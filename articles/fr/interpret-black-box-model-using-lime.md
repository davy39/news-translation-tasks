---
title: Comment interpr√©ter les mod√®les de bo√Æte noire en utilisant LIME (Local Interpretable
  Model-Agnostic Explanations)
subtitle: ''
author: Josua Naiborhu
co_authors: []
series: null
date: '2022-10-17T15:12:31.000Z'
originalURL: https://freecodecamp.org/news/interpret-black-box-model-using-lime
coverImage: https://www.freecodecamp.org/news/content/images/2022/10/LIME.jpeg
tags:
- name: Machine Learning
  slug: machine-learning
seo_title: Comment interpr√©ter les mod√®les de bo√Æte noire en utilisant LIME (Local
  Interpretable Model-Agnostic Explanations)
seo_desc: "Machine learning models are black box models. By giving input to these\
  \ models, we can get output based on the particular model we're using. \nThe way\
  \ humans interpret things is different from how machines interpret them. So it's\
  \ helpful to use tools t..."
---

Les mod√®les d'apprentissage automatique sont des mod√®les de bo√Æte noire. En donnant une entr√©e √† ces mod√®les, nous pouvons obtenir une sortie bas√©e sur le mod√®le particulier que nous utilisons. 

La mani√®re dont les humains interpr√®tent les choses est diff√©rente de la mani√®re dont les machines les interpr√®tent. Il est donc utile d'utiliser des outils qui peuvent transformer la sortie de certains mod√®les d'apprentissage automatique en quelque chose que les humains ou les utilisateurs non techniques peuvent comprendre.

Dans un contexte commercial, l'interpr√©tation du mod√®le joue un r√¥le important dans la prise de d√©cisions bas√©es sur les donn√©es. Plus nous interpr√©tons bien la sortie, plus il est facile pour les utilisateurs non techniques de comprendre cette sortie.

Dans ce tutoriel, je vais donc expliquer l'un des packages les plus populaires que vous pouvez utiliser pour interpr√©ter le mod√®le de bo√Æte noire de la sortie ‚Äì un package appel√© LIME (Local Interpretable Model-Agnostic Explanations).

## Qu'est-ce que LIME ?

LIME est un outil d'apprentissage automatique agnostique aux mod√®les qui vous aide √† interpr√©ter vos mod√®les de ML. Le terme **agnostique aux mod√®les** signifie que vous pouvez utiliser LIME avec n'importe quel mod√®le d'apprentissage automatique lors de l'entra√Ænement de vos donn√©es et de l'interpr√©tation des r√©sultats. 

LIME utilise des mod√®les "intrins√®quement interpr√©tables" tels que les arbres de d√©cision, les mod√®les lin√©aires et les mod√®les heuristiques bas√©s sur des r√®gles pour expliquer les r√©sultats aux utilisateurs non techniques sous forme visuelle. Vous pouvez utiliser LIME pour les probl√®mes de r√©gression et de classification afin d'interpr√©ter vos mod√®les de bo√Æte noire. 

## Collecte des donn√©es

Dans ce tutoriel, nous allons examiner un probl√®me de classification en utilisant le jeu de donn√©es Churn. Nous allons classer si les clients continueront √† utiliser les produits ou non (taux d'attrition) en examinant certaines caract√©ristiques du jeu de donn√©es. 

Vous pouvez t√©l√©charger le jeu de donn√©es [telco customers churn dataset ici](https://www.kaggle.com/datasets/blastchar/telco-customer-churn/code). 

## Pr√©traitement des donn√©es

√âtant donn√© que ce tutoriel se concentre sur la mise en ≈ìuvre de LIME en tant qu'outil d'interpr√©tabilit√©, nous allons effectuer quelques √©tapes de pr√©traitement pour diverses caract√©ristiques.

Nous commen√ßons le pr√©traitement en passant en revue les colonnes qui ne sont pas pertinentes pour le r√©sultat cible (attrition). Vous pouvez supprimer le CustomerID en utilisant ce code :

```py
# Suppression de toutes les colonnes non pertinentes
df.drop(columns=['customerID'], inplace = True)
```

Nous avons √©galement quelques valeurs manquantes qui ne sont pas correctement imput√©es. Pour simplifier, nous avons simplement supprim√© les valeurs manquantes en utilisant ce code :

```py
# Suppression des valeurs manquantes
df.dropna(inplace=True)
```

Une autre √©tape de pr√©traitement que nous devons effectuer est d'examiner les colonnes cat√©gorielles. La grande majorit√© des mod√®les d'apprentissage automatique ne peuvent pas g√©rer les caract√©ristiques cat√©gorielles. Nous devons donc pr√©traiter ce type de caract√©ristique en une repr√©sentation num√©rique. 

Il existe diverses transformations que nous pouvons effectuer telles que l'encodage par √©tiquettes, les astuces de hachage, l'encodage one-hot, l'encodage cible, l'encodage ordinal et l'encodage par fr√©quence. 

Pour ce tutoriel, nous allons utiliser la technique d'encodage par √©tiquettes en utilisant la biblioth√®que scikit-learn comme montr√© dans le code suivant :

```py
# Encodage par √©tiquettes des caract√©ristiques 
categorical_feat =list(df.select_dtypes(include=["object"]))

# Utilisation de l'encodeur d'√©tiquettes pour transformer les cat√©gories de cha√Ænes en √©tiquettes enti√®res
le = LabelEncoder()
for feat in categorical_feat:
    df[feat] = le.fit_transform(df[feat]).astype('int')
```

Il est important de noter que lorsque vous travaillez sur un probl√®me/jeu de donn√©es r√©el de ML, vous voudrez vous assurer de r√©aliser un pr√©traitement appropri√©, un g√©nie des caract√©ristiques, une validation crois√©e, un r√©glage des hyperparam√®tres, et ainsi de suite pour obtenir une meilleure pr√©diction.

## Mod√©lisation avec XGBoostClassifier et impl√©mentation de LIME

XGBoostClassifier est un algorithme XGBoost que vous pouvez utiliser pour r√©soudre des probl√®mes de classification. Il fonctionne en construisant les donn√©es dans un arbre de d√©cision et en utilisant les r√©sidus pour √™tre construits √† nouveau dans l'arbre de d√©cision suivant de mani√®re s√©quentielle.

Cet algorithme vous aide √† am√©liorer les performances des pr√©dictions de votre mod√®le qui peuvent ressembler √† la v√©rit√© terrain. Cela est d√ª au fait qu'il am√©liore les pr√©dictions mal class√©es (apprenants faibles) en les aidant √† devenir des apprenants forts. Il le fait en apprenant la pr√©diction mal class√©e qui est utilis√©e lors de l'it√©ration suivante de l'arbre de d√©cision suivant. 

Vous pouvez consulter le diagramme suivant pour voir comment cet algorithme fonctionne :

![Image](https://www.freecodecamp.org/news/content/images/2022/10/xgboost.png)
_Figure 4. Structure simplifi√©e de XGBoost ([source](https://www.researchgate.net/figure/Simplified-structure-of-XGBoost_fig2_348025909))_

Nous pouvons impl√©menter LIME apr√®s avoir travers√© le processus d'entra√Ænement sur nos donn√©es d'entra√Ænement. 

Nous commen√ßons le processus d'entra√Ænement en divisant les donn√©es en ensembles d'entra√Ænement et de test afin d'√©viter le surapprentissage. Nous pouvons utiliser la m√©thode de division disponible dans scikit-learn comme montr√© dans le code suivant :

```py
features = df.drop(columns=['Churn'])
labels = df['Churn']
# Division des donn√©es en ensemble d'entra√Ænement et de test avec un ratio de division de 80:20
x_train,x_test,y_train,y_test = train_test_split(features,labels,test_size=0.2, random_state=123)
```

Nous s√©lectionnons les caract√©ristiques et le r√©sultat cible (**churn**) et divisons les donn√©es en donn√©es d'entra√Ænement et de test. Ensuite, nous pouvons commencer l'entra√Ænement en ajustant **X_train et y_train** en fonction de l'objet d'instanciation du mod√®le d'apprentissage automatique que nous utilisons. Nous utilisons XGBoostClassifier pour ce tutoriel.

Nous initialisons n_estimators (nombre d'arbres de d√©cision) et random state pour la simplicit√© du processus d'entra√Ænement. Dans un projet r√©el de science des donn√©es, il y a de nombreux param√®tres que vous voudrez ajuster afin de maximiser la capacit√© de cet algorithme. Vous pouvez vous r√©f√©rer √† [**XGBoost parameters**](https://xgboost.readthedocs.io/en/stable/parameter.html) pour en savoir plus.

```py
model = XGBClassifier(n_estimators = 300, random_state = 123)
model.fit(x_train, y_train)
```

Apr√®s avoir ajust√© les donn√©es par le processus d'entra√Ænement, nous allons travailler sur l'interpr√©tation de l'interpr√©tabilit√© locale. L'interpr√©tabilit√© locale implique l'analyse de chaque caract√©ristique d'une instance de donn√©es particuli√®re. Nous pouvons s√©lectionner une instance/√©chantillon particulier pour v√©rifier comment les caract√©ristiques se corr√®lent au r√©sultat cible bas√© sur un √©chantillon particulier. 

```py
np.random.seed(123)
predict_fn = lambda x: model.predict_proba(x)
# D√©finition de l'objet explainer LIME
explainer = lime.lime_tabular.LimeTabularExplainer(df[features.columns].astype(int).values,                                               mode='classification',
class_names=['Did not Churn', 'Churn'],                                                 training_labels=df['Churn'],                                                  feature_names=features.columns)
# utilisation de LIME pour obtenir les explications
i = 5
exp=explainer.explain_instance(df.loc[i,features.columns].astype(int).values, predict_fn, num_features=5)
exp.show_in_notebook(show_table=True)
```

Pour obtenir l'explication pour une instance particuli√®re, nous commen√ßons par d√©finir une fonction comme le score de probabilit√© qui sera utilis√© dans le framework LIME. Nous instancions √©galement l'objet explainer LIME. 

LIME a un attribut lime_tabular qui peut interpr√©ter comment les caract√©ristiques se corr√®lent au r√©sultat cible. Nous pouvons √©galement sp√©cifier le mode en classification, training_label au r√©sultat cible (Churn), et les caract√©ristiques que nous avons s√©lectionn√©es dans le processus d'entra√Ænement. 

Nous choisissons l'√©chantillon 5 et nous obtiendrons l'explication pour cet √©chantillon particulier. Nous choisissons √©galement les 5 caract√©ristiques les plus importantes qui contribuent le plus au r√©sultat cible dans le param√®tre num_features. 

Ces caract√©ristiques sont √©galement appel√©es importance des caract√©ristiques. L'importance des caract√©ristiques est la caract√©ristique qui v√©rifie la corr√©lation entre les caract√©ristiques d'entr√©e et les caract√©ristiques cibles. Plus le score de la caract√©ristique dans le graphique d'importance des caract√©ristiques est √©lev√©, plus la caract√©ristique est importante pour √™tre ajust√©e dans le mod√®le d'apprentissage automatique.

## Comment interpr√©ter l'interpr√©tabilit√© locale

![Image](https://www.freecodecamp.org/news/content/images/2022/10/lime1.png)
_Figure 8. Pr√©diction d'interpr√©tabilit√© locale_

L'image ci-dessus montre trois graphiques qui montrent chacun des informations essentielles sur nos clients et leurs taux d'attrition. 

Le graphique de gauche montre que l'√©chantillon 5 dans les donn√©es montre l'intervalle de confiance indiquant que ces donn√©es sont √† 99% d'attrition alors que seulement 1% indique que l'instance n'a pas abandonn√©. 

Le graphique central montre les scores d'importance des caract√©ristiques sur cet √©chantillon particulier avec **MonthlyCharges** ayant un **score d'importance des caract√©ristiques de 21%**, suivi de **Contract** avec **19%** et **tenure** avec **11%**. Ces caract√©ristiques ont du sens en fonction de notre croyance que les clients tendent √† abandonner davantage avec **des MonthlyCharges plus √©lev√©s.** 

Le graphique de droite montre les cinq principales caract√©ristiques et leurs valeurs respectives. Les caract√©ristiques mises en √©vidence en orange contribuent √† la **classe 1 (Churn)** alors que les caract√©ristiques mises en √©vidence en bleu contribuent √† la **classe 0 (pas de Churn)**. 

Nous pouvons √©galement tracer une autre version du deuxi√®me graphique comme montr√© dans le graphique √† barres suivant. Il montre la plage des pr√©dictions d'interpr√©tabilit√© locale sur l'√©chantillon 5 dans lequel les MonthlyCharges pour cet √©chantillon particulier sont sup√©rieurs √† 89, le Contract est inf√©rieur √† 0, et les TotalCharges sont sup√©rieurs √† 401 et inf√©rieurs √† 1397.

![Image](https://www.freecodecamp.org/news/content/images/2022/10/lime2.png)
_Figure 9. La plage des pr√©dictions d'interpr√©tabilit√© locale_

## Comment interpr√©ter l'interpr√©tabilit√© globale

LIME fournit √©galement une autre explication via l'algorithme SP-LIME qui prend des √©chantillons repr√©sentatifs pour extraire la perspective globale du mod√®le de bo√Æte noire. 

Cette technique aide les utilisateurs non techniques √† comprendre les donn√©es non seulement dans une instance particuli√®re (interpr√©tabilit√© locale) mais aussi √† comprendre les donn√©es de mani√®re holistique. En comprenant de nombreux √©chantillons repr√©sentatifs et leurs interpr√©tations, les utilisateurs non techniques peuvent capturer la perspective globale des instances de donn√©es. 

```py
# Utilisons SP-LIME pour retourner des explications sur quelques ensembles de donn√©es d'√©chantillons 
# et obtenir une perspective de d√©cision globale non redondante du mod√®le de bo√Æte noire
sp_exp = submodular_pick.SubmodularPick(explainer, 
                                        df[features.columns].values,
                                        predict_fn, 
                                        num_features=5,
                                        num_exps_desired=5)
```

Nous utilisons les attributs sous-modulaires disponibles sur SP-LIME pour obtenir une perspective globale des instances de donn√©es. Ensuite, nous visualisons les donn√©es pour visualiser les √©chantillons repr√©sentatifs globaux extraits par l'algorithme SP-LIME en utilisant ce code :

```py
[exp.show_in_notebook() for exp in sp_exp.sp_explanations]
print('Explications SP-LIME.')
```

![Image](https://www.freecodecamp.org/news/content/images/2022/10/lime3-1.png)
_Figure 12. La plage des pr√©dictions d'interpr√©tabilit√© globale_

Vous pouvez voir comment SP-LIME construit des valeurs d'intervalle pour chaque **√©chantillon repr√©sentatif.** Par exemple, le premier √©chantillon repr√©sentatif montre que l'intervalle de confiance est de 81% d'attrition alors que 19% indique que les instances n'abandonnent pas. 

Les caract√©ristiques qui influencent la tendance de cette instance √† abandonner sont **MonthlyCharge, Contract, tenure, OnlineSecurity, et TechSupport**. Vous pouvez voir cela dans le graphique √† barres d'importance des caract√©ristiques pour le premier √©chantillon repr√©sentatif. Nous pouvons √©galement tracer une autre version du graphique central de chaque √©chantillon repr√©sentatif sur le graphique pr√©c√©dent en utilisant ce code :

```python
[exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_exp.sp_explanations]
print('Explications locales SP-LIME')
```

![Image](https://www.freecodecamp.org/news/content/images/2022/10/lime4-1.png)
_Figure 14. Premier et deuxi√®me √©chantillons repr√©sentatifs_

Sur le deuxi√®me √©chantillon repr√©sentatif de la **figure 12**, l'intervalle de confiance est de 100% pas d'attrition. Les caract√©ristiques qui influencent la tendance de cette instance √† ne pas abandonner sont **Contract, tenure, MonthlyCharge, TotalCharges, et OnlineSecurity** comme montr√© sur le graphique √† barres d'importance des caract√©ristiques suivant pour le deuxi√®me √©chantillon repr√©sentatif.

![Image](https://www.freecodecamp.org/news/content/images/2022/10/lab4.png)
_Figure 15. Troisi√®me √©chantillon repr√©sentatif_

Sur le troisi√®me √©chantillon repr√©sentatif de la **figure 12**, l'intervalle de confiance est de 100% pas d'attrition. Les caract√©ristiques qui influencent la tendance de cette instance √† ne pas abandonner sont **Contract, OnlineSecurity,** et **OnlineBackup** comme montr√© sur le graphique d'importance des caract√©ristiques ci-dessus pour le troisi√®me √©chantillon repr√©sentatif. 

Vous pouvez voir la mise en ≈ìuvre de LIME et SP-LIME sur les donn√©es [en regardant ce notebook](https://nbviewer.org/github/naiborhujosua/Blog_Notes/blob/main/notebook/interpreting-black-box-models.ipynb).

[Voici un article int√©ressant](https://arxiv.org/abs/1602.04938) sur la confiance dans les mod√®les.

# Merci d'avoir lu !

Je l'appr√©cie vraiment ! ü§ó. J'√©cris sur des sujets li√©s √† l'apprentissage automatique et √† l'apprentissage profond. J'essaie de garder mes publications simples mais pr√©cises, en fournissant toujours des visualisations et des simulations.