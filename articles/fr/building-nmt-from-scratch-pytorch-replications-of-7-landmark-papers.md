---
title: Créer une NMT de zéro – Réplications PyTorch de 7 articles de référence
author: Beau Carnes
date: '2025-12-10T15:23:41.238Z'
originalURL: https://freecodecamp.org/news/building-nmt-from-scratch-pytorch-replications-of-7-landmark-papers
description: 'Découvrez le parcours complet de la traduction automatique neuronale.

  Nous venons de publier un cours sur la chaîne YouTube de freeCodeCamp.org qui propose
  un voyage complet à travers l''évolution des modèles de séquence et de la traduction
  automatique neuronale (NMT). Il mêle...'
co_authors: []
series: null
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1765380195051/cafca462-96d6-49a4-b182-24d7a4f438f9.jpeg
tags:
- name: pytorch
  slug: pytorch
- name: youtube
  slug: youtube
seo_desc: 'Learn about the complete neural machine translation journey.

  We just posted a course on the freeCodeCamp.org YouTube channel that is a comprehensive
  journey through the evolution of sequence models and neural machine translation
  (NMT). It blends hist...'
---


Découvrez le parcours complet de la traduction automatique neuronale.

Nous venons de publier un cours sur la chaîne YouTube de freeCodeCamp.org qui constitue un voyage complet à travers l'évolution des modèles de séquence et de la traduction automatique neuronale (NMT). Il mêle percées historiques, innovations architecturales, perspectives mathématiques et réplications pratiques en PyTorch d'articles de référence qui ont façonné le NLP et l'IA modernes.

Le cours comprend :

*   Un récit détaillé retraçant l'histoire et les percées des RNN, LSTM, GRU, Seq2Seq, de l'Attention, du GNMT et de la NMT multilingue.
    
*   Des réplications de 7 articles NMT de référence en PyTorch, afin que les apprenants puissent coder en même temps et reconstruire l'histoire étape par étape.
    
*   Des explications sur les mathématiques derrière les RNN, LSTM, GRU et les Transformers.
    
*   Une clarté conceptuelle avec des comparaisons architecturales, des explications visuelles et des démos interactives comme le Transformer Playground.
    

Voici toutes les sections du cours :

*   Évolution des RNN
    
*   Évolution de la traduction automatique
    
*   Techniques de traduction automatique
    
*   Long Short-Term Memory (Aperçu)
    
*   Apprentissage de la représentation de phrases à l'aide de RNN (Encoder–Decoder pour la SMT)
    
*   Apprentissage de la représentation de phrases (Lab PyTorch – Réplication de Cho et al., 2014)
    
*   Apprentissage Seq2Seq avec les réseaux de neurones
    
*   Seq2Seq (Lab PyTorch – Réplication de Sutskever et al., 2014)
    
*   NMT par l'apprentissage conjoint de l'alignement (Bahdanau et al., 2015)
    
*   NMT par l'apprentissage conjoint de l'alignement et de la traduction (Lab PyTorch – Réplication de Bahdanau et al., 2015)
    
*   Sur l'utilisation d'un vocabulaire cible très large
    
*   NMT à large vocabulaire (Lab PyTorch – Réplication de Jean et al., 2015)
    
*   Approches efficaces de l'Attention (Luong et al., 2015)
    
*   Approches de l'Attention (Lab PyTorch – Réplication de Luong et al., 2015)
    
*   Réseau Long Short-Term Memory (Explication approfondie)
    
*   Attention Is All You Need (Vaswani et al., 2017)
    
*   Système de traduction automatique neuronale de Google (GNMT – Wu et al., 2016)
    
*   GNMT (Lab PyTorch – Réplication de Wu et al., 2016)
    
*   NMT multilingue de Google (Johnson et al., 2017)
    
*   NMT multilingue (Lab PyTorch – Réplication de Johnson et al., 2017)
    
*   Architectures Transformer vs GPT vs BERT
    
*   Transformer Playground (Démo de l'outil)
    
*   L'idée Seq2Seq de l'outil Google Translate
    
*   Architectures RNN, LSTM, GRU (Comparaisons)
    
*   Équations LSTM & GRU
    

Regardez le cours complet sur [la chaîne YouTube de freeCodeCamp.org](https://youtu.be/kRv2ElPNAdY) (7 heures de visionnage).

%[https://youtu.be/kRv2ElPNAdY]