---
title: 'Une introduction aux m√©thodes Advantage Actor Critic : jouons √† Sonic the
  Hedgehog !'
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2018-07-26T17:09:48.000Z'
originalURL: https://freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d
coverImage: https://cdn-media-1.freecodecamp.org/images/1*aRLyiDd3jEtCSHctP58Dvw.png
tags:
- name: Artificial Intelligence
  slug: artificial-intelligence
- name: Deep Learning
  slug: deep-learning
- name: Machine Learning
  slug: machine-learning
- name: technology
  slug: technology
- name: TensorFlow
  slug: tensorflow
seo_title: 'Une introduction aux m√©thodes Advantage Actor Critic : jouons √† Sonic
  the Hedgehog !'
seo_desc: 'By Thomas Simonini

  Since the beginning of this course, we‚Äôve studied two different reinforcement learning
  methods:


  Value based methods (Q-learning, Deep Q-learning): where we learn a value function
  that will map each state action pair to a value. Th...'
---

Par Thomas Simonini

Depuis le [d√©but de ce cours](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/), nous avons √©tudi√© deux m√©thodes diff√©rentes d'apprentissage par renforcement :

* **M√©thodes bas√©es sur la valeur** (Q-learning, Deep Q-learning) : o√π nous apprenons une fonction de valeur **qui mappe chaque paire √©tat-action √† une valeur.** Gr√¢ce √† ces m√©thodes, nous trouvons la meilleure action √† prendre pour chaque √©tat ‚Äî l'action avec la plus grande valeur. Cela fonctionne bien lorsque vous avez un ensemble fini d'actions.
* **M√©thodes bas√©es sur la politique** (REINFORCE avec Policy Gradients) : o√π nous optimisons directement la politique sans utiliser de fonction de valeur. Cela est utile lorsque l'espace d'action est continu ou stochastique. Le probl√®me principal est de trouver une bonne fonction de score pour calculer √† quel point une politique est bonne. Nous **utilisons les r√©compenses totales de l'√©pisode.**

Mais ces deux m√©thodes ont de grands inconv√©nients. C'est pourquoi, aujourd'hui, nous allons √©tudier un nouveau type de m√©thode d'apprentissage par renforcement que nous pouvons appeler une m√©thode "hybride" : **Actor Critic**. Nous allons utiliser deux r√©seaux de neurones :

* un Critic qui mesure √† quel point l'action prise est bonne (bas√© sur la valeur)
* un Actor qui contr√¥le comment notre agent se comporte (bas√© sur la politique)

Ma√Ætriser cette architecture est essentiel pour comprendre les algorithmes de pointe tels que **Proximal Policy Optimization (aka PPO). PPO est bas√© sur Advantage Actor Critic.**

Et vous allez impl√©menter un agent Advantage Actor Critic (A2C) qui apprend √† jouer √† Sonic the Hedgehog !

![Image](https://cdn-media-1.freecodecamp.org/images/1*F00fSSixgAp2CbzzI0_v7A.gif)
_Extrait de notre agent jouant √† Sonic apr√®s 10h d'entra√Ænement sur GPU._

### La qu√™te d'un meilleur mod√®le d'apprentissage

#### Le probl√®me avec les Policy Gradients

La [m√©thode Policy Gradient](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f) a un gros probl√®me. Nous sommes dans une situation de Monte Carlo, attendant jusqu'√† la fin de l'√©pisode pour calculer la r√©compense. Nous pouvons conclure que si nous avons une r√©compense √©lev√©e (**R(t)**), toutes les actions que nous avons prises √©taient bonnes, m√™me si certaines √©taient vraiment mauvaises.

![Image](https://cdn-media-1.freecodecamp.org/images/1*mKu7H4saf9pP7wfALYh6Kw.png)

Comme nous pouvons le voir dans cet exemple, m√™me si A3 √©tait une mauvaise action (a conduit √† des r√©compenses n√©gatives), **toutes les actions seront moyenn√©es comme bonnes car la r√©compense totale √©tait importante.**

Par cons√©quent, pour avoir une politique optimale, nous avons besoin de beaucoup d'√©chantillons. Cela produit un apprentissage lent, car il faut beaucoup de temps pour converger.

**Et si, au lieu de cela, nous pouvons faire une mise √† jour √† chaque √©tape ?**

### Introduction √† Actor Critic

Le mod√®le Actor Critic est une meilleure fonction de score. Au lieu d'attendre jusqu'√† la fin de l'√©pisode comme nous le faisons dans Monte Carlo REINFORCE, **nous faisons une mise √† jour √† chaque √©tape (TD Learning).**

![Image](https://cdn-media-1.freecodecamp.org/images/1*4TRtwlftFmWGNzZde45kaA.png)

Parce que nous faisons une mise √† jour √† chaque √©tape, nous ne pouvons pas utiliser les r√©compenses totales R(t). Au lieu de cela, nous devons entra√Æner un mod√®le Critic **qui approxime la fonction de valeur** (rappelons que la fonction de valeur calcule la r√©compense future maximale attendue √©tant donn√© un √©tat et une action). Cette fonction de valeur remplace la fonction de r√©compense dans le gradient de politique qui calcule les r√©compenses uniquement √† la fin de l'√©pisode.

#### Comment fonctionne Actor Critic

Imaginez que vous jouez √† un jeu vid√©o avec un ami qui vous donne des commentaires. Vous √™tes l'Actor et votre ami est le Critic.

![Image](https://cdn-media-1.freecodecamp.org/images/1*e1N-YzQmJt-5KwUkdUvAHg.png)

Au d√©but, vous ne savez pas comment jouer, alors vous essayez des actions au hasard. Le Critic observe votre action et fournit des commentaires.

Apprenant de ces commentaires, **vous mettrez √† jour votre politique et serez meilleur √† jouer √† ce jeu.**

D'autre part, votre ami (Critic) mettra √©galement √† jour sa propre fa√ßon de fournir des commentaires pour qu'ils soient meilleurs la prochaine fois.

Comme nous pouvons le voir, l'id√©e de Actor Critic est d'avoir deux r√©seaux de neurones. Nous estimons les deux :

![Image](https://cdn-media-1.freecodecamp.org/images/0*xoZipWE6lQgWyRh1.)
_**ACTOR** : Une fonction de politique, contr√¥le comment notre agent agit._

![Image](https://cdn-media-1.freecodecamp.org/images/0*vQZrik2laT8hdRMb.)
_**CRITIC** : Une fonction de valeur, mesure √† quel point ces actions sont bonnes._

Les deux fonctionnent en parall√®le.

Parce que nous avons deux mod√®les (Actor et Critic) qui doivent √™tre entra√Æn√©s, cela signifie que nous avons deux ensembles de poids (Œ∏ pour notre action et w pour notre Critic) **qui doivent √™tre optimis√©s s√©par√©ment :**

![Image](https://cdn-media-1.freecodecamp.org/images/1*KlX2-kNXRYLAYpdnI8VPiA.png)

#### Le processus Actor Critic

![Image](https://cdn-media-1.freecodecamp.org/images/1*zSsxcz9LjkCwFGcLgJZzdw.png)

√Ä chaque √©tape t, nous prenons l'√©tat actuel (St) de l'environnement et le passons en entr√©e √† travers notre Actor et notre Critic.

![Image](https://cdn-media-1.freecodecamp.org/images/1*ZwthrqP0X12yiYDraWoQMg.png)

Notre Politique prend l'√©tat, produit une action (At), et re√ßoit un nouvel √©tat (St+1) et une r√©compense (Rt+1).

Gr√¢ce √† cela :

* le Critic calcule la valeur de prendre cette action dans cet √©tat
* l'Actor met √† jour ses param√®tres de politique (poids) en utilisant cette valeur q

![Image](https://cdn-media-1.freecodecamp.org/images/1*ohA7iaViVAElqnSJvbYWpA.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*dGG7HIvsf_EKro2AOT6sKQ.png)

Gr√¢ce √† ses param√®tres mis √† jour, l'Actor produit l'action suivante √† prendre √† At+1 **√©tant donn√©** le nouvel √©tat St+1. Le Critic met ensuite √† jour ses param√®tres de valeur :

![Image](https://cdn-media-1.freecodecamp.org/images/1*Yd2F4KHmgn0lDA8nI9aSQw.png)

### A2C et A3C

#### Introduction de la fonction Advantage pour stabiliser l'apprentissage

Comme nous l'avons vu dans l'article sur [les am√©liorations du Deep Q Learning](https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682), les m√©thodes bas√©es sur la valeur ont **une grande variabilit√©.**

Pour r√©duire ce probl√®me, nous avons parl√© d'utiliser la fonction d'avantage au lieu de la fonction de valeur.

La fonction d'avantage est d√©finie comme suit :

![Image](https://cdn-media-1.freecodecamp.org/images/1*SvSFYWx5-u5zf38baqBgyQ.png)

Cette fonction nous dira **l'am√©lioration par rapport √† la moyenne de l'action prise dans cet √©tat.** En d'autres termes, cette fonction calcule la r√©compense suppl√©mentaire que j'obtiens si je prends cette action. La r√©compense suppl√©mentaire est celle au-del√† de la valeur attendue de cet √©tat.

Si A(s,a) > 0 : notre gradient est pouss√© dans cette direction.

Si A(s,a) < 0 (notre action fait pire que la valeur moyenne de cet √©tat), notre gradient est pouss√© dans la direction oppos√©e.

Le probl√®me de l'impl√©mentation de cette fonction d'avantage est qu'elle n√©cessite deux fonctions de valeur ‚Äî Q(s,a) et V(s). Heureusement, **nous pouvons utiliser l'erreur TD comme un bon estimateur de la fonction d'avantage.**

![Image](https://cdn-media-1.freecodecamp.org/images/1*fmWayfCY4QVIounYXWi2rg.png)

#### Deux strat√©gies diff√©rentes : Asynchrone ou Synchrone

Nous avons deux strat√©gies diff√©rentes pour impl√©menter un agent Actor Critic :

* A2C (aka Advantage Actor Critic)
* A3C (aka Asynchronous Advantage Actor Critic)

**√Ä cause de cela**, nous travaillerons avec A2C et non A3C. Si vous voulez voir une impl√©mentation compl√®te de A3C, consultez l'excellent article d'[Arthur Juliani](https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/undefined) sur [A3C](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2) et son [impl√©mentation de Doom](https://github.com/awjuliani/DeepRL-Agents/blob/master/A3C-Doom.ipynb).

Dans A3C, nous n'utilisons pas la m√©moire d'exp√©rience car cela n√©cessite beaucoup de m√©moire. Au lieu de cela, nous ex√©cutons de mani√®re asynchrone **diff√©rents agents en parall√®le sur plusieurs instances de l'environnement.** Chaque travailleur (copie du r√©seau) mettra √† jour le r√©seau global de mani√®re asynchrone.

D'autre part, la seule diff√©rence dans A2C est que nous mettons √† jour le r√©seau global de mani√®re synchrone. Nous attendons que tous les travailleurs aient termin√© leur entra√Ænement et calcul√© leurs gradients pour les moyenner, afin de mettre √† jour notre r√©seau global.

#### Choisir A2C ou A3C ?

Le probl√®me de A3C est expliqu√© dans [cet excellent article](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#a2c). √Ä cause de la nature asynchrone de A3C, certains travailleurs (copies de l'Agent) joueront avec des versions plus anciennes des param√®tres. Ainsi, la mise √† jour d'agr√©gation ne sera pas optimale.

C'est pourquoi A2C attend que chaque acteur termine son segment d'exp√©rience avant de mettre √† jour les param√®tres globaux. Ensuite, nous red√©marrons un nouveau segment d'exp√©rience avec tous les acteurs parall√®les ayant les m√™mes nouveaux param√®tres.

![Image](https://cdn-media-1.freecodecamp.org/images/1*0gZsoyvY01liRdZZXilZpA.png)
_Ce sch√©ma est inspir√© de [cet article](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#a2c)._

Par cons√©quent, l'entra√Ænement sera plus coh√©sif et plus rapide.

### Impl√©mentation d'un agent A2C qui joue √† Sonic the Hedgehog

#### A2C en pratique

En pratique, comme expliqu√© dans [ce post Reddit](https://www.reddit.com/r/reinforcementlearning/comments/7eljkx/understanding_a2c_and_a3c_multiple_actors/), la nature synchrone de A2C signifie **que nous n'avons pas besoin de diff√©rentes versions (diff√©rents travailleurs) de A2C.**

Chaque travailleur dans A2C aura le m√™me ensemble de poids puisque, contrairement √† A3C, A2C met √† jour tous ses travailleurs en m√™me temps.

En fait, nous cr√©ons **plusieurs versions d'environnements** (disons huit) et les ex√©cutons en parall√®le.

Le processus sera le suivant :

![Image](https://cdn-media-1.freecodecamp.org/images/1*bNw9TH5700_x3X64YXHPdQ.png)

* Cr√©e un vecteur de n environnements en utilisant la biblioth√®que multiprocessing
* Cr√©e un objet runner qui g√®re les diff√©rents environnements, ex√©cut√©s en parall√®le.
* A deux versions du r√©seau :

1. step_model : qui g√©n√®re des exp√©riences √† partir des environnements
2. train_model : qui entra√Æne les exp√©riences.

Lorsque le runner fait un pas (mod√®le √† pas unique), cela effectue un pas pour chacun des n environnements. Cela produit un lot d'exp√©rience.

Ensuite, nous calculons le gradient en une seule fois en utilisant train_model et notre lot d'exp√©rience.

Enfin, nous mettons √† jour le step model avec les nouveaux poids.

Rappelons que calculer le gradient en une seule fois est la m√™me chose que collecter des donn√©es, calculer le gradient pour chaque travailleur, puis faire la moyenne. Pourquoi ? **Parce que la somme des d√©riv√©es (somme des gradients) est la m√™me chose que prendre les d√©riv√©es de la somme.** Mais la deuxi√®me m√©thode est plus √©l√©gante et une meilleure fa√ßon d'utiliser le GPU.

#### A2C avec Sonic the Hedgehog

Maintenant que nous comprenons comment A2C fonctionne en g√©n√©ral, nous pouvons impl√©menter notre agent A2C jouant √† Sonic ! Cette vid√©o montre la diff√©rence de comportement de notre agent entre 10 minutes d'entra√Ænement (√† gauche) et 10 heures d'entra√Ænement (√† droite).

L'impl√©mentation est dans le d√©p√¥t GitHub [ici](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/A2C%20with%20Sonic%20the%20Hedgehog), et le notebook explique l'impl√©mentation. Je vous donne le mod√®le sauvegard√© entra√Æn√© avec environ 10h+ sur GPU.

Cette impl√©mentation est beaucoup plus complexe que les impl√©mentations pr√©c√©dentes. Nous commen√ßons √† impl√©menter des algorithmes de pointe, donc nous devons √™tre **de plus en plus efficaces avec notre code.** C'est pourquoi, dans cette impl√©mentation, nous allons s√©parer le code en diff√©rents objets et fichiers.

C'est tout ! Vous venez de cr√©er un agent qui apprend √† jouer √† Sonic the Hedgehog. C'est g√©nial ! Nous pouvons voir qu'avec 10 heures d'entra√Ænement, notre agent ne comprend pas les loopings, par exemple, donc nous devrons utiliser une architecture plus stable : PPO.

**Prenez le temps de consid√©rer tous les accomplissements que vous avez r√©alis√©s depuis le [premier chapitre de ce cours](https://medium.com/free-code-camp/an-introduction-to-reinforcement-learning-4339519de419) :** nous sommes pass√©s de simples jeux textuels (OpenAI taxi-v2) √† des jeux complexes comme Doom et Sonic the Hedgehog en utilisant des architectures de plus en plus puissantes. Et c'est fantastique !

La prochaine fois, nous apprendrons sur les Proximal Policy Gradients, l'architecture qui a gagn√© le [OpenAI Retro Contest](https://contest.openai.com/2018-1/). Nous entra√Ænerons notre agent √† jouer √† Sonic the Hedgehog 2 et 3 et cette fois, il terminera des niveaux entiers !

N'oubliez pas d'impl√©menter chaque partie du code par vous-m√™me. Il est vraiment important d'essayer de modifier le code que je vous ai donn√©. Essayez d'ajouter des √©poques, changez l'architecture, changez le taux d'apprentissage, et ainsi de suite. Exp√©rimenter est la meilleure fa√ßon d'apprendre, alors amusez-vous !

Si vous avez aim√© mon article, **veuillez cliquer sur le ? ci-dessous autant de fois que vous avez aim√© l'article** pour que d'autres personnes puissent le voir ici sur Medium. Et n'oubliez pas de me suivre !

Cet article fait partie de mon cours de Deep Reinforcement Learning avec TensorFlow ?. Consultez le programme [ici](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/).

Si vous avez des pens√©es, des commentaires, des questions, n'h√©sitez pas √† commenter ci-dessous ou √† m'envoyer un email : hello [at] simoninithomas [dot] com, ou me tweeter [@ThomasSimonini](https://twitter.com/ThomasSimonini).

![Image](https://cdn-media-1.freecodecamp.org/images/1*_yN1FzvEFDmlObiYsstIzg.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*mD-f5VN1SWYvhrZAbvSu_w.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*PqiptT-Cdi8uwosxuFn2DQ.png)

#### Cours de Deep Reinforcement Learning :

> Nous r√©alisons une **version vid√©o du cours de Deep Reinforcement Learning avec Tensorflow** ? o√π nous nous concentrons sur la partie impl√©mentation avec tensorflow [ici](https://youtu.be/q2ZOEFAaaI0).

_Partie 1 : [Une introduction √† l'apprentissage par renforcement](https://medium.com/p/4339519de419/edit)_

_Partie 2 : [Plonger plus profond√©ment dans l'apprentissage par renforcement avec Q-Learning](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)_

_Partie 3 : [Une introduction au Deep Q-Learning : jouons √† Doom](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)_

Partie 3+ : [Am√©liorations du Deep Q Learning : Dueling Double DQN, Prioritized Experience Replay, et Q-targets fixes](https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682)

Partie 4 : [Une introduction aux Policy Gradients avec Doom et Cartpole](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)

Partie 6 : [Proximal Policy Optimization (PPO) avec Sonic the Hedgehog 2 et 3](https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e)

Partie 7 : [L'apprentissage par curiosit√© rendu facile Partie I](https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359)