---
title: Une plong√©e approfondie dans les plongements de mots pour l'analyse de sentiment
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2020-01-05T14:27:33.000Z'
originalURL: https://freecodecamp.org/news/word-embeddings-for-sentiment-analysis
coverImage: https://www.freecodecamp.org/news/content/images/2020/01/1_u9pwb9JShvDIU7j1G9iszQ.jpeg
tags:
- name: keras
  slug: keras
- name: Machine Learning
  slug: machine-learning
- name: nlp
  slug: nlp
- name: Python
  slug: python
- name: Sentiment analysis
  slug: sentiment-analysis
- name: text mining
  slug: text-mining
seo_title: Une plong√©e approfondie dans les plongements de mots pour l'analyse de
  sentiment
seo_desc: "By Bert Carremans\nWhen applying one-hot encoding to words, we end up with\
  \ sparse (containing many zeros) vectors of high dimensionality. On large data sets,\
  \ this could cause performance issues. \nAdditionally, one-hot encoding does not\
  \ take into accou..."
---

Par Bert Carremans

Lorsque nous appliquons le codage one-hot aux mots, nous obtenons des vecteurs clairsem√©s (contenant de nombreux z√©ros) de grande dimensionnalit√©. Sur de grands ensembles de donn√©es, cela pourrait causer des probl√®mes de performance. 

De plus, le codage one-hot ne tient pas compte de la s√©mantique des mots. Ainsi, des mots comme _avion_ et _a√©ronef_ sont consid√©r√©s comme deux caract√©ristiques diff√©rentes alors que nous savons qu'ils ont un sens tr√®s similaire. Les plongements de mots abordent ces deux probl√®mes.

Les plongements de mots sont des vecteurs denses avec une dimensionnalit√© beaucoup plus faible. Deuxi√®mement, les relations s√©mantiques entre les mots sont refl√©t√©es dans la distance et la direction des vecteurs.

Nous travaillerons avec l'ensemble de donn√©es [TwitterAirlineSentiment sur Kaggle](https://www.kaggle.com/crowdflower/twitter-airline-sentiment). Cet ensemble de donn√©es contient environ 15K tweets avec 3 classes possibles pour le sentiment (positif, n√©gatif et neutre). Dans mon pr√©c√©dent article, nous avons essay√© de [classifier les tweets](https://www.freecodecamp.org/news/sentiment-analysis-with-text-mining/) en tokenisant les mots et en appliquant deux classificateurs. Voyons si les plongements de mots peuvent surpasser cela.

Apr√®s avoir lu ce tutoriel, vous saurez comment calculer des plongements de mots sp√©cifiques √† une t√¢che avec la couche Embedding de **Keras**. Deuxi√®mement, nous examinerons si les plongements de mots form√©s sur un corpus plus large peuvent am√©liorer la pr√©cision de notre mod√®le.

La structure de ce tutoriel est la suivante :

* Intuition derri√®re les plongements de mots
* Configuration du projet
* Pr√©paration des donn√©es
* Keras et sa couche Embedding
* Plongements de mots pr√©-entra√Æn√©s ‚Äî GloVe
* Entra√Ænement des plongements de mots avec plus de dimensions

# Intuition derri√®re les plongements de mots

Avant de pouvoir utiliser des mots dans un classificateur, nous devons les convertir en nombres. Une fa√ßon de le faire est de simplement mapper les mots √† des entiers. Une autre fa√ßon est de les encoder en one-hot. Chaque tweet pourrait alors √™tre repr√©sent√© comme un vecteur avec une dimension √©gale √† (un ensemble limit√© de) les mots dans le corpus. Les mots apparaissant dans le tweet ont une valeur de 1 dans le vecteur. Toutes les autres valeurs du vecteur sont √©gales √† z√©ro.

Les plongements de mots sont calcul√©s diff√©remment. Chaque mot est positionn√© dans un **_espace multi-dimensionnel_**. Le nombre de dimensions dans cet espace est choisi par le scientifique des donn√©es. Vous pouvez exp√©rimenter avec diff√©rentes dimensions et voir ce qui donne le meilleur r√©sultat.

Les **_valeurs du vecteur pour un mot repr√©sentent sa position_** dans cet espace de plongement. Les synonymes se trouvent proches les uns des autres tandis que les mots avec des significations oppos√©es ont une grande distance entre eux. Vous pouvez √©galement appliquer des op√©rations math√©matiques sur les vecteurs qui devraient produire des r√©sultats s√©mantiquement corrects. Un exemple typique est que la somme des plongements de mots de _roi_ et _femelle_ produit le plongement de mot de _reine_.

# Configuration du projet

Commen√ßons par importer tous les packages pour ce projet.

```python
import pandas as pd
import numpy as np
import re
import collections
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from keras import models
from keras import layers
```

Nous d√©finissons certains param√®tres et chemins utilis√©s tout au long du projet. La plupart d'entre eux sont auto-explicatifs. Mais d'autres seront expliqu√©s plus loin dans le code.

```python
NB_WORDS = 10000  # Param√®tre indiquant le nombre de mots que nous mettrons dans le dictionnaire
VAL_SIZE = 1000  # Taille de l'ensemble de validation
NB_START_EPOCHS = 10  # Nombre d'√©poques avec lesquelles nous commen√ßons g√©n√©ralement l'entra√Ænement
BATCH_SIZE = 512  # Taille des lots utilis√©s dans la descente de gradient mini-batch
MAX_LEN = 24  # Nombre maximum de mots dans une s√©quence
GLOVE_DIM = 100  # Nombre de dimensions des plongements de mots GloVe
root = Path('../')
input_path = root / 'input/'
ouput_path = root / 'output/'
source_path = root / 'source/'
```

Tout au long de ce code, nous utiliserons √©galement certaines fonctions auxiliaires pour la pr√©paration des donn√©es, la mod√©lisation et la visualisation. Ces d√©finitions de fonctions ne sont pas montr√©es ici pour garder l'article de blog sans encombrement. Vous pouvez toujours vous r√©f√©rer au [notebook sur Github](https://github.com/bertcarremans/TwitterUSAirlineSentiment/blob/master/source/Using%20Word%20Embeddings%20for%20Sentiment%20Analysis.ipynb) pour voir le code.

# Pr√©paration des donn√©es

## Lecture des donn√©es et nettoyage

Nous lisons le fichier CSV avec les tweets et appliquons un m√©lange al√©atoire sur ses index. Apr√®s cela, nous supprimons les mots vides et les @ mentions. Un ensemble de test de 10% est s√©par√© pour √©valuer le mod√®le sur de nouvelles donn√©es.

```python
df = pd.read_csv(input_path / 'Tweets.csv')
df = df.reindex(np.random.permutation(df.index))
df = df[['text', 'airline_sentiment']]
df.text = df.text.apply(remove_stopwords).apply(remove_mentions)
X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)
```

## Convertir les mots en entiers

Avec le **_Tokenizer_** de Keras, nous convertissons les tweets en s√©quences d'entiers. Nous limitons le nombre de mots aux **_NB_WORDS_** mots les plus fr√©quents. De plus, les tweets sont nettoy√©s avec certains filtres, mis en minuscules et divis√©s sur les espaces.

```python
tk = Tokenizer(num_words=NB_WORDS,
filters='!"#$%&()*+,-./:;<=>?@[\]^_`{"}~\t\n',lower=True, split=" ")
tk.fit_on_texts(X_train)
X_train_seq = tk.texts_to_sequences(X_train)
X_test_seq = tk.texts_to_sequences(X_test)
```

## Longueur √©gale des s√©quences

Chaque lot doit fournir des s√©quences de longueur √©gale. Nous y parvenons avec la m√©thode **_pad_sequences_**. En sp√©cifiant **_maxlen_**, les s√©quences sont remplies de z√©ros ou tronqu√©es.

```python
X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_LEN)
X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_LEN)
```

## Encodage de la variable cible

Les classes cibles sont des cha√Ænes de caract√®res qui doivent √™tre converties en vecteurs num√©riques. Cela est fait avec le **_LabelEncoder_** de Sklearn et la m√©thode **_to_categorical_** de Keras.

```python
le = LabelEncoder()
y_train_le = le.fit_transform(y_train)
y_test_le = le.transform(y_test)
y_train_oh = to_categorical(y_train_le)
y_test_oh = to_categorical(y_test_le)
```

## S√©paration de l'ensemble de validation

√Ä partir des donn√©es d'entra√Ænement, nous s√©parons un ensemble de validation de 10% √† utiliser pendant l'entra√Ænement.

```python
X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train_oh, test_size=0.1, random_state=37)
```

# Mod√©lisation

## Keras et la couche Embedding

Keras fournit un moyen pratique de convertir chaque mot en un vecteur multi-dimensionnel. Cela peut √™tre fait avec la couche **_Embedding_**. Elle calculera les plongements de mots (ou utilisera des plongements pr√©-entra√Æn√©s) et recherchera chaque mot dans un dictionnaire pour trouver sa repr√©sentation vectorielle. Ici, nous entra√Ænerons des plongements de mots avec 8 dimensions.

```python
emb_model = models.Sequential()
emb_model.add(layers.Embedding(NB_WORDS, 8, input_length=MAX_LEN))
emb_model.add(layers.Flatten())
emb_model.add(layers.Dense(3, activation='softmax'))
emb_history = deep_model(emb_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)
```

![Image](https://www.freecodecamp.org/news/content/images/2020/01/0_-XjJ4DTQ5RQ8jZOF.png)

Nous avons une pr√©cision de validation d'environ 74%. Le nombre de mots dans les tweets est plut√¥t faible, donc ce r√©sultat est assez bon. En comparant la perte d'entra√Ænement et de validation, nous voyons que le mod√®le commence √† **surapprendre** √† partir de l'√©poque 6.

Dans un article pr√©c√©dent, j'ai discut√© de la mani√®re dont nous pouvons [√©viter le surapprentissage](https://www.freecodecamp.org/news/handling-overfitting-in-deep-learning-models/). Vous pourriez vouloir lire cela si vous voulez approfondir ce sujet.

Lorsque nous entra√Ænons le mod√®le sur toutes les donn√©es (y compris les donn√©es de validation, mais √† l'exclusion des donn√©es de test) et fixons le nombre d'√©poques √† 6, nous obtenons une pr√©cision de test de 78%. Ce r√©sultat de test est correct, mais voyons si nous pouvons am√©liorer avec des plongements de mots pr√©-entra√Æn√©s.

```python
emb_results = test_model(emb_model, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 6)
print('/n')
print('Pr√©cision de test du mod√®le de plongements de mots : {0:.2f}%'.format(emb_results[1]*100))
```

## Plongements de mots pr√©-entra√Æn√©s ‚Äî GloVe

Parce que les donn√©es d'entra√Ænement ne sont pas si grandes, le mod√®le pourrait ne pas √™tre en mesure d'apprendre de bons plongements pour l'analyse de sentiment. Alternativement, nous pouvons charger des plongements de mots pr√©-entra√Æn√©s construits sur des donn√©es d'entra√Ænement beaucoup plus grandes.

La [base de donn√©es GloVe](https://nlp.stanford.edu/projects/glove/) contient plusieurs plongements de mots pr√©-entra√Æn√©s, et plus sp√©cifiquement des **_plongements entra√Æn√©s sur des tweets_**. Cela pourrait donc √™tre utile pour la t√¢che √† accomplir.

Tout d'abord, nous mettons les plongements de mots dans un dictionnaire o√π les cl√©s sont les mots et les valeurs sont les plongements de mots.

```python
glove_file = 'glove.twitter.27B.' + str(GLOVE_DIM) + 'd.txt'
emb_dict = {}
glove = open(input_path / glove_file)
for line in glove:
    values = line.split()
    word = values[0]
    vector = np.asarray(values[1:], dtype='float32')
    emb_dict[word] = vector
glove.close()
```

Avec les plongements GloVe charg√©s dans un dictionnaire, nous pouvons rechercher le plongement pour chaque mot dans le corpus des tweets de la compagnie a√©rienne. Ceux-ci seront stock√©s dans une matrice avec une forme de **_NB_WORDS_** et **_GLOVE_DIM_**. Si un mot n'est pas trouv√© dans le dictionnaire GloVe, les valeurs de plongement de mot pour le mot sont nulles.

```python
emb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))
for w, i in tk.word_index.items():
    if i < NB_WORDS:
        vect = emb_dict.get(w)
        if vect is not None:
        emb_matrix[i] = vect
    else:
        break
```

Ensuite, nous sp√©cifions le mod√®le comme nous l'avons fait avec le mod√®le ci-dessus.

```python
glove_model = models.Sequential()
glove_model.add(layers.Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))
glove_model.add(layers.Flatten())
glove_model.add(layers.Dense(3, activation='softmax'))
```

Dans la couche Embedding (qui est la couche 0 ici), nous **_d√©finissons les poids_** pour les mots √† ceux trouv√©s dans les plongements de mots GloVe. En d√©finissant **_trainable_** √† False, nous nous assurons que les plongements de mots GloVe ne peuvent pas √™tre modifi√©s. Apr√®s cela, nous ex√©cutons le mod√®le.

```python
glove_model.layers[0].set_weights([emb_matrix])
glove_model.layers[0].trainable = False
glove_history = deep_model(glove_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)
```

![Image](https://www.freecodecamp.org/news/content/images/2020/01/0_uhsGcl8UG_JYUycb.png)

Le mod√®le surapprend rapidement apr√®s 3 √©poques. De plus, la pr√©cision de validation est inf√©rieure par rapport aux plongements entra√Æn√©s sur les donn√©es d'entra√Ænement.

```python
glove_results = test_model(glove_model, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 3)
print('/n')
print('Pr√©cision de test du mod√®le glove : {0:.2f}%'.format(glove_results[1]*100))
```

En tant qu'exercice final, voyons quels r√©sultats nous obtenons lorsque nous entra√Ænons les plongements avec le m√™me nombre de dimensions que les donn√©es GloVe.

## Entra√Ænement des plongements de mots avec plus de dimensions

Nous entra√Ænerons les plongements de mots avec le m√™me nombre de dimensions que les plongements GloVe (c'est-√†-dire GLOVE_DIM).

```python
emb_model2 = models.Sequential()
emb_model2.add(layers.Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))
emb_model2.add(layers.Flatten())
emb_model2.add(layers.Dense(3, activation='softmax'))
emb_history2 = deep_model(emb_model2, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)
```

![Image](https://www.freecodecamp.org/news/content/images/2020/01/0_boJxTu7msbxWzexm.png)

```python
emb_results2 = test_model(emb_model2, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 3)
print('/n')
print('Pr√©cision de test du mod√®le de plongement de mots 2 : {0:.2f}%'.format(emb_results2[1]*100))
```

Sur les donn√©es de test, nous obtenons de bons r√©sultats, mais nous ne surpassons pas la r√©gression logistique avec le CountVectorizer. Il y a donc encore place √† l'am√©lioration.

# Conclusion

Le meilleur r√©sultat est obtenu avec des plongements de mots de 100 dimensions qui sont entra√Æn√©s sur les donn√©es disponibles. Cela surpasse m√™me l'utilisation de plongements de mots qui ont √©t√© entra√Æn√©s sur un corpus Twitter beaucoup plus grand.

Jusqu'√† pr√©sent, nous avons simplement mis une couche Dense sur les plongements aplatis. En faisant cela, **_nous ne tenons pas compte des relations entre les mots_** dans le tweet. Cela peut √™tre r√©alis√© avec un r√©seau de neurones r√©current ou un r√©seau de convolution 1D. Mais c'est quelque chose pour un futur article üòä