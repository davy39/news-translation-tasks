---
title: 'Comment cr√©er un agent IA avec LangChain et LangGraph : Construire un agent
  Starbucks autonome'
author: Djibril-MüçÄ
date: '2025-12-19T00:21:01.530Z'
originalURL: https://freecodecamp.org/news/how-to-build-a-starbucks-ai-agent-with-langchain
description: En 2023, quand j'ai commenc√© √† utiliser ChatGPT, ce n'√©tait qu'un chatbot
  de plus auquel je pouvais poser des questions complexes et qui identifiait les erreurs
  dans mes extraits de code. Tout allait bien. L'application n'avait aucune m√©moire
  des √©tats pr√©c√©dents ou de ce qui avait √©t√© dit la veille...
co_authors: []
series: null
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1765630477745/8dffec85-c3c4-4d83-9aa4-f332439d4663.png
tags:
- name: ai agents
  slug: ai-agents
- name: langchain
  slug: langchain
- name: nestjs
  slug: nestjs
- name: handbook
  slug: handbook
seo_desc: Back in 2023, when I started using ChatGPT, it was just another chatbot
  that I could ask complex questions to and it would identify errors in my code snippets.
  Everything was fine. The application had no memory of previous states or what was
  said the...
---


En 2023, quand j'ai commenc√© √† utiliser ChatGPT, ce n'√©tait qu'un chatbot de plus auquel je pouvais poser des questions complexes et qui identifiait les erreurs dans mes extraits de code. Tout allait bien. L'application n'avait aucune m√©moire des √©tats pr√©c√©dents ou de ce qui avait √©t√© dit la veille.

Puis, en 2024, tout a commenc√© √† changer. Nous sommes pass√©s d'un chatbot sans √©tat √† un agent IA capable d'appeler des outils, de faire des recherches sur Internet et de g√©n√©rer des liens de t√©l√©chargement.

√Ä ce moment-l√†, j'ai commenc√© √† devenir curieux. Comment un LLM peut-il faire des recherches sur Internet ? Une infinit√© de questions me traversaient l'esprit. Peut-il cr√©er ses propres outils, programmes, ou ex√©cuter son propre code ? J'avais l'impression que nous nous dirigions vers la r√©volution Skynet (Terminator).

J'√©tais simplement ignorant üòÖ. Mais c'est alors que j'ai commenc√© mes recherches et d√©couvert LangChain, un outil qui promet tous ces miracles sans un budget d'un milliard de dollars.

Dans cet article, vous allez construire un agent IA enti√®rement fonctionnel en utilisant LangChain et LangGraph. Vous commencerez par d√©finir des donn√©es structur√©es √† l'aide de sch√©mas Zod, puis vous les analyserez pour qu'elles soient compr√©hensibles par l'IA. Ensuite, vous apprendrez √† r√©sumer des donn√©es en texte, √† cr√©er des outils que l'agent peut appeler et √† configurer des n≈ìuds LangGraph pour orchestrer les workflows.

Vous verrez comment compiler le graphe de workflow, g√©rer l'√©tat et persister l'historique des conversations √† l'aide de MongoDB. √Ä la fin, vous aurez un barista Starbucks fonctionnel sous forme d'IA qui d√©montre comment combiner le raisonnement, l'ex√©cution d'outils et la m√©moire dans un seul agent.

## Table des mati√®res

* [Pr√©requis](#heading-prerequis)
    
* [Qu'est-ce qu'un agent LLM ?](#heading-qu-est-ce-qu-un-agent-llm)
    
* [Configuration du projet](#heading-configuration-du-projet)
    
* [Sch√©matisation des donn√©es avec Zod](#heading-schematisation-des-donnees-avec-zod)
    
* [Comment analyser le sch√©ma](#heading-comment-analyser-le-schema)
    
* [R√©sum√© de donn√©es en texte](#heading-resume-de-donnees-en-texte)
    
* [Comment persister les commandes avec MongoDB dans NestJS](#heading-comment-persister-les-commandes-avec-mongodb-dans-nestjs)
    
* [Termes d'√©tat et d'annotation LangGraph](#heading-termes-d-etat-et-d-annotation-langgraph)
    
* [Comment cr√©er des outils pour l'agent](#heading-comment-creer-des-outils-pour-l-agent)
    
* [N≈ìuds LangGraph (Composants de workflow)](#heading-noeuds-langgraph-composants-de-workflow)
    
* [D√©claration du graphe](#heading-declaration-du-graphe)
    
* [Compilation du workflow et persistance de l'√©tat (Partie finale)](#heading-compilation-du-workflow-et-persistance-de-l-etat-partie-finale)
    
* [Conclusion](#heading-conclusion)
    

## Pr√©requis

Pour profiter pleinement de cet article, vous devez avoir une compr√©hension de base de TypeScript, Node.js, et un peu de NestJS vous aidera, car c'est le framework backend que nous utiliserons.

## **Qu'est-ce qu'un agent LLM ?**

Par d√©finition, un agent LLM est un programme logiciel capable de percevoir son environnement, de prendre des d√©cisions et d'entreprendre des actions autonomes pour atteindre des objectifs sp√©cifiques. Il le fait souvent en interagissant avec des outils et des syst√®mes.

De nombreux frameworks et conventions ont √©t√© cr√©√©s pour y parvenir, et l'un des plus c√©l√®bres et des plus utilis√©s est le framework ReAct (Reason & Act).

Avec ce framework, le LLM re√ßoit un prompt, r√©fl√©chit, d√©cide de l'action suivante (cela peut √™tre l'appel d'un outil sp√©cifique) et re√ßoit les donn√©es de l'outil. Une fois la r√©ponse de l'outil re√ßue, le mod√®le d'IA observe la r√©ponse, g√©n√®re sa propre r√©ponse et planifie ses actions suivantes en fonction de la r√©ponse de l'outil.

Vous pouvez en savoir plus sur ce concept dans le [livre blanc officiel](https://arxiv.org/abs/2210.03629). Et voici un diagramme qui r√©sume l'ensemble du processus :

![Diagramme illustrant le workflow d'un agent LLM : l'agent re√ßoit un prompt, raisonne, d√©cide d'une action (comme l'appel d'un outil), observe la r√©ponse de l'outil, g√©n√®re sa propre r√©ponse et planifie de mani√®re it√©rative ses actions suivantes √† l'aide du framework ReAct](https://cdn.hashnode.com/res/hashnode/image/upload/v1765064426716/b1e6d7b2-4e4b-43c4-af5c-9cd49b27a864.png align="center")

Notez que le workflow n'est pas limit√© √† une seule invocation d'outil ‚Äì il peut passer par plusieurs cycles avant de r√©pondre √† l'utilisateur.

Mais pour qu'un agent LLM soit vraiment proche de l'humain et agisse avec la connaissance du pass√©, il n√©cessite une m√©moire. Cela lui permet de se souvenir des prompts et des r√©ponses pr√©c√©dents, maintenant ainsi la coh√©rence au sein d'un thread donn√©.

Il n'y a pas de source unique de v√©rit√© sur la fa√ßon d'aborder cela. La plupart des agents impl√©mentent une m√©moire √† court terme. Cela signifie que l'agent ajoutera chaque nouveau chat √† l'historique de la conversation, et lorsqu'un nouveau prompt est soumis, l'agent ajoutera les messages pr√©c√©dents au nouveau prompt.

Cette m√©thode est tr√®s efficace et donne au LLM une solide connaissance des √©tats pr√©c√©dents. Mais elle peut aussi introduire des probl√®mes, car plus la conversation s'allonge, plus le LLM devra parcourir tous les messages pr√©c√©dents afin de comprendre quelle action entreprendre ensuite.

Et cela peut introduire une certaine d√©rive contextuelle, tout comme les humains en font l'exp√©rience. Vous ne pouvez pas regarder un podcast de deux heures et vous souvenir de tous les mots prononc√©s, n'est-ce pas ? Dans ce sc√©nario, le LLM se concentrera sur les informations les plus pertinentes, finissant par perdre une partie du contexte.

![Illustration montrant le workflow d'un agent LLM avec m√©moire : l'agent traite plusieurs cycles de prompts et d'interactions avec les outils, maintient une m√©moire √† court terme des conversations pr√©c√©dentes et utilise ce contexte pour d√©cider des actions, tandis que le contexte plus ancien peut s'estomper avec le temps, causant une d√©rive contextuelle potentielle.](https://cdn.hashnode.com/res/hashnode/image/upload/v1765064542431/18b8d0a7-b9f1-4f7d-993d-76b3c4058ccf.png align="center")

Vous n'avez pas besoin d'impl√©menter cela de z√©ro. De nombreux outils et frameworks ont √©t√© d√©velopp√©s pour rendre l'impl√©mentation aussi facile que possible. Vous pouvez le construire de z√©ro si vous le souhaitez, bien s√ªr, mais nous ne ferons pas cela ici.

Dans cet article, nous allons construire un barista Starbucks qui collecte les informations de commande et appelle un outil `create_order` une fois que la commande r√©pond √† tous les crit√®res. C'est un outil que nous allons cr√©er et exposer √† l'IA.

## Configuration du projet

Commen√ßons par initialiser notre projet. Nous utiliserons Nest.js pour son efficacit√© et son support natif de TypeScript. Notez que rien ici n'est li√© √† Nest.js ‚Äì c'est juste une pr√©f√©rence de framework, et tout ce que nous ferons ici peut √™tre fait avec Node.js et Express.js.

Voici une liste de tous les outils que nous utiliserons :

1. `langchain/core` - **Toujours requis**
    
    C'est le moteur principal de Langchain qui d√©finit tous les outils de base et les fonctions fondamentales, contenant :
    
    * les templates de prompt
        
    * les types de messages
        
    * les runnables
        
    * les interfaces d'outils
        
    * les utilitaires de composition de cha√Ænes, et plus encore.
        
    
    La plupart des projets LangChain en ont besoin.
    
2. `langchain/google-genai` - Ce package est utilis√© pour interagir avec les mod√®les d'IA g√©n√©rative de Google, les mod√®les d'embeddings vectoriels et d'autres outils connexes.
    
3. `langchain/langgraph` - **Important pour construire un agent IA avec un contr√¥le total**
    
    Langgraph est un framework d'orchestration de bas niveau pour construire des agents contr√¥lables. Il peut √™tre utilis√© pour construire :
    
    * Des agents conversationnels.
        
    * Des automatisations de t√¢ches complexes.
        
    * La gestion du contexte de l'agent.
        
4. `langchain/langgraph-checkpoint-mongodb` - Ce package fournit un checkpointer bas√© sur MongoDB pour LangGraph, permettant la persistance de l'√©tat de l'agent et de la m√©moire √† court terme √† l'aide de MongoDB.
    
5. `@langchain/mongodb` - Ce package fournit des int√©grations MongoDB pour LangChain, vous permettant de :
    
    * Stocker et r√©cup√©rer des embeddings vectoriels.
        
    * Persister des documents LangChain, des agents ou des √©tats de m√©moire.
        
    * Int√©grer facilement MongoDB comme backend de base de donn√©es pour vos workflows d'IA.
        
6. `@nestjs/mongoose` - Un wrapper NestJS autour de Mongoose pour MongoDB. Fournit :
    
    * Le support de l'injection de d√©pendances pour les mod√®les Mongoose.
        
    * Une d√©finition de sch√©ma et une gestion de mod√®le simplifi√©es.
        
    * Une int√©gration transparente de MongoDB dans les applications NestJS, permettant la persistance des donn√©es structur√©es pour les applications d'IA ou tout backend.
        
7. `langchain` - C'est le package npm principal qui agr√®ge les fonctionnalit√©s de LangChain. Il fournit :
    
    * L'acc√®s aux connecteurs, utilitaires et modules de base.
        
    * L'importation facile de diff√©rents composants LangChain en un seul endroit.
        
    * Couramment utilis√© aux c√¥t√©s de `@langchain/core` pour construire des applications avec une configuration minimale.
        
8. `mongodb` - Le driver officiel MongoDB pour Node.js. Il fournit :
    
    * Un acc√®s flexible et de bas niveau aux bases de donn√©es MongoDB.
        
    * Le support des op√©rations CRUD, des transactions et de l'indexation.
        
    * Une d√©pendance requise si vous pr√©voyez de connecter des composants LangChain ou votre backend directement √† MongoDB.
        
9. `mongoose` - Une biblioth√®que ODM (Object Data Modeling) pour MongoDB. Offre :
    
    * Une mod√©lisation des donn√©es bas√©e sur des sch√©mas pour les documents MongoDB.
        
    * Des middlewares, de la validation et des hooks pour les op√©rations MongoDB.
        
    * Id√©al pour la gestion des donn√©es structur√©es dans NestJS ou d'autres applications Node.js.
        
10. `zod` - Une biblioth√®que de validation de sch√©ma orient√©e TypeScript. Utilis√©e pour :
    
    * D√©finir des sch√©mas de donn√©es stricts et valider les entr√©es/sorties.
        
    * Assurer la s√©curit√© des types au moment de l'ex√©cution.
        
    * Utile dans les applications d'IA pour valider les r√©ponses des mod√®les ou imposer la coh√©rence des donn√©es.
        

Commencez par initialiser votre projet Nest.js et installez toutes les d√©pendances requises :

```dart
$ npm i -g @nestjs/cli // Si vous n'avez pas Nest.js install√© sur votre machine
$ nest new project-name

"dependencies" : {
    "@langchain/core": "^0.3.75",
    "@langchain/google-genai": "^0.2.16",
    "@langchain/langgraph": "^0.4.8",
    "@langchain/langgraph-checkpoint-mongodb": "^0.1.1",
    "@langchain/mongodb": "^0.1.0",
    "@nestjs/mongoose": "^11.0.3",
    "langchain": "^0.3.33",
    "mongodb": "^6.19.0",
    "mongoose": "^8.18.1",
    "zod": "^4.1.8"
}

// Les versions peuvent ne pas √™tre les m√™mes au moment o√π vous lisez ceci, je recommande donc de v√©rifier
// la documentation officielle pour chaque package.
```

Maintenant que notre projet est cr√©√© et que tous les packages sont install√©s, voyons ce que nous devons faire pour transformer notre vision en projet. Pensez √† ce dont vous aurez besoin pour cr√©er un barista Starbucks :

* Tout d'abord, nous devons d√©finir la structure de nos donn√©es (cr√©ation de sch√©mas)
    
* Ensuite, nous devons cr√©er une liste de menu √† laquelle notre agent se r√©f√©rera.
    
* Apr√®s cela, nous ajouterons l'interaction avec le LLM
    
* Et enfin, nous ajouterons la possibilit√© de sauvegarder les conversations pr√©c√©dentes pour le contexte conversationnel.
    

### Structure des dossiers

Vous pouvez modifier cette structure de dossiers et l'adapter en fonction de votre framework de choix. Mais l'impl√©mentation de base est la m√™me pour tous les frameworks.

```plaintext
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .eslintrc.js
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .prettierrc
‚îú‚îÄ‚îÄ nest-cli.json
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ tsconfig.build.json
‚îú‚îÄ‚îÄ tsconfig.json
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app.controller.ts
‚îÇ   ‚îú‚îÄ‚îÄ app.module.ts
‚îÇ   ‚îú‚îÄ‚îÄ app.service.ts
‚îÇ   ‚îú‚îÄ‚îÄ main.ts
‚îÇ   ‚îú‚îÄ‚îÄ chat/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.controller.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.module.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.service.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dtos/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ chat.dto.ts
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ order.schema.ts
‚îÇ   ‚îî‚îÄ‚îÄ util/
‚îÇ       ‚îú‚îÄ‚îÄ constants/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ drinks_data.ts
‚îÇ       ‚îú‚îÄ‚îÄ schemas/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ drinks/
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Drink.schema.ts
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ orders/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ Order.schema.ts
‚îÇ       ‚îú‚îÄ‚îÄ summeries/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ drink.ts
‚îÇ       ‚îî‚îÄ‚îÄ types/
```

## Sch√©matisation des donn√©es avec Zod

Ce fichier contient toutes nos d√©finitions de sch√©mas concernant les boissons et toutes les modifications qu'elles peuvent recevoir. Cette partie est utile pour d√©finir la structure des donn√©es qui seront utilis√©es par l'agent IA.

### **Importation de Zod**

Dans le fichier `lib/util/schemas/drinks.ts`, avant de d√©finir tout sch√©ma, importez la biblioth√®que Zod, qui fournit des outils pour construire des sch√©mas orient√©s TypeScript.

```typescript
// Importe l'objet 'z' de la biblioth√®que 'zod'.
// Zod est une biblioth√®que de d√©claration et de validation de sch√©ma orient√©e TypeScript.
// 'z' est l'objet principal utilis√© pour d√©finir des sch√©mas (ex: z.object, z.string, z.boolean, z.array).
import z from "zod";
```

Zod vous offre un moyen simple et expressif de d√©finir et de valider la structure des donn√©es avec lesquelles notre agent interagira.

### **Sch√©ma de boisson (Drink Schema)**

Ce sch√©ma repr√©sente la structure d'une boisson dans le menu de style Starbucks. J'ai divis√© et expliqu√© chaque champ afin que le lecteur comprenne clairement ce que chaque propri√©t√© contr√¥le.

```typescript
export const DrinkSchema = z.object({
  name: z.string(),            // Nom requis de la boisson
  description: z.string(),     // Explication requise de ce qu'est la boisson
  supportMilk: z.boolean(),    // Si les options de lait sont disponibles
  supportSweeteners: z.boolean(), // Si des √©dulcorants peuvent √™tre ajout√©s
  supportSyrup: z.boolean(),   // Si les sirops aromatis√©s sont autoris√©s
  supportTopping: z.boolean(), // Si les garnitures sont support√©es
  supportSize: z.boolean(),    // Si la boisson peut √™tre command√©e en diff√©rentes tailles
  image: z.string().url().optional(), // URL d'image optionnelle
});
```

### **Ce que ce sch√©ma repr√©sente**

* Il garantit que chaque boisson a un nom et une description appropri√©s.
    
* Il d√©finit quelles personnalisations s'appliquent √† la boisson.
    
* Il pr√©pare l'agent √† raisonner sur les options de boissons dans un format structur√© et valid√©.
    

### **Sch√©ma d'√©dulcorant (Sweetener Schema)**

Chaque option d'√©dulcorant dans le menu est repr√©sent√©e par son propre sch√©ma.

```typescript
export const SweetenerSchema = z.object({
  name: z.string(),                // Nom de l'√©dulcorant
  description: z.string(),         // Ce que c'est / description du go√ªt
  image: z.string().url().optional(), // URL d'image optionnelle
});
```

Cela garantit la coh√©rence entre toutes les entr√©es d'√©dulcorants et √©vite les donn√©es malform√©es.

### **Sch√©ma de sirop (Syrup Schema)**

Similaire aux √©dulcorants, mais pour les saveurs de sirop :

```typescript

export const SyrupSchema = z.object({
  name: z.string(),
  description: z.string(),
  image: z.string().url().optional(),
});
```

Cela peut repr√©senter des saveurs comme Vanille, Caramel ou Noisette.

### **Sch√©ma de garniture (Topping Schema)**

Les garnitures telles que la cr√®me fouett√©e ou la cannelle sont d√©finies ici.

```typescript
export const ToppingSchema = z.object({
  name: z.string(),
  description: z.string(),
  image: z.string().url().optional(),
});
```

### **Sch√©ma de taille (Size Schema)**

Les tailles de boissons sont √©galement mod√©lis√©es comme des objets :

```typescript
export const SizeSchema = z.object({
  name: z.string(),               // ex: Petit, Moyen
  description: z.string(),        // Une courte explication
  image: z.string().url().optional(),
});
```

### **Sch√©ma de lait (Milk Schema)**

Repr√©sente les types de lait tels que Entier, √âcr√©m√©, Amande ou Avoine.

```typescript
export const MilkSchema = z.object({
  name: z.string(),
  description: z.string(),
  image: z.string().url().optional(),
});
```

### **Collections d'√©l√©ments**

Maintenant que les sch√©mas d'√©l√©ments individuels existent, nous pouvons en cr√©er des **collections**. Celles-ci repr√©sentent toutes les garnitures, tailles, types de lait, sirops, √©dulcorants disponibles, ainsi que l'int√©gralit√© du menu des boissons.

```typescript
export const ToppingsSchema = z.array(ToppingSchema);
export const SizesSchema = z.array(SizeSchema);
export const MilksSchema = z.array(MilkSchema);
export const SyrupsSchema = z.array(SyrupSchema);
export const SweetenersSchema = z.array(SweetenerSchema);
export const DrinksSchema = z.array(DrinkSchema);
```

Pourquoi des tableaux ? Parce que dans le monde r√©el, votre agent recevra des **listes** provenant d'une base de donn√©es ou d'une API ‚Äî pas des √©l√©ments uniques.

### **Types inf√©r√©s**

Zod permet √©galement √† TypeScript d'inf√©rer automatiquement les types √† partir des sch√©mas.

Cela garantit que :

* Les types TypeScript correspondent toujours aux sch√©mas.
    
* Vous √©vitez les d√©finitions dupliqu√©es.
    
* Le code de l'agent reste coh√©rent et s√ªr.
    

```typescript
export type Drink = z.infer<typeof DrinkSchema>;
export type SupportSweetener = z.infer<typeof SweetenerSchema>;
export type Syrup = z.infer<typeof SyrupSchema>;
export type Topping = z.infer<typeof ToppingSchema>;
export type Size = z.infer<typeof SizeSchema>;
export type Milk = z.infer<typeof MilkSchema>;

export type Toppings = z.infer<typeof ToppingsSchema>;
export type Sizes = z.infer<typeof SizesSchema>;
export type Milks = z.infer<typeof MilksSchema>;
export type Syrups = z.infer<typeof SyrupsSchema>;
export type Sweeteners = z.infer<typeof SweetenersSchema>;
export type Drinks = z.infer<typeof DrinksSchema>;
```

Ceux-ci fournissent au reste de votre code LangChain/LangGraph un typage fort bas√© sur vos d√©finitions de sch√©mas.

Ce fichier complet :

* Encode toutes les structures de donn√©es li√©es aux boissons.
    
* Fournit une validation pour garantir des donn√©es propres et pr√©visibles.
    
* G√©n√®re automatiquement des types TypeScript.
    
* Aide l'agent IA √† raisonner de mani√®re fiable sur les boissons et les options de personnalisation.
    

Vous utiliserez ces sch√©mas plus tard et les convertirez en repr√©sentations textuelles pour les prompts du LLM.

*Vous pouvez trouver le fichier contenant tout le code* [*ici*](https://github.com/DjibrilM/langgraph-starbucks-agent/blob/main/src/lib/schemas/drinks.ts)*.*

## Comment analyser le sch√©ma

Comme mentionn√© pr√©c√©demment, les LLM sont des **machines √† entr√©es-sorties textuelles**. Ils ne comprennent pas directement les types TypeScript ou les sch√©mas Zod. Si vous incluez un sch√©ma √† l'int√©rieur d'un prompt, le mod√®le le verra simplement comme du texte brut sans comprendre sa structure ou ses contraintes.

Pour cette raison, nous avons besoin d'un moyen de convertir les sch√©mas dans un format de cha√Æne lisible qui peut √™tre int√©gr√© √† l'int√©rieur d'un prompt, tel que :

> "La sortie doit √™tre un objet JSON avec les champs suivants..."

C'est exactement le probl√®me r√©solu par `StructuredOutputParser` de `langchain/output_parsers`. Il prend un sch√©ma Zod et le transforme en :

* Une description lisible par l'homme qui peut √™tre envoy√©e √† un LLM.
    
* Un validateur qui v√©rifie si la sortie du mod√®le correspond au sch√©ma.
    

En r√©sum√©, il agit comme un pont entre la logique d'application typ√©e et la sortie d'IA bas√©e sur le texte.

### D√©finition du sch√©ma de commande (Order Schema)

Nous allons commencer par un sch√©ma Zod simple qui repr√©sente la commande de boisson d'un client. Ce sch√©ma d√©finit la forme exacte et les contraintes des donn√©es que nous attendons du mod√®le.

```typescript
export const OrderSchema = z.object({
  drink: z.string(),
  size: z.string(),
  mil: z.string(),
  syrup: z.string(),
  sweeteners: z.string(),
  toppings: z.string(),
  quantity: z.number().min(1).max(10),
});

export type OrderType = z.infer<typeof OrderSchema>;
```

√Ä ce stade, le sch√©ma n'est utile qu'√† l'int√©rieur de notre application TypeScript. Le LLM n'a toujours aucune id√©e de ce que signifie cette structure.

### Analyse du sch√©ma en texte lisible par l'homme

C'est l√† qu'intervient l'analyse de sch√©ma. En utilisant `StructuredOutputParser.fromZodSchema`, nous pouvons transformer le sch√©ma Zod en :

* Des instructions que le LLM peut comprendre.
    
* Un validateur d'ex√©cution qui garantit que la r√©ponse est correcte.
    

```typescript
export const OrderParser =
  StructuredOutputParser.fromZodSchema(OrderSchema as any);
```

L'analyseur permet deux workflows critiques :

#### G√©n√©ration d'instructions de prompt

L'analyseur peut g√©n√©rer une description textuelle du sch√©ma qui ressemble approximativement √† : "Renvoyez un objet JSON avec les champs `drink`, `size`, `mil`, `syrup`, `sweeteners` et `toppings` sous forme de cha√Ænes, et `quantity` sous forme de nombre entre 1 et 10." Cette cha√Æne peut √™tre inject√©e directement dans votre prompt afin que le LLM sache exactement comment formater sa r√©ponse.

#### Validation de la sortie du mod√®le

Une fois que le LLM a r√©pondu, sa sortie n'est toujours que du texte. L'analyseur :

* Convertit ce texte en un objet JavaScript.
    
* Le valide par rapport au sch√©ma Zod original.
    
* Lance une erreur si quelque chose manque, est malform√© ou hors limites.
    

Cela emp√™che les donn√©es invalides g√©n√©r√©es par l'IA (par exemple, `quantity: 0`) d'entrer dans votre syst√®me.

### R√©utilisation de la m√™me approche pour d'autres sch√©mas

Une fois que vous avez compris ce mod√®le, l'appliquer √† d'autres sch√©mas est simple.

Par exemple, vous pouvez faire la m√™me chose pour un `DrinkSchema` :

```typescript
export const DrinkParser =
  StructuredOutputParser.fromZodSchema(DrinkSchema as any);
```

Maintenant, vous pouvez dire en toute confiance quelque chose comme : "H√© Gemini, voici √† quoi ressemble un objet boisson ‚Äî s'il te pla√Æt, r√©ponds en utilisant cette structure."

### Pourquoi c'est important

L'analyse de sch√©ma vous permet de :

* Conserver un typage fort dans votre application.
    
* Donner des instructions de formatage claires au LLM.
    
* Convertir en toute s√©curit√© une sortie d'IA non structur√©e en donn√©es valid√©es et pr√™tes pour la production.
    

Sans cette √©tape, travailler avec des LLM √† grande √©chelle devient peu fiable et sujet aux erreurs.

## R√©sum√© de donn√©es en texte

Dans le contexte des agents LLM, le **r√©sum√© de donn√©es en texte** consiste √† convertir des donn√©es structur√©es ‚Äî telles que des objets renvoy√©s par une base de donn√©es ou une API backend ‚Äî en **cha√Ænes de caract√®res claires et lisibles par l'homme** qui peuvent √™tre int√©gr√©es directement dans les prompts.

M√™me les LLM les plus avanc√©s fonctionnent uniquement sur du texte. Ils ne raisonnent pas sur des objets JavaScript, des lignes de base de donn√©es ou des structures JSON de la m√™me mani√®re que les humains ou les programmes. Plus votre entr√©e textuelle est claire et descriptive, plus la sortie du mod√®le sera pr√©cise et fiable.

Pour cette raison, un mod√®le courant et recommand√© lors de la construction de syst√®mes aliment√©s par LLM est :

**R√©cup√©rer les donn√©es structur√©es ‚Üí les r√©sumer en langage naturel ‚Üí passer le r√©sum√© dans le prompt**

Pour garder cet article focalis√©, nous stockerons nos donn√©es dans des constantes au lieu d'interroger une base de donn√©es r√©elle. La technique est exactement la m√™me, que les donn√©es proviennent de MongoDB, PostgreSQL ou d'une API.

### L'id√©e centrale

L'objectif du r√©sum√© de donn√©es en texte est simple :

* Prendre un objet avec des champs et des drapeaux bool√©ens
    
* Le convertir en un court paragraphe qui explique ce que l'objet repr√©sente
    
* √âliminer l'ambigu√Øt√© et les suppositions pour le LLM
    

Au lieu de forcer le mod√®le √† inf√©rer le sens √† partir de donn√©es brutes, nous l'*√©non√ßons explicitement*.

### R√©sumer un objet boisson

Consid√©rez l'objet boisson suivant :

```typescript
{
  name: 'Espresso',
  description: 'Shot de caf√© concentr√© et fort.',
  supportMilk: false,
  supportSweeteners: true,
  supportSyrup: true,
  supportTopping: false,
  supportSize: false,
}
```

Bien que cette structure soit facile √† comprendre pour les d√©veloppeurs, elle n'est pas id√©ale pour un prompt de LLM. Les drapeaux bool√©ens comme `supportMilk: false` n√©cessitent une interpr√©tation, ce qui augmente le risque de suppositions incorrectes.

Au lieu de cela, nous convertissons cet objet en un paragraphe descriptif :

"Une boisson nomm√©e Espresso. Elle est d√©crite comme un shot de caf√© fort et concentr√©. Elle ne peut pas √™tre pr√©par√©e avec du lait. Elle peut √™tre pr√©par√©e avec des √©dulcorants. Elle peut √™tre pr√©par√©e avec du sirop. Elle ne peut pas √™tre pr√©par√©e avec des garnitures. Elle ne peut pas √™tre pr√©par√©e en diff√©rentes tailles."

Cette transformation est exactement ce que fournit le r√©sum√© de donn√©es en texte.

### Un mod√®le de r√©sum√© standard

Voici un exemple simplifi√© de la fa√ßon dont nous convertissons un objet `Drink` en une description lisible.

```typescript
export const createDrinkItemSummary = (drink: Drink): string => {
  const name = `Une boisson nomm√©e ${drink.name}.`;
  const description = `Elle est d√©crite comme ${drink.description}.`;

  const milk = drink.supportMilk
    ? 'Elle peut √™tre pr√©par√©e avec du lait.'
    : 'Elle ne peut pas √™tre pr√©par√©e avec du lait.';

  const sweeteners = drink.supportSweeteners
    ? 'Elle peut √™tre pr√©par√©e avec des √©dulcorants.'
    : 'Elle ne peut pas contenir d\'√©dulcorants.';

  const syrup = drink.supportSyrup
    ? 'Elle peut √™tre pr√©par√©e avec du sirop.'
    : 'Elle ne peut pas √™tre pr√©par√©e avec du sirop.';

  const toppings = drink.supportTopping
    ? 'Elle peut √™tre pr√©par√©e avec des garnitures.'
    : 'Elle ne peut pas √™tre pr√©par√©e avec des garnitures.';

  const size = drink.supportSize
    ? 'Elle peut √™tre pr√©par√©e en diff√©rentes tailles.'
    : 'Elle ne peut pas √™tre pr√©par√©e en diff√©rentes tailles.';

  return `${name} ${description} ${milk} ${sweeteners} ${syrup} ${toppings} ${size}`;
};
```

### Pourquoi cela fonctionne bien pour les LLM

* La logique bool√©enne est convertie en **phrases explicites**
    
* Chaque capacit√© et limitation est clairement √©nonc√©e
    
* La sortie peut √™tre int√©gr√©e directement dans un prompt syst√®me ou utilisateur
    

### R√©sumer des collections de donn√©es

Cette m√™me approche s'applique aux listes de donn√©es telles que les laits, les sirops, les garnitures ou les tailles. Au lieu de passer un tableau d'objets au mod√®le, nous les convertissons en r√©sum√©s textuels sous forme de puces :

```typescript
export const createSweetenersSummary = (): string => {
  return `Les √©dulcorants disponibles sont :
${SWEETENERS.map(
  (s) => `- ${s.name} : ${s.description}`
).join('\n')}`;
};
```

Cela donne au mod√®le une **vue d'ensemble compl√®te et lisible** des options disponibles sans lui demander d'interpr√©ter des tableaux bruts.

### Appliquer la m√™me id√©e √† d'autres domaines

Ce mod√®le n'est pas limit√© aux boissons ou aux menus. Il fonctionne pour *n'importe quel* domaine. Par exemple, voici la m√™me technique de r√©sum√© appliqu√©e √† un objet repr√©sentant une chaussure dans un assistant de commande en ligne :

```typescript
export const createShoeItemSummary = (shoe: {
  name: string;
  description: string;
  genderCategory: string;
  styleType: string;
  material: string;
  availableInMultipleColors: boolean;
  limitedEdition: boolean;
  supportsCustomization: boolean;
}): string => {
  return `
Une chaussure nomm√©e ${shoe.name}.
Elle est d√©crite comme ${shoe.description}.
Elle est class√©e comme une chaussure ${shoe.genderCategory.toLowerCase()}.
Elle appartient au style de mode ${shoe.styleType.toLowerCase()}.
Elle est faite de ${shoe.material.toLowerCase()}.
${shoe.availableInMultipleColors ? 'Elle est disponible en plusieurs couleurs.' : 'Elle est disponible en une seule couleur.'}
${shoe.limitedEdition ? 'C\'est une √©dition limit√©e.' : 'Ce n\'est pas une √©dition limit√©e.'}
${shoe.supportsCustomization ? 'Elle supporte des options de personnalisation.' : 'Elle ne supporte pas d\'options de personnalisation.'}
`.trim();
};
```

Ce qui produit une sortie comme :

"Une chaussure nomm√©e Veloria Canvas Sneaker. Elle est d√©crite comme une basket minimaliste de tous les jours con√ßue pour une tenue d√©contract√©e. Elle est class√©e comme une chaussure unisexe. Elle appartient au style de mode d√©contract√©. Elle est faite de toile respirante. Elle est disponible en plusieurs couleurs. Ce n'est pas une √©dition limit√©e. Elle supporte des options de personnalisation l√©g√®re."

## Comment persister les commandes avec MongoDB dans NestJS

Maintenant que nous avons √©tabli les bases de notre application ‚Äî sch√©mas, analyseurs et r√©sum√©s de donn√©es en texte ‚Äî il est temps de **persister les donn√©es**. Dans un assistant r√©el, les commandes et les conversations ne devraient pas dispara√Ætre au red√©marrage du serveur. Elles doivent √™tre stock√©es de mani√®re fiable afin de pouvoir √™tre r√©cup√©r√©es, analys√©es ou poursuivies plus tard.

Pour y parvenir, nous utiliserons MongoDB comme base de donn√©es et l'int√©gration NestJS Mongoose pour g√©rer les mod√®les de donn√©es et les collections.

### Connexion de MongoDB √† une application NestJS

Dans NestJS, le `AppModule` est le module racine de l'application. C'est l√† que les d√©pendances globales ‚Äî telles que les connexions aux bases de donn√©es ‚Äî sont configur√©es.

```typescript
@Module({
  imports: [
    MongooseModule.forRoot(process.env.MONGO_URI),
    ChatsModule,
  ],
  controllers: [AppController],
  providers: [AppService],
})
export class AppModule {}
```

Que se passe-t-il ici ?

* `MongooseModule.forRoot(...)` √©tablit une connexion MongoDB globale.
    
* La cha√Æne de connexion est lue √† partir d'une variable d'environnement (`MONGO_URI`), ce qui est la pratique recommand√©e pour la s√©curit√©.
    
* Une fois configur√©e, cette connexion devient disponible dans toute l'application.
    
* `ChatsModule` est import√© afin qu'il puisse acc√©der √† la connexion √† la base de donn√©es et enregistrer ses propres sch√©mas.
    

Cette configuration garantit que chaque module de fonctionnalit√© peut interagir en toute s√©curit√© avec MongoDB sans cr√©er de multiples connexions.

### D√©finition d'un sch√©ma de commande avec Mongoose

NestJS utilise des d√©corateurs pour d√©finir les sch√©mas MongoDB de mani√®re propre et bas√©e sur des classes. Chaque classe repr√©sente un document MongoDB, et chaque propri√©t√© devient un champ dans la collection.

```typescript
@Schema()
export class Order {
  @Prop({ required: true })
  drink: string;

  @Prop({ default: null })
  size: string;

  @Prop({ default: null })
  milk: string;

  @Prop({ default: null })
  syrup: string;

  @Prop({ default: null })
  sweeter: string;

  @Prop({ default: null })
  toppings: string;

  @Prop({ default: 1 })
  quantity: number;
}
```

Pourquoi cette approche ?

* Chaque d√©corateur `@Prop()` correspond directement √† un champ MongoDB.
    
* Les valeurs par d√©faut permettent de sauvegarder des commandes partielles de mani√®re incr√©mentielle.
    
* Les champs obligatoires (comme `drink`) imposent une int√©grit√© de base des donn√©es.
    
* Le sch√©ma refl√®te √©troitement la sortie structur√©e produite par le LLM.
    

Une fois la classe d√©finie, elle est convertie en un sch√©ma Mongoose :

```typescript
export const OrderSchema = SchemaFactory.createForClass(Order);
```

Cette seule ligne cr√©e :

* Une collection MongoDB
    
* Une couche de validation
    
* Un sch√©ma que Mongoose peut utiliser pour cr√©er, lire et mettre √† jour des commandes
    

### Comment cela s'int√®gre dans l'architecture de l'agent LLM

√Ä ce stade, nous avons :

* **Des sch√©mas Zod** ‚Üí pour valider la sortie de l'IA
    
* **Des fonctions de r√©sum√©** ‚Üí pour convertir les donn√©es en prompts lisibles
    
* **Des sch√©mas MongoDB** ‚Üí pour persister les commandes finalis√©es
    

Cette s√©paration est intentionnelle :

* Zod g√®re la *validation c√¥t√© IA*
    
* Mongoose g√®re la *persistance en base de donn√©es*
    
* NestJS agit comme le liant qui unit le tout
    

### Pr√©paration de la logique de l'agent

Avec la base de donn√©es en place, nous sommes maintenant pr√™ts √† impl√©menter l'agent lui-m√™me.

Les responsabilit√©s de l'agent incluront :

* L'interpr√©tation des messages des utilisateurs
    
* L'appel d'outils
    
* La g√©n√©ration de commandes structur√©es
    
* Leur validation
    
* Leur persistance dans MongoDB
    
* Le maintien de l'√©tat conversationnel
    

Toute cette logique vivra √† l'int√©rieur du fichier `src/chats/chats.service.ts`. La section suivante pr√©sente la **logique centrale de l'agent**, et nous la parcourrons √©tape par √©tape pour que chaque partie soit facile √† suivre.

Commencez par importer les d√©pendances requises :

```tsx

import { Injectable } from '@nestjs/common';
import { InjectModel } from '@nestjs/mongoose';
import { MongoClient } from 'mongodb';
import { Model } from 'mongoose';

import { tool } from '@langchain/core/tools';
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from '@langchain/core/prompts';
import { AIMessage, BaseMessage, HumanMessage } from '@langchain/core/messages';

import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { StateGraph } from '@langchain/langgraph';
import { ToolNode } from '@langchain/langgraph/prebuilt';
import { Annotation } from '@langchain/langgraph';
import { START, END } from '@langchain/langgraph';

import { MongoDBSaver } from '@langchain/langgraph-checkpoint-mongodb';

import z from 'zod';

import { Order } from './schemas/order.schema';
import { OrderParser, OrderSchema, OrderType } from 'src/lib/schemas/orders';
import { DrinkParser } from 'src/lib/schemas/drinks';
import { DRINKS } from 'src/lib/utils/constants/menu_data';

import {
  createSweetenersSummary,
  availableToppingsSummary,
  createAvailableMilksSummary,
  createSyrupsSummary,
  createSizesSummary,
  createDrinkItemSummary,
} from 'src/lib/summaries';

const GOOGLE_API_KEY = process.env.GOOGLE_API_KEY || '';
const client: MongoClient = new MongoClient(process.env.MONGO_URI || '');
const database_name = 'drinks_db';
```

## Termes d'√©tat et d'annotation LangGraph

Dans LangGraph, l'**√©tat** (state) peut √™tre consid√©r√© comme un espace de travail temporaire qui existe pendant que l'agent s'ex√©cute. Il stocke toutes les informations auxquelles les n≈ìuds (nous verrons les n≈ìuds en d√©tail plus tard) pourraient avoir besoin d'acc√©der, comme le dernier message, l'historique de la conversation ou toute donn√©e interm√©diaire g√©n√©r√©e pendant l'ex√©cution.

Cet √©tat permet aux n≈ìuds de **le lire, de le mettre √† jour et de transmettre des informations** au fur et √† mesure que l'agent traite un workflow, ce qui en fait la m√©moire √† court terme de l'agent pour la dur√©e de l'ex√©cution.

```tsx
@Injectable()
export class ChatService {

  chatWithAgent = async ({
    thread_id,
    query,
  }: {
    thread_id: string;
    query: string;
  }) => {

    const graphState = Annotation.Root({
      messages: Annotation<BaseMessage[]>({
        reducer: (x, y) => [...x, ...y],
      }),
    });

  }

}
```

Ce code d√©finit l'**√©tat LangGraph** pour l'agent de chat. L'objet `graphState` agit comme une m√©moire centrale que chaque n≈ìud du workflow peut lire et mettre √† jour.

Le champ `messages` stocke sp√©cifiquement tous les messages de la conversation, y compris les messages de l'utilisateur, les r√©ponses de l'IA et les sorties des outils. La fonction r√©ductrice (reducer) `[...x, ...y]` ajoute les nouveaux messages au tableau existant, pr√©servant l'historique de la conversation √† travers plusieurs √©tapes.

Le m√©canisme de r√©ducteur de LangGraph permet aux d√©veloppeurs de contr√¥ler comment le nouvel √©tat fusionne avec l'ancien. Dans ce syst√®me de chat, l'approche est similaire √† la mise √† jour d'un √©tat React avec `setMessages(prev => [...prev, ...newMessages])` : elle conserve les anciens messages tout en ajoutant les nouveaux.

Ensemble, cet √©tat permet √† l'agent, aux outils et au syst√®me de checkpointing de maintenir une conversation coh√©rente, permettant √† chaque n≈ìud du workflow LangGraph d'acc√©der au contexte complet et de contribuer de mani√®re incr√©mentielle.

## Comment cr√©er des outils pour l'agent

Les chatbots modernes peuvent faire plus que simplement g√©n√©rer du texte - ils peuvent √©galement effectuer des recherches sur Internet, lire des fichiers ou effectuer des calculs. Bien que les LLM soient puissants, ils ne peuvent pas ex√©cuter de code ou compiler des programmes par eux-m√™mes.

Dans le contexte des agents LLM, un outil est un morceau de code √©crit par le d√©veloppeur de l'agent qu'un LLM peut invoquer sur la machine h√¥te. La machine h√¥te ex√©cute le code, et le LLM ne re√ßoit que la sortie finale du calcul.

Voici comment cr√©er un outil qui stocke les commandes dans la base de donn√©es. Toujours dans la fonction `chatWithAgent` au sein de la classe `ChatService`. Sous la d√©finition du store d'√©tat :

```tsx
const orderTool = tool(
  async ({ order }: { order: OrderType }) => {
    try {
      await this.orderModel.create(order);
      return 'Order created successfully';
    } catch (error) {
      console.log(error);
      return 'Failed to create the order';
    }
  },
  {
    schema: z.object({
      order: OrderSchema.describe('The order that will be stored in the DB'),
    }),
    name: 'create_order',
    description: 'This tool creates a new order in the database',
  }
);

const tools = [orderTool];
```

## N≈ìuds LangGraph (Composants de workflow)

D'un point de vue d√©finition, un n≈ìud LangGraph est un composant fondamental d'un workflow LangGraph, repr√©sentant une unit√© unique de calcul ou une √©tape individuelle dans le processus d'un agent IA.

Chaque n≈ìud peut effectuer une t√¢che sp√©cifique, telle que la g√©n√©ration d'un message, l'invocation d'un outil ou la transformation de donn√©es, et il interagit avec l'√©tat pour lire les entr√©es et √©crire les sorties. Ensemble, les n≈ìuds sont connect√©s pour former le workflow de l'agent ou le graphe d'ex√©cution, permettant un raisonnement complexe et des op√©rations en plusieurs √©tapes.

Dans notre projet, nous aurons quatre n≈ìuds.

1. **N≈ìud Agent :** Ce n≈ìud est charg√© d'interagir avec le LLM - il construit le template de message principal de l'agent et empile les anciens messages sur le nouveau prompt pour cr√©er du contexte.
    
2. **N≈ìud Tools :** Le n≈ìud d'outils introduit des capacit√©s externes, qui permettent au workflow d'interagir avec des API externes.
    
3. **N≈ìud** `START` **:** Ce n≈ìud indique le point d'entr√©e de notre workflow, ou pour √™tre pr√©cis, quel n≈ìud appeler lorsqu'un utilisateur initie une conversation avec l'agent. Il est assez simple √† d√©finir.
    
4. `addConditionalEdges` - `addConditionalEdges('agent', shouldContinue)` : Dans LangGraph, `.addConditionalEdges('agent', shouldContinue)` permet au workflow de bifurquer dynamiquement apr√®s l'ex√©cution du n≈ìud `'agent'`, en fonction d'une condition d√©finie dans `shouldContinue`. Contrairement √† une ar√™te fixe, qui va toujours d'un n≈ìud au suivant, une ar√™te conditionnelle √©value la sortie de l'agent et dirige le workflow vers diff√©rents n≈ìuds selon le r√©sultat, permettant √† l'agent IA de prendre des d√©cisions et d'adapter ses prochaines √©tapes.
    

## D√©claration du graphe

Dans LangGraph, un graphe est la structure centrale qui mod√©lise le workflow d'un agent IA sous forme de n≈ìuds interconnect√©s, o√π chaque n≈ìud repr√©sente une √©tape de calcul, un outil ou une d√©cision. Il orchestre le flux de donn√©es et de contr√¥le entre les n≈ìuds, g√®re les branchements conditionnels et maintient la boucle r√©cursive d'ex√©cution.

Essentiellement, le graphe est la colonne vert√©brale qui garantit que les interactions complexes et avec √©tat se produisent de mani√®re coordonn√©e et modulaire, connectant des n≈ìuds comme `agent`, `tools` et des ar√™tes conditionnelles en un workflow coh√©rent.

Avec ces connaissances en place, nous pouvons maintenant cr√©er le graphe de l'agent avec tous ses n≈ìuds.

```tsx
  const callModal = async (states: typeof graphState.State) => {
    const prompt = ChatPromptTemplate.fromMessages([
      {
        role: 'system',
        content: `
            You are a helpful assistant that helps users order drinks from Starbucks.
            Your job is to take the user's request and fill in any missing details based on how a complete order should look.
            A complete order follows this structure: ${OrderParser}.

            **TOOLS**
            You have access to a "create_order" tool.
            Use this tool when the user confirms the final order.
            After calling the tool, you should inform the user whether the order was successfully created or if it failed.

            **DRINK DETAILS**
            Each drink has its own set of properties such as size, milk, syrup, sweetener, and toppings.
            Here is the drink schema: ${DrinkParser}.

            You must ask for any missing details before creating the order.

            If the user requests a modification that is not supported for the selected drink, tell them that it is not possible.

            If the user asks for something unrelated to drink orders, politely tell them that you can only assist with drink orders.

            **AVAILABLE OPTIONS**
            List of available drinks and their allowed modifications:
            ${DRINKS.map((drink) => `- ${createDrinkItemSummary(drink)}`)}

            Sweeteners: ${createSweetenersSummary()}
            Toppings: ${availableToppingsSummary()}
            Milks: ${createAvailableMilksSummary()}
            Syrups: ${createSyrupsSummary()}
            Sizes: ${createSizesSummary()}

            Order schema: ${OrderParser}

            If the user's query is unclear, tell them that the request is not clear.

            **ORDER CONFIRMATION**
            Once the order is ready, you must ask the user to confirm it.
            If they confirm, immediately call the "create_order" tool.
            Only respond after the tool completes, indicating success or failure.

            **FRONTEND RESPONSE FORMAT**
            Every response must include:

            "message": "Your message to the user",
            "current_order": "The order currently being constructed",
            "suggestions": "Options the user can choose from",
            "progress": "Order status ('completed' after creation)"

            **IMPORTANT RULES**
            - Be friendly, use emojis, and add humor.
            - Use null for unfilled fields.
            - Never omit the JSON tracking object.
        `,
      },
      new MessagesPlaceholder('messages'),
    ]);

  const formattedPrompt = await prompt.formatMessages({
    time: new Date().toISOString(),
    messages: states.messages,
  });

  const chat = new ChatGoogleGenerativeAI({
    model: 'gemini-2.0-flash',
    temperature: 0,
    apiKey: GOOGLE_API_KEY,
  }).bindTools(tools);

  const result = await chat.invoke(formattedPrompt);
  return { messages: [result] };
  };     
    const shouldContinue = (state: typeof graphState.State) => {
      const lastMessage = state.messages[
        state.messages.length - 1
      ] as AIMessage;
      return lastMessage.tool_calls?.length ? 'tools' : END;
    };

    const toolsNode = new ToolNode<typeof graphState.State>(tools);

    /**
     * Build the conversation graph.
     */
    const graph = new StateGraph(graphState)
      .addNode('agent', callModal)
      .addNode('tools', toolsNode)
      .addEdge(START, 'agent')
      .addConditionalEdges('agent', shouldContinue)
      .addEdge('tools', 'agent');
```

### Explication

* **√âtat du graphe (**`graphState`)  
    L'objet `graphState` est la m√©moire partag√©e entre tous les n≈ìuds. Il stocke les `messages`, qui suivent l'historique de la conversation, y compris les entr√©es utilisateur, les r√©ponses de l'IA et les interactions avec les outils. Le r√©ducteur `[...x, ...y]` ajoute les nouveaux messages, pr√©servant le contexte pass√©. C'est similaire aux mises √† jour d'√©tat React : les anciens messages restent tandis que les nouveaux sont ajout√©s.
    
* **N≈ìud Agent (**`callModal`)  
    Ce n≈ìud g√®re l'**appel au LLM**. Il formate un prompt contenant les instructions syst√®me, les sch√©mas de boissons, les outils disponibles et les r√®gles de r√©ponse pour le frontend. En incluant `states.messages`, l'IA voit l'historique complet de la conversation, permettant un dialogue multi-tours.
    
* **Ex√©cution du LLM**  
    `ChatGoogleGenerativeAI` g√©n√®re la r√©ponse de l'IA. `.bindTools(tools)` permet √† l'IA d'appeler directement des outils comme `create_order` si n√©cessaire.
    
* **Flux conditionnel (**`shouldContinue`)  
    Apr√®s la r√©ponse de l'IA, la fonction `shouldContinue` v√©rifie si le message inclut des appels d'outils. Si c'est le cas, l'ex√©cution passe au n≈ìud `tools` ; sinon, le workflow se termine. Cela permet un branchement dynamique en fonction de la sortie de l'IA.
    
* **N≈ìud d'outil (**`ToolNode`)  
    Le n≈ìud `tools` ex√©cute l'outil demand√©, comme la sauvegarde de la commande dans la base de donn√©es. Une fois termin√©, le contr√¥le revient au n≈ìud agent, permettant √† l'IA de r√©pondre √† l'utilisateur avec les r√©sultats.
    
* **Construction du graphe (**`StateGraph`)  
    Les n≈ìuds sont connect√©s dans un workflow coh√©rent :
    
    * `START ‚Üí agent` commence la conversation
        
    * Les ar√™tes conditionnelles g√®rent l'ex√©cution des outils
        
    * `tools ‚Üí agent` garantit que l'agent peut r√©pondre apr√®s l'ex√©cution des outils
        
* **Flux global**  
    Ensemble, le graphe et l'√©tat partag√© garantissent une **conversation multi-tours avec √©tat**. L'IA peut demander des d√©tails manquants, appeler des outils quand c'est n√©cessaire et maintenir le contexte √† travers les interactions. Chaque n≈ìud lit et √©crit dans le m√™me √©tat.
    

## **Compilation du workflow et persistance de l'√©tat (Partie finale)**

Jusqu'√† pr√©sent, tous nos √©tats sont temporaires, ce qui signifie qu'ils n'existent que pendant la dur√©e de la requ√™te d'un utilisateur. Cependant, nous voulons que notre agent **se souvienne et rappelle le contexte de la conversation** m√™me lorsqu'une nouvelle requ√™te est envoy√©e avec le m√™me `thread_id` ou ID de conversation.

Pour y parvenir, nous utiliserons MongoDB en combinaison avec la biblioth√®que `langchain/langgraph-checkpoint-mongo`. Cette biblioth√®que simplifie la persistance de l'√©tat en associant chaque conversation √† un ID unique assign√© manuellement. Toutes les op√©rations ‚Äî de la r√©cup√©ration des messages pr√©c√©dents √† la sauvegarde des nouveaux ‚Äî sont g√©r√©es en interne, vous n'avez qu'√† fournir l'ID de conversation avec lequel vous souhaitez travailler.

```tsx
const graph = new StateGraph(graphState)
  .addNode('agent', callModal)
  .addNode('tools', toolsNode)
  .addEdge(START, 'agent')
  .addConditionalEdges('agent', shouldContinue)
  .addEdge('tools', 'agent');

  const checkpointer = new MongoDBSaver({ client, dbName: database_name });

  const app = graph.compile({ checkpointer });

  /**
     * Run the graph using the user's message.
     */
    const finalState = await app.invoke(
      { messages: [new HumanMessage(query)] },
      { recursionLimit: 15, configurable: { thread_id } },
    );

  /**
   * Extract JSON payload from AI response.
   */
  function extractJsonResponse(response: any) {
    const match = response.match(/```json\\s*([\\s\\S]*?)\\s*```/i);
    if (match && match[1] && typeof response === 'string') {
      return JSON.parse(match[1].trim());
    }
    throw response;
  }

  const lastMessage = finalState.messages.at(-1) as AIMessage; // Extrait le dernier message de la conversation
  return extractJsonResponse(lastMessage.content); // R√©ponse
```

Le code ci-dessus d√©montre comment initialiser un checkpoint, compiler un graphe et invoquer l'agent avec un prompt entrant.

La m√©thode `extractJsonResponse` est utilis√©e pour r√©cup√©rer la r√©ponse format√©e que nous avons demand√© au LLM de g√©n√©rer chaque fois qu'il renvoie quelque chose √† l'utilisateur.

Sur la base de cette instruction donn√©e dans le template principal, chaque r√©ponse doit inclure : "message" : "Votre message √† l'utilisateur", "current\_order" : "La commande en cours de construction", "suggestions" : "Options que l'utilisateur peut choisir", "progress" : "Statut de la commande ('completed' apr√®s cr√©ation)"

Chaque r√©ponse du LLM devrait ressembler √† ceci :

```tsx
'```json\\n' +
  '{\\n' +
  '"message": "Got it! To make sure I get your order just right, can you clarify which coffee drink you\\'d like? We have Latte, Cappuccino, Cold Brew, and Frappuccino. üòä",\\n' +
  '"current_order": {\\n' +
  '"drink": null,\\n' +
  '"size": null,\\n' +
  '"mil": null,\\n' +
  '"syrup": null,\\n' +
  '"sweeteners": null,\\n' +
  '"toppings": null,\\n' +
  '"quantity": null\\n' +
  '},\\n' +
  '"suggestions": [\\n' +
  '"Latte",\\n' +
  '"Cappuccino",\\n' +
  '"Cold Brew",\\n' +
  '"Frappuccino"\\n' +
  '],\\n' +
  '"progress": "incomplete"\\n' +
  '}\\n' +
  '```';
```

Cette structure permet au frontend de rendre facilement la r√©ponse du LLM et de suivre l'√©tat de la commande actuelle. C'est plus un choix de conception qu'une convention.

## **Conclusion**

Construire un agent IA autonome avec LangChain et LangGraph vous permet de combiner la puissance de raisonnement des LLM avec l'ex√©cution d'outils pratiques et une m√©moire persistante. En d√©finissant des sch√©mas, en analysant les donn√©es dans des formats lisibles par l'homme et en orchestrant les workflows via des n≈ìuds, vous pouvez cr√©er des agents intelligents capables de g√©rer des t√¢ches du monde r√©el ‚Äî comme notre barista Starbucks.

Avec l'int√©gration de MongoDB pour la persistance de l'√©tat, votre agent peut maintenir le contexte √† travers les conversations, rendant les interactions plus naturelles et humaines. Cette approche ouvre la porte √† la construction d'assistants IA plus sophistiqu√©s et sp√©cifiques √† un domaine sans repartir de z√©ro.

En r√©sum√© : **d√©finissez vos donn√©es, apprenez √† votre agent comment raisonner et laissez LangGraph orchestrer la magie.** ‚òïü§ñ

Code source ici : [https://github.com/DjibrilM/langgraph-starbucks-agent](https://github.com/DjibrilM/langgraph-starbucks-agent)

### **Ressources**

* Documentation LangGraph : [https://docs.langchain.com/oss/javascript/langgraph/quickstart](https://docs.langchain.com/oss/javascript/langgraph/quickstart)
    
* Synergizing Reasoning and Acting in Language Models : [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)