---
title: Coder un LLM de zéro – de la théorie au RLHF
subtitle: ''
author: Beau Carnes
co_authors: []
series: null
date: '2025-09-23T12:36:57.590Z'
originalURL: https://freecodecamp.org/news/code-an-llm-from-scratch-theory-to-rlhf
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1758631000000/c6abd414-46ed-4fec-af5f-3cd694f5af71.png
tags:
- name: llm
  slug: llm
- name: youtube
  slug: youtube
seo_title: Coder un LLM de zéro – de la théorie au RLHF
seo_desc: 'How do LLMs actually work?

  We just posted a course on the freeCodeCamp.org YouTube channel that will teach
  you how to build a large language model from scratch using pure PyTorch.

  This isn''t your typical course that just scratches the surface. It’s a...'
---

Comment fonctionnent réellement les LLM ?

Nous venons de publier un cours sur la chaîne YouTube de [freeCodeCamp.org](http://freeCodeCamp.org) qui vous apprendra à construire un grand modèle de langage (LLM) de zéro en utilisant uniquement PyTorch.

Ce n'est pas un cours ordinaire qui se contente d'effleurer le sujet. C'est une immersion profonde dans le fonctionnement interne des LLM, créée par un expert en IA ayant plus d'une décennie d'expérience en recherche et en entreprise. Vous passerez de la théorie fondamentale à la construction d'un modèle fonctionnel, et vous apprendrez même à l'aligner en utilisant des techniques modernes comme le RLHF. Vivek Kalyanarangan a créé ce cours.

Ce cours complet de six heures est conçu pour vous faire vivre un parcours full-stack complet. Vous commencerez par les bases de l'architecture Transformer, puis vous passerez à des concepts plus avancés et prêts pour la production.

Voici quelques-uns des sujets clés que vous aborderez :

* **Architecture Transformer de base :** Comprenez les blocs de construction fondamentaux des LLM.
    
* **Entraînement d'un Tiny LLM :** Pratiquez avec un modèle simple et voyez comment il fonctionne.
    
* **Améliorations modernes :** Implémentez des fonctionnalités avancées telles que RMSNorm, RoPE et le KV caching qui rendent les modèles plus efficaces.
    
* **Passage à l'échelle (Scaling Up) :** Apprenez à utiliser des techniques telles que la précision mixte (mixed precision) et le logging riche pour entraîner des modèles plus grands.
    
* **Couches Mixture-of-Experts (MoE) :** Découvrez comment utiliser ces couches puissantes pour construire des modèles plus performants.
    
* **Supervised Fine-Tuning (SFT) :** Apprenez à personnaliser le comportement de votre modèle.
    
* **Modélisation des récompenses et RLHF avec PPO :** C'est ici que vous apprendrez à aligner votre modèle et à façonner son comportement pour qu'il soit plus utile et sûr.
    

Chaque étape est expliquée clairement, et l'intégralité du code source est disponible sur GitHub pour que vous puissiez suivre et expérimenter.

L'objectif est de vous donner à la fois le "pourquoi" et le "comment" derrière les LLM afin que vous puissiez réellement assimiler les concepts et construire vos propres applications.

Vous pouvez regarder [le cours complet sur notre chaîne YouTube](https://youtu.be/p3sij8QzONQ) (durée de 6 heures).

%[https://youtu.be/p3sij8QzONQ]