---
title: Comment am√©liorer les performances d'un mod√®le de machine learning en combinant
  des caract√©ristiques cat√©gorielles
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2021-05-27T17:55:23.000Z'
originalURL: https://freecodecamp.org/news/improve-machine-learning-model-performance-by-combining-categorical-features
coverImage: https://www.freecodecamp.org/news/content/images/2021/05/zVaxL0LohRUpfDQhznRQ9z3y5tj1-g7c32kc.jpeg
tags:
- name: Machine Learning
  slug: machine-learning
- name: performance
  slug: performance
seo_title: Comment am√©liorer les performances d'un mod√®le de machine learning en combinant
  des caract√©ristiques cat√©gorielles
seo_desc: 'By Davis David

  When you''re training a machine learning model, you can have some features in your
  dataset that represent categorical values. Categorical features are types of data
  that you can divide into groups.

  There are three common categorical dat...'
---

Par Davis David

Lorsque vous entra√Ænez un mod√®le de machine learning, vous pouvez avoir certaines caract√©ristiques dans votre ensemble de donn√©es qui repr√©sentent des valeurs cat√©gorielles. Les caract√©ristiques cat√©gorielles sont des types de donn√©es que vous pouvez diviser en groupes.

Il existe trois types courants de donn√©es cat√©gorielles :

1. **Ordinal** ‚Äì un ensemble de valeurs dans l'ordre croissant ou d√©croissant. Exemple : √©valuer le bonheur sur une √©chelle de 1 √† 10
2. **Binaire** ‚Äì un ensemble avec seulement deux valeurs. Exemple : chaud ou froid.
3. **Nominal** ‚Äì un ensemble contenant des valeurs sans ordre particulier. Exemple : une liste de pays

La plupart des algorithmes de machine learning n√©cessitent des variables d'entr√©e et de sortie num√©riques. Cela signifie que vous devrez transformer les caract√©ristiques cat√©gorielles de votre ensemble de donn√©es en entiers ou en flottants afin que les algorithmes de machine learning puissent les utiliser.

Vous pouvez soit utiliser le [LabelEncoding](https://www.freecodecamp.org/news/feature-engineering-and-feature-selection-for-beginners/?ref=hackernoon.com) pour les caract√©ristiques binaires, soit la m√©thode [One-hot-encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f?ref=hackernoon.com) pour les caract√©ristiques nominales.

Dans cet article, vous apprendrez comment la combinaison de caract√©ristiques cat√©gorielles peut am√©liorer les performances de votre mod√®le de machine learning.

Alors, commen√ßons. üöÄ

## Comment combiner des caract√©ristiques cat√©gorielles dans les mod√®les de machine learning

Vous pouvez cr√©er une nouvelle caract√©ristique qui est une combinaison de deux autres caract√©ristiques cat√©gorielles. Vous pouvez √©galement combiner plus de trois, quatre ou m√™me plus de caract√©ristiques cat√©gorielles.

```python
df["new_feature"] = (
	df.feature_1.astype(str)
	 + "_"
	 + df.feature_2.astype(str)
	)
```

Dans le code ci-dessus, vous pouvez voir comment combiner deux caract√©ristiques cat√©gorielles en utilisant Pandas et former une nouvelle caract√©ristique dans votre ensemble de donn√©es.

Quelles caract√©ristiques cat√©gorielles devez-vous combiner ? Eh bien, il n'y a pas de r√©ponse facile √† cela. Cela d√©pend de vos donn√©es et des types de caract√©ristiques. Certaines connaissances du domaine peuvent √™tre utiles pour cr√©er de nouvelles caract√©ristiques comme celle-ci.

Pour illustrer tout le processus, nous allons utiliser l'ensemble de donn√©es [Financial Inclusion in Africa](https://zindi.africa/competitions/financial-inclusion-in-africa/data?ref=hackernoon.com) de la page de comp√©tition [Zindi](https://zindi.africa/competitions/financial-inclusion-in-africa?ref=hackernoon.com). Il contient de nombreuses caract√©ristiques cat√©gorielles que nous pouvons combiner pour voir si nous pouvons am√©liorer les performances du mod√®le.

L'objectif de cet ensemble de donn√©es est de pr√©dire qui est le plus susceptible d'avoir un compte bancaire. Il s'agit donc d'un probl√®me de classification.

## √âtape 1 ‚Äì Charger l'ensemble de donn√©es

Notre premi√®re √©tape consiste √† nous assurer que nous avons t√©l√©charg√© l'ensemble de donn√©es fourni dans la comp√©tition. Vous pouvez t√©l√©charger l'ensemble de donn√©es [ici](https://zindi.africa/competitions/financial-inclusion-in-africa/data?ref=hackernoon.com).

Importez les packages Python importants comme ceci :

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
np.random.seed(123)
warnings.filterwarnings('ignore')
%matplotlib inline

```

Ensuite, chargez l'ensemble de donn√©es.

```python
# Importer les donn√©es
data = pd.read_csv('data/Train_v2.csv')

```

Regardons la forme de notre ensemble de donn√©es :

```python
# afficher la forme
print('forme des donn√©es :', data.shape)

forme des donn√©es : (23524, 13)

```

La sortie ci-dessus montre le nombre de lignes et de colonnes dans l'ensemble de donn√©es. Nous avons 13 variables dans l'ensemble de donn√©es ‚Äì 12 variables ind√©pendantes et 1 variable d√©pendante.

Nous pouvons voir les cinq premi√®res lignes de notre ensemble de donn√©es en utilisant la m√©thode **`head()`** de la biblioth√®que Pandas.

```python
# inspecter les donn√©es 

data.head()
```

![Image](https://www.freecodecamp.org/news/content/images/2021/05/zVaxL0LohRUpfDQhznRQ9z3y5tj1-oz2132qe.jpeg)

Il est important de comprendre la signification de chaque caract√©ristique afin de bien comprendre l'ensemble de donn√©es. Vous pouvez lire le fichier **VariableDefinition.csv** pour comprendre la signification de chaque variable pr√©sent√©e dans l'ensemble de donn√©es.

## √âtape 2 ‚Äì Interpr√©ter l'ensemble de donn√©es

Nous pouvons obtenir plus d'informations sur les caract√©ristiques que nous avons en utilisant la m√©thode **`info()`** de Pandas.

```python
# afficher quelques informations sur l'ensemble de donn√©es

print(train_data.info())
```

![Image](https://www.freecodecamp.org/news/content/images/2021/05/zVaxL0LohRUpfDQhznRQ9z3y5tj1-h62f32t7.jpeg)

La sortie montre la liste des variables/caract√©ristiques, les tailles si elle contient des valeurs manquantes et le type de donn√©es pour chaque variable.

Nous n'avons aucune valeur manquante dans l'ensemble de donn√©es. Nous avons trois caract√©ristiques de type de donn√©es entier et 10 caract√©ristiques de type de donn√©es objet (la plupart sont des caract√©ristiques cat√©gorielles).

## √âtape 3 ‚Äì Pr√©parer les donn√©es pour les mod√®les de machine learning

L'√©tape suivante consiste √† s√©parer les variables ind√©pendantes et la cible (bank_account) des donn√©es. Ensuite, transformer les valeurs cibles du type de donn√©es objet en donn√©es num√©riques en utilisant [LabelEncoder](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd?ref=hackernoon.com).

```python
# importer le module de pr√©traitement
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler

# Convertir l'√©tiquette cible en donn√©es num√©riques
le = LabelEncoder()
data['bank_account'] = le.fit_transform(data['bank_account'])

# S√©parer les caract√©ristiques d'entra√Ænement de la cible
X = data.drop(['bank_account'], axis=1)
y = data['bank_account']

print(y)

```

![Image](https://www.freecodecamp.org/news/content/images/2021/05/zVaxL0LohRUpfDQhznRQ9z3y5tj1-el2r32yy--1-.jpeg)

Nous avons transform√© les valeurs cibles en types de donn√©es num√©riques ‚Äì 1 repr√©sente 'Oui' et 0 repr√©sente 'Non'.

J'ai cr√©√© une fonction de pr√©traitement simple pour :

* G√©rer la conversion des types de donn√©es.
* Convertir les caract√©ristiques cat√©gorielles en caract√©ristiques num√©riques en utilisant [One-hot Encoder et/ou Label Encoder](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd?ref=hackernoon.com).
* Supprimer la variable uniqueid.
* Effectuer une [mise √† l'√©chelle des caract√©ristiques](https://towardsdatascience.com/preprocessing-with-sklearn-a-complete-and-comprehensive-guide-670cb98fcfb9?ref=hackernoon.com).

```python
# fonction pour pr√©traiter nos donn√©es 

def preprocessing_data(data):

    # Convertir les √©tiquettes num√©riques suivantes de entier en flottant
    float_array = data[["household_size", "age_of_respondent", "year"]].values.astype(float
    )
    
    # caract√©ristiques cat√©gorielles √† convertir en One Hot Encoding
    categ = [
        "relationship_with_head",
        "marital_status",
        "education_level",
        "job_type",
        "country",
    ]
    
    # Conversion One Hot Encoding
    data = pd.get_dummies(data, prefix_sep="_", columns=categ)
    
    # Conversion Label Encoder
    data["location_type"] = le.fit_transform(data["location_type"])
    data["cellphone_access"] = le.fit_transform(data["cellphone_access"])
    data["gender_of_respondent"] = le.fit_transform(data["gender_of_respondent"])
    
    # supprimer la colonne uniquid
    data = data.drop(["uniquid"], axis=1)
    
    # mettre √† l'√©chelle nos donn√©es 
    scaler = StandardScaler()
    data = scaler.fit_transform(data)
    
    return data

```

Pr√©traitons notre ensemble de donn√©es.

```python
# pr√©traiter les donn√©es d'entra√Ænement 
processed_test_data = preprocessing_data(X_train)

```

## √âtape 4 ‚Äì Construction du mod√®le et exp√©riences

Nous utiliserons une partie de l'ensemble de donn√©es pour √©valuer nos mod√®les.

```python
# Diviser les donn√©es d'entra√Ænement
from sklearn.model_selection import train_test_split
X_Train, X_val, y_Train, y_val = train_test_split(processed_train_data, y_train, stratify = y, test_size = 0.1, random_state=42)

```

Nous n'utiliserons que **10%** de l'ensemble de donn√©es pour √©valuer les mod√®les de machine learning. Le param√®tre **stratify = y** garantira un √©quilibre √©gal des valeurs des deux classes ('oui' et 'non') pour les ensembles d'entra√Ænement et de validation.

Nous utiliserons l'algorithme **R√©gression Logistique** pour ce probl√®me de classification afin d'entra√Æner et de pr√©dire qui est le plus susceptible d'avoir un compte bancaire.

```python
# importer l'algorithme de classification ici
from sklearn.linear_model import LogisticRegression

# cr√©er le classificateur
lg_model = LogisticRegression()

# Entra√Æner le classificateur
lg_model.fit(X_Train,y_Train)

```

Apr√®s avoir entra√Æn√© le classificateur, utilisons le mod√®le entra√Æn√© pour pr√©dire notre ensemble d'√©valuation et voyons comment il performe. Nous utiliserons la pr√©cision comme m√©trique d'√©valuation.

```python
# importer les m√©triques d'√©valuation
from sklearn.metrics import confusion_matrix, accuracy_score

# √©valuer le mod√®le
y_pred = lg_model.predict(X_val)

# Obtenir la pr√©cision
print("Score de pr√©cision du classificateur de r√©gression logistique : ","{:.4f}".format(accuracy_score(y_val, lg_y_pred)))

```

Nous obtenons un score de pr√©cision de **0.8874** √† partir du classificateur de r√©gression logistique.

## Comment combiner les caract√©ristiques `education_level` et `job_type` pour am√©liorer les performances

Maintenant que nous connaissons les performances du mod√®le de base, voyons si nous pouvons les am√©liorer en combinant les caract√©ristiques **`education_level`** et **`job_type`**.

Dans notre premi√®re exp√©rience, nous devons mettre √† jour la fonction de pr√©traitement que nous avons cr√©√©e, puis ex√©cuter le reste du code.

```python
# fonction pour pr√©traiter nos donn√©es 
 
def preprocessing_data(data):

    # Convertir les √©tiquettes num√©riques suivantes de entier en flottant
    float_array = data[["household_size", "age_of_respondent", "year"]].values.astype(float)

    # combiner certaines caract√©ristiques cat
    data["features_combination"] = (data.education_level.astype(str) + "_" + data.job_type.astype(str) )

    # supprimer les caract√©ristiques individuelles qui sont combin√©es ensemble
    data = data.drop(['education_level','job_type'], axis=1)

    # caract√©ristiques cat√©gorielles √† convertir par One Hot Encoding
    categ = [
      "relationship_with_head",
      "marital_status",
      "features_combination",
      "country"
      ]

    # Conversion One Hot Encoding
    data = pd.get_dummies(data, prefix_sep="_", columns=categ)

    # Conversion Label Encoder
    data["location_type"] = le.fit_transform(data["location_type"])
    data["cellphone_access"] = le.fit_transform(data["cellphone_access"])
    data["gender_of_respondent"] = le.fit_transform(data["gender_of_respondent"])

    # supprimer la colonne uniquid
    data = data.drop(["uniqueid"], axis=1)

    # mettre √† l'√©chelle nos donn√©es 
    scaler = StandardScaler()
    data = scaler.fit_transform(data)

    return data

```

Dans la fonction de pr√©traitement ci-dessus, j'ai mis √† jour le code en :

* Combinant `education_level` et `job_type` pour cr√©er une nouvelle caract√©ristique appel√©e **`features_combination`**.
* Supprimant les caract√©ristiques individuelles (`education_level` et `job_type`) de l'ensemble de donn√©es.
* Ajoutant une nouvelle caract√©ristique appel√©e **`feature_combination`** dans la liste des caract√©ristiques cat√©gorielles que **One Hot Encoding** convertira.

**Note :** J'ai s√©lectionn√© uniquement les caract√©ristiques cat√©gorielles nominales (qui ont plus de deux valeurs uniques).

Apr√®s avoir r√©entra√Æn√© le classificateur de r√©gression logistique pour cette exp√©rience, les performances du mod√®le sont pass√©es de **0.8874** √† **0.8882**. Cela montre que la combinaison de caract√©ristiques cat√©gorielles peut am√©liorer les performances de notre mod√®le.

Gardez √† l'esprit que nous n'avons rien chang√©, comme les hyper-param√®tres dans notre classificateur de machine learning.

## Comment combiner les caract√©ristiques `relation_with_head` et `marital_status` pour am√©liorer les performances

Dans notre deuxi√®me exp√©rience, nous allons combiner deux autres caract√©ristiques cat√©gorielles qui sont `relationship_with_head` et **`marital_status`**.

Nous devons simplement mettre √† jour la fonction de pr√©traitement (comme dans la premi√®re exp√©rience) puis ex√©cuter le reste du code.

```python
# fonction pour pr√©traiter nos donn√©es 

def preprocessing_data(data):

    # Convertir les √©tiquettes num√©riques suivantes de entier en flottant
    float_array = data[["household_size", "age_of_respondent", "year"]].values.astype(
        float
    )
    
    # combiner certaines caract√©ristiques cat
    data["features_combination"] = (data.relationship_with_head.astype(str) + "_"
                           + data.marital_status.astype(str) 
                      )
    # supprimer les caract√©ristiques individuelles qui sont combin√©es ensemble
    data = data.drop(['relationship_with_head','marital_status'], axis=1)


    # caract√©ristiques cat√©gorielles √† convertir par One Hot Encoding
    categ = [
        "features_combination",
        "education_level",
        "job_type",
        "country",
    ]

    # Conversion One Hot Encoding
    data = pd.get_dummies(data, prefix_sep="_", columns=categ)

    # Conversion Label Encoder
    data["location_type"] = le.fit_transform(data["location_type"])
    data["cellphone_access"] = le.fit_transform(data["cellphone_access"])
    data["gender_of_respondent"] = le.fit_transform(data["gender_of_respondent"])

    # supprimer la colonne uniquid
    data = data.drop(["uniqueid"], axis=1)

    # mettre √† l'√©chelle nos donn√©es 
    scaler = StandardScaler()
    data = scaler.fit_transform(data)

    return data
```

Dans la fonction de pr√©traitement ci-dessus, j'ai mis √† jour le code en :

* Combinant `relation_with_head` et `marital_status` pour cr√©er une nouvelle caract√©ristique appel√©e **`features_combination`**.
* Supprimant les caract√©ristiques individuelles (`relation_with_head` et `marital_status`) de l'ensemble de donn√©es.
* Ajoutant une nouvelle caract√©ristique appel√©e **`feature_combination`** dans la liste des caract√©ristiques cat√©gorielles que **One Hot Encoding** convertira.

Apr√®s avoir r√©entra√Æn√© le classificateur de r√©gression logistique pour la deuxi√®me exp√©rience, les performances du mod√®le ont diminu√© de **0.8874** √† **0.8865**.

Cela montre que parfois, lorsque vous combinez des caract√©ristiques cat√©gorielles, votre mod√®le de machine learning ne s'am√©liorera pas comme vous l'esp√©riez. Par cons√©quent, vous devrez effectuer de nombreuses exp√©riences jusqu'√† obtenir des performances satisfaisantes de votre mod√®le de machine learning.

## Conclusion

Dans cet article, vous avez appris comment combiner des caract√©ristiques cat√©gorielles dans votre ensemble de donn√©es afin d'am√©liorer les performances de votre mod√®le de machine learning.

Souvenez-vous simplement ‚Äì pour obtenir des performances satisfaisantes de votre mod√®le, vous devez avoir certaines connaissances du domaine concernant le probl√®me que vous r√©solvez. De plus, vous devez effectuer de nombreuses exp√©riences qui n√©cessitent plus de ressources computationnelles.

F√©licitations üëèüëè, vous √™tes arriv√© √† la fin de cet article ! J'esp√®re que vous avez appris quelque chose de nouveau qui vous aidera dans votre prochain projet de machine learning ou de data science.

Si vous avez appris quelque chose de nouveau ou si vous avez appr√©ci√© la lecture de cet article, veuillez le partager afin que d'autres puissent le voir. En attendant, √† la prochaine !

Vous pouvez √©galement me trouver sur Twitter [@Davis_McDavid](https://twitter.com/Davis_McDavid?ref=hackernoon.com).

Vous pouvez lire [d'autres articles](https://hackernoon.com/u/davisdavid) ici.