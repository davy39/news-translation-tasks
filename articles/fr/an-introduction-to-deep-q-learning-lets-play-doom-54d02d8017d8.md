---
title: 'Une introduction au Deep Q-Learning : jouons √† Doom'
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2018-04-11T15:23:37.000Z'
originalURL: https://freecodecamp.org/news/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8
coverImage: https://cdn-media-1.freecodecamp.org/images/1*HtKKEcDPWBouD813vU_KJg.png
tags:
- name: Artificial Intelligence
  slug: artificial-intelligence
- name: Deep Learning
  slug: deep-learning
- name: Machine Learning
  slug: machine-learning
- name: General Programming
  slug: programming
- name: 'tech '
  slug: tech
seo_title: 'Une introduction au Deep Q-Learning : jouons √† Doom'
seo_desc: 'By Thomas Simonini


  This article is part of Deep Reinforcement Learning Course with Tensorflow ?Ô∏è. Check
  the syllabus here.


  Last time, we learned about Q-Learning: an algorithm which produces a Q-table that
  an agent uses to find the best action to t...'
---

Par Thomas Simonini

> Cet article fait partie du cours Deep Reinforcement Learning avec Tensorflow üèÅ. Consultez le programme [ici](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/).

[La derni√®re fois](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe), nous avons appris le Q-Learning : un algorithme qui produit une Q-table qu'un agent utilise pour trouver la meilleure action √† entreprendre √©tant donn√© un √©tat.

Mais comme nous le verrons, produire et mettre √† jour une Q-table peut devenir inefficace dans des environnements avec un grand espace d'√©tats.

Cet article est la troisi√®me partie d'une s√©rie de publications sur le Deep Reinforcement Learning. Pour plus d'informations et de ressources, consultez le [programme du cours](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/).

Aujourd'hui, nous allons cr√©er un Deep Q Neural Network. Au lieu d'utiliser une Q-table, nous allons impl√©menter un r√©seau de neurones qui prend un √©tat et approxime les Q-valeurs pour chaque action bas√©e sur cet √©tat.

Gr√¢ce √† ce mod√®le, nous pourrons cr√©er un agent qui apprend √† jouer √† [Doom](https://en.wikipedia.org/wiki/Doom_(1993_video_game)) !

![Image](https://cdn-media-1.freecodecamp.org/images/1*Q4XjhLC0IAOznnk5613PsQ.gif)
_Notre agent DQN_

Dans cet article, vous apprendrez :

* Qu'est-ce que le Deep Q-Learning (DQL) ?
* Quelles sont les meilleures strat√©gies √† utiliser avec le DQL ?
* Comment g√©rer le probl√®me de limitation temporelle
* Pourquoi nous utilisons l'exp√©rience replay
* Quelles sont les math√©matiques derri√®re le DQL
* Comment l'impl√©menter dans Tensorflow

### **Ajouter 'Deep' au Q-Learning**

Dans le [dernier article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe), nous avons cr√©√© un agent qui joue √† Frozen Lake gr√¢ce √† l'algorithme Q-learning.

Nous avons impl√©ment√© la fonction Q-learning pour cr√©er et mettre √† jour une Q-table. Pensez √† cela comme une "feuille de triche" pour nous aider √† trouver la r√©compense future maximale attendue d'une action, √©tant donn√© un √©tat actuel. C'√©tait une bonne strat√©gie, mais ce n'est pas scalable.

Imaginez ce que nous allons faire aujourd'hui. Nous allons cr√©er un agent qui apprend √† jouer √† Doom. Doom est un grand environnement avec un espace d'√©tats gigantesque (des millions d'√©tats diff√©rents). Cr√©er et mettre √† jour une Q-table pour cet environnement ne serait pas efficace du tout.

La meilleure id√©e dans ce cas est de cr√©er un [r√©seau de neurones](http://neuralnetworksanddeeplearning.com/) qui approximera, √©tant donn√© un √©tat, les diff√©rentes Q-valeurs pour chaque action.

![Image](https://cdn-media-1.freecodecamp.org/images/1*w5GuxedZ9ivRYqM_MLUxOQ.png)

### **Comment fonctionne le Deep Q-Learning ?**

Voici l'architecture de notre Deep Q Learning :

![Image](https://cdn-media-1.freecodecamp.org/images/1*LglEewHrVsuEGpBun8_KTg.png)

Cela peut sembler complexe, mais je vais expliquer l'architecture √©tape par √©tape.

Notre Deep Q Neural Network prend une pile de quatre frames en entr√©e. Ceux-ci passent √† travers son r√©seau et produisent un vecteur de Q-valeurs pour chaque action possible dans l'√©tat donn√©. Nous devons prendre la plus grande Q-valeur de ce vecteur pour trouver notre meilleure action.

Au d√©but, l'agent se comporte tr√®s mal. Mais avec le temps, il commence √† associer les frames (√©tats) avec les meilleures actions √† effectuer.

#### **Partie de pr√©traitement**

![Image](https://cdn-media-1.freecodecamp.org/images/1*QgGnC_0BkQEtPqMUftRC6A.png)

Le pr√©traitement est une √©tape importante. Nous voulons r√©duire la complexit√© de nos √©tats pour r√©duire le temps de calcul n√©cessaire √† l'entra√Ænement.

Tout d'abord, nous pouvons convertir chaque √©tat en niveaux de gris. La couleur n'ajoute pas d'information importante (dans notre cas, nous devons simplement trouver l'ennemi et le tuer, et nous n'avons pas besoin de couleur pour le trouver). C'est une √©conomie importante, car nous r√©duisons nos trois canaux de couleur (RGB) √† 1 (niveaux de gris).

Ensuite, nous recadrons le frame. Dans notre exemple, voir le plafond n'est pas vraiment utile.

Ensuite, nous r√©duisons la taille du frame et nous empilons quatre sous-frames ensemble.

#### **Le probl√®me de la limitation temporelle**

[Arthur Juliani](https://www.freecodecamp.org/news/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8/undefined) donne une excellente explication sur ce sujet dans [son article](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2). Il a une id√©e ing√©nieuse : utiliser des [r√©seaux de neurones LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) pour g√©rer le probl√®me.

Cependant, je pense qu'il est pr√©f√©rable pour les d√©butants d'utiliser des frames empil√©s.

La premi√®re question que vous pouvez poser est : pourquoi empiler les frames ensemble ?

Nous empilons les frames ensemble car cela nous aide √† g√©rer le probl√®me de la limitation temporelle.

Prenons un exemple, dans le jeu de Pong. Lorsque vous voyez ce frame :

![Image](https://cdn-media-1.freecodecamp.org/images/1*0lwyObh4p-jQjk19Q6qyIg.png)

Pouvez-vous me dire o√π la balle va ?

Non, car un seul frame ne suffit pas pour avoir un sens du mouvement !

Mais que se passe-t-il si j'ajoute trois frames suppl√©mentaires ? Ici, vous pouvez voir que la balle va vers la droite.

![Image](https://cdn-media-1.freecodecamp.org/images/1*MooQJUIkR_FVV2weeVPr8A.png)

C'est la m√™me chose pour notre agent Doom. Si nous lui donnons un seul frame √† la fois, il n'a aucune id√©e du mouvement. Et comment peut-il prendre une d√©cision correcte, s'il ne peut pas d√©terminer o√π et √† quelle vitesse les objets se d√©placent ?

#### **Utilisation des r√©seaux de convolution**

Les frames sont trait√©s par trois couches de convolution. Ces couches vous permettent d'exploiter les relations spatiales dans les images. Mais aussi, parce que les frames sont empil√©s ensemble, vous pouvez exploiter certaines propri√©t√©s spatiales √† travers ces frames.

Si vous n'√™tes pas familier avec la convolution, veuillez lire cet [excellent article intuitif](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721) par [Adam Geitgey](https://www.freecodecamp.org/news/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8/undefined).

Chaque couche de convolution utilisera ELU comme fonction d'activation. ELU s'est av√©r√© √™tre une bonne [fonction d'activation pour les couches de convolution](https://arxiv.org/pdf/1511.07289.pdf).

Nous utilisons une couche enti√®rement connect√©e avec la fonction d'activation ELU et une couche de sortie (une couche enti√®rement connect√©e avec une fonction d'activation lin√©aire) qui produit l'estimation de la Q-valeur pour chaque action.

#### **Experience Replay : faire un usage plus efficace de l'exp√©rience observ√©e**

L'exp√©rience replay nous aidera √† g√©rer deux choses :

* √âviter d'oublier les exp√©riences pr√©c√©dentes.
* R√©duire les corr√©lations entre les exp√©riences.

Je vais expliquer ces deux concepts.

Cette partie et les illustrations ont √©t√© inspir√©es par la grande explication dans le chapitre Deep Q Learning du Nanodegree Deep Learning Foundations par [Udacity](https://eu.udacity.com/).

#### **√âviter d'oublier les exp√©riences pr√©c√©dentes**

Nous avons un gros probl√®me : la variabilit√© des poids, car il y a une forte corr√©lation entre les actions et les √©tats.

Souvenez-vous, dans le premier article ([Introduction au Reinforcement Learning](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)), nous avons parl√© du processus de Reinforcement Learning :

![Image](https://cdn-media-1.freecodecamp.org/images/1*aKYFRoEmmKkybqJOvLt2JQ.png)

√Ä chaque √©tape, nous recevons un tuple (√©tat, action, r√©compense, nouvel_√©tat). Nous apprenons de celui-ci (nous alimentons le tuple dans notre r√©seau de neurones), puis nous jetons cette exp√©rience.

Notre probl√®me est que nous donnons des √©chantillons s√©quentiels des interactions avec l'environnement √† notre r√©seau de neurones. Et il tend √† oublier les exp√©riences pr√©c√©dentes car il les √©crase avec de nouvelles exp√©riences.

Par exemple, si nous sommes au premier niveau puis au deuxi√®me (qui est totalement diff√©rent), notre agent peut oublier comment se comporter au premier niveau.

![Image](https://cdn-media-1.freecodecamp.org/images/1*p4lfgKLiollqbkWYZ_jnlg.png)
_En apprenant √† jouer au niveau de l'eau, notre agent oubliera comment se comporter au premier niveau_

Par cons√©quent, il peut √™tre plus efficace d'utiliser les exp√©riences pr√©c√©dentes en apprenant avec elles plusieurs fois.

Notre solution : cr√©er un "tampon de replay". Celui-ci stocke les tuples d'exp√©rience lors de l'interaction avec l'environnement, puis nous √©chantillonnons un petit lot de tuples pour alimenter notre r√©seau de neurones.

Pensez au tampon de replay comme √† un dossier o√π chaque feuille est un tuple d'exp√©rience. Vous l'alimentez en interagissant avec l'environnement. Ensuite, vous prenez quelques feuilles al√©atoires pour alimenter le r√©seau de neurones.

![Image](https://cdn-media-1.freecodecamp.org/images/1*RFt8MBBkUSPZdolp_WfZFA.png)

Cela emp√™che le r√©seau d'apprendre uniquement ce qu'il a fait imm√©diatement.

#### **R√©duire la corr√©lation entre les exp√©riences**

Nous avons un autre probl√®me : nous savons que chaque action affecte l'√©tat suivant. Cela produit une s√©quence de tuples d'exp√©rience qui peuvent √™tre fortement corr√©l√©s.

Si nous entra√Ænons le r√©seau dans l'ordre s√©quentiel, nous risquons que notre agent soit influenc√© par l'effet de cette corr√©lation.

En √©chantillonnant √† partir du tampon de replay de mani√®re al√©atoire, nous pouvons briser cette corr√©lation. Cela emp√™che les valeurs d'action d'osciller ou de diverger de mani√®re catastrophique.

Il sera plus facile de comprendre cela avec un exemple. Supposons que nous jouons √† un jeu de tir √† la premi√®re personne, o√π un monstre peut appara√Ætre √† gauche ou √† droite. Le but de notre agent est de tirer sur le monstre. Il a deux armes et deux actions : tirer √† gauche ou tirer √† droite.

![Image](https://cdn-media-1.freecodecamp.org/images/1*IxrNQjJCa-WiLzoe0zPojQ.png)
_Le tableau repr√©sente les approximations des Q-valeurs_

Nous apprenons avec une exp√©rience ordonn√©e. Supposons que nous savons que si nous tirons sur un monstre, la probabilit√© que le monstre suivant vienne de la m√™me direction est de 70 %. Dans notre cas, c'est la corr√©lation entre nos tuples d'exp√©rience.

Commen√ßons l'entra√Ænement. Notre agent voit le monstre √† droite et tire dessus en utilisant l'arme de droite. C'est correct !

Ensuite, le monstre suivant vient √©galement de la droite (avec une probabilit√© de 70 %), et l'agent tirera avec l'arme de droite. Encore une fois, c'est bien !

Et ainsi de suite...

![Image](https://cdn-media-1.freecodecamp.org/images/1*Eg4HoqjJstVq9fhdfPQrKQ.png)
_L'arme rouge est l'action prise_

Le probl√®me est que cette approche augmente la valeur de l'utilisation de l'arme de droite dans tout l'espace d'√©tats.

![Image](https://cdn-media-1.freecodecamp.org/images/1*hHNtNnRnFWoVegKQdTRLNA.png)
_Nous pouvons voir que la Q-valeur pour le monstre √©tant √† gauche et tirant avec l'arme de droite est positive (m√™me si ce n'est pas rationnel)_

Et si notre agent ne voit pas beaucoup d'exemples √† gauche (puisque seulement 30 % viendront probablement de la gauche), notre agent finira par choisir la droite ind√©pendamment de l'endroit d'o√π vient le monstre. Ce n'est pas rationnel du tout.

![Image](https://cdn-media-1.freecodecamp.org/images/1*pTDJxVIg6GHn5gLv_myczw.png)
_M√™me si le monstre vient √† gauche, notre agent tirera avec l'arme de droite_

Nous avons deux strat√©gies parall√®les pour g√©rer ce probl√®me.

Tout d'abord, nous devons arr√™ter d'apprendre tout en interagissant avec l'environnement. Nous devrions essayer diff√©rentes choses et jouer un peu au hasard pour explorer l'espace d'√©tats. Nous pouvons sauvegarder ces exp√©riences dans le tampon de replay.

Ensuite, nous pouvons rappeler ces exp√©riences et apprendre d'elles. Apr√®s cela, retournez jouer avec la fonction de valeur mise √† jour.

Par cons√©quent, nous aurons un meilleur ensemble d'exemples. Nous pourrons g√©n√©raliser des motifs √† partir de ces exemples, les rappelant dans n'importe quel ordre.

Cela aide √† √©viter de se fixer sur une r√©gion de l'espace d'√©tats. Cela emp√™che de renforcer la m√™me action encore et encore.

Cette approche peut √™tre vue comme une forme d'apprentissage supervis√©.

Nous verrons dans les futurs articles que nous pouvons √©galement utiliser le "prioritized experience replay". Cela nous permet de pr√©senter des tuples rares ou "importants" au r√©seau de neurones plus fr√©quemment.

### **Notre algorithme de Deep Q-Learning**

D'abord un peu de math√©matiques :

[Souvenez-vous que nous mettons √† jour notre Q-valeur](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe) pour un √©tat et une action donn√©s en utilisant l'√©quation de Bellman :

![Image](https://cdn-media-1.freecodecamp.org/images/1*js8r4Aq2ZZoiLK0mMp_ocg.png)

Dans notre cas, nous voulons mettre √† jour les poids de notre r√©seau de neurones pour r√©duire l'erreur.

L'erreur (ou erreur TD) est calcul√©e en prenant la diff√©rence entre notre Q_target (valeur maximale possible de l'√©tat suivant) et Q_value (notre pr√©diction actuelle de la Q-valeur)

![Image](https://cdn-media-1.freecodecamp.org/images/1*Zplt-1wTWu_7BGmZCBFjbQ.png)

```
Initialiser l'environnement Doom E
Initialiser la m√©moire de replay M avec une capacit√© N (= capacit√© finie)
Initialiser les poids du DQN w
pour √©pisode dans max_√©pisode :
    s = √âtat de l'environnement
    pour √©tapes dans max_√©tapes :
        Choisir l'action a √† partir de l'√©tat s en utilisant epsilon greedy.
        Prendre l'action a, obtenir r (r√©compense) et s' (√©tat suivant)
        Stocker le tuple d'exp√©rience <s, a, r, s'> dans M
        s = s' (√©tat = nouvel_√©tat)
        Obtenir un minibatch al√©atoire de tuples d'exp√©rience de M
        D√©finir Q_target = r√©compense(s,a) + Œ≥maxQ(s')
        Mettre √† jour w = Œ±(Q_target - Q_value) * ‚àáw Q_value
```

Il y a deux processus qui se d√©roulent dans cet algorithme :

* Nous √©chantillonnons l'environnement o√π nous effectuons des actions et stockons les tuples d'exp√©rience observ√©s dans une m√©moire de replay.
* S√©lectionnez le petit lot de tuples al√©atoires et apprenez-en en utilisant une √©tape de mise √† jour de descente de gradient.

### **Impl√©mentons notre Deep Q Neural Network**

> Nous avons fait une vid√©o o√π nous impl√©mentons un agent Deep Q-learning avec Tensorflow qui apprend √† jouer √† Atari Space Invaders üéÆüöÄ.

Maintenant que nous savons comment cela fonctionne, nous allons impl√©menter notre Deep Q Neural Network √©tape par √©tape. Chaque √©tape et chaque partie du code est expliqu√©e directement dans le notebook Jupyter li√© ci-dessous.

Vous pouvez y acc√©der dans le [d√©p√¥t du cours Deep Reinforcement Learning](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/DQN/doom).

C'est tout ! Vous venez de cr√©er un agent qui apprend √† jouer √† Doom. G√©nial !

N'oubliez pas d'impl√©menter chaque partie du code par vous-m√™me. Il est vraiment important d'essayer de modifier le code que je vous ai donn√©. Essayez d'ajouter des √©poques, changez l'architecture, ajoutez des Q-valeurs fixes, changez le taux d'apprentissage, utilisez un environnement plus difficile (comme Health Gathering)... et ainsi de suite. Amusez-vous !

Dans le prochain article, je discuterai des derni√®res am√©liorations du Deep Q-learning :

* Q-valeurs fixes
* Prioritized Experience Replay
* Double DQN
* Dueling Networks

Mais la prochaine fois, nous travaillerons sur les Policy Gradients en entra√Ænant un agent qui joue √† Doom, et nous essaierons de survivre dans un environnement hostile en collectant de la sant√©.

![Image](https://cdn-media-1.freecodecamp.org/images/1*dNEZ6GX3Fp4DCLj59XrnFQ.gif)

Si vous avez aim√© mon article, **veuillez cliquer sur le üëè ci-dessous autant de fois que vous avez aim√© l'article** afin que d'autres personnes puissent le voir ici sur Medium. Et n'oubliez pas de me suivre !

Si vous avez des pens√©es, des commentaires, des questions, n'h√©sitez pas √† commenter ci-dessous ou √† m'envoyer un email : hello@simoninithomas.com, ou me tweeter [@ThomasSimonini](https://twitter.com/ThomasSimonini).

![Image](https://cdn-media-1.freecodecamp.org/images/1*_yN1FzvEFDmlObiYsstIzg.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*mD-f5VN1SWYvhrZAbvSu_w.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*PqiptT-Cdi8uwosxuFn2DQ.png)

Continuez √† apprendre, restez g√©nial !

#### Cours de Deep Reinforcement Learning avec Tensorflow üèÅ

üìú [Programme](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)

üé• [Version vid√©o](https://www.youtube.com/channel/UC8XuSf1eD9AF8x8J19ha5og?view_as=subscriber)

Partie 1 : [Une introduction au Reinforcement Learning](https://medium.com/p/4339519de419/edit)

Partie 2 : [Plonger plus profond√©ment dans le Reinforcement Learning avec le Q-Learning](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)

Partie 3 : [Une introduction au Deep Q-Learning : jouons √† Doom](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)

Partie 3+ : [Am√©liorations dans le Deep Q Learning : Dueling Double DQN, Prioritized Experience Replay, et Q-targets fixes](https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682)

Partie 4 : [Une introduction aux Policy Gradients avec Doom et Cartpole](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)

Partie 5 : [Une introduction aux m√©thodes Advantage Actor Critic : jouons √† Sonic the Hedgehog !](https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d)

Partie 6 : [Proximal Policy Optimization (PPO) avec Sonic the Hedgehog 2 et 3](https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e)

Partie 7 : [L'apprentissage par curiosit√© rendu facile Partie I](https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359)