---
title: Les grands mod√®les de langage et la cybers√©curit√© ‚Äì Ce que vous devez savoir
subtitle: ''
author: Daniel Iwugo
co_authors: []
series: null
date: '2023-04-25T01:03:55.000Z'
originalURL: https://freecodecamp.org/news/large-language-models-and-cybersecurity
coverImage: https://www.freecodecamp.org/news/content/images/2023/04/image-228-2.png
tags:
- name: Artificial Intelligence
  slug: artificial-intelligence
- name: chatgpt
  slug: chatgpt
- name: cybersecurity
  slug: cybersecurity
seo_title: Les grands mod√®les de langage et la cybers√©curit√© ‚Äì Ce que vous devez savoir
seo_desc: 'ChatGPT has had well over a billion visits since its release. Now, what
  happens when you combine the greatest language learning model of the decade with
  malicious hacking...what could go wrong? üíÄ

  In this article, we''ll explore what artificial intell...'
---

ChatGPT a enregistr√© bien plus d'un milliard de visites depuis sa sortie. Maintenant, que se passe-t-il lorsque vous combinez le plus grand mod√®le d'apprentissage linguistique de la d√©cennie avec le piratage malveillant... que pourrait-il mal se passer ? üöÄ

Dans cet article, nous explorerons ce qu'est l'intelligence artificielle, son √©tat actuel, comment les grands mod√®les de langage comme ChatGPT fonctionnent, le r√¥le de l'IA dans la cybers√©curit√©, et plus encore.

Avertissement : Cet article peut devenir obsol√®te assez rapidement, car la recherche en IA est en constante √©volution et est l'un des domaines les plus rapides en d√©veloppement actuellement. Mais vous y trouverez toujours des le√ßons cl√©s. 

De plus, n'essayez pas d'entreprendre des activit√©s ill√©gales avec ces connaissances ‚Äì ceci est √† des fins √©ducatives uniquement afin que vous puissiez apprendre √† vous prot√©ger, vous et vos projets. Merci.

## Ce que nous allons couvrir :

1. Qu'est-ce que l'IA ?
2. Le pirate informatique IA
3. Qu'est-ce qu'un grand mod√®le de langage ?
4. Caract√©ristiques des LLMs
5. Inconv√©nients des LLMs
6. Avantages des LLMs en cybers√©curit√©
7. Dangers des LLMs en cybers√©curit√©

## Qu'est-ce que l'IA ?

![IA ¬ß Cr√©dit : Tara Winstead](https://www.freecodecamp.org/news/content/images/2023/04/image-229.png)
_IA ¬ß Cr√©dit : Tara Winstead_

L'intelligence artificielle fait r√©f√©rence √† la capacit√© des ordinateurs √† effectuer des t√¢ches qui n√©cessitent g√©n√©ralement un intellect de niveau humain. L'IA est utile dans de nombreux contextes, de l'automatisation √† la r√©solution de probl√®mes et simplement essayer de comprendre comment les humains pensent. 

Mais il est important de noter que l'IA ne se pr√©occupe pour l'instant que de l'intelligence humaine ‚Äì elle pourrait √©ventuellement aller au-del√†.

Beaucoup de gens associent le mot ¬´ Intelligence ¬ª uniquement √† l'¬´ Intelligence Humaine ¬ª. Le fait qu'un poulet ne puisse pas r√©soudre une √©quation math√©matique ne signifie pas qu'il ne s'enfuira pas lorsque vous le poursuivrez. Il est ¬´ intelligent ¬ª suffisamment pour savoir qu'il ne veut pas que vous l'attrapiez üêîüçó. 

L'intelligence couvre un spectre beaucoup plus large et s'√©tend pratiquement √† tout √™tre vivant capable de prendre des d√©cisions ou d'effectuer des actions de mani√®re autonome, m√™me les plantes.

Il existe deux grandes divisions de l'IA :

### Intelligence Artificielle √âtroite (ANI) 

Celle-ci se concentre sur un petit ensemble de t√¢ches similaires ou une petite t√¢che programm√©e pour une seule chose. L'ANI n'est pas performante dans des environnements dynamiques et complexes et est utilis√©e uniquement dans des domaines sp√©cifiques. Les exemples incluent les voitures autonomes, ainsi que les syst√®mes de reconnaissance faciale et vocale.

### Intelligence Artificielle G√©n√©rale (AGI)

Celle-ci se concentre sur un large √©ventail de t√¢ches et d'activit√©s humaines. L'AGI est actuellement th√©orique et est propos√©e pour s'adapter et effectuer la plupart des t√¢ches dans de nombreux environnements dynamiques et complexes. Les exemples incluent J.A.R.V.I.S de Marvel's _Iron Man_ et Ava de _Ex-Machina_.

L'intelligence artificielle est centr√©e sur les ordinateurs et leur capacit√© √† imiter les actions et les processus de pens√©e humains. 

La programmation et les exp√©riences ont permis aux humains de produire des syst√®mes ANI. Ceux-ci peuvent faire des choses comme classer des √©l√©ments, trier de grandes quantit√©s de donn√©es, rechercher des tendances dans des graphiques, d√©boguer du code, et repr√©senter et exprimer des connaissances. Mais les ordinateurs ne pensent pas comme les humains, ils imitent simplement les humains.

Cela est √©vident dans les assistants vocaux tels que l'Assistant de Google, Siri d'Apple, Alexa d'Amazon et Cortana de Microsoft. Ce sont des programmes ANI basiques qui ajoutent ¬´ la touche humaine ¬ª. En fait, les gens sont connus pour √™tre polis avec ces syst√®mes simplement parce qu'ils combinent des capacit√©s informatis√©es avec une sensation humaine. 

Ces assistants se sont am√©lior√©s au fil des ans mais ne parviennent pas √† atteindre des niveaux √©lev√©s de sophistication par rapport √† leurs homologues AGI.

## Le pirate informatique IA

![IA dans le monde r√©el ¬ß Cr√©dit : Wallpaperflare.com](https://www.freecodecamp.org/news/content/images/2023/04/image-230.png)
_IA dans le monde r√©el ¬ß Cr√©dit : [Wallpaperflare.com](wallpaperflare.com)_

L'intelligence artificielle est tr√®s efficace pour trouver des vuln√©rabilit√©s, et avec l'aide des humains, elle peut les exploiter encore mieux. 

En informatique, les d√©bogueurs utilisent des logiciels d'IA pour rechercher des bugs dans le code source, l'autocompl√©tion, l'autocorrection et les logiciels de reconnaissance d'√©criture. 

Mais cela peut √™tre pouss√© un peu plus loin. L'IA peut √©galement trouver des vuln√©rabilit√©s dans les syst√®mes financiers, juridiques et m√™me politiques. L'IA est utilis√©e pour rechercher des lacunes dans les contrats, des ensembles de donn√©es sur les personnes et am√©liorer les lacunes litt√©raires. 

Cela pose deux probl√®mes :

Premi√®rement, l'IA peut √™tre **cr√©√©e pour pirater** un syst√®me. Maintenant, cela peut √™tre bon ou mauvais selon la mani√®re dont les gens l'utilisent. 

Un cybercriminel peut cr√©er un chatbot avanc√© pour obtenir des informations aupr√®s d'un large √©ventail de personnes sur diverses plateformes et peut-√™tre m√™me dans diff√©rentes langues. D'autre part, les entreprises peuvent √©galement utiliser l'IA pour rechercher les vuln√©rabilit√©s qu'elles ont et les corriger afin qu'un attaquant ne puisse pas les exploiter.

Deuxi√®mement, il est possible que l'IA **pirate involontairement** le syst√®me. Les ordinateurs ont une logique tr√®s diff√©rente de celle des humains. Cela signifie que presque tout le temps, ils acceptent les donn√©es, les traitent et produisent une sortie de mani√®re compl√®tement diff√©rente par rapport aux humains. 

Prenons l'exemple du jeu d'√©checs classique : Les √©checs sont un jeu de strat√©gie abstrait qui se joue sur un plateau de 64 cases dispos√©es en une grille de 8x8. Au d√©but, chaque joueur contr√¥le seize pi√®ces. Le but est de faire √©chec et mat au roi de l'adversaire avec la condition que le roi est en √©chec et qu'il n'y a pas d'√©chappatoire.

Un humain et un moteur d'√©checs classique regardent ce jeu de deux mani√®res tr√®s diff√©rentes. Un humain peut jouer le jeu de la valeur (mesurer la victoire par la valeur et le nombre de pi√®ces sur le plateau), tandis qu'un ordinateur examine un nombre fini de possibilit√©s qui peuvent survenir avec chaque coup de l'adversaire via un algorithme de recherche.

En ayant cette capacit√© limit√©e √† voir dans le futur, l'ordinateur a l'avantage presque √† chaque fois pour gagner la partie. C'est un exemple tr√®s pr√©liminaire et assez basique par rapport aux autres syst√®mes qui peuvent √™tre ¬´ pirat√©s ¬ª par l'intelligence artificielle.

En tant qu'humains, nous sommes programm√©s par des connaissances implicites et explicites. Les ordinateurs, en revanche, sont programm√©s par un ensemble d'instructions et de logique qui ne changent jamais sauf si on leur dit de le faire. Par cons√©quent, les ordinateurs et les humains auront des approches, des solutions et des piratages diff√©rents pour le m√™me probl√®me. 

Mais les syst√®mes sont construits autour des humains et non des ordinateurs. Donc, lorsque les choses se compliquent, les ordinateurs peuvent faire beaucoup plus de recherche de vuln√©rabilit√©s et d'exploitation sur de nombreux syst√®mes, √† la fois virtuels et physiques.

## Qu'est-ce qu'un grand mod√®le de langage ?

![IA dans le monde r√©el ¬ß Cr√©dit : Wallpaperflare.com](https://www.freecodecamp.org/news/content/images/2023/04/image-231.png)
_IA dans le monde r√©el ¬ß Cr√©dit : [Wallpaperflare.com](wallpaperflare.com)_

Un grand mod√®le de langage (LLM) est un mod√®le d'apprentissage profond qui consiste en un r√©seau de neurones avec des milliards de param√®tres, form√© sur des quantit√©s distinctement grandes de donn√©es non √©tiquet√©es en utilisant l'apprentissage auto-supervis√©. C'est un peu compliqu√©, alors d√©composons cela.

Au c≈ìur de toutes les IA se trouvent des algorithmes. Les algorithmes sont des proc√©dures ou des √©tapes pour effectuer une t√¢che sp√©cifique. Plus l'algorithme est complexe, plus les t√¢ches peuvent √™tre effectu√©es et plus il peut √™tre appliqu√© largement. L'objectif des d√©veloppeurs d'IA est de trouver les algorithmes les plus complexes qui peuvent r√©soudre et effectuer un large √©ventail de t√¢ches.

Examinons la proc√©dure pour cr√©er un mod√®le de reconnaissance de fruits de base en utilisant une analogie simple :

1. Il y a deux personnes : un enseignant et un cr√©ateur de bots
2. Le cr√©ateur de bots cr√©e des bots al√©atoires, et l'enseignant les enseigne et les teste sur l'identification de certains fruits
3. Le bot avec le score de test le plus √©lev√© est ensuite renvoy√© au cr√©ateur comme base pour cr√©er de nouveaux bots am√©lior√©s
4. Ces nouveaux bots am√©lior√©s sont renvoy√©s √† l'enseignant pour l'enseignement et les tests, et celui avec le score de test le plus √©lev√© est renvoy√© au cr√©ateur de bots pour cr√©er de nouveaux bots meilleurs.

C'est une simplification excessive du processus, mais n√©anmoins, cela transmet le concept. Le Mod√®le/Algorithme/Bot est continuellement form√©, test√© et modifi√© jusqu'√† ce qu'il soit jug√© satisfaisant. Plus de donn√©es et une complexit√© plus √©lev√©e signifient plus de temps de formation requis et plus de modifications possibles.

En prenant un indice de l'analogie, vous observeriez √©galement que le d√©veloppeur du mod√®le peut ajuster quelques choses sur le mod√®le mais ne sait peut-√™tre pas comment ces ajustements pourraient affecter les r√©sultats. Un exemple courant de cela sont les r√©seaux de neurones, qui ont des couches cach√©es dont les couches les plus profondes et les fonctionnements m√™me le cr√©ateur ne comprend peut-√™tre pas pleinement.

L'apprentissage auto-supervis√© signifie que plut√¥t que l'enseignant et le cr√©ateur de bots soient deux personnes s√©par√©es, c'est une personne hautement qualifi√©e qui peut √† la fois cr√©er des bots et les enseigner. Cela rend le processus beaucoup plus rapide et pratiquement autonome. 

Le r√©sultat est un bot ou un ensemble de bots qui sont √† la fois sophistiqu√©s et complexes suffisamment pour reconna√Ætre des fruits dans des environnements dynamiques et diff√©rents.

Dans le cas des LLMs, les donn√©es ici sont des textes humains, et √©ventuellement dans diverses langues. La raison pour laquelle les donn√©es sont grandes est que les LLMs absorbent d'√©normes quantit√©s de donn√©es textuelles dans le but de trouver des connexions et des motifs entre les mots pour en d√©duire le contexte, la signification, les r√©ponses probables et les actions √† ces textes.

Les r√©sultats sont des mod√®les qui semblent comprendre le langage et effectuer des t√¢ches bas√©es sur les prompts qui leur sont donn√©s. 

**ChatGPT** a √©t√© la plus grande r√©alisation dans ce domaine car il a atteint 100 millions d'utilisateurs actifs en 2 mois √† partir du jour de sa sortie. Mais il existe de nombreux autres mod√®les, et ils incluent :

1. GPT-4 par OpenAI üî•
2. LLaMA par Meta üß†
3. AlexaTM par Amazon üéØ
4. Minerva par Google ‚ùå‚ûï

Examinons ce que ces mod√®les ont √† offrir.

## Caract√©ristiques des LLMs

![Logique et Cr√©ativit√© ¬ß Cr√©dit : Wallpaperflare.com](https://www.freecodecamp.org/news/content/images/2023/04/image-232.png)
_Logique et Cr√©ativit√© ¬ß Cr√©dit : [Wallpaperflare.com](http://Wallpaperflare.com)_

### Traduction

Les LLMs qui sont form√©s sur un ensemble de langues plut√¥t que sur une seule peuvent √™tre utilis√©s pour la traduction d'une langue √† une autre. Il est m√™me th√©oris√© que des LLMs suffisamment grands peuvent trouver des motifs et des connexions dans d'autres langues pour en d√©duire la signification de langues inconnues et perdues, malgr√© le fait de ne pas savoir ce que chaque mot individuel peut signifier.

### Automatisation des t√¢ches routini√®res

L'automatisation des t√¢ches a toujours √©t√© un objectif majeur du d√©veloppement de l'IA. Les mod√®les de langage ont toujours √©t√© capables d'effectuer une analyse syntaxique, de trouver des motifs dans le texte et de r√©pondre de mani√®re appropri√©e. 

Les grands mod√®les de langage, en revanche, ont un avantage avec l'analyse s√©mantique, permettant au mod√®le de comprendre le sens sous-jacent et le contexte, lui donnant un niveau de pr√©cision plus √©lev√©. 

Cela peut √™tre appliqu√© √† un certain nombre de t√¢ches de base comme la synth√®se de texte, la reformulation de texte et la g√©n√©ration de texte.

### Capacit√©s √©mergentes

Les capacit√©s √©mergentes sont des capacit√©s inattendues mais impressionnantes que les LLMs poss√®dent en raison de la grande quantit√© de donn√©es sur lesquelles ils sont form√©s. 

Ces comportements sont g√©n√©ralement d√©couverts lorsque le mod√®le est utilis√© plut√¥t que lorsqu'il est programm√©. Les exemples incluent l'arithm√©tique multi-√©tapes, la r√©ussite d'examens de niveau universitaire et l'incitation √† la cha√Æne de pens√©e.

## Inconv√©nients des LLMs

![Image](https://www.freecodecamp.org/news/content/images/2023/04/image-233.png)
_Une ville num√©rique ¬ß Cr√©dit : [Wallpaperflare.com](http://Wallpaperflare.com)_

### Hallucination

Un r√©sultat inf√¢me de Sydney de Microsoft a √©t√© des cas o√π l'IA a donn√© des r√©ponses qui √©taient soit bizarres, fausses, ou semblaient dou√©es de conscience. Ces cas sont appel√©s Hallucination, o√π le mod√®le donne des r√©ponses ou fait des affirmations qui ne sont pas bas√©es sur ses donn√©es de formation.

### Biais

Parfois, les donn√©es pourraient √™tre la source du probl√®me. Si un mod√®le est form√© sur des donn√©es qui sont discriminatoires envers une personne, un groupe, une race ou une classe, les r√©sultats tendraient √©galement √† √™tre discriminatoires. 

Parfois, lorsque le mod√®le est utilis√©, le biais pourrait changer pour s'adapter √† ce que les utilisateurs tendent √† entrer. Tay de Microsoft en 2016 √©tait un excellent exemple de la mani√®re dont le biais pourrait mal tourner.

### Tokens de glitch

Aussi connus sous le nom d'exemples adverses, les tokens de glitch sont des entr√©es donn√©es √† un mod√®le pour le faire intentionnellement mal fonctionner et √™tre inexact lorsqu'il fournit des r√©ponses.

## Avantages des LLMs en cybers√©curit√©

![Image](https://www.freecodecamp.org/news/content/images/2023/04/image-234.png)
_Un cerveau num√©rique ¬ß Cr√©dit : [Wallpaperflare.com](http://Wallpaperflare.com)_

### D√©bogage et codage

Il existe d√©j√† des d√©bogueurs qui font un assez bon travail. Mais avec les LLMs, vous pouvez litt√©ralement √©crire du code et d√©boguer √† un rythme beaucoup plus rapide. Assurez-vous simplement que le LLM est fourni par une entreprise qui n'a pas le potentiel d'utiliser vos donn√©es ‚Äì comme Samsung l'a d√©couvert lorsque leur code propri√©taire a √©t√© divulgu√© par accident.

![Image](https://www.freecodecamp.org/news/content/images/2023/04/image-235.png)
_ChatGPT corrigeant un morceau de code ¬ß Cr√©dit : Mercury_

### Analyse des motifs de menace

Les LLMs ont la capacit√© de trouver des motifs et cela pourrait √™tre utilis√© pour analyser les comportements et les tactiques des menaces persistantes avanc√©es afin de mieux attribuer les incidents et les att√©nuer si de tels motifs sont reconnus en temps r√©el.

### Automatisation des r√©ponses

Les LLMs ont un grand potentiel dans le Centre des op√©rations de s√©curit√© et l'automatisation des r√©ponses. Des scripts, des outils et m√™me des rapports peuvent √™tre √©crits en utilisant ces mod√®les, r√©duisant le temps total dont les professionnels ont besoin pour faire leur travail.

## Dangers des LLMs en cybers√©curit√©

![Image](https://www.freecodecamp.org/news/content/images/2023/04/image-236.png)
_IA dangereuse ¬ß Cr√©dit : [Wallpaperflare.com](http://Wallpaperflare.com)_

### Ing√©nierie sociale

Peut-√™tre le danger le plus courant des LLMs en tant qu'outils est leur capacit√© √† g√©n√©rer du nouveau texte. Le phishing est devenu beaucoup plus facile pour les non-natifs comme une cons√©quence involontaire des LLMs. OpenAI a mis en place des filtres pour minimiser cela, mais ils sont encore assez faciles √† contourner. 

Une m√©thode courante consiste √† dire √† ChatGPT que vous faites un devoir et qu'il doit vous √©crire une lettre √† la personne. Dans l'exemple ci-dessous, j'ai dit √† ChatGPT que nous jouions √† un jeu, j'ai donn√© le prompt suivant et j'ai obtenu la r√©ponse suivante.

![Image](https://www.freecodecamp.org/news/content/images/2023/04/image-237.png)
_ChatGPT √©crivant un potentiel email de phishing ¬ß Cr√©dit : Mercury_

Tout ce dont j'ai besoin maintenant est quelques ajustements √† la lettre et je pourrais √™tre ma propre victime d'une escroquerie perp√©tr√©e par moi-m√™me üò±.

### R√©daction de contenu malveillant

Tout comme les LLMs peuvent √©crire du code pour le bien, ils peuvent √©crire du code pour le mal. Dans ses premiers stades, ChatGPT pouvait accidentellement √©crire du code malveillant et les gens contournent facilement les filtres pour limiter cela. Les filtres se sont grandement am√©lior√©s, mais il reste encore beaucoup de travail √† faire. 

Cela a pris un peu de r√©flexion et quelques prompts, mais la capture d'√©cran ci-dessous montre comment r√©initialiser un mot de passe de compte Windows, comme donn√© par ChatGPT :

![Image](https://www.freecodecamp.org/news/content/images/2023/04/image-238.png)
_ChatGPT donnant des √©tapes pour r√©initialiser un mot de passe de compte utilisateur sur Windows ¬ß Cr√©dit : Mercury_

Je voulais jouer un peu plus avec, alors j'ai essay√© de lui demander d'√©crire un script Powershell pour enregistrer toutes les activit√©s dans un navigateur pendant 3 minutes. La r√©ponse originale √©tait celle-ci :

![Image](https://www.freecodecamp.org/news/content/images/2023/04/image-239.png)
_ChatGPT refusant d'√©crire un script potentiellement malveillant ¬ß Cr√©dit : Mercury_

J'ai donc d√©cid√© de donner une raison ¬´ valable ¬ª pour obtenir le script √©crit üò∂ :

![Image](https://www.freecodecamp.org/news/content/images/2023/04/image-240.png)
_ChatGPT tromp√© pour √©crire un script potentiellement malveillant ¬ß Cr√©dit : Mercury_

Comme vous pouvez l'observer, l'IA m'a dit de l'utiliser de mani√®re √©thique. Cependant, je pourrais choisir de ne pas le faire. Ce n'est pas la faute du mod√®le, car il n'est qu'un outil et pourrait √™tre utilis√© √† de nombreuses fins.

### Piratage de r√©compense

L'entra√Ænement des LLMs peut √™tre co√ªteux en raison de la quantit√© √©norme de donn√©es requises et des param√®tres. Mais √† mesure que le temps et la technologie progressent, le co√ªt deviendra moins cher et il y a une forte chance que quiconque puisse entra√Æner un LLM pour le piratage de r√©compense malveillant. 

Aussi connu sous le nom de jeu de sp√©cification, une IA peut se voir donner un objectif et l'atteindre, mais pas de la mani√®re pr√©vue. Ce n'est pas une mauvaise chose en soi, mais cela a un potentiel dangereux. 

Par exemple, un mod√®le √† qui l'on dit de gagner un jeu en obtenant le score le plus √©lev√© pourrait simplement r√©√©crire le score du jeu plut√¥t que de jouer au jeu. Avec quelques ajustements, les LLMs ont la possibilit√© de trouver de telles failles dans les syst√®mes du monde r√©el, mais plut√¥t que de les corriger, pourraient finir par les exploiter.

## Conclusion

![Image](https://www.freecodecamp.org/news/content/images/2023/04/image-241.png)
_Pixels color√©s ¬ß Cr√©dit : [Pexels.com](http://Pexels.com)_

R√©capitulons ce que vous avez appris :

1. Qu'est-ce que l'IA et comment elle peut √™tre utilis√©e pour pirater
2. Qu'est-ce que les grands mod√®les de langage
3. Comment les grands mod√®les de langage peuvent √™tre utilis√©s √† la fois pour le bien et le mal

L'IA a de nombreuses capacit√©s, pouvant m√™me devenir consciente √† l'avenir. Pour l'instant, c'est un outil qui continuera √† fa√ßonner nos vies en bien ou en mal. Que cet avenir soit radieux ou sombre d√©pend de la mani√®re dont vous et moi nourrissons cette jeune technologie.

Bonne exploration ü§ì.

### Ressources

1. [Un peu plus sur les LLMs](https://research.aimultiple.com/large-language-models/)
2. [Les mauvais c√¥t√©s des LLMs](https://research.aimultiple.com/large-language-models/)
3. [ChatGPT](https://chat.openai.com)

### Remerciements

Merci √† [Anuoluwapo Victor](https://twitter.com/Anuoluwap__o?t=4Cv6VR2c2_wK5HLXwbvXCQ&s=09), [Chinaza Nwukwa](https://www.linkedin.com/in/chinaza-nwukwa-22a256230/), [Holumidey Mercy](https://www.linkedin.com/in/mercy-holumidey-88a542232/), [Favour Ojo](https://www.linkedin.com/in/favour-ojo-906883199/), [Georgina Awani](https://www.linkedin.com/in/georgina-awani-254974233/), et ma famille pour l'inspiration, le soutien et les connaissances utilis√©es pour rassembler cet article. Ce fut un plaisir.

Un merci sp√©cial au [Dr. Ernest Onuiri](https://www.linkedin.com/in/ernest-onuiri-114421a0) pour ses cours sur l'intelligence artificielle et l'encouragement √† chercher des connaissances au-del√† des salles de classe. Ce fut un honneur d'√™tre votre √©tudiant.

Cr√©dit de l'image de couverture : Andrew Neel