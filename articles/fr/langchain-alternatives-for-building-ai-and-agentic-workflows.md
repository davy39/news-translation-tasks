---
title: Alternatives Ã  LangChain pour construire des workflows IA et agentiques
subtitle: ''
author: Maham Codes
co_authors: []
series: null
date: '2025-01-30T18:59:27.703Z'
originalURL: https://freecodecamp.org/news/langchain-alternatives-for-building-ai-and-agentic-workflows
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1738696551914/6800aa71-d6f2-472a-8982-35af81509813.png
tags:
- name: AI
  slug: ai
- name: llm
  slug: llm
seo_title: Alternatives Ã  LangChain pour construire des workflows IA et agentiques
seo_desc: Building AI and agentic workflows is at the core of modern AI development
  in 2025. And LangChain has been the go-to framework for creating AI applications
  for a while now. But some developers are seeking alternatives that offer more flexibility,
  simp...
---

La construction de workflows IA et agentiques est au cÅ“ur du dÃ©veloppement moderne de l'IA en 2025. Et LangChain a Ã©tÃ© le framework de rÃ©fÃ©rence pour crÃ©er des applications IA depuis un certain temps. Mais certains dÃ©veloppeurs recherchent des alternatives offrant plus de flexibilitÃ©, de simplicitÃ© et de rentabilitÃ©.

Bien que LangChain ait permis un dÃ©veloppement rapide d'applications alimentÃ©es par des LLM avec des outils pour le chaÃ®nage, les agents et la mÃ©moire, son abstraction lourde, son dÃ©bogage complexe et sa difficultÃ© avec les cas d'utilisation rÃ©els en font souvent un outil plus adaptÃ© au prototypage qu'aux applications de niveau production.

Dans cet article, nous explorerons quelques alternatives puissantes Ã  LangChain que vous pouvez essayer et qui vous aideront Ã  construire des workflows IA et agentiques efficaces. Je passerai en revue les principales caractÃ©ristiques et les meilleurs cas d'utilisation de chacune afin que vous puissiez vous faire une bonne idÃ©e de la maniÃ¨re dont elles pourraient vous aider.

## Table des matiÃ¨res

1. [Qu'est-ce qu'un workflow IA et agentique ?](#heading-quest-ce-quun-workflow-ia-et-agentique)
    
2. [Langbase](#heading-langbase)
    
3. [LlamaIndex](#heading-llamaindex)
    
4. [AG2](#heading-ag2)
    
5. [Braintrust](#heading-braintrust)
    
6. [FlowiseAI](#heading-flowiseai)
    
7. [Conclusion ðŸ™Œ](#heading-conclusion)
    

## Qu'est-ce qu'un workflow IA et agentique ?

Un **workflow IA** fait rÃ©fÃ©rence Ã  une sÃ©rie de tÃ¢ches exÃ©cutÃ©es par des systÃ¨mes IA, suivant gÃ©nÃ©ralement une sÃ©quence prÃ©dÃ©finie. Il gÃ¨re des tÃ¢ches comme l'extraction de donnÃ©es, le traitement et la gÃ©nÃ©ration de sorties basÃ©es sur des instructions claires.

Un **workflow agentique** va plus loin. Il implique que l'IA prenne l'initiative, prenne des dÃ©cisions et gÃ¨re des tÃ¢ches de maniÃ¨re autonome. Dans les workflows agentiques, l'IA adapte ses actions en fonction de son environnement ou d'objectifs prÃ©dÃ©finis, souvent sans intervention humaine.

En bref, un workflow IA devient plus "agentique" lorsqu'il commence Ã  penser, dÃ©cider et agir de maniÃ¨re indÃ©pendante, agissant comme un agent intelligent. Plus l'IA peut prendre de dÃ©cisions par elle-mÃªme, moins elle a besoin d'Ãªtre sollicitÃ©e par un humain.

Maintenant que nous avons clarifiÃ© ce qu'est un workflow IA et agentique, examinons quelques autres outils et frameworks qui pourraient servir d'alternatives Ã  LangChain, chacun offrant des capacitÃ©s et des approches uniques que vous pouvez utiliser pour construire vos workflows IA et agentiques.

## Langbase

[Langbase](http://langbase.com) est une plateforme de dÃ©veloppement IA composable et serverless avec orchestration multi-agents et mÃ©moire Ã  long terme avancÃ©e. Elle est conÃ§ue pour un dÃ©veloppement et un dÃ©ploiement IA fluides. Langbase prend en charge plus de 100 LLM via une seule API, garantissant une expÃ©rience dÃ©veloppeur unifiÃ©e, avec un changement et une optimisation faciles des modÃ¨les.

ðŸ’¡ L'orchestration multi-agents fait rÃ©fÃ©rence Ã  la coordination de plusieurs agents IA pour travailler ensemble sur des tÃ¢ches. Elle implique de contrÃ´ler le flux des tÃ¢ches, de s'assurer que les agents travaillent dans la bonne sÃ©quence et de coordonner leurs actions pour maximiser l'efficacitÃ©.

### Produits Langbase

La plateforme offre les produits suivants :

1. **Pipe Agents** : Les agents Pipe sur Langbase sont diffÃ©rents des autres agents. Ce sont des agents IA serverless avec des outils agentiques qui peuvent fonctionner avec n'importe quel langage ou framework. Les agents Pipe sont facilement dÃ©ployables, et avec une seule API, ils vous permettent de connecter 100+ LLM Ã  n'importe quelle donnÃ©e pour construire n'importe quel workflow API dÃ©veloppeur.
    
2. **Memory Agents** : Les agents mÃ©moire de Langbase (solution de mÃ©moire Ã  long terme) sont conÃ§us pour acquÃ©rir, traiter, conserver et rÃ©cupÃ©rer des informations de maniÃ¨re fluide. Ils attachent dynamiquement des donnÃ©es privÃ©es Ã  n'importe quel LLM, permettant des rÃ©ponses contextuelles en temps rÃ©el et rÃ©duisant les hallucinations. La mÃ©moire, lorsqu'elle est connectÃ©e Ã  un agent Pipe, devient un agent mÃ©moire.
    
3. **BaseAI.dev** : BaseAI est le premier framework web IA open-source. Avec lui, vous pouvez commencer Ã  construire des pipes, outils et mÃ©moire agentiques locaux, et dÃ©ployer en serverless avec une seule commande.
    
4. **AI Studio** : Langbase AI Studio fournit un terrain de jeu pour collaborer sur des agents IA, la mÃ©moire et les outils. Avec lui, vous pouvez construire, collaborer, tester et dÃ©ployer des agents Pipe et mÃ©moire (RAG).
    
5. **LangUI** : LangUI est une bibliothÃ¨que Tailwind gratuite et open-source avec des composants prÃªts Ã  l'emploi conÃ§us pour les projets IA et GPT.
    
6. **Langbase SDK** : Langbase offre un SDK IA TypeScript qui simplifie le dÃ©veloppement. Il vous aide Ã  intÃ©grer facilement les LLM, Ã  crÃ©er des agents mÃ©moire et Ã  les enchaÃ®ner dans des pipelines, le tout avec un code minimal. Il prend en charge JavaScript, TypeScript, Node.js, Next.js, React et plus encore, permettant un dÃ©veloppement plus rapide avec une excellente expÃ©rience dÃ©veloppeur.
    

![Source : Langbase](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcniBaqv7TmuUdmUqWejNJiulwjZDMf6Jg_Eh-AajXof7kfPVDKbiPXTQ3kgP-mXLA8cLZM87HYiL0_e6txSimQlcoPg90DCQ2lVYte1D_fzWiEGMsDP6IQMGiAO42a3-011owoHg?key=U363uJCJj9tPrezMPf7fa2Fc align="left")

### FonctionnalitÃ©s clÃ©s

* **Plateforme API-first** : API simples (Pipe et agents mÃ©moire) pour une intÃ©gration facile avec une documentation claire et un support communautaire.
    
    ðŸ’¡ Utilisez l'API Pipe pour gÃ©rer les agents Pipe dans votre compte Langbase. Elle dispose de points de terminaison pour crÃ©er, mettre Ã  jour, lister et exÃ©cuter. L'API Memory de Langbase vous permet de gÃ©rer les mÃ©moires et les documents dans votre compte Langbase de maniÃ¨re programmatique.
    
* **Environnement serverless** : Langbase fonctionne dans un environnement entiÃ¨rement serverless, Ã©liminant le besoin pour les dÃ©veloppeurs de gÃ©rer l'infrastructure. Cela simplifie la mise Ã  l'Ã©chelle et le dÃ©ploiement, permettant aux dÃ©veloppeurs de tous niveaux de compÃ©tence, et pas seulement aux experts en IA/ML, de construire, mettre Ã  l'Ã©chelle et dÃ©ployer des agents IA de maniÃ¨re fluide.
    
* **Infrastructure composable** : Langbase est la **premiÃ¨re plateforme IA composable**. Elle est construite pour la flexibilitÃ© et la modularitÃ©. Les dÃ©veloppeurs peuvent combiner des modÃ¨les en pipelines, chacun se concentrant sur une tÃ¢che spÃ©cifique. Cela rend le dÃ©veloppement plus facile, montre le coÃ»t de chaque Ã©tape et permet de crÃ©er des expÃ©riences hautement personnalisÃ©es. En choisissant le meilleur modÃ¨le pour chaque tÃ¢che, Langbase vous aide Ã  construire des workflows efficaces qui rÃ©pondent Ã  diffÃ©rents besoins.
    
    ðŸ’¡ L'IA composable signifie combiner diffÃ©rents modÃ¨les IA comme des blocs de construction pour crÃ©er des solutions personnalisÃ©es. C'est simple, flexible et s'adapte Ã  vos besoins.
    
* **EfficacitÃ© des coÃ»ts** : Langbase offre significativement plus de valeur Ã  un coÃ»t infÃ©rieur Ã  celui de LangChain, avec des coÃ»ts de dÃ©passement de seulement 2 $ pour 1 000 exÃ©cutions contre 5 $ pour LangChain.
    

### Cas d'utilisation

Langbase est parfait pour les dÃ©veloppeurs recherchant des solutions rentables avec un changement de modÃ¨le fluide via une seule API. Il est bien adaptÃ© pour les projets nÃ©cessitant une infrastructure IA composable/modulaire et des fonctionnalitÃ©s avancÃ©es de mÃ©moire Ã  long terme. Il excelle Ã©galement dans la construction de workflows autonomes avec collaboration multi-agents.

Voici quelques applications spÃ©cifiques que vous pouvez construire en utilisant Langbase :

* **Agent de support client** : Construisez des agents/applications de support client capables de gÃ©rer des conversations complexes et contextuelles Ã  travers les tickets de support, les e-mails et les chats, fournissant des rÃ©solutions prÃ©cises et efficaces. Consultez les agents de support client [ici](https://langbase.com/docs/solutions/customer-support).
    
* **Agent de codage** : CrÃ©ez des applications multi-agents qui assistent les dÃ©veloppeurs en gÃ©nÃ©rant des extraits de code, en dÃ©boguant et en rÃ©visant le code en temps rÃ©el, amÃ©liorant la productivitÃ© dans les workflows de dÃ©veloppement logiciel. Voici un exemple de dÃ©monstration d'agent de codage [demo](https://code-alchemist.langbase.dev/).
    

### Commencer avec Langbase

* Pour commencer avec Langbase, inscrivez-vous gratuitement [ici](https://langbase.com/signup).
    
* Pour crÃ©er un agent Pipe, tapez simplement pipe.new dans la barre de recherche.
    
* Pour crÃ©er un agent mÃ©moire, tapez rag.new dans la barre de recherche.

## LlamaIndex

[LlamaIndex](https://www.llamaindex.ai/) est un framework open-source construit pour les applications RAG et les systÃ¨mes basÃ©s sur des agents. Il fournit des outils essentiels pour ingÃ©rer, structurer et connecter des donnÃ©es privÃ©es ou spÃ©cifiques Ã  un domaine aux LLM, permettant une gÃ©nÃ©ration de texte plus prÃ©cise et fiable.

Avec son support pour la construction d'agents et l'intÃ©gration de pipelines RAG dans le cadre d'un ensemble d'outils plus large, LlamaIndex offre la flexibilitÃ© nÃ©cessaire pour gÃ©rer des tÃ¢ches complexes.

### FonctionnalitÃ©s clÃ©s

* **Chargement de donnÃ©es** : LlamaIndex rend l'importation de donnÃ©es fluide avec le support de 150+ sources, y compris les API, les PDF, les documents et les bases de donnÃ©es SQL. En utilisant des **connecteurs de donnÃ©es (LlamaHub)**, les dÃ©veloppeurs peuvent intÃ©grer des donnÃ©es diverses dans leurs applications LLM sans effort. Les exemples incluent l'extraction de donnÃ©es en temps rÃ©el Ã  partir d'API, le chargement d'informations structurÃ©es Ã  partir de MySQL ou PostgreSQL, et l'ingestion de texte Ã  partir de PDF ou de rapports.
    
    ðŸ’¡ Les chargeurs de donnÃ©es sont des utilitaires qui permettent d'ingÃ©rer facilement des donnÃ©es pour la recherche et la rÃ©cupÃ©ration par un grand modÃ¨le de langage.
    
* **Indexation** : L'indexation organise et stocke les donnÃ©es pour une rÃ©cupÃ©ration facile et rapide, crÃ©ant des structures comme des index vectoriels ou documentaires. Avec LlamaIndex, vous pouvez stocker et indexer des donnÃ©es sur plusieurs fournisseurs (par exemple, des bases de donnÃ©es vectorielles, documentaires, graphiques et SQL).
    
* **RequÃªtage** : Le requÃªtage rÃ©cupÃ¨re des informations spÃ©cifiques Ã  partir de donnÃ©es indexÃ©es, permettant des recherches et des workflows avancÃ©s comme les pipelines RAG pour des rÃ©ponses contextuelles. Pour cela, LlamaIndex vous permet de construire des workflows de requÃªte avancÃ©s avec rÃ©cupÃ©ration, post-traitement et synthÃ¨se de rÃ©ponse pour les chaÃ®nes de prompts et les pipelines RAG.
    
    ðŸ’¡ Un **Pipeline de requÃªte** dans LlamaIndex est un moyen simple de concevoir des workflows de requÃªte pour diffÃ©rentes tÃ¢ches comme le RAG et l'extraction de donnÃ©es structurÃ©es. Il vous aide Ã  dÃ©finir comment les requÃªtes interagissent avec vos donnÃ©es, rendant facile la gestion des workflows de base et avancÃ©s. Lisez Ã  propos des pipelines de requÃªte LlamaIndex [ici](https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537).
    
* **Ã‰valuations** : Inclut des modules pour Ã©valuer la qualitÃ© de la rÃ©cupÃ©ration et des rÃ©ponses, amÃ©liorant le suivi des performances.
    

### Cas d'utilisation

LlamaIndex est prÃ©fÃ©rÃ© pour l'indexation fluide des donnÃ©es et la rÃ©cupÃ©ration rapide, le rendant plus adaptÃ© aux applications RAG prÃªtes pour la production. D'autre part, LangChain fournit plus de composants prÃªts Ã  l'emploi, facilitant la crÃ©ation de diverses architectures LLM.

Voici quelques applications RAG spÃ©cifiques que vous pouvez construire en utilisant LlamaIndex :

* **Assistant d'analyses financiÃ¨res** : Construisez un assistant de connaissances pour les analystes financiers afin de rÃ©cupÃ©rer des informations en temps rÃ©el Ã  partir des donnÃ©es de marchÃ©, des rapports de rÃ©sultats et des documents financiers internes, permettant une prise de dÃ©cision plus rapide et une Ã©valuation des risques.
    
* **Conseiller en fabrication** : CrÃ©ez un assistant alimentÃ© par l'IA pour rationaliser les workflows de production en accÃ©dant aux manuels d'Ã©quipement, aux journaux de maintenance et aux donnÃ©es de la chaÃ®ne d'approvisionnement, amÃ©liorant l'efficacitÃ© opÃ©rationnelle et rÃ©duisant les temps d'arrÃªt.
    

![Source : LlamaIndex](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeaMsX_VAc5eaX5XfO5fbZLa3Ak5yxQyKApHm1NcNR2iBtDovLuLBTXE2tUax73e9DZprnllnBdGva14hjGPPb1pzwiPn-TFa0Ckb0w6JbCeWQr8HAbWAq62sPdX8xfXCyFZ2UvYw?key=U363uJCJj9tPrezMPf7fa2Fc align="left")

### Commencer avec LlamaIndex

Vous pouvez commencer avec LlamaIndex en Python ou TypeScript en seulement 5 lignes de code.

1. DÃ©finissez la variable d'environnement `OPENAI_API_KEY` avec votre clÃ© API OpenAI.
    
2. Installez la bibliothÃ¨que Python : `pip install llama-index`
    
3. Placez vos documents dans un dossier nommÃ© `data`, puis utilisez ce code de dÃ©marrage pour les interroger :
    

```python
pythonCopyEditfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("data").load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()
response = query_engine.query("Votre question ici")
print(response)
```

Pour plus de dÃ©tails, consultez leur [documentation](https://docs.llamaindex.ai/en/stable/#getting-started).

## AG2

[AG2](https://ag2.ai/) (anciennement AutoGen) est un framework open-source pour construire des agents IA et permettre la collaboration multi-agents. AG2 fournit un framework pour construire des workflows autonomes et la collaboration d'agents, simplifiant la crÃ©ation d'agents spÃ©cialisÃ©s qui peuvent travailler ensemble de maniÃ¨re fluide.

ðŸ’¡ La **collaboration multi-agents** fait rÃ©fÃ©rence Ã  plusieurs agents travaillant ensemble vers un objectif commun, chacun effectuant des tÃ¢ches et partageant des informations selon les besoins. Les agents peuvent Ãªtre indÃ©pendants et spÃ©cialisÃ©s, mais ils collaborent pour accomplir une tÃ¢che.

### FonctionnalitÃ©s clÃ©s

* **Collaboration d'agents** : Prend en charge l'orchestration multi-agents pour une communication et une gestion des tÃ¢ches fluides.
    
* **RÃ´les d'agents flexibles** : DÃ©finissez les comportements, rÃ´les et workflows des agents avec un code intuitif. Attribuez des rÃ´les spÃ©cifiques aux agents, tels que collecteur de donnÃ©es, analyste ou dÃ©cideur, et faites-les interagir dans des conversations ou travailler de maniÃ¨re indÃ©pendante. Par exemple, un agent peut rassembler des informations, tandis qu'un autre les traite et fournit des insights. Ces conversations entre agents peuvent conduire Ã  l'accomplissement de tÃ¢ches, chaque agent contribuant en fonction de son rÃ´le dÃ©signÃ©, rendant les workflows plus dynamiques et efficaces.
    
* **Support humain dans la boucle** : AG2 permet une implication humaine fluide dans le workflow en permettant des mÃ©thodes d'entrÃ©e personnalisables, telles que des remplacements manuels ou des boucles de feedback. Il offre une transmission contextuelle, ce qui signifie que le systÃ¨me peut passer des tÃ¢ches Ã  un humain au bon moment, en fonction de conditions ou d'exigences spÃ©cifiques. De plus, des interfaces interactives sont fournies, permettant aux humains de rÃ©viser, approuver ou ajuster les actions des agents en temps rÃ©el, garantissant que le systÃ¨me reste alignÃ© avec le jugement et la supervision humains.
    
* **ModÃ¨les de conversation** : Des modÃ¨les intÃ©grÃ©s automatisent les tÃ¢ches de coordination comme le routage des messages, la gestion d'Ã©tat et la sÃ©lection dynamique des intervenants.
    

### Cas d'utilisation

AG2 se distingue par sa capacitÃ© Ã  gÃ©rer des interactions complexes entre agents, ce qui en fait un excellent choix pour les workflows multi-agents nÃ©cessitant une collaboration humaine.

Voici quelques applications IA que vous pouvez construire en utilisant AG2 :

* **Pipelines de crÃ©ation et de rÃ©vision de contenu** : Construisez des workflows collaboratifs oÃ¹ un agent gÃ©nÃ¨re du contenu Ã©crit ou visuel, un autre s'assure de la conformitÃ© avec les directives, et un rÃ©viseur humain fournit des contributions crÃ©atives ou une approbation finale.
    
* **Plateformes d'Ã©ducation personnalisÃ©es** : CrÃ©ez des assistants d'apprentissage oÃ¹ un agent sÃ©lectionne du contenu Ã©ducatif, un autre conÃ§oit des parcours d'apprentissage personnalisÃ©s, et un troisiÃ¨me surveille les progrÃ¨s des Ã©tudiants. Les enseignants ou mentors peuvent intervenir pour fournir des feedbacks personnalisÃ©s ou des ajustements au programme.
    

![Source : AG2](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcuX2p7__EXrxmGP6pFHPKNtQDz6_d6vUU3PFpSy-viwSBIYpti3JCdCQzw3vUmrGOhnUG7Z9JqVd_PKgil3OI6KD5gnEe5yBDA4ImZ_5Lq-f16iIJ5RiaD6DEJuyneXT6TLw-oCQ?key=U363uJCJj9tPrezMPf7fa2Fc align="left")

### Commencer avec AG2

AG2 nÃ©cessite **Python version >= 3.9, < 3.14**. Il peut Ãªtre installÃ© depuis pip :

```python
pip install ag2
```

Pour plus de dÃ©tails, visitez la [documentation](https://docs.ag2.ai/docs/Getting-Started).

## Braintrust

[Braintrust](https://www.braintrust.dev/) est une plateforme complÃ¨te pour Ã©valuer, amÃ©liorer et dÃ©ployer des grands modÃ¨les de langage (LLM) avec des outils pour l'ingÃ©nierie de prompts, la gestion des donnÃ©es et l'Ã©valuation continue. ConÃ§ue pour rendre la construction d'applications IA plus robuste et itÃ©rative, Braintrust vous aide Ã  prototyper rapidement avec diffÃ©rents prompts et modÃ¨les, Ã  Ã©valuer les performances avec des outils intÃ©grÃ©s et Ã  surveiller les interactions en temps rÃ©el.

### FonctionnalitÃ©s clÃ©s

* **ExpÃ©rimentation itÃ©rative** : Prototypage et test rapides de prompts avec diffÃ©rents modÃ¨les dans le [playground](https://www.braintrust.dev/docs/guides/playground) intÃ©grÃ©. Vous pouvez expÃ©rimenter avec des entrÃ©es de jeux de donnÃ©es rÃ©els, comparer les rÃ©ponses entre les modÃ¨les (OpenAI, Anthropic, Mistral, Google, Meta, et plus), et affiner les performances dans le playground.
    
* **Insights de performance** : Ã‰valuez les performances des modÃ¨les et des prompts avec des outils intÃ©grÃ©s comme le playground de prompts, les imports de jeux de donnÃ©es et les fonctions de notation. Vous pouvez tester les sorties contre des donnÃ©es rÃ©elles, comparer les modÃ¨les et affiner les prompts de maniÃ¨re itÃ©rative. Utilisez des heuristiques ou des notations basÃ©es sur les LLM pour Ã©valuer la prÃ©cision, suivre les rÃ©sultats et amÃ©liorer les performances au fil du temps dans l'UI ou le SDK de Braintrust.
    
* **Surveillance en temps rÃ©el** : Suivez les interactions IA avec des logs dÃ©taillÃ©s, capturant les entrÃ©es, les sorties et les mÃ©tadonnÃ©es pour chaque requÃªte. Braintrust log les traces des appels IA, les dÃ©composant en spans pour identifier les problÃ¨mes, surveiller le comportement des utilisateurs et affiner les performances. Les logs s'intÃ¨grent de maniÃ¨re fluide avec les Ã©valuations, crÃ©ant une boucle de feedback pour l'amÃ©lioration continue des modÃ¨les.
    
* **Gestion centralisÃ©e des donnÃ©es** : Braintrust intÃ¨gre les donnÃ©es de production, de staging et d'Ã©valuations, vous permettant de suivre les changements, comparer les itÃ©rations et affiner les modÃ¨les au fil du temps. Le versioning garantit que vous pouvez revenir en arriÃ¨re, auditer et Ã©pingler les Ã©valuations Ã  des versions spÃ©cifiques de jeux de donnÃ©es, soutenant l'expÃ©rimentation structurÃ©e et les rÃ©visions humaines dans la boucle pour une amÃ©lioration continue.
    
    ðŸ’¡ Les jeux de donnÃ©es vous permettent de collecter des donnÃ©es Ã  partir de la production, du staging, des Ã©valuations et mÃªme manuellement, puis d'utiliser ces donnÃ©es pour exÃ©cuter des Ã©valuations et suivre les amÃ©liorations au fil du temps.
    

### Cas d'utilisation

Braintrust est idÃ©al pour le dÃ©veloppement et l'Ã©valuation itÃ©ratifs de modÃ¨les, en particulier pour les projets nÃ©cessitant des pipelines de test et de dÃ©ploiement robustes. Il se distingue pour la construction d'applications LLM scalables, offrant des insights basÃ©s sur les donnÃ©es qui permettent une optimisation prÃ©cise et une amÃ©lioration continue.

Voici quelques applications que vous pouvez construire avec Braintrust :

* **Ã‰valuation d'un assistant de chat** : Avec Braintrust, vous pouvez Ã©valuer un assistant de chat en vous assurant que l'IA conversationnelle maintient le contexte pour des rÃ©ponses prÃ©cises. Il permet des Ã©valuations automatisÃ©es pour Ã©valuer la qualitÃ© des rÃ©ponses, gÃ¨re les jeux de donnÃ©es pour affiner les cas de test et suit les performances pour une amÃ©lioration continue.
    
* **Barre de recherche IA** : Braintrust aide Ã  optimiser la recherche alimentÃ©e par l'IA en garantissant la prÃ©cision et la conscience du contexte. Il log les requÃªtes pour identifier les lacunes, Ã©value les rÃ©sultats de recherche pour la pertinence et compare les versions de modÃ¨les pour suivre les amÃ©liorations.
    

### Commencer avec Braintrust

* Pour commencer, [inscrivez-vous](https://www.braintrust.dev/signup) sur Braintrust.
    
* Une fois inscrit, vous serez invitÃ© Ã  crÃ©er une organisation gratuitement.
    
* Pour exÃ©cuter votre premiÃ¨re Ã©valuation, vous pouvez utiliser soit l'[UI](https://www.braintrust.dev/docs/start/eval-ui) soit le [code de dÃ©marrage](https://www.braintrust.dev/docs/start/eval-sdk) disponible. Installez le SDK Braintrust en utilisant cette commande :
    
    ```bash
    npm install braintrust autoevals
    ```
    
    Pour plus de dÃ©tails, visitez la [documentation](https://www.braintrust.dev/docs/start).
    

![Source : Braintrust](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcwXfdLQfJnKfYu0kNauwNFALM0fZfDPiuajC8DgcImgdUL-UmGpzhFQhRVUPj9LPLoJM_UWz7eAEU_yqwepSfgyIbFQTNIqxnABuueLNWke74CMewIe1nsp9FzO8p7WqYZbYMHpA?key=U363uJCJj9tPrezMPf7fa2Fc align="left")

## FlowiseAI

[FlowiseAI](https://flowiseai.com/) est un outil open source low-code pour les dÃ©veloppeurs afin de construire des flux d'orchestration LLM personnalisÃ©s et des agents IA. Avec son interface intuitive de glisser-dÃ©poser, Flowise rend la technologie LLM accessible Ã  un public plus large, y compris ceux ayant peu ou pas d'expÃ©rience en codage.

### FonctionnalitÃ©s clÃ©s

* **ItÃ©ration rapide** : L'approche low-code permet des itÃ©rations rapides, facilitant le passage des tests Ã  la production en une fraction du temps.
    
* **Chatflow et orchestration LLM** : Connectez de maniÃ¨re fluide les LLM avec la mÃ©moire, les chargeurs de donnÃ©es, la mise en cache et les outils de modÃ©ration pour gÃ©rer la maniÃ¨re dont les modÃ¨les traitent les entrÃ©es, rÃ©cupÃ¨rent les donnÃ©es pertinentes et gÃ©nÃ¨rent des rÃ©ponses. Cela garantit des interactions contrÃ´lÃ©es entre les modÃ¨les, les entrÃ©es utilisateur et les sources de donnÃ©es externes pour des performances optimales.
    
* **100+ intÃ©grations** : Connectez-vous facilement avec des outils comme Langchain et LlamaIndex pour amÃ©liorer vos workflows. Ces intÃ©grations vous aident Ã  lier des sources de donnÃ©es, gÃ©rer des tÃ¢ches et ajouter des fonctionnalitÃ©s supplÃ©mentaires, vous permettant de construire des applications IA personnalisÃ©es. Utilisez-les pour automatiser le travail, amÃ©liorer les performances des modÃ¨les ou Ã©tendre les capacitÃ©s de votre systÃ¨me en fonction de vos besoins.
    
* **Agents et assistants** : Construisez des agents autonomes qui exÃ©cutent des tÃ¢ches en utilisant des outils comme les multi-agents ou les agents sÃ©quentiels, amÃ©liorant les capacitÃ©s de votre application. Ces agents peuvent interagir avec des sources de donnÃ©es externes et des outils, leur permettant d'effectuer des tÃ¢ches spÃ©cialisÃ©es de maniÃ¨re efficace. Par exemple, Flowise propose deux approches pour crÃ©er des systÃ¨mes basÃ©s sur des agents : les Multi-Agents, qui travaillent ensemble de maniÃ¨re collaborative et spÃ©cialisÃ©e, et les Agents SÃ©quentiels, qui traitent les tÃ¢ches de maniÃ¨re structurÃ©e, Ã©tape par Ã©tape. En intÃ©grant ces systÃ¨mes, vous pouvez automatiser des workflows complexes et amÃ©liorer l'exÃ©cution des tÃ¢ches au sein de votre application.
    
* **Convivial pour les dÃ©veloppeurs** : Ã‰tendez et intÃ©grez avec vos applications en utilisant des API, des SDK et des options de chat intÃ©grÃ©es, y compris le SDK React et les widgets intÃ©grÃ©s.
    

### Cas d'utilisation

Flowise est idÃ©al pour les dÃ©veloppeurs ayant peu d'expÃ©rience en codage construisant des workflows LLM et les Ã©quipes ayant besoin de mises Ã  jour rapides sans perdre de fonctionnalitÃ©s. Il rend les workflows IA avancÃ©s faciles Ã  utiliser, mÃªme pour les non-experts.

Il s'intÃ¨gre avec des frameworks comme LangChain et LlamaIndex, ce qui en fait un choix idÃ©al pour un dÃ©veloppement IA simplifiÃ©. Mais il peut poser des dÃ©fis pour ceux qui sont nouveaux dans les LLM, et les approches code-first peuvent Ãªtre mieux adaptÃ©es pour les tÃ¢ches hautement spÃ©cialisÃ©es.

Voici quelques exemples pratiques que vous pouvez construire en utilisant Flowise :

* **Interroger plusieurs documents** : Avec Flowise, vous pouvez construire des systÃ¨mes qui interrogent plusieurs documents en les tÃ©lÃ©chargeant sur Pinecone avec des mÃ©tadonnÃ©es. Les agents outils aident le LLM Ã  sÃ©lectionner le document appropriÃ© en fonction de la question.
    
* **Assistants personnels** : DÃ©veloppez des assistants capables de gÃ©rer des tÃ¢ches, de planifier des rendez-vous et de fournir des rappels avec Flowise.
    

### Commencer avec FlowiseAI

* Pour commencer, installez Flowise localement en utilisant NPM.
    
    ```bash
    npm install -g flowise
    ```
    
    ðŸ’¡ PrÃ©requis : assurez-vous que [NodeJS](https://nodejs.org/en/download) est installÃ© sur la machine. Node `v18.15.0` ou `v20` et versions supÃ©rieures sont supportÃ©es.
    
* DÃ©marrez Flowise en utilisant cette commande et ouvrez [localhost:3000](http://localhost:3000) :
    
    ```bash
    npx flowise start
    ```
    

Pour plus de dÃ©tails, consultez ces Ã©tapes de [dÃ©marrage](https://docs.flowiseai.com/getting-started).

![Source : FlowiseAI](https://docs.flowiseai.com/~gitbook/image?url=https%3A%2F%2F1820151947-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252Fy8ifwt9BYklr92KDdr48%252Fuploads%252Fgit-blob-b24c3b670833a97778c5374146e905d026e0b122%252Fmulti-docs-upload.png%3Falt%3Dmedia&width=768&dpr=4&quality=100&sign=1196a9d4&sv=2 align="left")

## Conclusion ðŸ™Œ

Les workflows IA et agentiques Ã©voluent rapidement, et LangChain n'est plus la seule option. Le choix du bon outil dÃ©pend des besoins de votre projet : orchestration flexible des agents, rÃ©duction des coÃ»ts ou intÃ©gration fluide. Alors que nous avanÃ§ons en 2025, ces alternatives mÃ©ritent votre attention pour construire l'avenir de l'IA.

Merci d'avoir lu !