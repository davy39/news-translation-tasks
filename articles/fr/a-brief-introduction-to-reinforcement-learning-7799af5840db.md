---
title: Une br√®ve introduction √† l'apprentissage par renforcement
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2018-08-27T21:17:00.000Z'
originalURL: https://freecodecamp.org/news/a-brief-introduction-to-reinforcement-learning-7799af5840db
coverImage: https://cdn-media-1.freecodecamp.org/images/0*7i8JA5t1Nx3HlK4E
tags:
- name: Artificial Intelligence
  slug: artificial-intelligence
- name: Deep Learning
  slug: deep-learning
- name: Machine Learning
  slug: machine-learning
- name: Reinforcement Learning
  slug: reinforcement-learning
- name: 'tech '
  slug: tech
seo_title: Une br√®ve introduction √† l'apprentissage par renforcement
seo_desc: 'By ADL

  Reinforcement Learning is an aspect of Machine learning where an agent learns to
  behave in an environment, by performing certain actions and observing the rewards/results
  which it get from those actions.

  With the advancements in Robotics Arm M...'
---

Par ADL

L'apprentissage par renforcement est un aspect de l'apprentissage automatique o√π un agent apprend √† se comporter dans un environnement, en effectuant certaines actions et en observant les r√©compenses/r√©sultats qu'il obtient de ces actions.

Avec les avanc√©es en manipulation de bras robotiques, Google Deep Mind battant un joueur professionnel d'Alpha Go, et r√©cemment l'√©quipe OpenAI battant un joueur professionnel de DOTA, le domaine de l'apprentissage par renforcement a vraiment explos√© ces derni√®res ann√©es.

![Image](https://cdn-media-1.freecodecamp.org/images/1*EM8x5jAL-SeUUG7b4anCQg.gif)

![Image](https://cdn-media-1.freecodecamp.org/images/1*rvGVriKT_aLeLKvAP16S0A.gif)
_Exemples_

Dans cet article, nous discuterons :

* Ce qu'est l'apprentissage par renforcement et ses d√©tails comme les r√©compenses, les t√¢ches, etc.
* 3 cat√©gorisations de l'apprentissage par renforcement

#### Qu'est-ce que l'apprentissage par renforcement ?

Commen√ßons l'explication avec un exemple ‚Äî disons qu'il y a un petit b√©b√© qui commence √† apprendre √† marcher.

Divisons cet exemple en deux parties :

#### 1. **Le b√©b√© commence √† marcher et atteint avec succ√®s le canap√©**

Puisque le canap√© est le but final, le b√©b√© et les parents sont heureux.

![Image](https://cdn-media-1.freecodecamp.org/images/1*sDMJA6qzlo59o7iivh6U6Q.jpeg)

Donc, le b√©b√© est heureux et re√ßoit des f√©licitations de ses parents. C'est positif ‚Äî le b√©b√© se sent bien _(R√©compense positive +n)._

#### 2. **Le b√©b√© commence √† marcher et tombe √† cause d'un obstacle et se blesse.**

![Image](https://cdn-media-1.freecodecamp.org/images/1*i_999FG_Y-DnlCtpEKb5Vw.jpeg)

A√Øe ! Le b√©b√© se blesse et a mal. C'est n√©gatif ‚Äî le b√©b√© pleure _(R√©compense n√©gative -n)._

C'est ainsi que nous, humains, apprenons ‚Äî par essai et erreur. L'apprentissage par renforcement est conceptuellement le m√™me, mais c'est une approche computationnelle pour apprendre par les actions.

### Apprentissage par renforcement

Supposons que notre agent d'apprentissage par renforcement apprend √† jouer √† Mario comme exemple. Le processus d'apprentissage par renforcement peut √™tre mod√©lis√© comme une boucle it√©rative qui fonctionne comme suit :

![Image](https://cdn-media-1.freecodecamp.org/images/1*vz3AN1mBUR2cr_jEG8s7Mg.png)

* L'agent RL re√ßoit l'**√©tat S‚ÇÄ** de l'**environnement** c'est-√†-dire Mario
* Sur la base de cet **√©tat S‚ÇÄ**, l'agent RL prend une **action A‚ÇÄ**, disons ‚Äî notre agent RL se d√©place vers la droite. Initialement, cela est al√©atoire.
* Maintenant, l'environnement est dans un nouvel √©tat **S‚ÇÅ** (nouvelle frame de Mario ou du moteur de jeu)
* L'environnement donne une certaine **r√©compense R‚ÇÅ** √† l'agent RL. Il donne probablement un +1 parce que l'agent n'est pas encore mort.

Cette boucle RL continue jusqu'√† ce que nous soyons morts ou que nous atteignions notre destination, et elle produit en continu une s√©quence d'**√©tat, action et r√©compense.**

Le but de base de notre agent RL est de maximiser la r√©compense.

### Maximisation de la r√©compense

L'agent RL fonctionne essentiellement sur une hypoth√®se de maximisation de la r√©compense. **C'est pourquoi l'apprentissage par renforcement doit avoir la meilleure action possible afin de maximiser la r√©compense.**

Les r√©compenses cumulatives √† chaque √©tape de temps avec l'action respective sont √©crites comme suit :

![Image](https://cdn-media-1.freecodecamp.org/images/1*up3hsG1ToqndcnmdA8tbRw.png)

Cependant, les choses ne fonctionnent pas de cette mani√®re lorsque l'on additionne toutes les r√©compenses.

Comprenons cela en d√©tail :

![Image](https://cdn-media-1.freecodecamp.org/images/1*l8wl4hZvZAiLU56hT9vLlg.png)

Disons que notre agent RL (souris robotique) est dans un labyrinthe qui contient **du fromage, des chocs √©lectriques et des chats**. Le but est de manger la quantit√© maximale de fromage avant d'√™tre mang√© par le chat ou de recevoir un choc √©lectrique.

Il semble √©vident de manger le fromage pr√®s de nous plut√¥t que le fromage proche du chat ou du choc √©lectrique, car plus nous sommes proches du choc √©lectrique ou du chat, plus le danger de mourir augmente. Par cons√©quent, la r√©compense pr√®s du chat ou du choc √©lectrique, m√™me si elle est plus grande (plus de fromage), sera r√©duite. Cela est fait en raison du facteur d'incertitude.

Cela a du sens, n'est-ce pas ?

#### **L'actualisation des r√©compenses fonctionne comme suit :**

Nous d√©finissons un taux d'actualisation appel√© **gamma**. Il doit √™tre compris entre 0 et 1. Plus le gamma est grand, plus l'actualisation est faible et vice versa.

Donc, nos r√©compenses cumulatives attendues (actualis√©es) sont :

![Image](https://cdn-media-1.freecodecamp.org/images/1*ef-5D-aBUShEnvMjiCujNw.png)
_R√©compenses cumulatives attendues_

### T√¢ches et leurs types dans l'apprentissage par renforcement

Une **t√¢che** est une instance unique d'un probl√®me d'apprentissage par renforcement. Nous avons essentiellement deux types de t√¢ches : **continues et √©pisodiques.**

#### T√¢ches continues

**Ce sont les types de t√¢ches qui continuent ind√©finiment.** Par exemple, un agent RL qui fait du trading Forex/Actions automatis√©.

![Image](https://cdn-media-1.freecodecamp.org/images/0*Rpz3cfDnays7p4-e)
_Photo par [Unsplash](https://unsplash.com/@chrisliverani?utm_source=medium&amp;utm_medium=referral" rel="noopener" target="_blank" title="">Chris Liverani</a> sur <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener" target="_blank" title=")_

Dans ce cas, l'agent doit apprendre √† choisir les meilleures actions et interagir simultan√©ment avec l'environnement. Il n'y a pas de point de d√©part et d'√©tat final.

**L'agent RL doit continuer √† fonctionner jusqu'√† ce que nous d√©cidions de l'arr√™ter manuellement.**

#### T√¢che √©pisodique

Dans ce cas, nous avons un point de d√©part et un point final **appel√© l'√©tat terminal. Cela cr√©e un √©pisode** : une liste d'√âtats (S), d'Actions (A), de R√©compenses (R).

Par exemple, jouer √† un jeu de _counter strike_, o√π nous tirons sur nos adversaires ou nous sommes tu√©s par eux. Nous les tuons tous et compl√©tons l'√©pisode ou nous sommes tu√©s. Donc, il n'y a que deux cas pour compl√©ter les √©pisodes.

### Compromis entre exploration et exploitation

Il y a un concept important de compromis entre exploration et exploitation dans l'apprentissage par renforcement. L'exploration consiste √† trouver plus d'informations sur un environnement, tandis que l'exploitation consiste √† exploiter des informations d√©j√† connues pour maximiser les r√©compenses.

**Exemple de la vie r√©elle :** Disons que vous allez au m√™me restaurant tous les jours. Vous exploitez essentiellement. Mais d'un autre c√¥t√©, si vous cherchez un nouveau restaurant chaque fois avant d'aller dans l'un d'eux, alors c'est de l'**exploration**. L'exploration est tr√®s importante pour la recherche de futures r√©compenses qui pourraient √™tre plus √©lev√©es que les r√©compenses proches.

![Image](https://cdn-media-1.freecodecamp.org/images/1*R9hA8rKx52oByN5Xa7Aqng.png)

Dans le jeu ci-dessus, notre souris robotique peut avoir une bonne quantit√© de petit fromage (+0,5 chacun). Mais en haut du labyrinthe, il y a une grande somme de fromage (+100). Donc, si nous nous concentrons uniquement sur la r√©compense la plus proche, notre souris robotique n'atteindra jamais la grande somme de fromage ‚Äî elle exploitera simplement.

Mais si la souris robotique fait un peu d'exploration, elle peut trouver la grande r√©compense, c'est-√†-dire le gros fromage.

C'est le concept de base du **compromis entre exploration et exploitation.**

### Approches de l'apprentissage par renforcement

Comprenons maintenant les approches pour r√©soudre les probl√®mes d'apprentissage par renforcement. Il y a essentiellement 3 approches, mais nous ne prendrons que 2 approches majeures dans cet article :

#### 1. Approche bas√©e sur les politiques

Dans l'apprentissage par renforcement bas√© sur les politiques, nous avons une politique que nous devons optimiser. La politique d√©finit essentiellement comment l'agent se comporte :

![Image](https://cdn-media-1.freecodecamp.org/images/1*0eMOC89KDSeJAPxEpOZi5Q.png)

Nous apprenons une fonction de politique qui nous aide √† mapper chaque √©tat √† la meilleure action.

En approfondissant les politiques, nous divisons davantage les politiques en deux types :

* **D√©terministe** : une politique √† un √©tat donn√© (s) retournera toujours la m√™me action (a). **Cela signifie qu'elle est pr√©-mapp√©e comme S=(s) ‚ûî A=(a).**
* **Stochastique** : Elle donne une distribution de probabilit√© sur diff√©rentes actions. **c'est-√†-dire Politique Stochastique ‚ûî p( A = a | S = s )**

#### 2. Bas√©e sur la valeur

Dans l'apprentissage par renforcement bas√© sur la valeur, le but de l'agent est d'optimiser la fonction de valeur _V(s)_ qui est d√©finie comme une fonction qui nous indique la r√©compense future maximale attendue que l'agent doit obtenir √† chaque √©tat.

La valeur de chaque √©tat est le montant total de la r√©compense qu'un agent RL peut s'attendre √† collecter √† l'avenir, √† partir d'un √©tat particulier.

![Image](https://cdn-media-1.freecodecamp.org/images/0*kvtRAhBZO-h77Iw1.)

L'agent utilisera la fonction de valeur ci-dessus pour s√©lectionner quel √©tat choisir √† chaque √©tape. L'agent choisira toujours l'√©tat avec la plus grande valeur.

Dans l'exemple ci-dessous, nous voyons qu'√† chaque √©tape, nous prendrons la plus grande valeur pour atteindre notre objectif : 1 ‚ûî 3 ‚ûî 4 ‚ûî 6 et ainsi de suite...

![Image](https://cdn-media-1.freecodecamp.org/images/1*96F7YC253a5-mXNPVUTCSg.png)
_Labyrinthe_

### Le jeu de Pong ‚Äî Une √©tude de cas intuitive

![Image](https://cdn-media-1.freecodecamp.org/images/1*6D27X-9bipEPrgHrrjwIRA.gif)

Prenons un exemple de la vie r√©elle de jouer √† Pong. Cette √©tude de cas vous introduira simplement √† l'intuition de **comment fonctionne l'apprentissage par renforcement**. Nous n'entrerons pas dans les d√©tails dans cet exemple, mais dans le prochain article, nous approfondirons certainement.

Supposons que nous apprenons √† notre agent RL √† jouer au jeu de Pong.

Essentiellement, nous alimentons les frames du jeu (nouveaux √©tats) dans l'algorithme RL et laissons l'algorithme d√©cider o√π aller en haut ou en bas. Ce r√©seau est dit √™tre un **r√©seau de politiques**, que nous discuterons dans notre prochain article.

![Image](https://cdn-media-1.freecodecamp.org/images/1*nGQ4cQneWpgbUpl7aREGwg.jpeg)

La m√©thode utilis√©e pour entra√Æner cet algorithme est appel√©e le **gradient de politique**. Nous alimentons des frames al√©atoires du moteur de jeu, et l'algorithme produit une sortie al√©atoire qui donne une r√©compense et cela est r√©inject√© dans l'algorithme/r√©seau. C'est un **processus it√©ratif.**

Nous discuterons des **gradients de politique** dans le prochain article avec plus de d√©tails.

![Image](https://cdn-media-1.freecodecamp.org/images/1*-SwnWvR-VhZRhX-a9ruF6Q.png)
_Environnement = Moteur de jeu et Agent = Agent RL_

Dans le contexte du jeu, le tableau de score agit comme une r√©compense ou un retour pour l'agent. Chaque fois que l'agent marque +1, il comprend que l'action prise √©tait suffisamment bonne √† cet √©tat.

Maintenant, nous allons entra√Æner l'agent √† jouer au jeu de Pong. Pour commencer, nous allons alimenter un ensemble de frames de jeu **(√©tats)** dans le r√©seau/algorithme et laisser l'algorithme d√©cider de l'action. Les actions initiales de l'agent seront √©videmment mauvaises, mais notre agent peut parfois avoir assez de chance pour marquer un point et cela pourrait √™tre un √©v√©nement al√©atoire. Mais gr√¢ce √† cet √©v√©nement al√©atoire chanceux, il re√ßoit une r√©compense et cela aide l'agent √† comprendre que la s√©rie d'actions √©tait suffisamment bonne pour obtenir une r√©compense.

![Image](https://cdn-media-1.freecodecamp.org/images/1*cdq5CaGCJCU6ePiXS9GbYg.png)
_R√©sultats pendant l'entra√Ænement_

Donc, √† l'avenir, l'agent est susceptible de prendre les actions qui lui rapporteront une r√©compense plut√¥t qu'une action qui ne le fera pas. Intuitivement, l'agent RL apprend √† jouer au jeu.

![Image](https://cdn-media-1.freecodecamp.org/images/1*roRyfK2mmV1E_MsN0cRzcg.gif)
_Source : OLEGIF.com_

#### Limites

Pendant l'entra√Ænement de l'agent, lorsqu'un agent perd un √©pisode, l'algorithme rejettera ou r√©duira la probabilit√© de prendre toutes les s√©ries d'actions qui existaient dans cet √©pisode.

![Image](https://cdn-media-1.freecodecamp.org/images/1*H6wuWYx1wlGRTNfiFGWvhA.png)
_La d√©marcation rouge montre toutes les actions prises dans un √©pisode perdant_

Mais si l'agent se comportait **bien** depuis le d√©but de l'√©pisode, mais a perdu le jeu √† cause des deux derni√®res actions, il n'a pas de sens de rejeter toutes les actions. Il serait plus judicieux de simplement supprimer les deux derni√®res actions qui ont entra√Æn√© la perte.

![Image](https://cdn-media-1.freecodecamp.org/images/1*ZSPXbb8q_2zZiVEQdbDX9A.png)
_La d√©marcation verte montre toutes les actions qui √©taient correctes et la d√©marcation rouge montre les actions qui devraient √™tre supprim√©es._

Cela s'appelle le **probl√®me d'attribution de cr√©dit**. Ce probl√®me survient en raison d'un **param√©trage de r√©compense clairsem√©**. C'est-√†-dire, au lieu de recevoir une r√©compense √† chaque √©tape, nous recevons la r√©compense √† la fin de l'√©pisode. Donc, c'est √† l'agent d'apprendre quelles actions √©taient correctes et quelle action r√©elle a conduit √† perdre le jeu.

Ainsi, en raison de ce param√©trage de r√©compense clairsem√© en RL, l'algorithme est tr√®s inefficace en termes d'√©chantillons. Cela signifie qu'un grand nombre d'exemples d'entra√Ænement doivent √™tre aliment√©s afin d'entra√Æner l'agent. Mais le fait est que les param√©trages de r√©compense clairsem√©s √©chouent dans de nombreuses circonstances en raison de la complexit√© de l'environnement.

Il existe donc quelque chose appel√© **fa√ßonnage des r√©compenses** qui est utilis√© pour r√©soudre cela. Mais encore une fois, le fa√ßonnage des r√©compenses souffre √©galement de certaines limitations, car nous devons concevoir une fonction de r√©compense personnalis√©e pour chaque jeu.

#### Note de cl√¥ture

Aujourd'hui, l'apprentissage par renforcement est un domaine d'√©tude passionnant. Des d√©veloppements majeurs ont √©t√© r√©alis√©s dans ce domaine, dont l'apprentissage par renforcement profond est l'un d'eux.

Nous aborderons l'apprentissage par renforcement profond dans nos prochains articles. Cet article couvre de nombreux concepts. Prenez votre temps pour comprendre les concepts de base de l'apprentissage par renforcement.

Mais je tiens √† mentionner que le renforcement n'est pas une bo√Æte noire secr√®te. Toutes les avanc√©es que nous voyons aujourd'hui dans le domaine de l'apprentissage par renforcement sont le r√©sultat d'esprits brillants travaillant jour et nuit sur des applications sp√©cifiques.

La prochaine fois, nous travaillerons sur un agent Q-learning et aborderons √©galement d'autres notions de base de l'apprentissage par renforcement.

En attendant, profitez de l'IA ü§ñ...

> **Important** : Cet article est la 1√®re partie de la s√©rie sur l'apprentissage par renforcement profond. La s√©rie compl√®te sera disponible √† la fois sous forme de texte lisible sur [Medium](https://medium.com/@alamba093) et sous forme de vid√©o explicative sur [ma cha√Æne YouTube](https://www.youtube.com/channel/UCRkxhh51YKqpn2gaUI3MXjg).

Pour une compr√©hension plus approfondie et intuitive de l'apprentissage par renforcement, je vous recommande de regarder la vid√©o ci-dessous :

Abonnez-vous √† ma cha√Æne YouTube pour plus de vid√©os sur l'IA : [**ADL**](https://goo.gl/u72j6u).

_Si vous avez aim√© mon article, veuillez cliquer sur le **?** car je reste motiv√© √† √©crire des articles et veuillez me suivre sur Medium._

![Image](https://cdn-media-1.freecodecamp.org/images/1*z8B3R6kZjTkMKPv3MnUYxg.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*-etmF1WRWkvWO6cSol7f1w.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*7DWddirTA0TDNoAL34xjag.png)

Si vous avez des questions, veuillez me le faire savoir dans un commentaire ci-dessous ou sur [**Twitter**](https://twitter.com/I_AM_ADL). Abonnez-vous √† ma cha√Æne YouTube pour plus de vid√©os tech : [**ADL**](https://goo.gl/u72j6u).