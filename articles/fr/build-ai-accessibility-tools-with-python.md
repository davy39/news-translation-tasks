---
title: Comment cr√©er des outils d'accessibilit√© IA de reconnaissance vocale et de
  synth√®se vocale avec Python
subtitle: ''
author: OMOTAYO OMOYEMI
co_authors: []
series: null
date: '2025-09-01T19:50:40.943Z'
originalURL: https://freecodecamp.org/news/build-ai-accessibility-tools-with-python
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1756755907758/3568b7ab-f659-45c9-8c1a-e877d1a0a166.png
tags:
- name: Accessibility
  slug: accessibility
- name: Python
  slug: python
- name: AI
  slug: ai
seo_title: Comment cr√©er des outils d'accessibilit√© IA de reconnaissance vocale et
  de synth√®se vocale avec Python
seo_desc: Learn how to use AI to make classrooms more inclusive. Build simple Python
  tools for speech-to-text and text-to-speech that support diverse learners.
---

Les salles de classe d'aujourd'hui sont plus diversifi√©es que jamais. Parmi les √©l√®ves se trouvent des apprenants neurodivergents ayant des besoins d'apprentissage diff√©rents. Bien que ces apprenants apportent des forces uniques, les m√©thodes d'enseignement traditionnelles ne r√©pondent pas toujours √† leurs besoins.

C'est l√† que les outils d'accessibilit√© pilot√©s par l'IA peuvent faire la diff√©rence. Du sous-titrage en temps r√©el au support de lecture adaptatif, l'intelligence artificielle transforme les salles de classe en espaces plus inclusifs.

Dans cet article, vous allez :

* Comprendre ce que signifie l'√©ducation inclusive en pratique.
    
* Voir comment l'IA peut soutenir les apprenants neurodivergents.
    
* Essayer deux d√©mos Python pratiques :
    
    * **Reconnaissance vocale (Speech-to-Text)** en utilisant Whisper en local (gratuit, sans cl√© API).
        
    * **Synth√®se vocale (Text-to-Speech)** en utilisant Hugging Face SpeechT5.
        
* Obtenir une structure de projet pr√™te √† l'emploi, les pr√©requis et des conseils de d√©pannage pour les utilisateurs Windows et macOS/Linux.
    

## Table des mati√®res

* [Pr√©requis](#heading-prerequis)
    
* [Remarque sur les fichiers manquants](#heading-remarque-sur-les-fichiers-manquants)
    
* [Ce que l'√©ducation inclusive signifie r√©ellement](#heading-ce-que-leducation-inclusive-signifie-reellement)
    
* [Bo√Æte √† outils : Cinq outils d'accessibilit√© IA que les enseignants peuvent essayer d√®s aujourd'hui](#heading-boite-a-outils-cinq-outils-daccessibilite-ia-pour-les-enseignants)
    
* [Remarques par plateforme (Windows vs macOS/Linux)](#heading-remarques-par-plateforme-windows-vs-macoslinux)
    
* [Guide pratique : Cr√©er une bo√Æte √† outils d'accessibilit√© simple (Python)](#heading-guide-pratique-creer-une-boite-a-outils-daccessibilite-python)
    
* [Antis√®che de configuration rapide](#heading-antiseche-de-configuration-rapide)
    
* [Du code √† l'impact en classe](#heading-du-code-a-limpact-en-classe)
    
* [D√©fi pour les d√©veloppeurs : Coder pour l'inclusion](#heading-defi-pour-les-developpeurs-coder-pour-linclusion)
    
* [D√©fis et consid√©rations](#heading-defis-et-considerations)
    
* [Perspectives d'avenir](#heading-perspectives-davenir)
    
* [Conclusion](#heading-conclusion)
    

## Pr√©requis

Avant de commencer, assurez-vous de disposer des √©l√©ments suivants :

* **Python 3.8** ou version ult√©rieure install√© (pour les utilisateurs Windows, si vous ne l'avez pas, vous pouvez t√©l√©charger la derni√®re version sur : [python.org](http://python.org). Les utilisateurs macOS l'ont g√©n√©ralement d√©j√† via `python3`).
    
* **Environnement virtuel** configur√© (`venv`) ‚Äî recommand√© pour garder votre syst√®me propre.
    
* **Installation de** [**FFmpeg**](https://www.gyan.dev/ffmpeg/builds/#) (requis pour que Whisper puisse lire les fichiers audio).
    
* **PowerShell** (Windows) ou **Terminal** (macOS/Linux).
    
* **Connaissances de base** sur l'ex√©cution de scripts Python.
    

**Conseil** : Si vous d√©butez avec les environnements Python, ne vous inqui√©tez pas car les commandes de configuration seront incluses dans chaque √©tape ci-dessous.

## Remarque sur les fichiers manquants

Certains fichiers ne sont pas inclus dans le [r√©p√¥t GitHub](https://github.com/tayo4christ/inclusive-ai-toolkit). C'est intentionnel, ils sont soit g√©n√©r√©s automatiquement, soit doivent √™tre cr√©√©s/install√©s localement :

* `.venv/` ‚Üí Votre dossier d'environnement virtuel. Chaque lecteur doit cr√©er le sien localement avec :
    
    ```python
    python -m venv .venv
    ```
    
    1. **Installation de FFmpeg** :
        
        * **Windows** : FFmpeg n'est pas inclus dans les fichiers du projet car il est volumineux (environ 90 Mo). Les utilisateurs doivent t√©l√©charger le build FFmpeg s√©par√©ment.
            
        * **macOS** : Les utilisateurs peuvent installer FFmpeg via le gestionnaire de paquets Homebrew avec la commande `brew install ffmpeg`.
            
        * **Linux** : Les utilisateurs peuvent installer FFmpeg via le gestionnaire de paquets avec la commande `sudo apt install ffmpeg`.
            
    2. **Fichier de sortie** :
        
        * `output.wav` est un fichier g√©n√©r√© lorsque vous ex√©cutez le script de synth√®se vocale. Ce fichier n'est pas inclus dans le d√©p√¥t GitHub, il est cr√©√© localement sur votre machine lors de l'ex√©cution du script.
            

Pour garder le d√©p√¥t propre, ces √©l√©ments sont exclus via `.gitignore` :

```python
# Ignorer les environnements virtuels
.venv/
env/
venv/

# Ignorer les fichiers binaires
ffmpeg.exe
*.dll
*.lib

# Ignorer l'audio g√©n√©r√© (mais garder l'entr√©e d'exemple)
*.wav
*.mp3
!lesson_recording.mp3
```

Le d√©p√¥t inclut tous les fichiers essentiels n√©cessaires pour suivre :

* `requirements.txt` (voir ci-dessous)
    
* `transcribe.py` et `tts.py` (couverts √©tape par √©tape dans la section pratique).
    

`requirements.txt`

```python
openai-whisper
transformers
torch
soundfile
sentencepiece
numpy
```

De cette fa√ßon, vous aurez tout ce dont vous avez besoin pour reproduire le projet.

## Ce que l'√©ducation inclusive signifie r√©ellement

L'√©ducation inclusive va au-del√† du simple placement d'√©l√®ves aux besoins divers dans une m√™me classe. Il s'agit de concevoir des environnements d'apprentissage o√π chaque √©l√®ve peut s'√©panouir.

Les barri√®res courantes incluent :

* Difficult√©s de lecture (par exemple, la dyslexie).
    
* D√©fis de communication (troubles de la parole ou de l'audition).
    
* Surcharge sensorielle ou troubles de l'attention (autisme, TDAH).
    
* Difficult√©s de prise de notes et de compr√©hension.
    

L'IA peut aider √† r√©duire ces barri√®res gr√¢ce au sous-titrage, √† la lecture √† voix haute, au rythme adaptatif et aux outils de communication alternative.

## Bo√Æte √† outils : Cinq outils d'accessibilit√© IA que les enseignants peuvent essayer d√®s aujourd'hui

1. [**Lecteur Immersif Microsoft**](https://support.microsoft.com/en-gb/office/use-immersive-reader-in-word-a857949f-c91e-4c97-977c-a4efcaf9b3c1) ‚Äì Synth√®se vocale, guides de lecture et traduction.
    
2. [**Google Live Transcribe**](https://cloud.google.com/speech-to-text) ‚Äì Sous-titres en temps r√©el pour le support auditif.
    
3. [**Otter.ai**](http://Otter.ai) ‚Äì Prise de notes automatique et r√©sum√©.
    
4. [**Grammarly**](https://www.grammarly.com/) **/** [**Quillbot**](https://quillbot.com/login?returnUrl=%2F&triggerOneTap=true) ‚Äì Assistance √† l'√©criture pour la lisibilit√© et la clart√©.
    
5. [**Seeing AI (Microsoft)**](https://blogs.microsoft.com/accessibility/seeing-ai/) ‚Äì D√©crit le texte et les sc√®nes pour les apprenants malvoyants.
    

### Exemples concrets

Un √©l√®ve dyslexique peut utiliser le Lecteur Immersif pour √©couter un manuel tout en suivant visuellement. Un autre √©l√®ve malentendant peut utiliser Live Transcribe pour suivre les discussions en classe. Ce sont de petits changements technologiques qui cr√©ent de grandes victoires en mati√®re d'inclusion.

## Remarques par plateforme (Windows vs macOS/Linux)

La plupart du code fonctionne de la m√™me mani√®re sur tous les syst√®mes, mais les commandes de configuration diff√®rent l√©g√®rement :

**Cr√©ation d'un environnement virtuel**

Pour cr√©er et activer un environnement virtuel dans PowerShell en utilisant Python 3.8 ou sup√©rieur, vous pouvez suivre ces √©tapes :

1. **Cr√©er un environnement virtuel** :
    
    ```powershell
    py -3.12 -m venv .venv
    ```
    
2. **Activer l'environnement virtuel** :
    
    ```powershell
    .\.venv\Scripts\Activate
    ```
    

Une fois activ√©e, votre invite PowerShell devrait changer pour indiquer que vous travaillez maintenant dans l'environnement virtuel. Cette configuration aide √† g√©rer les d√©pendances et √† isoler votre projet.

Pour les utilisateurs macOS, pour cr√©er et activer un environnement virtuel dans un shell bash en utilisant Python 3, vous pouvez suivre ces √©tapes :

1. **Cr√©er un environnement virtuel** :
    
    ```bash
    python3 -m venv .venv
    ```
    
2. **Activer l'environnement virtuel** :
    
    ```bash
    source .venv/bin/activate
    ```
    

**Pour installer FFmpeg sur Windows, suivez ces √©tapes :**

1. **T√©l√©charger le Build FFmpeg** : Visitez le [site officiel](https://www.gyan.dev/ffmpeg/builds/#) de FFmpeg pour t√©l√©charger le dernier build pour Windows.
    
2. **D√©compresser le fichier** : Une fois t√©l√©charg√©, d√©compressez le fichier pour extraire son contenu. Vous trouverez plusieurs fichiers, dont `ffmpeg.exe`.
    
3. **Copier** `ffmpeg.exe` : Vous avez deux options :
    
    * **Dossier du projet** : Copiez `ffmpeg.exe` directement dans le dossier de votre projet.
        
    * **Ajouter au PATH** : Ajoutez le r√©pertoire contenant `ffmpeg.exe` √† la variable d'environnement PATH de votre syst√®me.
        

Le dossier complet du projet est disponible au [t√©l√©chargement sur GitHub](https://github.com/tayo4christ/inclusive-ai-toolkit).

Pour les utilisateurs macOS :

Utilisez Homebrew :

1. **Ouvrir le Terminal**.
    
2. **Installer Homebrew** (si non install√©) : `/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"`
    
3. **Installer FFmpeg** :
    
    ```bash
    brew install ffmpeg
    ```
    

Pour les utilisateurs Linux (Debian/Ubuntu) :

1. **Ouvrir le Terminal**.
    
2. **Mettre √† jour la liste des paquets** : `sudo apt update`
    
3. **Installer FFmpeg** :
    
    ```bash
    sudo apt install ffmpeg
    ```

**Ex√©cuter les scripts Python**

* Windows : `python script.py` ou `py script.py`
    
* macOS/Linux : `python3 script.py`

## Guide pratique : Cr√©er une bo√Æte √† outils d'accessibilit√© simple (Python)

Vous allez cr√©er deux petites d√©mos :

* **Reconnaissance vocale** avec Whisper (local, gratuit).
    
* **Synth√®se vocale** avec Hugging Face SpeechT5.
    

### 1) Reconnaissance vocale avec Whisper (Local et gratuit)

**Ce que vous allez construire :**  
Un script Python qui prend un court enregistrement MP3 et affiche la transcription dans votre terminal.

**Pourquoi Whisper ?**  
C'est un mod√®le de reconnaissance vocale open-source robuste. La version locale est parfaite pour les d√©butants car elle √©vite les cl√©s API et fonctionne hors ligne.

**Comment installer Whisper (PowerShell) :**

```powershell
# Activez votre environnement virtuel
# Exemple : .\venv\Scripts\Activate

# Installez le paquet openai-whisper
pip install openai-whisper

# V√©rifiez si FFmpeg est disponible
ffmpeg -version
```

**Remarque :** Les utilisateurs macOS peuvent utiliser le m√™me code dans leur terminal.

### Cr√©er `transcribe.py` :

```python
import whisper

# Charger le mod√®le Whisper
model = whisper.load_model("base")  # Utilisez "tiny" ou "small" pour plus de rapidit√©

# Transcrire le fichier audio
result = model.transcribe("lesson_recording.mp3", fp16=False)

# Afficher la transcription
print("Transcription :", result["text"])
```

**Fonctionnement du code :**

* `whisper.load_model("base")` ‚Äî t√©l√©charge/charge le mod√®le une fois, puis utilise le cache.
    
* `model.transcribe(...)` ‚Äî g√®re le d√©codage audio, la d√©tection de la langue et l'inf√©rence.
    
* `fp16=False` ‚Äî √©vite les calculs GPU en demi-pr√©cision pour fonctionner sur CPU.
    

Lancez-le :

```bash
python transcribe.py
```

### 2) Synth√®se vocale avec SpeechT5

**Ce que vous allez construire :**  
Un script Python qui convertit une courte phrase en un fichier audio WAV nomm√© `output.wav`.

**Pourquoi SpeechT5 ?**  
C'est un mod√®le ouvert largement utilis√© qui fonctionne sur votre CPU. Aucune cl√© API n'est requise.

**Installer les paquets requis (PowerShell Windows) :**

```powershell
# Activer votre environnement virtuel
# Installer les paquets requis
pip install transformers torch soundfile sentencepiece
```

Cr√©er `tts.py`

```python
from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan
import soundfile as sf
import torch
import numpy as np

# Charger les mod√®les
processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")
vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")

# Embedding du locuteur (graine al√©atoire fixe pour une voix synth√©tique constante)
g = torch.Generator().manual_seed(42)
speaker_embeddings = torch.randn((1, 512), generator=g)

# Texte √† synth√©tiser
text = "Bienvenue dans l'√©ducation inclusive avec l'IA."
inputs = processor(text=text, return_tensors="pt")

# G√©n√©rer la parole
with torch.no_grad():
    speech = model.generate_speech(inputs["input_ids"], speaker_embeddings, vocoder=vocoder)

# Sauvegarder en WAV
sf.write("output.wav", speech.numpy(), samplerate=16000)
print("‚úÖ Audio sauvegard√© sous le nom output.wav")
```

Lancez-le :

```bash
python tts.py
```

**Note :** macOS/Linux utilisent `python3 tts.py`.

### 3) Optionnel : Whisper via l'API OpenAI

**Ce que cela fait :**  
Au lieu d'ex√©cuter Whisper localement, vous appelez l'API OpenAI (`whisper-1`). Votre fichier est transcrit sur les serveurs d'OpenAI.

**Pourquoi utiliser l'API ?**

* Pas besoin d'installer de gros mod√®les localement.
    
* Plus rapide si votre ordinateur est lent.
    

**Configuration de la cl√© API (PowerShell) :**

```powershell
$env:OPENAI_API_KEY="votre_cle_api_ici"
```

**Cr√©er** `transcribe_api.py`

```python
from openai import OpenAI

# Initialiser le client OpenAI
client = OpenAI()

# Ouvrir le fichier audio et cr√©er une transcription
with open("lesson_recording.mp3", "rb") as f:
    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=f
    )

print("Transcription :", transcript.text)
```

**Local Whisper vs API Whisper ‚Äî Lequel utiliser ?**

| Caract√©ristique | Whisper Local (sur votre machine) | API Whisper OpenAI (cloud) |
| --- | --- | --- |
| **Configuration** | N√©cessite paquets Python + FFmpeg | Client `openai` + cl√© API |
| **Mat√©riel** | Utilise votre CPU/GPU | Serveurs OpenAI |
| **Co√ªt** | ‚úÖ Gratuit | üí≥ Payant √† la minute |
| **Internet requis** | ‚ùå Non (une fois install√©) | ‚úÖ Oui |
| **Confidentialit√©** | L'audio reste sur votre machine | L'audio est envoy√© √† OpenAI |

## **Antis√®che de configuration rapide**

| T√¢che | Windows (PowerShell) | macOS / Linux (Terminal) |
| --- | --- | --- |
| **Cr√©er venv** | `py -3.12 -m venv .venv` | `python3 -m venv .venv` |
| **Activer venv** | `.\.venv\Scripts\Activate` | `source .venv/bin/activate` |
| **Installer Whisper** | `pip install openai-whisper` | `pip install openai-whisper` |
| **Installer FFmpeg** | [T√©l√©charger](https://www.gyan.dev/ffmpeg/builds/) -> extraire -> PATH | `brew install ffmpeg` / `sudo apt install ffmpeg` |
| **Lancer STT** | `python transcribe.py` | `python3 transcribe.py` |
| **Lancer TTS** | `python tts.py` | `python3 tts.py` |

## Du code √† l'impact en classe

Vous avez maintenant un prototype fonctionnel capable de :

* Convertir des le√ßons orales en texte.
    
* Lire du texte √† voix haute pour les √©l√®ves qui pr√©f√®rent l'entr√©e auditive.

## D√©fi pour les d√©veloppeurs : Coder pour l'inclusion

Essayez de combiner les deux scripts pour cr√©er une **application compagnon de classe** qui :

* **Sous-titre** ce que l'enseignant dit en temps r√©el.
    
* **Lit √† voix haute** les passages de manuels sur demande.

## D√©fis et consid√©rations

* **Confidentialit√©** : Les donn√©es des √©l√®ves doivent √™tre prot√©g√©es.
    
* **Co√ªt** : Les solutions doivent rester abordables pour les √©coles.
    
* **Formation** : Les enseignants ont besoin de soutien pour utiliser ces outils en toute confiance.

## Perspectives d'avenir

L'avenir de l'√©ducation inclusive passera par l'IA multimodale combinant parole, gestes, symboles et reconnaissance des √©motions, cr√©ant une communication fluide pour tous les apprenants.

## Conclusion

L'IA n'est pas l√† pour remplacer les enseignants, mais pour les aider √† atteindre chaque √©l√®ve. En adoptant l'accessibilit√© pilot√©e par l'IA, les salles de classe deviennent des espaces o√π les apprenants neurodivergents ne sont plus laiss√©s pour compte, mais propuls√©s vers la r√©ussite.

**Code source complet sur GitHub :** [Inclusive AI Toolkit](https://github.com/tayo4christ/inclusive-ai-toolkit)