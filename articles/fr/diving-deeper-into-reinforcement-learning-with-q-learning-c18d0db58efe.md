---
title: Plonger plus profond√©ment dans l'apprentissage par renforcement avec le Q-Learning
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2018-04-10T20:04:07.000Z'
originalURL: https://freecodecamp.org/news/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe
coverImage: https://cdn-media-1.freecodecamp.org/images/1*sYFG8AhKTVnmv_VLRK0c0A.png
tags:
- name: Artificial Intelligence
  slug: artificial-intelligence
- name: Deep Learning
  slug: deep-learning
- name: Machine Learning
  slug: machine-learning
- name: General Programming
  slug: programming
- name: 'tech '
  slug: tech
seo_title: Plonger plus profond√©ment dans l'apprentissage par renforcement avec le
  Q-Learning
seo_desc: 'By Thomas Simonini


  This article is part of Deep Reinforcement Learning Course with Tensorflow ?Ô∏è. Check
  the syllabus here.


  Today we‚Äôll learn about Q-Learning. Q-Learning is a value-based Reinforcement Learning
  algorithm.

  This article is the second ...'
---

Par Thomas Simonini

> Cet article fait partie du cours d'apprentissage par renforcement profond avec Tensorflow üöÄ. Consultez le programme [ici](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/).

Aujourd'hui, nous allons apprendre le Q-Learning. Le Q-Learning est un algorithme d'apprentissage par renforcement bas√© sur la valeur.

Cet article est la deuxi√®me partie d'une s√©rie gratuite de publications de blog sur l'apprentissage par renforcement profond. Pour plus d'informations et de ressources, consultez le [programme du cours](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/). Voir [le premier article ici](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419).

Dans cet article, vous apprendrez :

* Qu'est-ce que le Q-Learning
* Comment l'impl√©menter avec Numpy

### La vue d'ensemble : le Chevalier et la Princesse

![Image](https://cdn-media-1.freecodecamp.org/images/1*h7B4EVx3B-sv5OvHH8nrNw.png)

Imaginons que vous √™tes un chevalier et que vous devez sauver la princesse pi√©g√©e dans le ch√¢teau montr√© sur la carte ci-dessus.

Vous pouvez vous d√©placer d'une case √† la fois. L'ennemi ne peut pas, mais si vous atterrissez sur la m√™me case que l'ennemi, vous mourrez. Votre objectif est d'aller au ch√¢teau par le chemin le plus rapide possible. Cela peut √™tre √©valu√© en utilisant un syst√®me de "notation de points".

* Vous perdez -1 √† chaque √©tape (perdre des points √† chaque √©tape aide notre agent √† √™tre rapide).
* Si vous touchez un ennemi, vous perdez -100 points, et l'√©pisode se termine.
* Si vous √™tes dans le ch√¢teau, vous gagnez, vous obtenez +100 points.

La question est : comment cr√©er un agent capable de faire cela ?

Voici une premi√®re strat√©gie. Disons que notre agent essaie d'aller sur chaque case, puis colorie chaque case. Vert pour "s√ªr", et rouge sinon.

![Image](https://cdn-media-1.freecodecamp.org/images/1*imHK8jFkt6udrUwm8RvOhA.png)
_La m√™me carte, mais color√©e pour montrer quelles cases sont s√ªres √† visiter._

Ensuite, nous pouvons dire √† notre agent de ne prendre que les cases vertes.

Mais le probl√®me est que ce n'est pas vraiment utile. Nous ne savons pas quelle est la meilleure case √† prendre lorsque les cases vertes sont adjacentes les unes aux autres. Ainsi, notre agent peut tomber dans une boucle infinie en essayant de trouver le ch√¢teau !

### Introduction √† la Q-table

Voici une deuxi√®me strat√©gie : cr√©er une table o√π nous calculerons la r√©compense future maximale attendue, pour chaque action √† chaque √©tat.

Gr√¢ce √† cela, nous saurons quelle est la meilleure action √† prendre pour chaque √©tat.

Chaque √©tat (case) permet quatre actions possibles. Il s'agit de se d√©placer √† gauche, √† droite, en haut ou en bas.

![Image](https://cdn-media-1.freecodecamp.org/images/1*kwu9TImqAWZCiooj3pLyCA.png)
_0 sont des mouvements impossibles (si vous √™tes dans le coin sup√©rieur gauche, vous ne pouvez pas aller √† gauche ou en haut !)_

En termes de calcul, nous pouvons transformer cette grille en une table.

Cela s'appelle une **Q-table** ("Q" pour "qualit√©" de l'action). Les colonnes seront les quatre actions (gauche, droite, haut, bas). Les lignes seront les √©tats. La valeur de chaque cellule sera la r√©compense future maximale attendue pour cet √©tat et cette action donn√©s.

![Image](https://cdn-media-1.freecodecamp.org/images/1*fBjmzVXBYdx2-QOXZhnzFQ.png)

Chaque score de la Q-table sera la r√©compense future maximale attendue que j'obtiendrai si je prends cette action dans cet √©tat avec la meilleure politique donn√©e.

Pourquoi disons-nous "avec la politique donn√©e" ? C'est parce que **nous n'impl√©mentons pas de politique.** Au lieu de cela, nous am√©liorons simplement notre Q-table pour toujours choisir la meilleure action.

Pensez √† cette Q-table comme une "feuille de triche" de jeu. Gr√¢ce √† cela, nous savons pour chaque √©tat (chaque ligne dans la Q-table) quelle est la meilleure action √† prendre, en trouvant le score le plus √©lev√© dans cette ligne.

Oui ! Nous avons r√©solu le probl√®me du ch√¢teau ! Mais attendez... Comment calculons-nous les valeurs pour chaque √©l√©ment de la Q-table ?

Pour apprendre chaque valeur de cette Q-table, **nous utiliserons l'algorithme d'apprentissage Q.**

### Algorithme Q-learning : apprendre la fonction de valeur d'action

La fonction de valeur d'action (ou "Q-fonction") prend deux entr√©es : "√©tat" et "action". Elle retourne la r√©compense future attendue de cette action dans cet √©tat.

![Image](https://cdn-media-1.freecodecamp.org/images/1*6IqzImIFK1oEiVWmlu1Esw.png)

Nous pouvons voir cette fonction Q comme un lecteur qui parcourt la Q-table pour trouver la ligne associ√©e √† notre √©tat, et la colonne associ√©e √† notre action. Elle retourne la valeur Q de la cellule correspondante. C'est la "r√©compense future attendue".

![Image](https://cdn-media-1.freecodecamp.org/images/1*yklmxNRdXleiDbv6aSZUIg.png)

Mais avant d'explorer l'environnement, la Q-table donne la m√™me valeur fixe arbitraire (la plupart du temps 0). √Ä mesure que nous explorons l'environnement, la Q-table nous donnera une approximation de mieux en mieux en mettant √† jour de mani√®re it√©rative Q(s,a) en utilisant l'√©quation de Bellman (voir ci-dessous !).

#### Le processus de l'algorithme Q-learning

![Image](https://cdn-media-1.freecodecamp.org/images/1*QeoQEqWYYPs1P8yUwyaJVQ.png)

![Image](https://cdn-media-1.freecodecamp.org/images/0*voKUaGu68-cDuncy.)
_Le pseudo-code de l'algorithme Q-learning_

**√âtape 1 : Initialiser les valeurs Q**  
Nous construisons une Q-table, avec _m_ colonnes (m = nombre d'actions), et _n_ lignes (n = nombre d'√©tats). Nous initialisons les valeurs √† 0.

![Image](https://cdn-media-1.freecodecamp.org/images/1*ut7-8VVa-TWC40_YAeqZ7Q.png)

**√âtape 2 : Pour la vie (ou jusqu'√† ce que l'apprentissage soit arr√™t√©)**  
Les √©tapes 3 √† 5 seront r√©p√©t√©es jusqu'√† ce que nous ayons atteint un nombre maximum d'√©pisodes (sp√©cifi√© par l'utilisateur) ou jusqu'√† ce que nous arr√™tions manuellement l'entra√Ænement.

**√âtape 3 : Choisir une action**  
Choisir une action _a_ dans l'√©tat actuel _s_ en fonction des estimations actuelles des valeurs Q.

Mais... quelle action pouvons-nous prendre au d√©but, si toutes les valeurs Q sont √©gales √† z√©ro ?

C'est l√† que le compromis exploration/exploitation dont nous avons parl√© dans [le dernier article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419) sera important.

L'id√©e est qu'au d√©but, nous utiliserons la strat√©gie epsilon-greedy :

* Nous sp√©cifions un taux d'exploration "epsilon", que nous fixons √† 1 au d√©but. C'est le taux d'√©tapes que nous ferons de mani√®re al√©atoire. Au d√©but, ce taux doit √™tre √† sa valeur la plus √©lev√©e, car nous ne savons rien des valeurs dans la Q-table. Cela signifie que nous devons faire beaucoup d'exploration, en choisissant nos actions de mani√®re al√©atoire.
* Nous g√©n√©rons un nombre al√©atoire. Si ce nombre > epsilon, alors nous ferons de "l'exploitation" (cela signifie que nous utilisons ce que nous savons d√©j√† pour s√©lectionner la meilleure action √† chaque √©tape). Sinon, nous ferons de l'exploration.
* L'id√©e est que nous devons avoir un epsilon √©lev√© au d√©but de l'entra√Ænement de la fonction Q. Ensuite, le r√©duire progressivement √† mesure que l'agent devient plus confiant dans l'estimation des valeurs Q.

![Image](https://cdn-media-1.freecodecamp.org/images/1*9StLEbor62FUDSoRwxyJrg.png)

**√âtape 4-5 : √âvaluer !**  
Prenez l'action _a_ et observez l'√©tat r√©sultant _s'_ et la r√©compense _r_. Maintenant, mettez √† jour la fonction Q(s,a).

Nous prenons l'action _a_ que nous avons choisie √† l'√©tape 3, et l'ex√©cution de cette action nous retourne un nouvel √©tat _s'_ et une r√©compense _r_ (comme nous l'avons vu dans le processus d'apprentissage par renforcement dans [le premier article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)).

Ensuite, pour mettre √† jour Q(s,a), nous utilisons **l'√©quation de Bellman** :

![Image](https://cdn-media-1.freecodecamp.org/images/1*jmcVWHHbzCxDc-irBy9JTw.png)

L'id√©e ici est de mettre √† jour notre Q(√©tat, action) comme suit :

```
Nouvelle valeur Q =    Valeur Q actuelle +    lr * [R√©compense + taux_actualisation * (valeur Q la plus √©lev√©e entre les actions possibles √† partir du nouvel √©tat s' ) ‚Äî Valeur Q actuelle ]
```

Prenons un exemple :

![Image](https://cdn-media-1.freecodecamp.org/images/1*-3MsnOxnipUICgRUWVz9Ng.png)

* Un fromage = +1
* Deux fromages = +2
* Gros tas de fromage = +10 (fin de l'√©pisode)
* Si vous mangez du poison pour rat = -10 (fin de l'√©pisode)

**√âtape 1 : Nous initialisons notre Q-table**

![Image](https://cdn-media-1.freecodecamp.org/images/1*UYB4uCHcwfa2SYlik9HNaQ.png)
_La Q-table initialis√©e_

**√âtape 2 : Choisir une action**   
√Ä partir de la position de d√©part, vous pouvez choisir entre aller √† droite ou en bas. Parce que nous avons un taux epsilon √©lev√© (puisque nous ne savons rien de l'environnement pour l'instant), nous choisissons al√©atoirement. Par exemple... aller √† droite.

![Image](https://cdn-media-1.freecodecamp.org/images/1*IyjuM__mnP-as7m5KTdUyA.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*VY6VFj3RnBMi9sPshouF8A.png)
_Nous nous d√©pla√ßons al√©atoirement (par exemple, √† droite)_

Nous avons trouv√© un morceau de fromage (+1), et nous pouvons maintenant mettre √† jour la valeur Q de l'√©tat de d√©part et de l'action √† droite. Nous faisons cela en utilisant l'√©quation de Bellman.

**√âtape 4-5 : Mettre √† jour la fonction Q**

![Image](https://cdn-media-1.freecodecamp.org/images/1*jmcVWHHbzCxDc-irBy9JTw.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*wzI7Y0s26kw3fQTZx8HZ8A.png)

* Tout d'abord, nous calculons le changement de valeur Q ŒîQ(d√©part, droite)
* Ensuite, nous ajoutons la valeur Q initiale au ŒîQ(d√©part, droite) multipli√© par un taux d'apprentissage.

Pensez au taux d'apprentissage comme une mani√®re de d√©terminer √† quelle vitesse un r√©seau abandonne l'ancienne valeur pour la nouvelle. Si le taux d'apprentissage est 1, la nouvelle estimation sera la nouvelle valeur Q.

![Image](https://cdn-media-1.freecodecamp.org/images/1*IAhKNvQBreGJj2jWN7fleQ.png)
_La Q-table mise √† jour_

Bien ! Nous venons de mettre √† jour notre premi√®re valeur Q. Maintenant, nous devons faire cela encore et encore jusqu'√† ce que l'apprentissage soit arr√™t√©.

### Impl√©menter un algorithme Q-learning

> Nous avons fait une vid√©o o√π nous impl√©mentons un agent Q-learning qui apprend √† jouer √† Taxi-v2 avec Numpy.

Maintenant que nous savons comment cela fonctionne, nous allons impl√©menter l'algorithme Q-learning √©tape par √©tape. Chaque partie du code est expliqu√©e directement dans le notebook Jupyter ci-dessous.

Vous pouvez y acc√©der dans le [d√©p√¥t du cours d'apprentissage par renforcement profond](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/Q%20learning/FrozenLake).

Ou vous pouvez y acc√©der directement sur Google Colaboratory :

[**Q* Learning avec Frozen Lake**](https://colab.research.google.com/drive/17iM0vx848VYWFwW3Du-l-FCn3Y1VhCgx)  
[colab.research.google.com](https://colab.research.google.com/drive/17iM0vx848VYWFwW3Du-l-FCn3Y1VhCgx)

### Un r√©capitulatif...

* Le Q-learning est un algorithme d'apprentissage par renforcement bas√© sur la valeur qui est utilis√© pour trouver la politique optimale de s√©lection d'actions en utilisant une fonction q.
* Il √©value quelle action prendre en fonction d'une fonction de valeur d'action qui d√©termine la valeur d'√™tre dans un certain √©tat et de prendre une certaine action dans cet √©tat.
* Objectif : maximiser la fonction de valeur Q (r√©compense future attendue donn√©e un √©tat et une action).
* La Q-table nous aide √† trouver la meilleure action pour chaque √©tat.
* Pour maximiser la r√©compense attendue en s√©lectionnant la meilleure de toutes les actions possibles.
* Le Q vient de la qualit√© d'une certaine action dans un certain √©tat.
* Fonction Q(√©tat, action) ‚Üí retourne la r√©compense future attendue de cette action dans cet √©tat.
* Cette fonction peut √™tre estim√©e en utilisant le Q-learning, qui met √† jour de mani√®re it√©rative Q(s,a) en utilisant l'√©quation de Bellman.
* Avant d'explorer l'environnement : la Q-table donne la m√™me valeur fixe arbitraire ‚Üí mais √† mesure que nous explorons l'environnement ‚Üí Q nous donne une approximation de mieux en mieux.

C'est tout ! N'oubliez pas d'impl√©menter chaque partie du code par vous-m√™me ‚Äî il est vraiment important d'essayer de modifier le code que je vous ai donn√©.

Essayez d'ajouter des √©poques, changez le taux d'apprentissage, et utilisez un environnement plus difficile (comme Frozen-lake avec des cases de 8x8). Amusez-vous !

La prochaine fois, nous travaillerons sur le Deep Q-learning, l'une des plus grandes avanc√©es en apprentissage par renforcement profond en 2015. Et nous entra√Ænerons un agent qui joue √† Doom et tue des ennemis !

![Image](https://cdn-media-1.freecodecamp.org/images/1*Q4XjhLC0IAOznnk5613PsQ.gif)
_Doom !_

Si vous avez aim√© mon article, **veuillez cliquer sur le üëè ci-dessous autant de fois que vous avez aim√© l'article** afin que d'autres personnes puissent le voir ici sur Medium. Et n'oubliez pas de me suivre !

Si vous avez des pens√©es, des commentaires, des questions, n'h√©sitez pas √† commenter ci-dessous ou √† m'envoyer un email : hello@simoninithomas.com, ou me tweeter [@ThomasSimonini](https://twitter.com/ThomasSimonini).

![Image](https://cdn-media-1.freecodecamp.org/images/1*_yN1FzvEFDmlObiYsstIzg.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*mD-f5VN1SWYvhrZAbvSu_w.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*PqiptT-Cdi8uwosxuFn2DQ.png)

Continuez √† apprendre, restez g√©nial !

#### Cours d'apprentissage par renforcement profond avec Tensorflow üöÄ

üìú [Programme](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)

üìπ [Version vid√©o](https://www.youtube.com/channel/UC8XuSf1eD9AF8x8J19ha5og?view_as=subscriber)

Partie 1 : [Une introduction √† l'apprentissage par renforcement](https://medium.com/p/4339519de419/edit)

Partie 2 : [Plonger plus profond√©ment dans l'apprentissage par renforcement avec le Q-Learning](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)

Partie 3 : [Une introduction au Deep Q-Learning : jouons √† Doom](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)

Partie 3+ : [Am√©liorations dans le Deep Q Learning : Dueling Double DQN, Prioritized Experience Replay, et Q-targets fixes](https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682)

Partie 4 : [Une introduction aux Policy Gradients avec Doom et Cartpole](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)

Partie 5 : [Une introduction aux m√©thodes Advantage Actor Critic : jouons √† Sonic the Hedgehog !](https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d)

Partie 6 : [Proximal Policy Optimization (PPO) avec Sonic the Hedgehog 2 et 3](https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e)

Partie 7 : [L'apprentissage par curiosit√© rendu facile Partie I](https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359)