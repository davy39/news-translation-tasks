---
title: Comment diffuser des t√©l√©chargements de fichiers vers le stockage d'objets
  S3 et r√©duire les co√ªts
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2023-05-23T19:51:05.000Z'
originalURL: https://freecodecamp.org/news/stream-file-uploads-to-s3-object-storage-and-reduce-costs
coverImage: https://www.freecodecamp.org/news/content/images/2023/05/pexels-pixabay-219717.jpg
tags:
- name: Cloud Computing
  slug: cloud-computing
- name: node js
  slug: node-js
- name: S3
  slug: s3
seo_title: Comment diffuser des t√©l√©chargements de fichiers vers le stockage d'objets
  S3 et r√©duire les co√ªts
seo_desc: "By Austin Gil\nTo support file uploads in your application, you will have\
  \ to learn how to send files from the frontend and receive files on the backend.\
  \ \nThis tutorial is going to take a step back and explore architectural changes\
  \ that'll help you red..."
---

Par Austin Gil

Pour prendre en charge les t√©l√©chargements de fichiers dans votre application, vous devrez apprendre √† [envoyer des fichiers depuis le frontend](https://austingil.com/uploading-files-with-html/) et √† [recevoir des fichiers sur le backend](https://austingil.com/file-uploads-in-node/). 

Ce tutoriel va faire un pas en arri√®re et explorer les changements architecturaux qui vous aideront √† r√©duire les co√ªts lors de l'ajout de t√©l√©chargements de fichiers √† vos applications.

## Voici ce que nous allons couvrir :

1. [Qu'est-ce que le stockage d'objets ?](#heading-quest-ce-que-le-stockage-dobjets)
2. [Qu'est-ce que S3 ?](#heading-quest-ce-que-s3)
3. [Commencer avec une application Node.js existante](#id="commencer-avec-une-application-node-js-existante")
4. [Configurer le client S3](#heading-configurer-le-client-s3)
5. [Comment modifier formidable](#heading-comment-modifier-formidable)
6. [Parcours de l'ensemble du flux](#heading-parcours-de-lensemble-du-flux)
7. [Avertissements](#heading-avertissements)
8. [R√©flexions finales](#heading-reflexions-finales)

Et voici une vid√©o que vous pouvez utiliser pour compl√©ter ce tutoriel si vous le souhaitez :

%[https://www.youtube.com/watch?v=cJ6IuSJabXk]

Avant d'aller plus loin, vous devriez d√©j√† √™tre familier avec l'envoi et la r√©ception d'une requ√™te `multipart/form-data`, l'analyse de la requ√™te, l'acc√®s au flux de fichiers et l'√©criture de ce fichier sur le disque **sur le serveur d'application**.

Notez que le flux d√©crit ci-dessus √©crit les fichiers sur le serveur d'application. C'est assez courant, mais il y a quelques probl√®mes avec cette approche.

Premi√®rement, cette approche ne fonctionne pas pour les syst√®mes distribu√©s qui peuvent d√©pendre de plusieurs machines diff√©rentes. Si un utilisateur t√©l√©charge un fichier, il peut √™tre difficile (ou impossible) de savoir quelle machine a re√ßu la requ√™te, et donc, o√π le fichier est enregistr√©. Cela est particuli√®rement vrai si vous utilisez du serverless ou du edge compute.

Deuxi√®mement, stocker les t√©l√©chargements sur le serveur d'application peut entra√Æner un manque d'espace disque sur le serveur. √Ä ce stade, nous devrions mettre √† niveau notre serveur. Cela pourrait √™tre beaucoup plus co√ªteux que d'autres solutions rentables.

Et c'est l√† que le [stockage d'objets](https://www.linode.com/products/object-storage/) entre en jeu.

## Qu'est-ce que le stockage d'objets ?

Vous pouvez penser au stockage d'objets comme √† un dossier sur un ordinateur. Vous pouvez y mettre tous les fichiers (aka "objets") que vous voulez, mais les dossiers (aka "buckets") vivent dans un fournisseur de services cloud. Vous pouvez √©galement acc√©der aux fichiers via une URL.

Le stockage d'objets offre plusieurs avantages :

* C'est un endroit central unique pour stocker et acc√©der √† tous vos t√©l√©chargements.
* Il est con√ßu pour √™tre hautement disponible, facilement scalable et tr√®s rentable.

Par exemple, si vous consid√©rez les [serveurs CPU partag√©s](https://www.linode.com/products/shared/), vous pourriez ex√©cuter une application pour 5 $/mois et obtenir 25 Go d'espace disque. Si votre serveur commence √† manquer d'espace, vous pourriez mettre √† niveau votre serveur pour obtenir 25 Go suppl√©mentaires, mais cela vous co√ªtera 7 $/mois de plus.

Alternativement, vous pourriez mettre cet argent dans le stockage d'objets et vous obtiendriez 250 Go pour 5 $/mois. Donc 10 fois plus d'espace de stockage pour un co√ªt moindre.

Bien s√ªr, il y a d'autres raisons de mettre √† niveau votre serveur d'application. Vous pourriez avoir besoin de plus de RAM ou de CPU, mais si nous parlons uniquement d'espace disque, le stockage d'objets est une solution beaucoup moins ch√®re.

Avec cela √† l'esprit, le reste de cet article couvrira la connexion d'une application Node.js existante √† un fournisseur de stockage d'objets. Nous utiliserons [formidable](https://github.com/node-formidable/formidable) pour analyser les requ√™tes multipart, mais nous le configurerons pour t√©l√©charger les fichiers vers le stockage d'objets au lieu d'√©crire sur le disque.

Si vous souhaitez suivre, vous devrez avoir un bucket de stockage d'objets configur√©, ainsi que les cl√©s d'acc√®s. Tout fournisseur de stockage d'objets compatible S3 devrait fonctionner. 

Aujourd'hui, j'utiliserai les [services de cloud computing d'Akamai](https://bit.ly/austinode) (anciennement Linode). Si vous souhaitez faire de m√™me, [voici un guide qui vous montre comment commencer](https://www.linode.com/docs/products/storage/object-storage/get-started/).

[Et voici un lien pour obtenir 100 $ de cr√©dits gratuits pendant 60 jours](https://bit.ly/austinode).

## Qu'est-ce que S3 ?

Nous allons bient√¥t nous mettre au travail avec du code, mais avant cela, il y a un autre concept que je devrais expliquer : S3. S3 signifie "Simple Storage Service", et c'est un produit de stockage d'objets initialement d√©velopp√© chez AWS.

Avec leur produit, AWS a invent√© un protocole de communication standard pour interagir avec leur solution de stockage d'objets.

Alors que de plus en plus d'entreprises ont commenc√© √† offrir des services de stockage d'objets, elles ont √©galement d√©cid√© d'adopter le m√™me protocole de communication S3 pour leur service de stockage d'objets, et S3 est devenu une norme.

En cons√©quence, nous avons plus d'options pour choisir parmi les fournisseurs de stockage d'objets et moins d'options √† explorer pour les outils. Nous pouvons utiliser les m√™mes biblioth√®ques (maintenues par AWS) avec d'autres fournisseurs. C'est une excellente nouvelle car cela signifie que le code que nous √©crivons aujourd'hui devrait fonctionner avec n'importe quel service compatible S3.

Aujourd'hui, nous allons travailler avec une application Node.js et les biblioth√®ques dont nous aurons besoin sont [`@aws-sdk/client-s3`](https://www.npmjs.com/package/@aws-sdk/client-s3) et [`@aws-sdk/lib-storage`](https://www.npmjs.com/package/@aws-sdk/lib-storage) :

```
npm install @aws-sdk/client-s3 @aws-sdk/lib-storage
```

Ces biblioth√®ques nous aideront √† t√©l√©charger des objets dans nos buckets.

D'accord, √©crivons un peu de code !

## Commencer avec une application Node.js existante

Nous allons commencer avec un exemple de gestionnaire d'√©v√©nements [Nuxt.js](https://nuxt.com/) qui √©crit des fichiers sur le disque en utilisant formidable. Il v√©rifie si une requ√™te contient `multipart/form-data` et, si c'est le cas, il passe l'objet de requ√™te Node.js sous-jacent (aka `IncomingMessage`) √† une fonction personnalis√©e `parseMultipartNodeRequest`. Puisque cette fonction utilise la requ√™te Node.js, elle fonctionnera dans n'importe quel environnement Node.js et avec des outils comme formidable.

```
import formidable from 'formidable';

/* global defineEventHandler, getRequestHeaders, readBody */

/**
 * @see https://nuxt.com/docs/guide/concepts/server-engine
 * @see https://github.com/unjs/h3
 */
export default defineEventHandler(async (event) => {
  let body;
  const headers = getRequestHeaders(event);

  if (headers['content-type']?.includes('multipart/form-data')) {
    body = await parseMultipartNodeRequest(event.node.req);
  } else {
    body = await readBody(event);
  }
  console.log(body);

  return { ok: true };
});

/**
 * @param {import('http').IncomingMessage} req
 */
function parseMultipartNodeRequest(req) {
  return new Promise((resolve, reject) => {
    const form = formidable({ multiples: true });
    form.parse(req, (error, fields, files) => {
      if (error) {
        reject(error);
        return;
      }
      resolve({ ...fields, ...files });
    });
  });
}
```

Nous allons modifier ce code pour envoyer les fichiers vers un bucket S3 au lieu de les √©crire sur le disque.

## Configurer le client S3

La premi√®re chose que nous devons faire est de configurer un client S3 pour effectuer les requ√™tes de t√©l√©chargement pour nous, afin de ne pas avoir √† les √©crire manuellement. Nous allons importer le constructeur `S3Client` de `@aws-sdk/client-s3` ainsi que la commande `Upload` de `@aws-sdk/lib-storage`. Nous allons √©galement importer le module `stream` de Node pour l'utiliser plus tard.

```
import stream from 'node:stream';
import { S3Client } from '@aws-sdk/client-s3';
import { Upload } from '@aws-sdk/lib-storage';
```

Ensuite, nous devons configurer notre client en utilisant l'**endpoint** de notre bucket S3, la [cl√© d'acc√®s](https://www.linode.com/docs/products/storage/object-storage/guides/access-keys/), la [cl√© d'acc√®s secr√®te](https://www.linode.com/docs/products/storage/object-storage/guides/access-keys/), et la **r√©gion**. Encore une fois, vous devriez d√©j√† avoir configur√© un bucket S3 et savoir o√π trouver ces informations. Si ce n'est pas le cas, [consultez ce guide](https://www.linode.com/docs/products/storage/object-storage/get-started/) ([100 $ de cr√©dit](https://bit.ly/austinode)).

J'aime stocker ces informations dans des variables d'environnement et ne pas les coder en dur dans le code source. Nous pouvons acc√©der √† ces variables en utilisant `process.env` pour les utiliser dans notre application.

```
const { S3_URL, S3_ACCESS_KEY, S3_SECRET_KEY, S3_REGION } = process.env;
```

Si vous n'avez jamais utilis√© de variables d'environnement, c'est un bon endroit pour mettre des informations secr√®tes telles que les identifiants d'acc√®s. Vous pouvez [en lire plus √† ce sujet ici](https://nodejs.dev/en/learn/how-to-read-environment-variables-from-nodejs/).

Avec nos variables configur√©es, je peux maintenant instancier le client S3 que nous utiliserons pour communiquer avec notre bucket.

```
const s3Client = new S3Client({
  endpoint: `https://${S3_URL}`,
  credentials: {
    accessKeyId: S3_ACCESS_KEY,
    secretAccessKey: S3_SECRET_KEY,
  },
  region: S3_REGION,
});
```

Il est important de noter que l'endpoint doit inclure le protocole HTTPS. Dans le tableau de bord du stockage d'objets d'Akamai, lorsque vous copiez l'URL du bucket, elle n'inclut pas le protocole (`bucket-name.bucket-region.linodeobjects.com`). Je l'ajoute donc ici comme pr√©fixe.

Avec notre client S3 configur√©, nous pouvons commencer √† l'utiliser.

## Comment modifier formidable

Dans notre application, nous passons toute requ√™te multipart Node √† notre fonction personnalis√©e, `parseMultipartNodeRequest`. Cette fonction retourne une promesse et passe la requ√™te √† formidable, qui analyse la requ√™te, √©crit les fichiers sur le disque et r√©sout la promesse avec les donn√©es des champs de formulaire et des fichiers.

```
function parseMultipartNodeRequest(req) {
  return new Promise((resolve, reject) => {
    const form = formidable({ multiples: true });
    form.parse(req, (error, fields, files) => {
      if (error) {
        reject(error);
        return;
      }
      resolve({ ...fields, ...files });
    });
  });
}
```

C'est la partie qui doit changer. Au lieu de traiter la requ√™te et d'√©crire les fichiers sur le disque, nous voulons rediriger les flux de fichiers vers une requ√™te de t√©l√©chargement S3. Ainsi, chaque fragment de fichier re√ßu est pass√© par notre gestionnaire √† la requ√™te de t√©l√©chargement S3.

Nous allons toujours retourner une promesse et utiliser formidable pour analyser le formulaire, mais nous devons changer les [options de configuration](https://github.com/node-formidable/formidable#options) de formidable. Nous allons d√©finir l'option `fileWriteStreamHandler` sur une fonction appel√©e `fileWriteStreamHandler` que nous allons √©crire prochainement.

```
/** @param {import('formidable').File} file */
function fileWriteStreamHandler(file) {
  // TODO
}
const form = formidable({
  multiples: true,
  fileWriteStreamHandler: fileWriteStreamHandler,
});
```

Voici ce que dit leur documentation sur `fileWriteStreamHandler` :

> `options.fileWriteStreamHandler` **{function}** ‚Äì par d√©faut `null`, ce qui par d√©faut √©crit sur le syst√®me de fichiers de la machine h√¥te chaque fichier analys√© ; La fonction doit retourner une instance d'un [flux Writable](https://nodejs.org/api/stream.html#stream_class_stream_writable) qui recevra les donn√©es du fichier t√©l√©charg√©. Avec cette option, vous pouvez avoir n'importe quel comportement personnalis√© concernant l'endroit o√π les donn√©es du fichier t√©l√©charg√© seront diffus√©es. Si vous souhaitez √©crire le fichier t√©l√©charg√© dans d'autres types de stockages cloud (AWS S3, Azure blob storage, Google cloud storage) ou de stockage de fichiers priv√©, c'est l'option que vous recherchez. Lorsque cette option est d√©finie, le comportement par d√©faut d'√©criture du fichier dans le syst√®me de fichiers de la machine h√¥te est perdu.

Alors que formidable analyse chaque fragment de donn√©es de la requ√™te, il redirigera ce fragment vers le flux Writable qui est retourn√© par cette fonction. Ainsi, notre fonction `fileWriteStreamHandler` est l'endroit o√π la magie op√®re.

Avant d'√©crire le code, comprenons quelques choses :

1. Cette fonction doit retourner un [flux Writable](https://nodejs.org/api/stream.html#stream_class_stream_writable) pour √©crire chaque fragment de t√©l√©chargement.
2. Elle **doit √©galement** rediriger chaque fragment de donn√©es vers un stockage d'objets S3.
3. Nous pouvons utiliser la commande `Upload` de `@aws-sdk/lib-storage` pour cr√©er la requ√™te.
4. Le corps de la requ√™te peut √™tre un flux, mais il doit √™tre un [flux Readable](https://nodejs.org/api/stream.html#stream_class_stream_readable), et non un flux Writable.
5. Un [flux Passthrough](https://nodejs.org/api/stream.html#stream_class_stream_passthrough) peut √™tre utilis√© √† la fois comme flux Readable et Writable.
6. Chaque requ√™te que formidable analysera peut contenir plusieurs fichiers, donc nous devons peut-√™tre suivre plusieurs requ√™tes de t√©l√©chargement S3.
7. `fileWriteStreamHandler` re√ßoit un param√®tre de type [`formidable.File` interface](https://github.com/node-formidable/formidable#file) avec des propri√©t√©s comme `originalFilename`, `size`, `mimetype`, et plus encore.

D'accord, √©crivons maintenant le code. Nous allons commencer par un `Array` pour stocker et suivre toutes les requ√™tes de t√©l√©chargement S3 en dehors de la port√©e de `fileWriteStreamHandler`. 

√Ä l'int√©rieur de `fileWriteStreamHandler`, nous allons cr√©er le flux `Passthrough` qui servira √† la fois de corps Readable du t√©l√©chargement S3 et de valeur de retour Writable de cette fonction. 

Nous allons cr√©er la requ√™te `Upload` en utilisant les biblioth√®ques S3, et lui indiquer le nom de notre bucket, la cl√© de l'objet (qui peut inclure des dossiers), le type de contenu de l'objet, le niveau de contr√¥le d'acc√®s pour cet objet, et le flux `Passthrough` comme corps de la requ√™te. 

Nous allons instancier la requ√™te en utilisant `Upload.done()` et ajouter la `Promise` retourn√©e √† notre `Array` de suivi. Nous pourrions vouloir ajouter la propri√©t√© `Location` de la r√©ponse √† l'objet `file` lorsque le t√©l√©chargement est termin√©, afin de pouvoir utiliser cette information plus tard. 

Enfin, nous allons retourner le flux `Passthrough` de cette fonction :

```
/** @type {Promise<any>[]} */
const s3Uploads = [];

/** @param {import('formidable').File} file */
function fileWriteStreamHandler(file) {
  const body = new stream.PassThrough();
  const upload = new Upload({
    client: s3Client,
    params: {
      Bucket: 'austins-bucket',
      Key: `files/${file.originalFilename}`,
      ContentType: file.mimetype,
      ACL: 'public-read',
      Body: body,
    },
  });
  const uploadRequest = upload.done().then((response) => {
    file.location = response.Location;
  });
  s3Uploads.push(uploadRequest);
  return body;
}
```

Quelques points √† noter :

* `Key` est le nom et l'emplacement o√π l'objet existera. Il peut inclure des dossiers qui seront cr√©√©s s'ils n'existent pas actuellement. Si un fichier existe avec le m√™me nom et le m√™me emplacement, il sera √©cras√© (ce qui me convient aujourd'hui). Vous pouvez √©viter les collisions en utilisant des noms hach√©s ou des horodatages.
* `ContentType` n'est pas obligatoire, mais il est utile de l'inclure. Il permet aux navigateurs de cr√©er la r√©ponse de t√©l√©chargement de mani√®re appropri√©e en fonction du type de contenu.
* `ACL` : est √©galement facultatif, mais par d√©faut, chaque objet est priv√©. Si vous voulez que les gens puissent acc√©der aux fichiers via une URL (comme un √©l√©ment `<img>`), vous devrez le rendre public.
* Bien que `@aws-sdk/client-s3` prenne en charge les t√©l√©chargements, vous avez besoin de `@aws-sdk/lib-storage` pour prendre en charge les flux Readable.
* Vous pouvez en lire plus sur les param√®tres [sur NPM](https://www.npmjs.com/package/@aws-sdk/client-s3).

De cette mani√®re, formidable devient la plomberie qui connecte la requ√™te cliente entrante √† la requ√™te de t√©l√©chargement S3.

Il ne reste plus qu'un changement √† faire. Nous suivons toutes les requ√™tes de t√©l√©chargement, mais nous n'attendons pas qu'elles se terminent.

Nous pouvons corriger cela en modifiant la fonction `parseMultipartNodeRequest`. Elle doit continuer √† utiliser formidable pour analyser la requ√™te cliente, mais au lieu de r√©soudre la promesse imm√©diatement, nous pouvons utiliser `Promise.all` pour attendre que toutes les requ√™tes de t√©l√©chargement soient r√©solues.

La fonction compl√®te ressemble √† ceci :

```
/**
 * @param {import('http').IncomingMessage} req
 */
function parseMultipartNodeRequest(req) {
  return new Promise((resolve, reject) => {
    /** @type {Promise<any>[]} */
    const s3Uploads = [];

    /** @param {import('formidable').File} file */
    function fileWriteStreamHandler(file) {
      const body = new PassThrough();
      const upload = new Upload({
        client: s3Client,
        params: {
          Bucket: 'austins-bucket',
          Key: `files/${file.originalFilename}`,
          ContentType: file.mimetype,
          ACL: 'public-read',
          Body: body,
        },
      });
      const uploadRequest = upload.done().then((response) => {
        file.location = response.Location;
      });
      s3Uploads.push(uploadRequest);
      return body;
    }
    const form = formidable({
      multiples: true,
      fileWriteStreamHandler: fileWriteStreamHandler,
    });
    form.parse(req, (error, fields, files) => {
      if (error) {
        reject(error);
        return;
      }
      Promise.all(s3Uploads)
        .then(() => {
          resolve({ ...fields, ...files });
        })
        .catch(reject);
    });
  });
}
```

La valeur r√©solue `files` contiendra √©galement la propri√©t√© `location` que nous avons incluse, pointant vers l'URL du stockage d'objets.

## Parcours de l'ensemble du flux

Nous avons couvert beaucoup de choses, et je pense qu'il est bon de revoir comment tout fonctionne ensemble. Si nous regardons en arri√®re le gestionnaire d'√©v√©nements original, nous pouvons voir que toute requ√™te `multipart/form-data` sera re√ßue et pass√©e √† notre fonction `parseMultipartNodeRequest`. La valeur r√©solue de cette fonction sera enregistr√©e dans la console :

```
export default defineEventHandler(async (event) => {
  let body;
  const headers = getRequestHeaders(event);

  if (headers['content-type']?.includes('multipart/form-data')) {
    body = await parseMultipartNodeRequest(event.node.req);
  } else {
    body = await readBody(event);
  }
  console.log(body);

  return { ok: true };
});
```

Avec cela √† l'esprit, d√©composons ce qui se passe si je veux t√©l√©charger [une photo mignonne de Nugget faisant un grand b√¢illement](https://www.instagram.com/p/CmUxW6jDmWO/).

1. Pour que le navigateur envoie le fichier sous forme de donn√©es binaires, il doit faire une requ√™te `multiplart/form-data` avec un [formulaire HTML](https://austingil.com/uploading-files-with-html/) ou [avec JavaScript](https://austingil.com/upload-files-with-javascript/).
2. Notre application [Nuxt.js](https://nuxt.com/) re√ßoit la requ√™te `multipart/form-data` et passe l'objet de requ√™te Node.js sous-jacent √† notre fonction personnalis√©e `parseMultipartNodeRequest`.
3. `parseMultipartNodeRequest` retourne une `Promise` qui sera √©ventuellement r√©solue avec les donn√©es. √Ä l'int√©rieur de cette `Promise`, nous instancions la biblioth√®que formidable et passons l'objet de requ√™te √† formidable pour l'analyse.
4. Alors que formidable analyse la requ√™te, lorsqu'il rencontre un fichier, il √©crit les fragments de donn√©es du flux de fichiers dans le flux `Passthrough` qui est retourn√© par la fonction `fileWriteStreamHandler`.
5. √Ä l'int√©rieur de `fileWriteStreamHandler`, nous configurons √©galement une requ√™te pour t√©l√©charger le fichier vers notre bucket compatible S3, et nous utilisons le m√™me flux `Passthrough` comme corps de la requ√™te. Ainsi, alors que formidable √©crit des fragments de donn√©es de fichiers dans le flux `Passthrough`, ils sont √©galement lus par la requ√™te de t√©l√©chargement S3.
6. Une fois que formidable a termin√© l'analyse de la requ√™te, tous les fragments de donn√©es des flux de fichiers sont pris en charge, et nous attendons que la liste des requ√™tes S3 termine le t√©l√©chargement.
7. Apr√®s que tout cela est fait, nous r√©solvons la `Promise` de `parseMultipartNodeRequest` avec les donn√©es modifi√©es de formidable. La variable `body` est assign√©e √† la valeur r√©solue.
8. Les donn√©es repr√©sentant les champs et les fichiers (pas les fichiers eux-m√™mes) sont enregistr√©es dans la console.

Donc maintenant, si notre requ√™te de t√©l√©chargement originale contenait un seul champ appel√© "file1" avec la photo de Nugget, nous pourrions voir quelque chose comme ceci :

```
{
  file1: {
    _events: [Object: null prototype] { error: [Function (anonymous)] },
    _eventsCount: 1,
    _maxListeners: undefined,
    lastModifiedDate: null,
    filepath: '/tmp/93374f13c6cab7a01f7cb5100',
    newFilename: '93374f13c6cab7a01f7cb5100',
    originalFilename: 'nugget.jpg',
    mimetype: 'image/jpeg',
    hashAlgorithm: false,
    createFileWriteStream: [Function: fileWriteStreamHandler],
    size: 82298,
    _writeStream: PassThrough {
      _readableState: [ReadableState],
      _events: [Object: null prototype],
      _eventsCount: 6,
      _maxListeners: undefined,
      _writableState: [WritableState],
      allowHalfOpen: true,
      [Symbol(kCapture)]: false,
      [Symbol(kCallback)]: null
    },
    hash: null,
    location: 'https://austins-bucket.us-southeast-1.linodeobjects.com/files/nugget.jpg',
    [Symbol(kCapture)]: false
  }
}
```

Cela ressemble beaucoup √† l'objet que formidable retourne lorsqu'il √©crit directement sur le disque. Mais cette fois, il a une propri√©t√© suppl√©mentaire, `location`, qui est l'URL du stockage d'objets pour notre fichier t√©l√©charg√©.

Jetez ce truc dans votre navigateur et qu'obtenez-vous ?

![Capture d'√©cran de mon navigateur montrant une photo mignonne de Nugget faisant un grand b√¢illement, et il y a une bo√Æte mettant en √©vidence l'URL d'Akamai Object Storage.](https://austingil.com/wp-content/uploads/image-65-1080x608.png)

C'est √ßa ! Une photo mignonne de Nugget faisant un grand b√¢illement üòä

Je peux √©galement aller dans mon bucket dans mon [tableau de bord Object Storage](https://cloud.linode.com/object-storage) et voir que j'ai maintenant un dossier appel√© "files" contenant un fichier appel√© "nugget.jpg".

![Capture d'√©cran de mon tableau de bord Akamai Object Storage montrant "nugget.jpg" √† l'int√©rieur du dossier "files" √† l'int√©rieur de l'instance de stockage d'objets "austins-bucket".](https://austingil.com/wp-content/uploads/image-62-1080x568.png)

## Avertissements

Je serais n√©gligent si je ne mentionnais pas ce qui suit. (En fait, j'ai **√©t√©** n√©gligent parce que je ne l'ai pas mentionn√© jusqu'√† ce que quelqu'un me l'ait fait remarquer üò≥)

Le streaming des t√©l√©chargements via votre backend vers le stockage d'objets n'est pas la seule fa√ßon de t√©l√©charger des fichiers vers S3. Vous pouvez √©galement utiliser des **URL sign√©es**. 

Les URL sign√©es sont essentiellement la m√™me URL dans le bucket o√π le fichier vivra, mais elles incluent une signature d'authentification qui peut √™tre utilis√©e par n'importe qui pour t√©l√©charger un fichier, tant que la signature n'a pas expir√© (g√©n√©ralement tr√®s bient√¥t).

Voici comment le flux fonctionne g√©n√©ralement :

1. Le frontend fait une requ√™te au backend pour une URL sign√©e.
2. Le backend fait une requ√™te authentifi√©e au fournisseur de stockage d'objets pour une URL sign√©e avec une expiration donn√©e.
3. Le fournisseur de stockage d'objets fournit une URL sign√©e au backend.
4. Le backend retourne l'URL sign√©e au frontend.
5. Le frontend t√©l√©charge le fichier directement vers le stockage d'objets gr√¢ce √† l'URL sign√©e.
6. Optionnel : Le frontend peut faire une autre requ√™te au backend si vous devez mettre √† jour une base de donn√©es indiquant que le t√©l√©chargement est termin√©.

Ce flux n√©cessite un peu plus de chor√©graphie que Frontend -> Backend -> Stockage d'objets, mais il a quelques avantages.

* Il d√©place le travail de vos serveurs, ce qui peut r√©duire la charge et am√©liorer les performances.
* Il d√©place la bande passante de t√©l√©chargement de fichiers de votre serveur. Si vous payez pour l'ingress et que vous avez plusieurs t√©l√©chargements de gros fichiers tout le temps, cela peut s'additionner.

Cela comporte √©galement ses propres co√ªts.

* Vous avez beaucoup moins de contr√¥le sur ce que les utilisateurs peuvent t√©l√©charger. Cela peut inclure des logiciels malveillants.
* Si vous devez effectuer des fonctions sur les fichiers comme l'optimisation, vous ne pouvez pas le faire avec des URL sign√©es.
* Le flux complexe rend beaucoup plus difficile la construction d'un flux de t√©l√©chargement avec une am√©lioration progressive √† l'esprit.

Comme pour la plupart des choses en d√©veloppement web, il n'y a pas une seule solution correcte. Cela d√©pendra largement de votre cas d'utilisation. J'aime passer par mon backend, afin d'avoir plus de contr√¥le sur les fichiers et de simplifier le frontend.

Je voulais partager cette option de streaming, largement parce qu'il y a √† peine du contenu sur le streaming. La plupart du contenu utilise des URL sign√©es (peut-√™tre que je manque quelque chose). Si vous souhaitez en savoir plus sur l'utilisation des URL sign√©es, [voici une documentation](https://www.linode.com/docs/products/storage/object-storage/guides/urls/#signed-urls) et [voici un tutoriel pratique](https://dev.to/gathoni/how-to-upload-an-image-to-a-linode-storage-bucket-using-a-pre-signed-url-1ba7) par [Mary Gathoni](https://twitter.com/remigathoni).

## R√©flexions finales

D'accord, nous avons couvert beaucoup de choses aujourd'hui. J'esp√®re que tout cela avait du sens. Si ce n'est pas le cas, n'h√©sitez pas √† me contacter avec vos questions. De plus, contactez-moi et faites-moi savoir si vous l'avez fait fonctionner dans votre propre application.

J'adorerais avoir de vos nouvelles, car l'utilisation du stockage d'objets est une excellente d√©cision architecturale si vous avez besoin d'un endroit unique et rentable pour stocker des fichiers.

Merci beaucoup d'avoir lu. Si vous avez aim√© cet article et souhaitez me soutenir, les meilleures fa√ßons de le faire sont de [le partager](https://twitter.com/share?via=heyAustinGil), [de vous inscrire √† ma newsletter](https://austingil.com/newsletter/), et [de me suivre sur Twitter](https://twitter.com/heyAustinGil).