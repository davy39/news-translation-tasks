---
title: 'Big Picture Machine Learning: Classifier du texte avec des r√©seaux de neurones
  et TensorFlow'
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2017-04-09T21:49:41.000Z'
originalURL: https://freecodecamp.org/news/big-picture-machine-learning-classifying-text-with-neural-networks-and-tensorflow-d94036ac2274
coverImage: https://cdn-media-1.freecodecamp.org/images/1*W2vzGrXR1ua5KN-X0m9oMw.jpeg
tags:
- name: Artificial Intelligence
  slug: artificial-intelligence
- name: Data Science
  slug: data-science
- name: Machine Learning
  slug: machine-learning
- name: technology
  slug: technology
- name: TensorFlow
  slug: tensorflow
seo_title: 'Big Picture Machine Learning: Classifier du texte avec des r√©seaux de
  neurones et TensorFlow'
seo_desc: 'By D√©borah Mesquita

  Developers often say that if you want to get started with machine learning, you
  should first learn how the algorithms work. But my experience shows otherwise.

  I say you should first be able to see the big picture: how the applicat...'
---

Par D√©bora Mesquita

Les d√©veloppeurs disent souvent que si vous voulez commencer avec le machine learning, vous devriez d'abord apprendre comment les algorithmes fonctionnent. Mais mon exp√©rience montre le contraire.

Je dis que vous devriez d'abord √™tre capable de voir le big picture : **comment les applications fonctionnent**. Une fois que vous comprenez cela, il devient beaucoup plus facile de plonger en profondeur et d'explorer le fonctionnement interne des algorithmes.

Alors, comment d√©velopper une intuition et atteindre cette compr√©hension globale du machine learning ? Une bonne fa√ßon de faire cela est de **cr√©er des mod√®les de machine learning**.

En supposant que vous ne savez toujours pas comment cr√©er tous ces algorithmes √† partir de z√©ro, vous voudrez utiliser une biblioth√®que qui a d√©j√† tous ces algorithmes impl√©ment√©s pour vous. Et cette biblioth√®que est **TensorFlow**.

Dans cet article, nous allons cr√©er un mod√®le de machine learning pour classer des textes en cat√©gories. Nous allons couvrir les sujets suivants :

1. **Comment TensorFlow fonctionne**
2. **Qu'est-ce qu'un mod√®le de machine learning**
3. **Qu'est-ce qu'un r√©seau de neurones**
4. **Comment le r√©seau de neurones apprend**
5. **Comment manipuler les donn√©es et les passer aux entr√©es du r√©seau de neurones**
6. **Comment ex√©cuter le mod√®le et obtenir les r√©sultats de pr√©diction**

Vous allez probablement apprendre beaucoup de nouvelles choses, alors commen√ßons ! üöÄ

### TensorFlow

[TensorFlow](https://www.tensorflow.org/) est une biblioth√®que open-source pour le machine learning, cr√©√©e √† l'origine par Google. Le nom de la biblioth√®que nous aide √† comprendre comment nous travaillons avec elle : les tenseurs sont des tableaux multidimensionnels qui circulent √† travers les n≈ìuds d'un graphe.

#### tf.Graph

Chaque calcul dans TensorFlow est repr√©sent√© comme un graphe de flux de donn√©es. Ce graphe a deux √©l√©ments :

* un ensemble de `tf.Operation`, qui repr√©sente des unit√©s de calcul
* un ensemble de `tf.Tensor`, qui repr√©sente des unit√©s de donn√©es

Pour voir comment tout cela fonctionne, vous allez cr√©er ce graphe de flux de donn√©es :

![Image](https://cdn-media-1.freecodecamp.org/images/FqZ0WWUano-1PMy9ip69CsKrZYFjlaC4iozV)
_Un graphe qui calcule x+y_

Vous allez d√©finir `x = [1,3,6]` et `y = [1,1,1]`. Comme le graphe fonctionne avec `tf.Tensor` pour repr√©senter les unit√©s de donn√©es, vous allez cr√©er des tenseurs constants :

```
import tensorflow as tf
```

```
x = tf.constant([1,3,6]) y = tf.constant([1,1,1])
```

Maintenant, vous allez d√©finir l'unit√© d'op√©ration :

```
import tensorflow as tf
```

```
x = tf.constant([1,3,6]) y = tf.constant([1,1,1])
```

```
op = tf.add(x,y)
```

Vous avez tous les √©l√©ments du graphe. Maintenant, vous devez construire le graphe :

```
import tensorflow as tf
```

```
my_graph = tf.Graph()
```

```
with my_graph.as_default():    x = tf.constant([1,3,6])     y = tf.constant([1,1,1])
```

```
    op = tf.add(x,y)
```

Voici comment fonctionne le flux de travail de TensorFlow : vous cr√©ez d'abord un graphe, et seulement ensuite vous pouvez effectuer les calculs (ex√©cuter r√©ellement les n≈ìuds du graphe avec des op√©rations). Pour ex√©cuter le graphe, vous devrez cr√©er une `tf.Session`.

#### tf.Session

Un objet `tf.Session` encapsule l'environnement dans lequel les objets `Operation` sont ex√©cut√©s et les objets `Tensor` sont √©valu√©s (d'apr√®s [la documentation](https://www.tensorflow.org/api_docs/python/tf/Session)). Pour cela, nous devons d√©finir quel graphe sera utilis√© dans la Session :

```
import tensorflow as tf
```

```
my_graph = tf.Graph()
```

```
with tf.Session(graph=my_graph) as sess:    x = tf.constant([1,3,6])     y = tf.constant([1,1,1])
```

```
    op = tf.add(x,y)
```

Pour ex√©cuter les op√©rations, vous utiliserez la m√©thode `tf.Session.run()`. Cette m√©thode ex√©cute une "√©tape" du calcul TensorFlow, en ex√©cutant le fragment de graphe n√©cessaire pour ex√©cuter chaque objet `Operation` et √©valuer chaque `Tensor` pass√© dans l'argument `fetches`. Dans votre cas, vous allez ex√©cuter une √©tape des op√©rations de somme :

```
import tensorflow as tf
```

```
my_graph = tf.Graph()
```

```
with tf.Session(graph=my_graph) as sess:    x = tf.constant([1,3,6])     y = tf.constant([1,1,1])
```

```
    op = tf.add(x,y)    result = sess.run(fetches=op)    print(result)
```

```
>>>; [2 4 7]
```

### Un mod√®le pr√©dictif

Maintenant que vous savez comment TensorFlow fonctionne, vous devez apprendre √† cr√©er un mod√®le pr√©dictif. En bref,

**Algorithme de machine learning** + **donn√©es** = **mod√®le pr√©dictif**

Le processus de construction d'un mod√®le est le suivant :

![Image](https://cdn-media-1.freecodecamp.org/images/gdMcFEDuaj8M7U0IQsxh77naKKo8vRKbsROi)
_Le processus de cr√©ation d'un mod√®le pr√©dictif_

Comme vous pouvez le voir, le mod√®le consiste en un algorithme de machine learning "entra√Æn√©" avec des donn√©es. Lorsque vous avez le mod√®le, vous obtiendrez des r√©sultats comme ceci :

![Image](https://cdn-media-1.freecodecamp.org/images/GKPyWdL-Z31sZhmSFqGGH6y6Bw6VAqiu7PaJ)
_Flux de travail de pr√©diction_

Le but du mod√®le que vous allez cr√©er est de classer les textes en cat√©gories, nous d√©finissons cela :

**entr√©e** : texte, **r√©sultat** : cat√©gorie

Nous avons un ensemble de donn√©es d'entra√Ænement avec tous les textes √©tiquet√©s (chaque texte a une √©tiquette indiquant √† quelle cat√©gorie il appartient). En machine learning, ce type de t√¢che est appel√© **apprentissage supervis√©**.

> "Nous connaissons les bonnes r√©ponses. L'algorithme fait des pr√©dictions de mani√®re it√©rative sur les donn√©es d'entra√Ænement et est corrig√© par l'enseignant." ‚Äî [Jason Brownlee](http://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/)

Vous allez classer les donn√©es en cat√©gories, donc c'est aussi une t√¢che de **classification**.

Pour cr√©er le mod√®le, nous allons utiliser des r√©seaux de neurones.

### R√©seaux de neurones

Un r√©seau de neurones est un mod√®le computationnel (une fa√ßon de d√©crire un syst√®me en utilisant le langage math√©matique et les concepts math√©matiques). Ces syst√®mes sont auto-apprenants et entra√Æn√©s, plut√¥t que programm√©s explicitement.

Les r√©seaux de neurones sont inspir√©s par notre syst√®me nerveux central. Ils ont des n≈ìuds connect√©s qui sont similaires √† nos neurones.

![Image](https://cdn-media-1.freecodecamp.org/images/ngj7GZs1s5M0eOI3RyNVKqqj17menI4lJjMr)
_Un r√©seau de neurones_

Le Perceptron √©tait le premier algorithme de r√©seau de neurones. [Cet article](https://appliedgo.net/perceptron/) explique tr√®s bien le fonctionnement interne d'un perceptron (l'animation "Inside an artificial neuron" est fantastique).

Pour comprendre comment fonctionne un r√©seau de neurones, nous allons en fait construire une architecture de r√©seau de neurones avec TensorFlow. Cette architecture a √©t√© utilis√©e par [Aymeric Damien](https://github.com/aymericdamien) dans [cet exemple](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/multilayer_perceptron.ipynb).

#### Architecture du r√©seau de neurones

Le r√©seau de neurones aura 2 couches cach√©es ([vous devez choisir](http://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw) combien de couches cach√©es le r√©seau aura, cela fait partie de la conception de l'architecture). Le travail de chaque couche cach√©e est de [transformer les entr√©es en quelque chose que la couche de sortie peut utiliser](http://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute).

**Couche cach√©e 1**

![Image](https://cdn-media-1.freecodecamp.org/images/6YQgGFOd-HLll7zGB7gA-sjCkVb4RYuZLnka)
_Couche d'entr√©e et 1√®re couche cach√©e_

Vous devez √©galement d√©finir combien de n≈ìuds la 1√®re couche cach√©e aura. Ces n≈ìuds sont √©galement appel√©s caract√©ristiques ou neurones, et dans l'image ci-dessus, ils sont repr√©sent√©s par chaque cercle.

Dans la couche d'entr√©e, chaque n≈ìud correspond √† un mot du jeu de donn√©es (nous verrons comment cela fonctionne plus tard).

Comme expliqu√© [ici](https://appliedgo.net/perceptron/), chaque n≈ìud (neurone) est multipli√© par un poids. Chaque n≈ìud a une valeur de poids, et pendant la phase d'entra√Ænement, le r√©seau de neurones ajuste ces valeurs afin de produire une sortie correcte (attendez, nous en apprendrons plus √† ce sujet dans une minute).

En plus de multiplier chaque n≈ìud d'entr√©e par un poids, le r√©seau ajoute √©galement un biais ([r√¥le du biais dans les r√©seaux de neurones](http://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks)).

Dans votre architecture, apr√®s avoir multipli√© les entr√©es par les poids et additionn√© les valeurs au biais, les donn√©es passent √©galement par une **fonction d'activation**. Cette fonction d'activation d√©finit la sortie finale de chaque n≈ìud. Une analogie : imaginez que chaque n≈ìud est une lampe, la fonction d'activation indique si la lampe s'allumera ou non.

Il existe [de nombreux types de fonctions d'activation](https://en.wikipedia.org/wiki/Activation_function). Vous utiliserez l'unit√© lin√©aire rectifi√©e (ReLu). Cette fonction est d√©finie ainsi :

_f(x)_ = _max(0,x)_ [la sortie est x ou 0 (z√©ro), selon la valeur la plus grande]

Exemples : si **_x_ = -1**, alors **_f(x) = 0_**_(z√©ro) ; si **x = 0.7**, alors **f(x) = 0.7**_.

**Couche cach√©e 2**

La 2√®me couche cach√©e fait exactement ce que la 1√®re couche cach√©e fait, mais maintenant l'entr√©e de la 2√®me couche cach√©e est la sortie de la 1√®re.

![Image](https://cdn-media-1.freecodecamp.org/images/hvIts6lxc2bXXMQzEA8af3Fk760Ih2zjoFl3)
_1√®re et 2√®me couches cach√©es_

**Couche de sortie**

Et nous arrivons enfin √† la derni√®re couche, la couche de sortie. Vous utiliserez le [codage one-hot](https://en.wikipedia.org/wiki/One-hot) pour obtenir les r√©sultats de cette couche. Dans ce codage, un seul bit a la valeur 1 et tous les autres ont une valeur z√©ro. Par exemple, si nous voulons coder trois cat√©gories (sports, espace et infographie) :

```
+-------------------+-----------+|    cat√©gorie       |   valeur   |+-------------------|-----------+|      sports       |    001    ||      espace        |    010    || infographie       |    100    ||-------------------|-----------|
```

Ainsi, le nombre de n≈ìuds de sortie est le nombre de classes du jeu de donn√©es d'entr√©e.

Les valeurs de la couche de sortie sont √©galement multipli√©es par les poids et nous ajoutons √©galement le biais, mais maintenant la fonction d'activation est diff√©rente.

Vous voulez √©tiqueter chaque texte avec une cat√©gorie, et ces cat√©gories sont mutuellement exclusives (un texte n'appartient pas √† deux cat√©gories en m√™me temps). Pour tenir compte de cela, au lieu d'utiliser la fonction d'activation ReLu, vous utiliserez la fonction [Softmax](https://en.wikipedia.org/wiki/Softmax_function). Cette fonction transforme la sortie de chaque unit√© en une valeur comprise entre 0 et 1 et s'assure √©galement que la somme de toutes les unit√©s est √©gale √† 1. De cette mani√®re, la sortie nous indiquera la probabilit√© de chaque texte pour chaque cat√©gorie.

```
| 1.2                    0.46|| 0.9   -> [softmax] ->  0.34|| 0.4                    0.20|
```

Et maintenant vous avez le graphe de flux de donn√©es de votre r√©seau de neurones. En traduisant tout ce que nous avons vu jusqu'√† pr√©sent en code, le r√©sultat est :

```
# Param√®tres du r√©seau
n_hidden_1 = 10        # Nombre de caract√©ristiques de la 1√®re couche
n_hidden_2 = 5         # Nombre de caract√©ristiques de la 2√®me couche
n_input = total_words  # Mots dans le vocabulaire
n_classes = 3          # Cat√©gories : infographie, espace et baseball
```

```
def multilayer_perceptron(input_tensor, weights, biases):    layer_1_multiplication = tf.matmul(input_tensor, weights['h1'])    layer_1_addition = tf.add(layer_1_multiplication, biases['b1'])    layer_1_activation = tf.nn.relu(layer_1_addition)
```

```
# Couche cach√©e avec activation RELU    layer_2_multiplication = tf.matmul(layer_1_activation, weights['h2'])    layer_2_addition = tf.add(layer_2_multiplication, biases['b2'])    layer_2_activation = tf.nn.relu(layer_2_addition)
```

```
# Couche de sortie avec activation lin√©aire    out_layer_multiplication = tf.matmul(layer_2_activation, weights['out'])    out_layer_addition = out_layer_multiplication + biases['out']
```

```
return out_layer_addition
```

(Nous parlerons du code pour la fonction d'activation de la couche de sortie plus tard.)

### Comment le r√©seau de neurones apprend

Comme nous l'avons vu pr√©c√©demment, les valeurs de poids sont mises √† jour pendant que le r√©seau est entra√Æn√©. Maintenant, nous allons voir comment cela se passe dans l'environnement TensorFlow.

#### tf.Variable

Les poids et les biais sont stock√©s dans des variables (`tf.Variable`). Ces variables maintiennent l'√©tat dans le graphe √† travers les appels √† `run()`. En machine learning, nous commen√ßons g√©n√©ralement les valeurs de poids et de biais par une [distribution normale](https://en.wikipedia.org/wiki/Normal_distribution).

```
weights = {    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))}biases = {    'b1': tf.Variable(tf.random_normal([n_hidden_1])),    'b2': tf.Variable(tf.random_normal([n_hidden_2])),    'out': tf.Variable(tf.random_normal([n_classes]))}
```

Lorsque nous ex√©cutons le r√©seau pour la premi√®re fois (c'est-√†-dire que les valeurs de poids sont celles d√©finies par la distribution normale) :

```
valeurs d'entr√©e : x
poids : w
biais : b
valeurs de sortie : z
```

```
valeurs attendues : expected
```

Pour savoir si le r√©seau apprend ou non, vous devez comparer les valeurs de sortie (_z_) avec les valeurs attendues (_expected_). Et comment calculons-nous cette diff√©rence (perte) ? Il existe de nombreuses m√©thodes pour cela. Parce que nous travaillons avec une t√¢che de classification, la meilleure mesure pour la perte est l'[erreur d'entropie crois√©e](https://en.wikipedia.org/wiki/Cross_entropy).

[James D. McCaffrey](https://jamesmccaffrey.wordpress.com/) a √©crit [une explication brillante](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/) sur pourquoi c'est la meilleure m√©thode pour ce type de t√¢che.

Avec TensorFlow, vous allez calculer l'erreur d'entropie crois√©e en utilisant la m√©thode `tf.nn.softmax_cross_entropy_with_logits()` (ici se trouve la fonction d'activation softmax) et calculer l'erreur moyenne (`tf.reduce_mean()`).

```
# Construire le mod√®le
prediction = multilayer_perceptron(input_tensor, weights, biases)
```

```
# D√©finir la perte
entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=output_tensor)
loss = tf.reduce_mean(entropy_loss)
```

Vous voulez trouver les meilleures valeurs pour les poids et les biais afin de minimiser l'erreur de sortie (la diff√©rence entre la valeur que nous avons obtenue et la valeur correcte). Pour cela, vous allez utiliser la m√©thode de [descente de gradient](https://en.wikipedia.org/wiki/Gradient_descent). Plus pr√©cis√©ment, vous allez utiliser la [descente de gradient stochastique](https://en.wikipedia.org/wiki/Stochastic_gradient_descent).

![Image](https://cdn-media-1.freecodecamp.org/images/MI54d2-a1xt1hEAvaTeNh-K8BkC7Y716wsvt)
_Descente de gradient. Source : [https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html](https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html" rel="noopener" target="_blank" title=")_

Il existe √©galement de nombreux algorithmes pour calculer la descente de gradient, vous allez utiliser l'[Estimation Adaptative du Moment (Adam)](http://sebastianruder.com/optimizing-gradient-descent/index.html#adam). Pour utiliser cet algorithme dans TensorFlow, vous devez passer la valeur learning_rate, qui d√©termine les √©tapes incr√©mentielles des valeurs pour trouver les meilleures valeurs de poids.

La m√©thode `tf.train.AdamOptimizer(learning_rate)**.minimize(loss)**` est un [sucre syntaxique](https://en.wikipedia.org/wiki/Syntactic_sugar) qui fait deux choses :

1. **compute_gradients**(loss, <liste de variables>)
2. **apply_gradients**(<liste de variables>)

La m√©thode met √† jour toutes les `tf.Variables` avec les nouvelles valeurs, donc nous n'avons pas besoin de passer la liste des variables. Et maintenant vous avez le code pour entra√Æner le r√©seau :

```
learning_rate = 0.001
```

```
# Construire le mod√®le
prediction = multilayer_perceptron(input_tensor, weights, biases)
```

```
# D√©finir la perte
entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=output_tensor)
loss = tf.reduce_mean(entropy_loss)
```

```
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)
```

### Manipulation des donn√©es

Le jeu de donn√©es que vous allez utiliser contient de nombreux textes en anglais et nous devons manipuler ces donn√©es pour les passer au r√©seau de neurones. Pour cela, vous allez faire deux choses :

1. Cr√©er un index pour chaque mot
2. Cr√©er une matrice pour chaque texte, o√π les valeurs sont 1 si un mot est dans le texte et 0 sinon

Voyons le code pour comprendre ce processus :

```
import numpy as np    #numpy est un package pour le calcul scientifique
from collections import Counter
```

```
vocab = Counter()
```

```
text = "Hi from Brazil"
```

```
#Obtenir tous les mots
for word in text.split(' '):    vocab[word]+=1        #Convertir les mots en index
def get_word_2_index(vocab):    word2index = {}    for i,word in enumerate(vocab):        word2index[word] = i            return word2index
```

```
#Maintenant nous avons un index
word2index = get_word_2_index(vocab)
```

```
total_words = len(vocab)
```

```
#C'est ainsi que nous cr√©ons un tableau numpy (notre matrice)
matrix = np.zeros((total_words),dtype=float)
```

```
#Maintenant nous remplissons les valeurs
for word in text.split():    matrix[word2index[word]] += 1
```

```
print(matrix)
```

```
>>> [ 1.  1.  1.]
```

Dans l'exemple ci-dessus, le texte √©tait "Hi from Brazil" et la matrice √©tait **[ 1. 1. 1.]**. Que se passe-t-il si le texte √©tait seulement "Hi" ?

```
matrix = np.zeros((total_words),dtype=float)
```

```
text = "Hi"
```

```
for word in text.split():    matrix[word2index[word.lower()]] += 1
```

```
print(matrix)
```

```
>>> [ 1.  0.  0.]
```

Vous allez faire la m√™me chose avec les √©tiquettes (cat√©gories des textes), mais maintenant vous allez utiliser le codage one-hot :

```
y = np.zeros((3),dtype=float)
```

```
if category == 0:    y[0] = 1.        # [ 1.  0.  0.]elif category == 1:    y[1] = 1.        # [ 0.  1.  0.]else:     y[2] = 1.       # [ 0.  0.  1.]
```

### Ex√©cution du graphe et obtention des r√©sultats

Maintenant vient la meilleure partie : obtenir les r√©sultats du mod√®le. D'abord, examinons de plus pr√®s le jeu de donn√©es d'entr√©e.

#### Le jeu de donn√©es

Vous allez utiliser les [20 Newsgroups](http://qwone.com/~jason/20Newsgroups/), un jeu de donn√©es avec 18 000 messages sur 20 sujets. Pour charger ce jeu de donn√©es, vous allez utiliser la biblioth√®que [scikit-learn](http://scikit-learn.org/stable/index.html). Nous allons utiliser seulement 3 cat√©gories : **comp.graphics**, **sci.space** et **rec.sport.baseball**. Le scikit-learn a deux sous-ensembles : un pour l'entra√Ænement et un pour les tests. La recommandation est que **vous ne devriez jamais regarder les donn√©es de test**, car cela peut interf√©rer avec vos choix lors de la cr√©ation du mod√®le. Vous ne voulez pas cr√©er un mod√®le pour pr√©dire ces donn√©es de test sp√©cifiques, vous voulez cr√©er un mod√®le avec une bonne **g√©n√©ralisation**.

Voici comment vous allez charger les jeux de donn√©es :

```
from sklearn.datasets import fetch_20newsgroups
```

```
categories = ["comp.graphics","sci.space","rec.sport.baseball"]
```

```
newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)
newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)
```

#### Entra√Ænement du mod√®le

Dans la [terminologie des r√©seaux de neurones](http://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks), une √©poque = une passe avant (obtenir les valeurs de sortie) et une passe arri√®re (mettre √† jour les poids) de _tous_ les exemples d'entra√Ænement.

Vous vous souvenez de la m√©thode `tf.Session.run()` ? Examinons-la de plus pr√®s :

`tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)`

Dans le graphe de flux de donn√©es du d√©but de cet article, vous avez utilis√© l'op√©ration de somme, mais nous pouvons √©galement passer une liste de choses √† ex√©cuter. Dans cette ex√©cution du r√©seau de neurones, vous allez passer deux choses : le calcul de la perte et l'√©tape d'optimisation.

Le param√®tre `feed_dict` est l'endroit o√π nous passons les donn√©es pour chaque √©tape d'ex√©cution. Pour passer ces donn√©es, nous devons d√©finir des `tf.placeholders` (pour alimenter le `feed_dict`).

Comme le dit la documentation de TensorFlow :

> "Un placeholder existe uniquement pour servir de cible aux feeds. Il n'est pas initialis√© et ne contient aucune donn√©e." ‚Äî [Source](https://www.tensorflow.org/programmers_guide/reading_data)

Vous allez donc d√©finir vos placeholders comme ceci :

```
n_input = total_words # Mots dans le vocabulaire
n_classes = 3         # Cat√©gories : infographie, sci.space et baseball
```

```
input_tensor = tf.placeholder(tf.float32,[None, n_input],name="input")
output_tensor = tf.placeholder(tf.float32,[None, n_classes],name="output")
```

Vous allez s√©parer les donn√©es d'entra√Ænement en lots :

> "Si vous utilisez des placeholders pour **alimenter l'entr√©e**, vous pouvez sp√©cifier une **dimension de lot variable** en cr√©ant le placeholder avec tf.placeholder(‚Ä¶, shape=[**None**, ‚Ä¶]). L'√©l√©ment None de la forme correspond √† une dimension de taille variable." ‚Äî [Source](https://www.tensorflow.org/versions/r0.11/resources/faq)

Nous allons alimenter le dict avec un lot plus grand lors du test du mod√®le, c'est pourquoi vous devez d√©finir une dimension de lot variable.

La fonction `get_batches()` nous donne le nombre de textes avec la taille du lot. Et maintenant nous pouvons ex√©cuter le mod√®le :

```
training_epochs = 10
```

```
# Lancer le graphe
with tf.Session() as sess:    sess.run(init) #init les variables (distribution normale, vous vous souvenez ?)
```

```
    # Cycle d'entra√Ænement    for epoch in range(training_epochs):        avg_cost = 0.        total_batch = int(len(newsgroups_train.data)/batch_size)        # Boucle sur tous les lots        for i in range(total_batch):            batch_x,batch_y = get_batch(newsgroups_train,i,batch_size)            # Ex√©cuter l'op√©ration d'optimisation (r√©tropropagation) et l'op√©ration de co√ªt (pour obtenir la valeur de perte)            c,_ = sess.run([loss,optimizer], feed_dict={input_tensor: batch_x, output_tensor:batch_y})
```

Maintenant vous avez le mod√®le, entra√Æn√©. Pour le tester, vous devrez √©galement cr√©er des √©l√©ments de graphe. Nous allons mesurer la pr√©cision du mod√®le, donc vous devez obtenir l'index de la valeur pr√©dite et l'index de la valeur correcte (parce que nous utilisons le codage one-hot), v√©rifier s'ils sont √©gaux et calculer la moyenne pour tout le jeu de donn√©es de test :

```
    # Tester le mod√®le    index_prediction = tf.argmax(prediction, 1)    index_correct = tf.argmax(output_tensor, 1)    correct_prediction = tf.equal(index_prediction, index_correct)
```

```
    # Calculer la pr√©cision    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))    total_test_data = len(newsgroups_test.target)    batch_x_test,batch_y_test = get_batch(newsgroups_test,0,total_test_data)    print("Pr√©cision :", accuracy.eval({input_tensor: batch_x_test, output_tensor: batch_y_test}))
```

```
>>> Epoch: 0001 loss= 1133.908114347    Epoch: 0002 loss= 329.093700409    Epoch: 0003 loss= 111.876660109    Epoch: 0004 loss= 72.552971845    Epoch: 0005 loss= 16.673050320    Epoch: 0006 loss= 16.481995190    Epoch: 0007 loss= 4.848220565    Epoch: 0008 loss= 0.759822878    Epoch: 0009 loss= 0.000000000    Epoch: 0010 loss= 0.079848485    Optimisation termin√©e !
```

```
    Pr√©cision : 0.75
```

Et c'est tout ! Vous avez cr√©√© un mod√®le utilisant un r√©seau de neurones pour classer des textes en cat√©gories. F√©licitations ! üéâ

Vous pouvez voir le notebook avec le **code final** [ici](https://github.com/dmesquita/understanding_tensorflow_nn).

Conseil : modifiez les valeurs que nous avons d√©finies pour voir comment les changements affectent le temps d'entra√Ænement et la pr√©cision du mod√®le.

Des questions ou des suggestions ? Laissez-les dans les commentaires. Oh, et merci d'avoir lu ! üòä ‚úçÔ∏è

Avez-vous trouv√© cet article utile ? Je fais de mon mieux pour √©crire un article approfondi chaque mois, vous pouvez [recevoir un email lorsque j'en publie un nouveau](https://goo.gl/forms/SLrJDrGtxgAoILkt1).

_Cela signifierait beaucoup si vous cliquez sur le üëè et partagez avec des amis. Suivez-moi pour plus d'articles sur la Data Science et le Machine Learning._