---
title: 'Pr√©traitement pour le deep learning : de la matrice de covariance au blanchiment
  d''image'
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2018-09-16T22:44:23.000Z'
originalURL: https://freecodecamp.org/news/https-medium-com-hadrienj-preprocessing-for-deep-learning-9e2b9c75165c
coverImage: https://cdn-media-1.freecodecamp.org/images/1*ehXogigFyLpyy2q2sz80HA.png
tags:
- name: Data Science
  slug: data-science
- name: Deep Learning
  slug: deep-learning
- name: Machine Learning
  slug: machine-learning
- name: Python
  slug: python
- name: 'tech '
  slug: tech
seo_title: 'Pr√©traitement pour le deep learning : de la matrice de covariance au blanchiment
  d''image'
seo_desc: 'By hadrienj

  The goal of this post is to go from the basics of data preprocessing to modern techniques
  used in deep learning. My point is that we can use code (such as Python/NumPy) to
  better understand abstract mathematical notions. Thinking by codin...'
---

Par hadrienj

Le but de cet article est de passer des bases du pr√©traitement des donn√©es aux techniques modernes utilis√©es en deep learning. Mon id√©e est que nous pouvons utiliser du code (comme Python/NumPy) pour mieux comprendre des notions math√©matiques abstraites. Penser en codant ! üí°

Nous commencerons par des concepts basiques mais tr√®s utiles en science des donn√©es et en machine learning/deep learning, comme les matrices de variance et de covariance. Nous irons plus loin avec certaines techniques de pr√©traitement utilis√©es pour alimenter des images dans des r√©seaux de neurones. Nous essaierons d'obtenir des informations plus concr√®tes en utilisant du code pour voir ce que chaque √©quation fait r√©ellement.

Le **pr√©traitement** fait r√©f√©rence √† toutes les transformations appliqu√©es aux donn√©es brutes avant qu'elles ne soient fournies √† l'algorithme de machine learning ou de deep learning. Par exemple, entra√Æner un r√©seau de neurones convolutionnel sur des images brutes conduira probablement √† de mauvaises performances de classification ([Pal & Sudeep, 2016](https://ieeexplore.ieee.org/document/7808140/)). Le pr√©traitement est √©galement important pour acc√©l√©rer l'entra√Ænement (par exemple, les techniques de centrage et de mise √† l'√©chelle, voir [Lecun et al., 2012 ; voir 4.3](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)).

Voici le plan de ce tutoriel :

**1. Contexte :** Dans la premi√®re partie, nous rappellerons quelques notions sur la variance et la covariance. Nous verrons comment g√©n√©rer et tracer des donn√©es fictives pour mieux comprendre ces concepts.

**2. Pr√©traitement :** Dans la deuxi√®me partie, nous verrons les bases de certaines techniques de pr√©traitement qui peuvent √™tre appliqu√©es √† tout type de donn√©es ‚Äî **normalisation par la moyenne**, **standardisation** et **blanchiment**.

**3. Blanchiment d'images :** Dans la troisi√®me partie, nous utiliserons les outils et concepts acquis dans **1.** et **2.** pour effectuer un type sp√©cial de blanchiment appel√© **Analyse des Composantes √† Z√©ro (ZCA)**. Il peut √™tre utilis√© pour pr√©traiter des images pour le deep learning. Cette partie sera tr√®s pratique et amusante ‚òÄÔ∏è!

N'h√©sitez pas √† forker [le notebook associ√© √† cet article](https://github.com/hadrienj/Preprocessing-for-deep-learning)! Par exemple, v√©rifiez les formes des matrices chaque fois que vous avez un doute.

### 1. Contexte

#### A. Variance et covariance

La variance d'une variable d√©crit √† quel point les valeurs sont dispers√©es. La covariance est une mesure qui indique le degr√© de d√©pendance entre deux variables.

Une covariance positive signifie que les valeurs de la premi√®re variable sont grandes lorsque les valeurs de la deuxi√®me variable sont √©galement grandes. Une covariance n√©gative signifie l'inverse : les grandes valeurs d'une variable sont associ√©es √† de petites valeurs de l'autre.

La valeur de la covariance d√©pend de l'√©chelle de la variable, ce qui la rend difficile √† analyser. Il est possible d'utiliser le coefficient de corr√©lation, qui est plus facile √† interpr√©ter. Le coefficient de corr√©lation est simplement la covariance normalis√©e.

![Image](https://cdn-media-1.freecodecamp.org/images/1*GH0ou22oJEwAw89GkrS8-w.png)
_Une covariance positive signifie que les grandes valeurs d'une variable sont associ√©es √† de grandes valeurs de l'autre (√† gauche). Une covariance n√©gative signifie que les grandes valeurs d'une variable sont associ√©es √† de petites valeurs de l'autre (√† droite)._

La matrice de covariance est une matrice qui r√©sume les variances et covariances d'un ensemble de vecteurs et peut en dire long sur vos variables. La diagonale correspond √† la variance de chaque vecteur :

![Image](https://cdn-media-1.freecodecamp.org/images/1*5V2y7dyc7YclTRqdVjoOrQ.png)
_Une matrice **A** et sa matrice de covariance. La diagonale correspond √† la variance de chaque vecteur colonne._

V√©rifions simplement avec la formule de la variance :

![Image](https://cdn-media-1.freecodecamp.org/images/1*EpBVFBmFboZeAxANYe6PEg.png)

avec **n** la longueur du vecteur, et **xÃÑ** la moyenne du vecteur. Par exemple, la variance du premier vecteur colonne de **A** est :

![Image](https://cdn-media-1.freecodecamp.org/images/1*nIpi1287Raa-n9NKwVHrsA.png)

C'est la premi√®re cellule de notre matrice de covariance. Le deuxi√®me √©l√©ment sur la diagonale correspond √† la variance du deuxi√®me vecteur colonne de **A**, et ainsi de suite.

**Note** : les vecteurs extraits de la matrice **A** correspondent aux colonnes de **A**.

Les autres cellules correspondent √† la covariance entre deux vecteurs colonnes de **_A_**. Par exemple, la covariance entre la premi√®re et la troisi√®me colonne se trouve dans la matrice de covariance √† la colonne 1 et √† la ligne 3 (ou √† la colonne 3 et √† la ligne 1).

![Image](https://cdn-media-1.freecodecamp.org/images/1*Ce3wTRBXCJUG7fFf95CQ9Q.png)
_La position dans la matrice de covariance. La colonne correspond √† la premi√®re variable et la ligne √† la seconde (ou l'inverse). La covariance entre le premier et le troisi√®me vecteur colonne de **A** est l'√©l√©ment √† la colonne 1 et √† la ligne 3 (ou l'inverse = m√™me valeur)._

V√©rifions que la covariance entre le premier et le troisi√®me vecteur colonne de **A** est √©gale √† -2,67. La formule de la covariance entre deux variables **_X_** et **Y** est :

![Image](https://cdn-media-1.freecodecamp.org/images/1*Y1kVDzXPCxhRRsk8snmzTQ.png)

Les variables **X** et **Y** sont les premier et troisi√®me vecteurs colonnes dans le dernier exemple. D√©composons cette formule pour √™tre s√ªr qu'elle est claire :

![Image](https://cdn-media-1.freecodecamp.org/images/1*BWZDmC8GrNL-xNqGG1CjyA.png)

1. Le symbole somme (**Œ£**) signifie que nous allons it√©rer sur les √©l√©ments des vecteurs. Nous commencerons avec le premier √©l√©ment (**i=1**) et calculerons le premier √©l√©ment de **X** moins la moyenne du vecteur **X**.

![Image](https://cdn-media-1.freecodecamp.org/images/1*aFL3dzKMDXf9vj2_J5tNPQ.png)

2. Multipliez le r√©sultat par le premier √©l√©ment de **Y** moins la moyenne du vecteur **_Y_**. 

![Image](https://cdn-media-1.freecodecamp.org/images/1*AKo4naYravnW3-3NrwSXTg.png)

3. R√©p√©tez le processus pour chaque √©l√©ment des vecteurs et calculez la somme de tous les r√©sultats.

![Image](https://cdn-media-1.freecodecamp.org/images/1*lk6JZa0lHqswjKwD5wKziQ.png)

4. Divisez par le nombre d'√©l√©ments dans le vecteur.

**Exemple 1.**

Commen√ßons par la matrice **A** :

![Image](https://cdn-media-1.freecodecamp.org/images/1*o6NqwIfr6XlHSL_NIXtXsA.png)

Nous allons calculer la covariance entre les premier et troisi√®me vecteurs colonnes :

![Image](https://cdn-media-1.freecodecamp.org/images/1*BvbRAxHeb40LU5goEDsoLg.png)

et

![Image](https://cdn-media-1.freecodecamp.org/images/1*Jx4vbnRDKW95fF2nYSygZg.png)

**xÃÑ=3**, **»≥=4**, et **n=3** donc nous avons :

![Image](https://cdn-media-1.freecodecamp.org/images/1*PcvOUuCgOCY_qQhLb5AimA.png)

OK, super ! C'est la valeur de la matrice de covariance.

**Maintenant, la m√©thode facile**. Avec NumPy, la matrice de covariance peut √™tre calcul√©e avec la fonction `np.cov`.

**Il est √† noter** que si vous voulez que NumPy utilise les colonnes comme vecteurs, le param√®tre `rowvar=False` doit √™tre utilis√©. De plus, `bias=True` divise par **n** et non par **n-1**.

Cr√©ons d'abord le tableau :

```
array([[1, 3, 5],       [5, 4, 1],       [3, 8, 6]])
```

Maintenant, nous allons calculer la covariance avec la fonction NumPy :

```
array([[ 2.66666667, 0.66666667, -2.66666667],       [ 0.66666667, 4.66666667, 2.33333333],       [-2.66666667, 2.33333333, 4.66666667]])
```

Cela semble bon !

**Trouver la matrice de covariance avec le produit scalaire**

Il existe une autre fa√ßon de calculer la matrice de covariance de **A**. Vous pouvez centrer **A** autour de 0. La moyenne du vecteur est soustraite de chaque √©l√©ment du vecteur pour avoir un vecteur avec une moyenne √©gale √† 0. Il est multipli√© par sa propre transpos√©e, et divis√© par le nombre d'observations.

Commen√ßons par une impl√©mentation et ensuite nous essaierons de comprendre le lien avec l'√©quation pr√©c√©dente :

Testons cela sur notre matrice **A** :

```
array([[ 2.66666667, 0.66666667, -2.66666667],       [ 0.66666667, 4.66666667, 2.33333333],       [-2.66666667, 2.33333333, 4.66666667]])
```

Nous obtenons le m√™me r√©sultat qu'avant.

L'explication est simple. Le produit scalaire entre deux vecteurs peut √™tre exprim√© :

![Image](https://cdn-media-1.freecodecamp.org/images/1*hdHYlHiK3s0IDwwWytJO0A.png)

C'est exact, c'est la somme des produits de chaque √©l√©ment des vecteurs :

![Image](https://cdn-media-1.freecodecamp.org/images/1*6zDuuYJtL6yiuE1CatrYDQ.png)
_Le produit scalaire correspond √† la somme des produits de chaque √©l√©ment des vecteurs._

Si **n** est le nombre d'√©l√©ments dans nos vecteurs et que nous divisons par **n** :

![Image](https://cdn-media-1.freecodecamp.org/images/1*XNMJtFhQF2v56K1OLE0_Yw.png)

Vous pouvez noter que cela n'est pas trop √©loign√© de la formule de la covariance que nous avons vue pr√©c√©demment :

![Image](https://cdn-media-1.freecodecamp.org/images/1*RYVpFx0lrkTEl_R92ocGgQ.png)

La seule diff√©rence est que, dans la formule de la covariance, nous soustrayons la moyenne d'un vecteur de chacun de ses √©l√©ments. C'est pourquoi nous devons centrer les donn√©es avant de faire le produit scalaire.

Maintenant, si nous avons une matrice **A**, le produit scalaire entre **A** et sa transpos√©e vous donnera une nouvelle matrice :

![Image](https://cdn-media-1.freecodecamp.org/images/1*1Qw42RtGhHQWXD4rkA-MTQ.png)
_Si vous commencez avec une matrice centr√©e sur z√©ro, le produit scalaire entre cette matrice et sa transpos√©e vous donnera la variance de chaque vecteur et la covariance entre eux, c'est-√†-dire la matrice de covariance._

C'est la matrice de covariance !

#### B. Visualiser les donn√©es et les matrices de covariance

Afin d'obtenir plus d'informations sur la matrice de covariance et comment elle peut √™tre utile, nous allons cr√©er une fonction pour la visualiser avec des donn√©es 2D. Vous pourrez voir le lien entre la matrice de covariance et les donn√©es.

Cette fonction calculera la matrice de covariance comme nous l'avons vu ci-dessus. Elle cr√©era deux sous-graphiques ‚Äî un pour la matrice de covariance et un pour les donn√©es. La fonction `heatmap()` de [Seaborn](https://seaborn.pydata.org) est utilis√©e pour cr√©er des gradients de couleur ‚Äî les petites valeurs seront color√©es en vert clair et les grandes valeurs en bleu fonc√©. Nous avons choisi l'une de nos couleurs de palette, mais vous pouvez pr√©f√©rer d'autres couleurs. Les donn√©es sont repr√©sent√©es sous forme de nuage de points.

#### C. Simulation de donn√©es

**Donn√©es non corr√©l√©es**

Maintenant que nous avons la fonction de tra√ßage, nous allons g√©n√©rer quelques donn√©es al√©atoires pour visualiser ce que la matrice de covariance peut nous dire. Nous commencerons par quelques donn√©es tir√©es d'une distribution normale avec la fonction NumPy `np.random.normal()`.

![Image](https://cdn-media-1.freecodecamp.org/images/1*C5wwjainirV9mQHDlei9SQ.png)
_Tirage d'√©chantillon d'une distribution normale avec NumPy._

Cette fonction n√©cessite la moyenne, l'√©cart-type et le nombre d'observations de la distribution en entr√©e. Nous allons cr√©er deux variables al√©atoires de 300 observations avec un √©cart-type de 1. La premi√®re aura une moyenne de 1 et la seconde une moyenne de 2. Si nous tirons al√©atoirement deux ensembles de 300 observations d'une distribution normale, les deux vecteurs seront non corr√©l√©s.

```
(300, 2)
```

**Note 1** : Nous transposons les donn√©es avec `.T` car la forme originale est `(2, 300)` et nous voulons le nombre d'observations comme lignes (donc avec la forme `(300, 2)`).

**Note 2** : Nous utilisons la fonction `np.random.seed` pour la reproductibilit√©. Le m√™me nombre al√©atoire sera utilis√© la prochaine fois que nous ex√©cuterons la cellule.

V√©rifions √† quoi ressemblent les donn√©es :

```
array([[ 2.47143516, 1.52704645],       [ 0.80902431, 1.7111124 ],       [ 3.43270697, 0.78245452],       [ 1.6873481 , 3.63779121],       [ 1.27941127, -0.74213763],       [ 2.88716294, 0.90556519],       [ 2.85958841, 2.43118375],       [ 1.3634765 , 1.59275845],       [ 2.01569637, 1.1702969 ],       [-0.24268495, -0.75170595]])
```

Bien, nous avons deux vecteurs colonnes.

Maintenant, nous pouvons v√©rifier que les distributions sont normales :

![Image](https://cdn-media-1.freecodecamp.org/images/1*Wb8r7PRje6nunmN-iMrUyQ.png)

Cela semble bon !

Nous pouvons voir que les distributions ont des √©carts-types √©quivalents mais des moyennes diff√©rentes (1 et 2). Donc c'est exactement ce que nous avons demand√©.

Maintenant, nous pouvons tracer notre ensemble de donn√©es et sa matrice de covariance avec notre fonction :

```
Matrice de covariance :[[ 0.95171641 -0.0447816 ] [-0.0447816 0.87959853]]
```

![Image](https://cdn-media-1.freecodecamp.org/images/1*Kq4mIBv4hFGzOuhoftcFVQ.png)

Nous pouvons voir sur le nuage de points que les deux dimensions ne sont pas corr√©l√©es. Notez que nous avons une dimension avec une moyenne de 1 (axe y) et l'autre avec une moyenne de 2 (axe x).

De plus, la matrice de covariance montre que la variance de chaque variable est tr√®s grande (autour de 1) et la covariance des colonnes 1 et 2 est tr√®s petite (autour de 0). Puisque nous avons assur√© que les deux vecteurs sont ind√©pendants, cela est coh√©rent. L'inverse n'est pas n√©cessairement vrai : une covariance de 0 ne garantit pas l'ind√©pendance (voir [ici](https://stats.stackexchange.com/questions/12842/covariance-and-independence)).

**Donn√©es corr√©l√©es**

Maintenant, construisons des donn√©es d√©pendantes en sp√©cifiant une colonne √† partir de l'autre.

```
Matrice de covariance :[[ 0.95171641 0.92932561] [ 0.92932561 1.12683445]]
```

![Image](https://cdn-media-1.freecodecamp.org/images/1*r-OmoGkWJvkWqjFO0ltV3w.png)

La corr√©lation entre les deux dimensions est visible sur le nuage de points. Nous pouvons voir qu'une ligne pourrait √™tre trac√©e et utilis√©e pour pr√©dire **y** √† partir de **x** et vice versa. La matrice de covariance n'est pas diagonale (il y a des cellules non nulles en dehors de la diagonale). Cela signifie que la covariance entre les dimensions est non nulle.

C'est super ! Nous avons maintenant tous les outils pour voir diff√©rentes techniques de pr√©traitement.

### 2. Pr√©traitement

#### A. Normalisation par la moyenne

La normalisation par la moyenne consiste simplement √† soustraire la moyenne de chaque observation.

![Image](https://cdn-media-1.freecodecamp.org/images/1*ym0P7PyUgZlML1QlGeUlvQ.png)

o√π **X'** est l'ensemble de donn√©es normalis√©, **X** est l'ensemble de donn√©es original, et **xÃÑ** est la moyenne de **X**.

La normalisation par la moyenne a pour effet de centrer les donn√©es autour de 0. Nous allons cr√©er la fonction `center()` pour faire cela :

Essayons avec la matrice **B** que nous avons cr√©√©e pr√©c√©demment :

```
Avant :
```

```
Matrice de covariance :[[ 0.95171641 0.92932561] [ 0.92932561 1.12683445]]
```

![Image](https://cdn-media-1.freecodecamp.org/images/1*PaAR5buk8ICGSyiTKAeh_w.png)

```
Apr√®s :
```

```
Matrice de covariance :[[ 0.95171641 0.92932561] [ 0.92932561 1.12683445]]
```

![Image](https://cdn-media-1.freecodecamp.org/images/1*QeCA2GooKYzbVqevDVG0FA.png)

Le premier graphique montre √† nouveau les donn√©es originales **B** et le deuxi√®me graphique montre les donn√©es centr√©es (regardez l'√©chelle).

#### B. Standardisation ou normalisation

La standardisation est utilis√©e pour mettre toutes les caract√©ristiques √† la m√™me √©chelle. Chaque dimension centr√©e sur z√©ro est divis√©e par son √©cart-type.

![Image](https://cdn-media-1.freecodecamp.org/images/1*2PX6slhDPJkjibJiecX25Q.png)

o√π **X'** est l'ensemble de donn√©es standardis√©, **X** est l'ensemble de donn√©es original, **xÃÑ** est la moyenne de **X**, et **œÉ** est l'√©cart-type de **_X_**. 

Cr√©ons un autre ensemble de donn√©es avec une √©chelle diff√©rente pour v√©rifier que cela fonctionne.

```
Matrice de covariance :[[ 0.95171641 0.83976242] [ 0.83976242 6.22529922]]
```

![Image](https://cdn-media-1.freecodecamp.org/images/1*vTAyUBAcepxHyvX449hESQ.png)

Nous pouvons voir que les √©chelles de **x** et **y** sont diff√©rentes. Notez √©galement que la corr√©lation semble plus faible en raison des diff√©rences d'√©chelle. Maintenant, standardisons cela :

```
Matrice de covariance :[[ 1.          0.34500274] [ 0.34500274  1.        ]]
```

![Image](https://cdn-media-1.freecodecamp.org/images/1*0a6gXhWnLPBv8i-L7PbYKA.png)

Cela semble bon. Vous pouvez voir que les √©chelles sont les m√™mes et que l'ensemble de donn√©es est centr√© sur z√©ro selon les deux axes.

Maintenant, jetez un coup d'≈ìil √† la matrice de covariance. Vous pouvez voir que la variance de chaque coordonn√©e ‚Äî la cellule en haut √† gauche et la cellule en bas √† droite ‚Äî est √©gale √† 1.

Cette nouvelle matrice de covariance est en fait la matrice de corr√©lation. Le coefficient de corr√©lation de Pearson entre les deux variables (**c1** et **c2**) est 0,54220151.

#### C. Blanchiment

Le blanchiment, ou sph√©risation, des donn√©es signifie que nous voulons les transformer pour avoir une matrice de covariance qui est la matrice identit√© ‚Äî 1 sur la diagonale et 0 pour les autres cellules. Il est appel√© blanchiment en r√©f√©rence au bruit blanc.

[Voici plus de d√©tails sur la matrice identit√©.](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.3-Identity-and-Inverse-Matrices/)

Le blanchiment est un peu plus compliqu√© que les autres pr√©traitements, mais nous avons maintenant tous les outils dont nous avons besoin pour le faire. Il implique les √©tapes suivantes :

* Centrer les donn√©es sur z√©ro
* D√©corr√©lation des donn√©es
* Redimensionnement des donn√©es

Prenons √† nouveau **C** et essayons de faire ces √©tapes.

1. **Centrage sur z√©ro**

Cela fait r√©f√©rence √† la normalisation par la moyenne (**2. A**). V√©rifiez les d√©tails sur la fonction `center()`.

```
Matrice de covariance :[[ 0.95171641  0.83976242] [ 0.83976242  6.22529922]]
```

![Image](https://cdn-media-1.freecodecamp.org/images/1*AXOzdgC8gjiwpg-9AsfzKw.png)

**2. D√©corr√©lation**

√Ä ce stade, nous devons d√©corr√©lationner nos donn√©es. Intuitivement, cela signifie que nous voulons faire tourner les donn√©es jusqu'√† ce qu'il n'y ait plus de corr√©lation. Regardez l'image suivante pour voir ce que je veux dire :

![Image](https://cdn-media-1.freecodecamp.org/images/1*ehXogigFyLpyy2q2sz80HA.png)

Le graphique de gauche montre des donn√©es corr√©l√©es. Par exemple, si vous prenez un point de donn√©es avec une grande valeur **x**, il y a des chances que le **y** associ√© soit √©galement assez grand.

Maintenant, prenez tous les points de donn√©es et faites une rotation (peut-√™tre de 45 degr√©s dans le sens inverse des aiguilles d'une montre). Les nouvelles donn√©es, trac√©es √† droite, ne sont plus corr√©l√©es. Vous pouvez voir que les grandes et petites valeurs de **y** sont li√©es au m√™me type de valeurs de **x**.

La question est : comment pourrions-nous trouver la bonne rotation afin d'obtenir les donn√©es non corr√©l√©es ?

En fait, c'est exactement ce que font les vecteurs propres de la matrice de covariance. Ils indiquent la direction o√π la dispersion des donn√©es est √† son maximum :

![Image](https://cdn-media-1.freecodecamp.org/images/1*1SAoJ_o70IygSmDnKiCkmw.png)

Les vecteurs propres de la matrice de covariance vous donnent la direction qui maximise la variance. La direction de la **ligne verte** est celle o√π la variance est maximale. Regardez simplement les points les plus petits et les plus grands projet√©s sur cette ligne ‚Äî la dispersion est grande. Comparez cela avec la projection sur la **ligne orange** ‚Äî la dispersion est tr√®s petite.

Pour plus de d√©tails sur la d√©composition propre, voir [cet article](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.7-Eigendecomposition/).

Nous pouvons donc d√©corr√©lationner les donn√©es en les projetant √† l'aide des vecteurs propres. Cela aura pour effet d'appliquer la rotation n√©cessaire et de supprimer les corr√©lations entre les dimensions. Voici les √©tapes :

* Calculer la matrice de covariance
* Calculer les vecteurs propres de la matrice de covariance
* Appliquer la matrice des vecteurs propres aux donn√©es ‚Äî cela appliquera la rotation

Emballons cela dans une fonction :

Essayons de d√©corr√©lationner notre matrice centr√©e sur z√©ro **C** pour voir cela en action :

```
Matrice de covariance :[[ 0.95171641 0.83976242] [ 0.83976242 6.22529922]]
```

![Image](https://cdn-media-1.freecodecamp.org/images/1*Ok5JSx7zN4lg9FAcZOE6Ew.png)

```
Matrice de covariance :[[ 5.96126981e-01 -1.48029737e-16] [ -1.48029737e-16 3.15205774e+00]]
```

![Image](https://cdn-media-1.freecodecamp.org/images/1*HmJGGe0cP6X-p0W-VuRfBg.png)

Bien ! Cela fonctionne.

Nous pouvons voir que la corr√©lation n'est plus pr√©sente. La matrice de covariance, maintenant une matrice diagonale, confirme que la covariance entre les deux dimensions est √©gale √† 0.

**3. Redimensionnement des donn√©es**

L'√©tape suivante consiste √† mettre √† l'√©chelle la matrice non corr√©l√©e afin d'obtenir une matrice de covariance correspondant √† la matrice identit√©. Pour ce faire, nous mettons √† l'√©chelle nos donn√©es d√©corr√©l√©es en divisant chaque dimension par la racine carr√©e de sa valeur propre correspondante.

**Note** : nous ajoutons une petite valeur (ici 10^-5) pour √©viter la division par 0.

```
Matrice de covariance :[[ 9.99983225e-01 -1.06581410e-16] [ -1.06581410e-16 9.99996827e-01]]
```

![Image](https://cdn-media-1.freecodecamp.org/images/1*vFleYrVKknN5zwO3SNA5hQ.png)

Hourra ! Nous pouvons voir avec la matrice de covariance que tout est bon. Nous avons quelque chose qui ressemble √† une matrice identit√© ‚Äî 1 sur la diagonale et 0 ailleurs.

### 3. Blanchiment d'images

Nous allons voir comment le blanchiment peut √™tre appliqu√© pour pr√©traiter un ensemble de donn√©es d'images. Pour ce faire, nous utiliserons l'article de [Pal & Sudeep (2016)](https://ieeexplore.ieee.org/document/7808140/) o√π ils donnent quelques d√©tails sur le processus. Cette technique de pr√©traitement est appel√©e Analyse des Composantes √† Z√©ro (ZCA).

Consultez l'article, mais voici le type de r√©sultat qu'ils ont obtenu. Les images originales (√† gauche) et les images apr√®s le ZCA (√† droite) sont montr√©es.

![Image](https://cdn-media-1.freecodecamp.org/images/1*YyKLLSzcAMX_9cCBbjP2sg.png)
_Blanchiment d'images du jeu de donn√©es CIFAR10. R√©sultats de l'article de [Pal & Sudeep (2016)](https://ieeexplore.ieee.org/document/7808140/)._

Commen√ßons par le commencement. Nous allons charger des images du jeu de donn√©es CIFAR. Ce jeu de donn√©es est disponible √† partir de Keras et vous pouvez √©galement le t√©l√©charger [ici](https://www.cs.toronto.edu/~kriz/cifar.html).

```
(50000, 32, 32, 3)
```

L'ensemble d'entra√Ænement du jeu de donn√©es CIFAR10 contient 50000 images. La forme de `X_train` est `(50000, 32, 32, 3)`. Chaque image est de 32px par 32px et chaque pixel contient 3 dimensions (R, G, B). Chaque valeur est la luminosit√© de la couleur correspondante entre 0 et 255.

Nous allons commencer par s√©lectionner seulement un sous-ensemble des images, disons 1000 :

```
(1000, 32, 32, 3)
```

C'est mieux. Maintenant, nous allons remodeler le tableau pour avoir des donn√©es d'image plates avec une image par ligne. Chaque image sera `(1, 3072)` car 32 x 32 x 3 = 3072. Ainsi, le tableau contenant toutes les images sera `(1000, 3072)` :

```
(1000, 3072)
```

L'√©tape suivante est de pouvoir voir les images. La fonction `imshow()` de Matplotlib ([doc](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html)) peut √™tre utilis√©e pour afficher les images. Elle a besoin d'images avec la forme (M x N x 3), donc cr√©ons une fonction pour remodeler les images et pouvoir les visualiser √† partir de la forme `(1, 3072)`.

Par exemple, tra√ßons une des images que nous avons charg√©es :

![Image](https://cdn-media-1.freecodecamp.org/images/1*ylvIdrsU1GyVkJwP6cwTdA.png)

Mignon !

Nous pouvons maintenant impl√©menter le blanchiment des images. [Pal & Sudeep (2016)](https://ieeexplore.ieee.org/document/7808140/) d√©crivent le processus :

**1.** La premi√®re √©tape consiste √† redimensionner les images pour obtenir la plage [0, 1] en divisant par 255 (la valeur maximale des pixels).

Rappelons que la formule pour obtenir la plage [0, 1] est :

![Image](https://cdn-media-1.freecodecamp.org/images/1*g8aDx7zkR7G4GoZzLJONhA.png)

mais, ici, la valeur minimale est 0, donc cela donne :

![Image](https://cdn-media-1.freecodecamp.org/images/1*QSSqNJa6rbJGOEQMk_Jp0A.png)

```
X.min() 0.0X.max() 1.0
```

**Soustraction de la moyenne : par pixel ou par image ?**

OK, cool, la plage de nos valeurs de pixel est maintenant entre 0 et 1. L'√©tape suivante est :

**2.** Soustraire la moyenne de toutes les images.

**Faites attention ici.**

Une fa√ßon de faire est de prendre chaque image et de soustraire la moyenne de cette image de chaque pixel ([Jarrett et al., 2009](https://www.computer.org/csdl/proceedings/iccv/2009/4420/00/05459469.pdf)). L'intuition derri√®re ce processus est qu'il centre les pixels de chaque image autour de 0.

Une autre fa√ßon de faire est de prendre chacun des 3072 pixels que nous avons (32 par 32 pixels pour R, G et B) pour chaque image et de soustraire la moyenne de ce pixel √† travers toutes les images. Cela s'appelle la soustraction de la moyenne par pixel. Cette fois, chaque pixel sera centr√© autour de 0 **selon toutes les images**. Lorsque vous alimenterez votre r√©seau avec les images, chaque pixel est consid√©r√© comme une caract√©ristique diff√©rente. Avec la soustraction de la moyenne par pixel, nous avons centr√© chaque caract√©ristique (pixel) autour de 0. Cette technique est couramment utilis√©e (par exemple [Wan et al., 2013](http://proceedings.mlr.press/v28/wan13.html)).

Nous allons maintenant effectuer la soustraction de la moyenne par pixel √† partir de nos 1000 images. Nos donn√©es sont organis√©es avec ces dimensions `(images, pixels)`. C'√©tait `(1000, 3072)` car il y a 1000 images avec 32 x 32 x 3 = 3072 pixels. La moyenne par pixel peut ainsi √™tre obtenue √† partir du premier axe :

```
(3072,)
```

Cela nous donne 3072 valeurs qui est le nombre de moyennes ‚Äî une par pixel. Voyons le type de valeurs que nous avons :

```
array([ 0.5234 , 0.54323137, 0.5274 , ‚Ä¶, 0.50369804, 0.50011765, 0.45227451])
```

Cela est proche de 0,5 car nous avons d√©j√† normalis√© √† la plage [0, 1]. Cependant, nous devons encore soustraire la moyenne de chaque pixel :

Juste pour nous convaincre que cela a fonctionn√©, nous allons calculer la moyenne du premier pixel. Esp√©rons que ce soit 0.

```
array([ -5.30575583e-16, -5.98021632e-16, -4.23439062e-16, ‚Ä¶, -1.81965554e-16, -2.49800181e-16, 3.98570066e-17])
```

Ce n'est pas exactement 0 mais c'est suffisamment petit pour que nous puissions consid√©rer que cela a fonctionn√© !

Maintenant, nous voulons calculer la matrice de covariance des donn√©es centr√©es sur z√©ro. Comme nous l'avons vu ci-dessus, nous pouvons la calculer avec la fonction `np.cov()` de NumPy.

**Veuillez noter** que nos variables sont nos diff√©rentes images. Cela implique que les variables sont les lignes de la matrice **X**. Juste pour √™tre clair, nous allons donner cette information √† NumPy avec le param√®tre `rowvar=TRUE` m√™me si c'est `True` par d√©faut (voir la [doc](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html)) :

**Maintenant, la partie magique** ‚Äî nous allons calculer les valeurs singuli√®res et les vecteurs de la matrice de covariance et les utiliser pour faire tourner notre ensemble de donn√©es. Jetez un coup d'≈ìil √† [mon article](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.8-Singular-Value-Decomposition/) sur la d√©composition en valeurs singuli√®res (SVD) si vous avez besoin de plus de d√©tails.

**Note** : Cela peut prendre un peu de temps avec beaucoup d'images et c'est pourquoi nous utilisons seulement 1000. Dans l'article, ils ont utilis√© 10000 images. N'h√©sitez pas √† comparer les r√©sultats en fonction du nombre d'images que vous utilisez :

Dans l'article, ils ont utilis√© l'√©quation suivante :

![Image](https://cdn-media-1.freecodecamp.org/images/1*ucN3UJIcEWsuYu1EZjA3KA.png)

avec **U** les vecteurs singulaires gauches et **S** les valeurs singuli√®res de la covariance de l'ensemble de donn√©es initial normalis√© d'images, et **X** l'ensemble de donn√©es normalis√©. **Œµ** est un hyper-param√®tre appel√© le coefficient de blanchiment. **diag(a)** correspond √† une matrice avec le vecteur **a** comme diagonale et 0 dans toutes les autres cellules.

Nous allons essayer d'impl√©menter cette √©quation. Commen√ßons par v√©rifier les dimensions de la SVD :

```
(1000, 1000) (1000,)
```

**S** est un vecteur contenant 1000 √©l√©ments (les valeurs singuli√®res). **diag(S)** sera ainsi de forme `(1000, 1000)` avec **S** comme diagonale :

```
[[ 8.15846654e+00 0.00000000e+00 0.00000000e+00 ‚Ä¶, 0.00000000e+00 0.00000000e+00 0.00000000e+00] [ 0.00000000e+00 4.68234845e+00 0.00000000e+00 ‚Ä¶, 0.00000000e+00 0.00000000e+00 0.00000000e+00] [ 0.00000000e+00 0.00000000e+00 2.41075267e+00 ‚Ä¶, 0.00000000e+00 0.00000000e+00 0.00000000e+00] ‚Ä¶,  [ 0.00000000e+00 0.00000000e+00 0.00000000e+00 ‚Ä¶, 3.92727365e-05 0.00000000e+00 0.00000000e+00] [ 0.00000000e+00 0.00000000e+00 0.00000000e+00 ‚Ä¶, 0.00000000e+00 3.52614473e-05 0.00000000e+00] [ 0.00000000e+00 0.00000000e+00 0.00000000e+00 ‚Ä¶, 0.00000000e+00 0.00000000e+00 1.35907202e-15]]
```

```
forme : (1000, 1000)
```

V√©rifiez cette partie :

![Image](https://cdn-media-1.freecodecamp.org/images/1*l5DfX7eqQvIgndc4B749Gg.png)

Cela est √©galement de forme `(1000, 1000)` ainsi que **U** et **U^T**. Nous avons vu aussi que **X** a la forme `(1000, 3072)`. La forme de **X_ZCA** est donc :

![Image](https://cdn-media-1.freecodecamp.org/images/1*2aSZTPJiOgdApva29IZyUg.png)

ce qui correspond √† la forme de l'ensemble de donn√©es initial. Bien.

Nous avons :

![Image](https://cdn-media-1.freecodecamp.org/images/1*TdmO3dBq-ne84RqY-EtZ2Q.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*mEVlWSSed1Ne_eQ7UiqrZQ.png)

D√©cevant ! Si vous regardez l'article, ce n'est pas le type de r√©sultat qu'ils montrent. En fait, c'est parce que nous n'avons pas redimensionn√© les pixels et qu'il y a des valeurs n√©gatives. Pour ce faire, nous pouvons les remettre dans la plage [0, 1] avec la m√™me technique que ci-dessus :

```
min : 0.0max : 1.0
```

![Image](https://cdn-media-1.freecodecamp.org/images/1*ylvIdrsU1GyVkJwP6cwTdA.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*7Cd3O07GnABlmvfGflJDqg.png)

Hourra ! C'est super ! Cela ressemble √† une image de l'article. Comme mentionn√© pr√©c√©demment, ils ont utilis√© 10000 images et non 1000 comme nous.

Pour voir les diff√©rences dans les r√©sultats en fonction du nombre d'images que vous utilisez et de l'effet de l'hyper-param√®tre **Œµ**, voici les r√©sultats pour diff√©rentes valeurs :

![Image](https://cdn-media-1.freecodecamp.org/images/1*ZN5tPhzho7QCbL4VCXYKGQ.png)

Le r√©sultat du blanchiment est diff√©rent en fonction du nombre d'images que nous utilisons et de la valeur de l'hyper-param√®tre **Œµ**. L'image de gauche est l'image originale. Dans l'article, [Pal & Sudeep (2016)](https://ieeexplore.ieee.org/document/7808140/) ont utilis√© 10000 images et epsilon = 0,1. Cela correspond √† l'image en bas √† gauche.

C'est tout !

J'esp√®re que vous avez trouv√© quelque chose d'int√©ressant dans cet article. Vous pouvez le lire sur mon [blog](https://hadrienj.github.io/posts/Preprocessing-for-deep-learning/), avec LaTeX pour les maths, ainsi que d'autres articles.

Vous pouvez √©galement forker le notebook Jupyter sur Github [ici](https://github.com/hadrienj/Preprocessing-for-deep-learning).

#### R√©f√©rences

[K. Jarrett, K. Kavukcuoglu, M. Ranzato, et Y. LeCun, ¬´ What is the best multi-stage architecture for object recognition ? ¬ª, dans 2009 IEEE 12th International Conference on Computer Vision, 2009, pp. 2146‚Äì2153.](https://www.computer.org/csdl/proceedings/iccv/2009/4420/00/05459469.pdf)

[A. Krizhevsky, ¬´ Learning Multiple Layers of Features from Tiny Images ¬ª, M√©moire de ma√Ætrise, Universit√© de Tront, 2009.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.222.9220&rep=rep1&type=pdf)

[Y. A. LeCun, L. Bottou, G. B. Orr, et K.-R. M√ºller, ¬´ Efficient BackProp ¬ª, dans Neural Networks: Tricks of the Trade, Springer, Berlin, Heidelberg, 2012, pp. 9‚Äì48.](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)

[K. K. Pal et K. S. Sudeep, ¬´ Preprocessing for image classification by convolutional neural networks ¬ª, dans 2016 IEEE International Conference on Recent Trends in Electronics, Information Communication Technology (RTEICT), 2016, pp. 1778‚Äì1781.](https://ieeexplore.ieee.org/document/7808140/)

[L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, et R. Fergus, ¬´ Regularization of Neural Networks using DropConnect ¬ª, dans International Conference on Machine Learning, 2013, pp. 1058‚Äì1066.](http://proceedings.mlr.press/v28/wan13.html)

**Excellent ressources et QA**

[Wikipedia ‚Äî Transformation de blanchiment](https://en.wikipedia.org/wiki/Whitening_transformation)

[CS231 ‚Äî R√©seaux de neurones convolutionnels pour la reconnaissance visuelle](http://cs231n.github.io/neural-networks-2/)

[Dustin Stansbury ‚Äî The Clever Machine](https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/)

[Quelques d√©tails sur la matrice de covariance](http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/)

[SO ‚Äî Blanchiment d'image en Python](https://stackoverflow.com/questions/41635737/is-this-the-correct-way-of-whitening-an-image-in-python)

[Normalisation par la moyenne par image ou √† partir de l'ensemble de donn√©es entier](http://ufldl.stanford.edu/wiki/index.php/Data_Preprocessing)

[Soustraction de la moyenne ‚Äî toutes les images ou par image ?](https://stackoverflow.com/questions/29743523/subtract-mean-from-image)

[Pourquoi le centrage est important ‚Äî Voir la section 4.3](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)

[Kernel Kaggle sur ZCA](https://www.kaggle.com/nicw102168/exploring-zca-and-color-image-whitening/notebook)

[Comment ZCA est impl√©ment√© dans Keras](https://github.com/keras-team/keras-preprocessing/blob/b9d142456a64ef228475f07cb2f2d38fd05bd249/keras_preprocessing/image.py#L1254:L1257)