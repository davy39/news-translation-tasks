---
title: Une introduction aux Policy Gradients avec Cartpole et Doom
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2018-05-09T18:25:15.000Z'
originalURL: https://freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f
coverImage: https://cdn-media-1.freecodecamp.org/images/1*q00eKh5Tl9325LyfZrMwZA.png
tags:
- name: Artificial Intelligence
  slug: artificial-intelligence
- name: Deep Learning
  slug: deep-learning
- name: Machine Learning
  slug: machine-learning
- name: General Programming
  slug: programming
- name: 'tech '
  slug: tech
seo_title: Une introduction aux Policy Gradients avec Cartpole et Doom
seo_desc: 'By Thomas Simonini


  This article is part of Deep Reinforcement Learning Course with Tensorflow ?Ô∏è. Check
  the syllabus here.


  In the last two articles about Q-learning and Deep Q learning, we worked with value-based
  reinforcement learning algorithms. ...'
---

Par Thomas Simonini

> Cet article fait partie du cours Deep Reinforcement Learning avec Tensorflow üèÅ. Consultez le programme [ici](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/).

Dans les deux derniers articles sur [Q-learning](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe) et [Deep Q learning](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8), nous avons travaill√© avec des algorithmes d'apprentissage par renforcement bas√©s sur la valeur. Pour choisir quelle action entreprendre √©tant donn√© un √©tat, nous prenons l'action avec la valeur Q la plus √©lev√©e (r√©compense future maximale attendue que je vais obtenir √† chaque √©tat). Par cons√©quent, dans l'apprentissage bas√© sur la valeur, une politique n'existe que gr√¢ce √† ces estimations de valeur d'action.

Aujourd'hui, nous allons apprendre une technique d'apprentissage par renforcement bas√©e sur les politiques appel√©e Policy Gradients.

Nous allons impl√©menter deux agents. Le premier apprendra √† garder la barre en √©quilibre.

![Image](https://cdn-media-1.freecodecamp.org/images/1*Wj5RZ_EqKIeCQ4E7DgdvCw.gif)

Le second sera un agent qui apprend √† survivre dans un environnement hostile de Doom en collectant des points de vie.

![Image](https://cdn-media-1.freecodecamp.org/images/1*dNEZ6GX3Fp4DCLj59XrnFQ.gif)
_Notre agent Policy Gradients_

Dans les m√©thodes bas√©es sur les politiques, au lieu d'apprendre une fonction de valeur qui nous dit quelle est la somme attendue des r√©compenses √©tant donn√© un √©tat et une action, nous apprenons directement la fonction de politique qui mappe l'√©tat √† l'action (s√©lectionner des actions sans utiliser une fonction de valeur).

Cela signifie que nous essayons directement d'optimiser notre fonction de politique œÄ sans nous soucier d'une fonction de valeur. Nous allons directement param√©trer œÄ (s√©lectionner une action sans une fonction de valeur).

Bien s√ªr, nous pouvons utiliser une fonction de valeur pour optimiser les param√®tres de la politique. Mais la fonction de valeur ne sera pas utilis√©e pour s√©lectionner une action.

Dans cet article, vous apprendrez :

* Qu'est-ce que Policy Gradient, et ses avantages et inconv√©nients
* Comment l'impl√©menter dans Tensorflow.

### Pourquoi utiliser des m√©thodes bas√©es sur les politiques ?

#### Deux types de politiques

Une politique peut √™tre soit d√©terministe soit stochastique.

Une politique d√©terministe est une politique qui mappe les √©tats aux actions. Vous lui donnez un √©tat et la fonction retourne une action √† entreprendre.

![Image](https://cdn-media-1.freecodecamp.org/images/1*NDEGtK42rEpYLkTPg2LBPA.png)

Les politiques d√©terministes sont utilis√©es dans des environnements d√©terministes. Ce sont des environnements o√π les actions entreprises d√©terminent le r√©sultat. Il n'y a pas d'incertitude. Par exemple, lorsque vous jouez aux √©checs et que vous d√©placez votre pion de A2 √† A3, vous √™tes s√ªr que votre pion se d√©placera √† A3.

D'autre part, une politique stochastique produit une distribution de probabilit√© sur les actions.

![Image](https://cdn-media-1.freecodecamp.org/images/1*YCABimP7x1wZZZKqz2CoyQ.png)

Cela signifie qu'au lieu d'√™tre s√ªr de prendre l'action _a_ (par exemple √† gauche), il y a une probabilit√© que nous prenions une action diff√©rente (dans ce cas, 30 % que nous prenions sud).

La politique stochastique est utilis√©e lorsque l'environnement est incertain. Nous appelons ce processus un Processus de D√©cision de Markov Partiellement Observable (POMDP).

La plupart du temps, nous utiliserons ce second type de politique.

#### Avantages

> Mais le Deep Q Learning est vraiment g√©nial ! Pourquoi utiliser des m√©thodes d'apprentissage par renforcement bas√©es sur les politiques ?

Il y a trois avantages principaux √† utiliser les Policy Gradients.

#### Convergence

Tout d'abord, les m√©thodes bas√©es sur les politiques ont de meilleures propri√©t√©s de convergence.

Le probl√®me avec les m√©thodes bas√©es sur la valeur est qu'elles peuvent avoir de grandes oscillations pendant l'entra√Ænement. Cela est d√ª au fait que le choix de l'action peut changer de mani√®re dramatique pour un changement arbitrairement petit dans les valeurs d'action estim√©es.

D'autre part, avec le gradient de politique, nous suivons simplement le gradient pour trouver les meilleurs param√®tres. Nous voyons une mise √† jour fluide de notre politique √† chaque √©tape.

Parce que nous suivons le gradient pour trouver les meilleurs param√®tres, nous sommes garantis de converger vers un maximum local (pire cas) ou un maximum global (meilleur cas).

![Image](https://cdn-media-1.freecodecamp.org/images/1*0lYcY5TBSqfNwdu8TduB6g.png)

#### Les gradients de politique sont plus efficaces dans les espaces d'action de haute dimension

Le deuxi√®me avantage est que les gradients de politique sont plus efficaces dans les espaces d'action de haute dimension, ou lors de l'utilisation d'actions continues.

Le probl√®me avec le Deep Q-learning est que leurs pr√©dictions attribuent un score (r√©compense future maximale attendue) pour chaque action possible, √† chaque √©tape de temps, √©tant donn√© l'√©tat actuel.

Mais que se passe-t-il si nous avons une possibilit√© infinie d'actions ?

Par exemple, avec une voiture autonome, √† chaque √©tat, vous pouvez avoir un choix (presque) infini d'actions (tourner le volant √† 15¬∞, 17,2¬∞, 19,4¬∞, klaxonner‚Ä¶). Nous devrons produire une valeur Q pour chaque action possible !

D'autre part, dans les m√©thodes bas√©es sur les politiques, vous ajustez simplement les param√®tres directement : gr√¢ce √† cela, vous commencerez √† comprendre ce que sera le maximum, plut√¥t que de calculer (estimer) le maximum directement √† chaque √©tape.

![Image](https://cdn-media-1.freecodecamp.org/images/1*_hAkM4RIxjKjKqAYFR_9CQ.png)

#### Les gradients de politique peuvent apprendre des politiques stochastiques

Un troisi√®me avantage est que le gradient de politique peut apprendre une politique stochastique, alors que les fonctions de valeur ne le peuvent pas. Cela a deux cons√©quences.

L'une d'entre elles est que nous n'avons pas besoin d'impl√©menter un compromis exploration/exploitation. Une politique stochastique permet √† notre agent d'explorer l'espace d'√©tat sans toujours prendre la m√™me action. Cela est d√ª au fait qu'elle produit une distribution de probabilit√© sur les actions. Par cons√©quent, elle g√®re le compromis exploration/exploitation sans le coder en dur.

Nous nous d√©barrassons √©galement du probl√®me d'aliasing perceptuel. L'aliasing perceptuel se produit lorsque nous avons deux √©tats qui semblent √™tre (ou sont r√©ellement) les m√™mes, mais qui n√©cessitent des actions diff√©rentes.

Prenons un exemple. Nous avons un aspirateur intelligent, et son objectif est d'aspirer la poussi√®re et d'√©viter de tuer les hamsters.

![Image](https://cdn-media-1.freecodecamp.org/images/1*Zy9JMzCF3zwWDbjPaiKd2w.png)
_Cet exemple a √©t√© inspir√© par l'excellent cours de David Silver : [http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf" rel="noopener" target="_blank" title=")_

Notre aspirateur ne peut percevoir que l'emplacement des murs.

Le probl√®me : les deux cas rouges sont des √©tats alias√©s, car l'agent per√ßoit un mur sup√©rieur et un mur inf√©rieur pour chacun des deux.

Sous une politique d√©terministe, la politique sera soit de se d√©placer √† droite lorsque l'√©tat est rouge, soit de se d√©placer √† gauche. Dans les deux cas, notre agent sera bloqu√© et ne pourra jamais aspirer la poussi√®re.

![Image](https://cdn-media-1.freecodecamp.org/images/1*V-jY8KezWKfsca_DExtXPQ.png)

Sous un algorithme RL bas√© sur la valeur, nous apprenons une politique quasi-d√©terministe (strat√©gie "epsilon greedy"). Par cons√©quent, notre agent peut passer beaucoup de temps avant de trouver la poussi√®re.

D'autre part, une politique stochastique optimale se d√©placera al√©atoirement √† gauche ou √† droite dans les √©tats gris. Par cons√©quent, elle ne sera pas bloqu√©e et atteindra l'√©tat objectif avec une probabilit√© √©lev√©e.

![Image](https://cdn-media-1.freecodecamp.org/images/1*zwe5kBczuErX8c0TH3rAmg.png)

#### Inconv√©nients

Naturellement, les Policy Gradients ont un grand inconv√©nient. La plupart du temps, ils convergent vers un maximum local plut√¥t que vers l'optimum global.

Au lieu du Deep Q-Learning, qui essaie toujours d'atteindre le maximum, les Policy Gradients convergent plus lentement, √©tape par √©tape. Ils peuvent prendre plus de temps √† s'entra√Æner.

Cependant, nous verrons qu'il existe des solutions √† ce probl√®me.

### Recherche de politique

Nous avons notre politique œÄ qui a un param√®tre Œ∏. Ce œÄ produit une distribution de probabilit√© des actions.

![Image](https://cdn-media-1.freecodecamp.org/images/0*354cfoILK19WFTWa.)
_Probabilit√© de prendre l'action a √©tant donn√© l'√©tat s avec les param√®tres theta._

G√©nial ! Mais comment savons-nous si notre politique est bonne ?

Rappelez-vous que la politique peut √™tre vue comme un probl√®me d'optimisation. Nous devons trouver les meilleurs param√®tres (Œ∏) pour maximiser une fonction de score, J(Œ∏).

![Image](https://cdn-media-1.freecodecamp.org/images/0*PfUAJaIGoEsvfbCG.)

Il y a deux √©tapes :

* Mesurer la qualit√© d'une œÄ (politique) avec une fonction de score de politique J(Œ∏)
* Utiliser l'ascension du gradient de politique pour trouver le meilleur param√®tre Œ∏ qui am√©liore notre œÄ.

L'id√©e principale ici est que J(Œ∏) nous dira √† quel point notre œÄ est bonne. L'ascension du gradient de politique nous aidera √† trouver les meilleurs param√®tres de politique pour maximiser l'√©chantillon des bonnes actions.

#### Premi√®re √©tape : la fonction de score de politique J(Œ∏)

Pour mesurer √† quel point notre politique est bonne, nous utilisons une fonction appel√©e fonction objectif (ou fonction de score de politique) qui calcule la r√©compense attendue de la politique.

Trois m√©thodes fonctionnent √©galement bien pour optimiser les politiques. Le choix d√©pend uniquement de l'environnement et des objectifs que vous avez.

Tout d'abord, dans un environnement √©pisodique, nous pouvons utiliser la valeur de d√©part. Calculez la moyenne du retour √† partir de la premi√®re √©tape de temps (G1). Il s'agit de la r√©compense cumul√©e actualis√©e pour l'√©pisode entier.

![Image](https://cdn-media-1.freecodecamp.org/images/1*tP4l4IrIG3aMLTrMt-1-HA.png)

L'id√©e est simple. Si je commence toujours dans un certain √©tat s1, quelle est la r√©compense totale que j'obtiendrai √† partir de cet √©tat de d√©part jusqu'√† la fin ?

Nous voulons trouver la politique qui maximise G1, car elle sera la politique optimale. Cela est d√ª √† l'hypoth√®se de r√©compense [expliqu√©e dans le premier article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419).

Par exemple, dans Breakout, je joue une nouvelle partie, mais j'ai perdu la balle apr√®s 20 briques d√©truites (fin de la partie). Les nouveaux √©pisodes commencent toujours au m√™me √©tat.

![Image](https://cdn-media-1.freecodecamp.org/images/0*bNljRIeIigzMKh_F.png)

Je calcule le score en utilisant J1(Œ∏). Frapper 20 briques est bien, mais je veux am√©liorer le score. Pour cela, je devrai am√©liorer les distributions de probabilit√© de mes actions en ajustant les param√®tres. Cela se produit dans l'√©tape 2.

Dans un environnement continu, nous pouvons utiliser la valeur moyenne, car nous ne pouvons pas nous fier √† un √©tat de d√©part sp√©cifique.

Chaque valeur d'√©tat est maintenant pond√©r√©e (parce que certaines se produisent plus que d'autres) par la probabilit√© de l'occurrence de l'√©tat respect√©.

![Image](https://cdn-media-1.freecodecamp.org/images/1*S-XLkrvPuVUqLrFW1hmIMg.png)

Troisi√®mement, nous pouvons utiliser la r√©compense moyenne par √©tape de temps. L'id√©e ici est que nous voulons obtenir le plus de r√©compense par √©tape de temps.

![Image](https://cdn-media-1.freecodecamp.org/images/1*3SejRRby6vAnThZ8c2UaQg.png)

#### Deuxi√®me √©tape : l'ascension du gradient de politique

Nous avons une fonction de score de politique qui nous dit √† quel point notre politique est bonne. Maintenant, nous voulons trouver un param√®tre Œ∏ qui maximise cette fonction de score. Maximiser la fonction de score signifie trouver la politique optimale.

Pour maximiser la fonction de score J(Œ∏), nous devons faire une ascension de gradient sur les param√®tres de la politique.

L'ascension de gradient est l'inverse de la descente de gradient. Rappelez-vous que le gradient pointe toujours vers le changement le plus raide.

Dans la descente de gradient, nous prenons la direction de la diminution la plus raide de la fonction. Dans l'ascension de gradient, nous prenons la direction de l'augmentation la plus raide de la fonction.

Pourquoi l'ascension de gradient et non la descente de gradient ? Parce que nous utilisons la descente de gradient lorsque nous avons une fonction d'erreur que nous voulons minimiser.

Mais la fonction de score n'est pas une fonction d'erreur ! C'est une fonction de score, et parce que nous voulons maximiser le score, nous avons besoin de l'ascension de gradient.

L'id√©e est de trouver le gradient de la politique actuelle œÄ qui met √† jour les param√®tres dans la direction de la plus grande augmentation, et it√©rer.

![Image](https://cdn-media-1.freecodecamp.org/images/0*oh-lF13hYWt2Bd6V.)

D'accord, maintenant impl√©mentons cela math√©matiquement. Cette partie est un peu difficile, mais il est fondamental de comprendre comment nous arrivons √† notre formule de gradient.

Nous voulons trouver les meilleurs param√®tres Œ∏*, qui maximisent le score :

![Image](https://cdn-media-1.freecodecamp.org/images/1*xoGZI5v6lBS8s5OtBteJMA.png)

Notre fonction de score peut √™tre d√©finie comme :

![Image](https://cdn-media-1.freecodecamp.org/images/1*dl4Fp0Izhv6bC0-qgThByA.png)

Qui est la somme totale de la r√©compense attendue √©tant donn√© la politique.

Maintenant, parce que nous voulons faire une ascension de gradient, nous devons diff√©rencier notre fonction de score J(Œ∏).

Notre fonction de score J(Œ∏) peut √©galement √™tre d√©finie comme :

![Image](https://cdn-media-1.freecodecamp.org/images/1*qySDorYr55KgVJ6H3bu_6Q.png)

Nous avons √©crit la fonction de cette mani√®re pour montrer le probl√®me auquel nous sommes confront√©s ici.

Nous savons que les param√®tres de la politique changent la mani√®re dont les actions sont choisies, et par cons√©quent, les r√©compenses que nous obtenons et les √©tats que nous verrons et √† quelle fr√©quence.

Il peut donc √™tre difficile de trouver les changements de politique de mani√®re √† garantir une am√©lioration. Cela est d√ª au fait que la performance d√©pend des s√©lections d'actions et de la distribution des √©tats dans lesquels ces s√©lections sont faites.

Ces deux aspects sont affect√©s par les param√®tres de la politique. L'effet des param√®tres de la politique sur les actions est simple √† trouver, mais comment trouver l'effet de la politique sur la distribution des √©tats ? La fonction de l'environnement est inconnue.

Par cons√©quent, nous sommes confront√©s √† un probl√®me : comment estimer le ‚àá (gradient) par rapport √† la politique Œ∏, lorsque le gradient d√©pend de l'effet inconnu des changements de politique sur la distribution des √©tats ?

La solution sera d'utiliser le Th√©or√®me du Gradient de Politique. Cela fournit une expression analytique pour le gradient ‚àá de J(Œ∏) (performance) par rapport √† la politique Œ∏ qui ne n√©cessite pas la diff√©rentiation de la distribution des √©tats.

Calculons donc :

![Image](https://cdn-media-1.freecodecamp.org/images/1*dl4Fp0Izhv6bC0-qgThByA.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*i72jd_Hrimu9Aag70WGDmQ.png)

Rappelez-vous, nous sommes dans une situation de politique stochastique. Cela signifie que notre politique produit une distribution de probabilit√© œÄ(œÑ ; Œ∏). Elle produit la probabilit√© de prendre cette s√©rie d'√©tapes (s0, a0, r0‚Ä¶), √©tant donn√© nos param√®tres actuels Œ∏.

Mais diff√©rencier une fonction de probabilit√© est difficile, sauf si nous pouvons la transformer en logarithme. Cela la rend beaucoup plus simple √† diff√©rencier.

Ici, nous utiliserons l'[astuce du rapport de vraisemblance](http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/) qui remplace la fraction r√©sultante par une probabilit√© logarithmique.

![Image](https://cdn-media-1.freecodecamp.org/images/1*iKhO5anOAfc3oqJOM2i_8A.png)

Maintenant, convertissons la somme en une attente :

![Image](https://cdn-media-1.freecodecamp.org/images/1*4Y7BwUu2JBRIJ8bxXkzDjg.png)

Comme vous pouvez le voir, nous devons seulement calculer la d√©riv√©e de la fonction de politique logarithmique.

Maintenant que nous avons fait cela, et c'√©tait beaucoup, nous pouvons conclure sur les gradients de politique :

![Image](https://cdn-media-1.freecodecamp.org/images/1*zjEh737KfmDUzNECjW4e4w.png)

Ce gradient de politique nous dit comment nous devons d√©placer la distribution de la politique en changeant les param√®tres Œ∏ si nous voulons obtenir un score plus √©lev√©.

R(œÑ) est comme un score de valeur scalaire :

* Si R(œÑ) est √©lev√©, cela signifie qu'en moyenne nous avons pris des actions qui ont conduit √† des r√©compenses √©lev√©es. Nous voulons augmenter les probabilit√©s des actions vues (augmenter la probabilit√© de prendre ces actions).
* D'autre part, si R(œÑ) est faible, nous voulons diminuer les probabilit√©s des actions vues.

Ce gradient de politique fait que les param√®tres se d√©placent le plus dans la direction qui favorise les actions qui ont le retour le plus √©lev√©.

### Monte Carlo Policy Gradients

Dans notre notebook, nous utiliserons cette approche pour concevoir l'algorithme de gradient de politique. Nous utilisons Monte Carlo parce que nos t√¢ches peuvent √™tre divis√©es en √©pisodes.

```
Initialiser Œ∏
pour chaque √©pisode œÑ = S0, A0, R1, S1, ‚Ä¶, ST :
    pour t <-- 1 √† T-1 :
        ŒîŒ∏ = Œ± ‚àáŒ∏(log œÄ(St, At, Œ∏)) Gt
        Œ∏ = Œ∏ + ŒîŒ∏
```

```
Pour chaque √©pisode :
    √Ä chaque √©tape de temps dans cet √©pisode :
         Calculer les probabilit√©s logarithmiques produites par notre fonction de politique. Multiplier par la fonction de score.
         Mettre √† jour les poids
```

Mais nous sommes confront√©s √† un probl√®me avec cet algorithme. Parce que nous ne calculons R qu'√† la fin de l'√©pisode, nous faisons la moyenne de toutes les actions. M√™me si certaines des actions entreprises √©taient tr√®s mauvaises, si notre score est assez √©lev√©, nous ferons la moyenne de toutes les actions comme bonnes.

Ainsi, pour avoir une politique correcte, nous avons besoin de beaucoup d'√©chantillons‚Ä¶ ce qui entra√Æne un apprentissage lent.

### Comment am√©liorer notre mod√®le ?

Nous verrons dans les prochains articles quelques am√©liorations :

* Actor Critic : un hybride entre les algorithmes bas√©s sur la valeur et les algorithmes bas√©s sur les politiques.
* Proximal Policy Gradients : garantit que la d√©viation par rapport √† la politique pr√©c√©dente reste relativement faible.

### Impl√©mentons-le avec Cartpole et Doom

> Nous avons fait une vid√©o o√π nous impl√©mentons un **agent Policy Gradient avec Tensorflow qui apprend √† jouer √† Doom üíÄ dans un environnement Deathmatch.**

**Vous pouvez acc√©der directement aux notebooks dans le [d√©p√¥t du cours Deep Reinforcement Learning](https://github.com/simoninithomas/Deep_reinforcement_learning_Course).**

**Cartpole :**

**Doom :**

**C'est tout ! Vous venez de cr√©er un agent qui apprend √† survivre dans un environnement Doom. G√©nial !**

**N'oubliez pas d'impl√©menter chaque partie du code par vous-m√™me. Il est vraiment important d'essayer de modifier le code que je vous ai donn√©. Essayez d'ajouter des √©poques, de changer l'architecture, de changer le taux d'apprentissage, d'utiliser un environnement plus difficile‚Ä¶ et ainsi de suite. Amusez-vous !**

**Dans le prochain article, je discuterai des derni√®res am√©liorations du Deep Q-learning :**

* **Valeurs Q fixes**
* **Replay d'exp√©rience prioritaire**
* **Double DQN**
* **R√©seaux Dueling**

**Si vous avez aim√© mon article, **cliquez sur le üíô ci-dessous autant de fois que vous avez aim√© l'article** afin que d'autres personnes puissent le voir ici sur Medium. Et n'oubliez pas de me suivre !**

**Si vous avez des pens√©es, des commentaires, des questions, n'h√©sitez pas √† commenter ci-dessous ou √† m'envoyer un email : hello@simoninithomas.com, ou √† me tweeter [@ThomasSimonini](https://twitter.com/ThomasSimonini).**

![Image](https://cdn-media-1.freecodecamp.org/images/1*_yN1FzvEFDmlObiYsstIzg.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*mD-f5VN1SWYvhrZAbvSu_w.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*PqiptT-Cdi8uwosxuFn2DQ.png)

#### **Continuez √† apprendre, restez g√©nial !**

#### **Cours de Deep Reinforcement Learning avec Tensorflow üèÅ**

**üìã S[yllabus](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)**

**üé• V[ersion vid√©o](https://www.youtube.com/channel/UC8XuSf1eD9AF8x8J19ha5og?view_as=subscriber)**

**Partie 1 : [Une introduction √† l'apprentissage par renforcement](https://medium.com/p/4339519de419/edit)**

**Partie 2 : [Plonger plus profond√©ment dans l'apprentissage par renforcement avec Q-Learning](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)**

**Partie 3 : [Une introduction au Deep Q-Learning : jouons √† Doom](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)**

**Partie 3+ : [Am√©liorations du Deep Q Learning : Dueling Double DQN, Prioritized Experience Replay, et Q-targets fixes](https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682)**

**Partie 4 : [Une introduction aux Policy Gradients avec Doom et Cartpole](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)**

**Partie 5 : [Une introduction aux m√©thodes Advantage Actor Critic : jouons √† Sonic the Hedgehog !](https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d)**

**Partie 6 : [Proximal Policy Optimization (PPO) avec Sonic the Hedgehog 2 et 3](https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e)**

**Partie 7 : [L'apprentissage par curiosit√© rendu facile Partie I](https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359)**