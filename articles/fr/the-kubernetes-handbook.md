---
title: Le manuel Kubernetes ‚Äì Apprendre Kubernetes pour les d√©butants
subtitle: ''
author: Farhan Hasin Chowdhury
co_authors: []
series: null
date: '2020-08-20T15:53:43.000Z'
originalURL: https://freecodecamp.org/news/the-kubernetes-handbook
coverImage: https://www.freecodecamp.org/news/content/images/2023/07/Kubernetes-Handbook-Mockup.png
tags:
- name: containerization
  slug: containerization
- name: containers
  slug: containers
- name: Devops
  slug: devops
- name: Docker
  slug: docker
- name: Kubernetes
  slug: kubernetes
seo_title: Le manuel Kubernetes ‚Äì Apprendre Kubernetes pour les d√©butants
seo_desc: "Kubernetes is an open-source container orchestration platform that automates\
  \ the deployment, management, scaling, and networking of containers. \nIt was developed\
  \ by Google using the Go Programming Language, and this amazing technology has been\
  \ open-s..."
---

[Kubernetes](https://kubernetes.io/) est une plateforme open-source d'orchestration de conteneurs qui automatise le d√©ploiement, la gestion, la mise √† l'√©chelle et la mise en r√©seau des conteneurs. 

Il a √©t√© d√©velopp√© par [Google](https://opensource.google/projects/kubernetes) en utilisant le [langage de programmation Go](https://golang.org/), et cette technologie incroyable est open-source depuis 2014.

Selon le [Stack Overflow Developer Survey - 2020](https://insights.stackoverflow.com/survey/2020#overview), Kubernetes est la [3√®me plateforme la plus aim√©e](https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-platforms-loved5) et la [3√®me plateforme la plus souhait√©e](https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-platforms-wanted5).

En plus d'√™tre tr√®s puissant, Kubernetes est connu pour √™tre assez difficile √† prendre en main. Je ne dirai pas que c'est facile, mais si vous √™tes √©quip√© des pr√©requis et que vous parcourez ce guide attentivement et avec patience, vous devriez √™tre capable de :

* Obtenir une compr√©hension solide des fondamentaux.
* Cr√©er et g√©rer des clusters Kubernetes.
* D√©ployer (presque) n'importe quelle application sur un cluster Kubernetes.

## Pr√©requis

* Familiarit√© avec JavaScript
* Familiarit√© avec le terminal Linux
* Familiarit√© avec Docker (lecture sugg√©r√©e : [The Docker Handbook](https://www.freecodecamp.org/news/the-docker-handbook/))

## Code du projet

Le code des projets d'exemple peut √™tre trouv√© dans le d√©p√¥t suivant :

%[https://github.com/fhsinchy/kubernetes-handbook-projects]

Vous pouvez trouver le code complet dans la branche `completed`.

## Table des mati√®res

* [Introduction √† l'orchestration de conteneurs et Kubernetes](#heading-introduction-a-l-orchestration-de-conteneurs-et-kubernetes)
* [Installation de Kubernetes](#heading-installation-de-kubernetes)
* [Hello World dans Kubernetes](#heading-hello-world-dans-kubernetes)
    * [Architecture de Kubernetes](#heading-architecture-de-kubernetes)
    * [Composants du plan de contr√¥le](#heading-composants-du-plan-de-controle)
    * [Composants des n≈ìuds](#heading-composants-des-noeuds)
    * [Objets Kubernetes](#heading-objets-kubernetes)
    * [Pods](#heading-pods)
    * [Services](#heading-services)
    * [Le tableau complet](#heading-le-tableau-complet-1)
    * [Se d√©barrasser des ressources Kubernetes](#heading-se-debarrasser-des-ressources-kubernetes)
* [Approche de d√©ploiement d√©clarative](#heading-approche-de-deploiement-declarative)
    * [√âcrire votre premier ensemble de configurations](#heading-ecrire-votre-premier-ensemble-de-configurations)
    * [Le tableau de bord Kubernetes](#heading-le-tableau-de-bord-kubernetes)
* [Travailler avec des applications multi-conteneurs](#heading-travailler-avec-des-applications-multi-conteneurs)
    * [Plan de d√©ploiement](#heading-plan-de-deploiement)
    * [Contr√¥leurs de r√©plication, Replica Sets et Deployments](#heading-controleurs-de-replication-replica-sets-et-deployments)
    * [Cr√©er votre premier d√©ploiement](#heading-creer-votre-premier-deploiement)
    * [Inspecter les ressources Kubernetes](#heading-inspecter-les-ressources-kubernetes)
    * [Obtenir les logs des conteneurs depuis les pods](#heading-obtenir-les-logs-des-conteneurs-depuis-les-pods)
    * [Variables d'environnement](#heading-variables-d-environnement)
    * [Cr√©er le d√©ploiement de la base de donn√©es](#heading-creer-le-deploiement-de-la-base-de-donnees)
    * [Volumes persistants et Persistent Volume Claims](#heading-volumes-persistants-et-persistent-volume-claims)
    * [Provisionnement dynamique des volumes persistants](#heading-provisionnement-dynamique-des-volumes-persistants)
    * [Connecter les volumes avec les pods](#heading-connecter-les-volumes-avec-les-pods)
    * [Relier tout ensemble](#heading-relier-tout-ensemble)
* [Travailler avec les contr√¥leurs d'entr√©e](#heading-travailler-avec-les-controleurs-d-entree)
    * [Configuration du contr√¥leur d'entr√©e NGINX](#heading-configuration-du-controleur-d-entree-nginx)
    * [Secrets et Config Maps dans Kubernetes](#heading-secrets-et-config-maps-dans-kubernetes)
    * [Effectuer des d√©ploiements de mises √† jour dans Kubernetes](#heading-effectuer-des-deploiements-de-mises-a-jour-dans-kubernetes)
    * [Combiner les configurations](#heading-combiner-les-configurations)
* [D√©pannage](#heading-depannage-1)
* [Conclusion](#heading-conclusion)

## Introduction √† l'orchestration de conteneurs et Kubernetes

Selon [Red Hat](https://www.redhat.com/en/topics/containers/what-is-container-orchestration) ‚Äî

> "L'orchestration de conteneurs est le processus d'automatisation du d√©ploiement, de la gestion, de la mise √† l'√©chelle et des t√¢ches de mise en r√©seau des conteneurs.   
>   
> Il peut √™tre utilis√© dans tout environnement o√π vous utilisez des conteneurs et peut vous aider √† d√©ployer la m√™me application dans diff√©rents environnements sans n√©cessiter de reconception".

Permettez-moi de vous montrer un exemple. Supposons que vous avez d√©velopp√© une application incroyable qui sugg√®re aux gens ce qu'ils devraient manger en fonction de l'heure de la journ√©e. 

Maintenant, supposons que vous avez conteneuris√© l'application en utilisant Docker et que vous l'avez d√©ploy√©e sur AWS.

![Image](https://www.freecodecamp.org/news/content/images/2020/07/food-suggestion-application-single-instance.svg)

Si l'application tombe en panne pour une raison quelconque, les utilisateurs perdent imm√©diatement l'acc√®s √† votre service. 

Pour r√©soudre ce probl√®me, vous pouvez cr√©er plusieurs copies ou r√©plicas de la m√™me application et la rendre hautement disponible.

![Image](https://www.freecodecamp.org/news/content/images/2020/07/food-suggestion-application-multi-instance.svg)

M√™me si l'une des instances tombe en panne, les deux autres seront disponibles pour les utilisateurs. 

Maintenant, supposons que votre application est devenue extr√™mement populaire parmi les noctambules et que vos serveurs sont submerg√©s de requ√™tes la nuit, pendant que vous dormez. 

Que se passe-t-il si toutes les instances tombent en panne en raison d'une surcharge ? Qui va faire la mise √† l'√©chelle ? M√™me si vous mettez √† l'√©chelle et cr√©ez 50 r√©plicas de votre application, qui va v√©rifier leur √©tat de sant√© ? Comment allez-vous configurer la mise en r√©seau pour que les requ√™tes atteignent le bon point de terminaison ? L'√©quilibrage de charge va √©galement √™tre un grand probl√®me, n'est-ce pas ?

Kubernetes peut rendre les choses beaucoup plus faciles pour ces types de situations. C'est une plateforme d'orchestration de conteneurs qui se compose de plusieurs composants et qui travaille sans rel√¢che pour maintenir vos serveurs dans l'√©tat que vous souhaitez. 

Supposons que vous souhaitez avoir 50 r√©plicas de votre application en cours d'ex√©cution en continu. M√™me s'il y a une augmentation soudaine du nombre d'utilisateurs, le serveur doit √™tre mis √† l'√©chelle automatiquement. 

Vous dites simplement vos besoins √† Kubernetes et il fera le reste du travail pour vous.

![Image](https://www.freecodecamp.org/news/content/images/2020/07/kube-representation.svg)

Kubernetes ne se contentera pas de mettre en ≈ìuvre l'√©tat, il le maintiendra √©galement. Il cr√©era des r√©plicas suppl√©mentaires si l'un des anciens meurt, g√©rera la mise en r√©seau et le stockage, d√©ployera ou annulera les mises √† jour, ou m√™me mettra √† l'√©chelle le serveur si n√©cessaire.

## Installation de Kubernetes

Ex√©cuter Kubernetes sur votre machine locale est en fait tr√®s diff√©rent de l'ex√©cuter sur le cloud. Pour d√©marrer Kubernetes, vous avez besoin de deux programmes.

* [minikube](https://kubernetes.io/docs/tasks/tools/install-minikube/) - il ex√©cute un cluster Kubernetes √† n≈ìud unique √† l'int√©rieur d'une machine virtuelle (VM) sur votre ordinateur local.
* [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) - L'outil en ligne de commande Kubernetes, qui vous permet d'ex√©cuter des commandes contre les clusters Kubernetes.

En plus de ces deux programmes, vous aurez √©galement besoin d'un hyperviseur et d'une plateforme de conteneurisation. [Docker](https://www.docker.com/) est le choix √©vident pour la plateforme de conteneurisation. Les hyperviseurs recommand√©s sont les suivants :

* [Hyper-V](https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/quick-start/enable-hyper-v) pour Windows
* [HyperKit](https://github.com/moby/hyperkit) pour Mac
* [Docker](https://www.docker.com/) pour Linux

Hyper-V est int√©gr√© √† Windows 10 (Pro, Enterprise et Education) en tant que fonctionnalit√© optionnelle et peut √™tre activ√© depuis le panneau de configuration. 

HyperKit est fourni avec Docker Desktop pour Mac en tant que composant principal. 

Et sur Linux, vous pouvez contourner toute la couche hyperviseur en utilisant Docker directement. C'est beaucoup plus rapide que d'utiliser un hyperviseur et c'est la m√©thode recommand√©e pour ex√©cuter Kubernetes sur Linux.

Vous pouvez installer l'un des hyperviseurs mentionn√©s ci-dessus. Ou si vous voulez garder les choses simples, obtenez simplement [VirtualBox](https://www.virtualbox.org/). 

Pour le reste de l'article, je supposerai que vous utilisez VirtualBox. Ne vous inqui√©tez pas, m√™me si vous utilisez autre chose, il ne devrait pas y avoir beaucoup de diff√©rence.

> J'utiliserai `minikube` avec le pilote Docker sur une machine [Ubuntu](https://www.freecodecamp.org/news/p/c4f90e6f-97af-41ce-b775-b6e52a5a5152/ubuntu.com/) tout au long de l'article.

Une fois que vous avez install√© l'hyperviseur et la plateforme de conteneurisation, il est temps d'installer les programmes `minikube` et `kubectl`. 

`kubectl` est g√©n√©ralement fourni avec Docker Desktop sur Mac et Windows. Les instructions d'installation pour Linux peuvent √™tre trouv√©es [ici](https://kubernetes.io/docs/tasks/tools/install-kubectl/). 

`minikube`, en revanche, doit √™tre install√© sur les trois syst√®mes. Vous pouvez utiliser [Homebrew](https://brew.sh/) sur Mac, et [Chocolatey](https://chocolatey.org/) sur Windows pour installer `minikube`. Les instructions d'installation pour Linux peuvent √™tre trouv√©es [ici](https://kubernetes.io/docs/tasks/tools/install-minikube/).

Une fois que vous les avez install√©s, vous pouvez tester les deux programmes en ex√©cutant les commandes suivantes :

```bash
minikube version

# minikube version: v1.12.1
# commit: 5664228288552de9f3a446ea4f51c6f29bbdd0e0

kubectl version

# Client Version: version.Info{Major:"1", Minor:"18", GitVersion:"v1.18.6", GitCommit:"dff82dc0de47299ab66c83c626e08b245ab19037", GitTreeState:"clean", BuildDate:"2020-07-16T00:04:31Z", GoVersion:"go1.14.4", Compiler:"gc", Platform:"darwin/amd64"}
# Server Version: version.Info{Major:"1", Minor:"18", GitVersion:"v1.18.3", GitCommit:"2e7996e3e2712684bc73f0dec0200d64eec7fe40", GitTreeState:"clean", BuildDate:"2020-05-20T12:43:34Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"}
```

Si vous avez t√©l√©charg√© les bonnes versions pour votre syst√®me d'exploitation et que vous avez correctement configur√© les chemins, vous devriez √™tre pr√™t √† partir.

Comme je l'ai d√©j√† mentionn√©, `minikube` ex√©cute un cluster Kubernetes √† n≈ìud unique √† l'int√©rieur d'une machine virtuelle (VM) sur votre ordinateur local. J'expliquerai les clusters et les n≈ìuds en d√©tail dans une section √† venir. 

Pour l'instant, comprenez que `minikube` cr√©e une VM r√©guli√®re en utilisant votre hyperviseur de choix et traite cela comme un cluster Kubernetes.

> Si vous rencontrez des probl√®mes dans cette section, veuillez consulter la section [D√©pannage](#heading-depannage-1) √† la fin de cet article.

Avant de d√©marrer `minikube`, vous devez d√©finir le bon pilote d'hyperviseur √† utiliser. Pour d√©finir VirtualBox comme pilote par d√©faut, ex√©cutez la commande suivante :

```bash
minikube config set driver virtualbox

# ‚ö° Ces changements prendront effet apr√®s un minikube delete et ensuite un minikube start
```

Vous pouvez remplacer `virtualbox` par `hyperv`, `hyperkit`, ou `docker` selon votre pr√©f√©rence. Cette commande est n√©cessaire uniquement pour la premi√®re fois. 

Pour d√©marrer `minikube`, ex√©cutez la commande suivante :

```bash
minikube start

# üòÑ minikube v1.12.1 sur Ubuntu 20.04
# ‚ö° Utilisation du pilote virtualbox bas√© sur le profil existant
# üî• D√©marrage du n≈ìud de plan de contr√¥le minikube dans le cluster minikube
# üê≥ Mise √† jour de la VM virtualbox "minikube" en cours d'ex√©cution...
# üöÄ Pr√©paration de Kubernetes v1.18.3 sur Docker 19.03.12...
# ‚úÖ V√©rification des composants Kubernetes...
# üîó Addons activ√©s : default-storageclass, storage-provisioner
# üéâ Termin√© ! kubectl est maintenant configur√© pour utiliser "minikube"
```

Vous pouvez arr√™ter `minikube` en ex√©cutant la commande `minikube stop`.

## Hello World dans Kubernetes

Maintenant que vous avez Kubernetes sur votre syst√®me local, il est temps de mettre les mains dans le cambouis. Dans cet exemple, vous allez d√©ployer une application tr√®s simple sur votre cluster local et vous familiariser avec les fondamentaux.

> Il y aura des terminologies comme **pod**, **service**, **load balancer**, et ainsi de suite dans cette section. Ne stressez pas si vous ne les comprenez pas tout de suite. Je vais entrer dans les d√©tails en expliquant chacun d'eux dans la sous-section [Le tableau complet](#heading-le-tableau-complet-1).

Si vous avez d√©marr√© `minikube` dans la section pr√©c√©dente, vous √™tes pr√™t √† partir. Sinon, vous devrez le d√©marrer maintenant. Une fois que `minikube` a d√©marr√©, ex√©cutez la commande suivante dans votre terminal :

```bash
kubectl run hello-kube --image=fhsinchy/hello-kube --port=80

# pod/hello-kube created
```

Vous verrez le message `pod/hello-kube created` presque imm√©diatement. La commande [run](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#run) ex√©cute l'image de conteneur donn√©e √† l'int√©rieur d'un pod. 

Les pods sont comme une bo√Æte qui encapsule un conteneur. Pour vous assurer que le pod a √©t√© cr√©√© et est en cours d'ex√©cution, ex√©cutez la commande suivante :

```bash
kubectl get pod

# NAME         READY   STATUS    RESTARTS   AGE
# hello-kube   1/1     Running   0          3m3s
```

Vous devriez voir `Running` dans la colonne `STATUS`. Si vous voyez quelque chose comme `ContainerCreating`, attendez une minute ou deux et v√©rifiez √† nouveau. 

Les pods sont par d√©faut inaccessibles depuis l'ext√©rieur du cluster. Pour les rendre accessibles, vous devez les exposer en utilisant un service. Donc, une fois que le pod est op√©rationnel, ex√©cutez la commande suivante pour exposer le pod :

```bash
kubectl expose pod hello-kube --type=LoadBalancer --port=80

# service/hello-kube exposed
```

Pour vous assurer que le service de l'√©quilibreur de charge a √©t√© cr√©√© avec succ√®s, ex√©cutez la commande suivante :

```bash
kubectl get service

# NAME         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
# hello-kube   LoadBalancer   10.109.60.75   <pending>     80:30848/TCP   119s
# kubernetes   ClusterIP      10.96.0.1      <none>        443/TCP        7h47m
```

Assurez-vous de voir le service `hello-kube` dans la liste. Maintenant que vous avez un pod en cours d'ex√©cution qui est expos√©, vous pouvez aller de l'avant et y acc√©der. Ex√©cutez la commande suivante pour ce faire :

```bash
minikube service hello-kube

# |-----------|------------|-------------|-----------------------------|
# | NAMESPACE |    NAME    | TARGET PORT |             URL             |
# |-----------|------------|-------------|-----------------------------|
# | default   | hello-kube |          80 | http://192.168.99.101:30848 |
# |-----------|------------|-------------|-----------------------------|
# üéâ Opening service default/hello-kube in default browser...
```

Votre navigateur web par d√©faut devrait s'ouvrir automatiquement et vous devriez voir quelque chose comme ceci :

![Image](https://www.freecodecamp.org/news/content/images/2020/08/image-85.png)

Ceci est une application JavaScript tr√®s simple que j'ai assembl√©e en utilisant [vite](https://github.com/vitejs/vite) et un peu de CSS. Pour comprendre ce que vous venez de faire, vous devez acqu√©rir une bonne compr√©hension de l'architecture Kubernetes.

### Architecture de Kubernetes

Dans le monde de Kubernetes, un **n≈ìud** peut √™tre soit une machine physique, soit une machine virtuelle avec un r√¥le donn√©. Une collection de telles machines ou serveurs utilisant un r√©seau partag√© pour communiquer entre eux est appel√©e un **cluster**.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/nodes-cluster-1.svg)

Dans votre configuration locale, `minikube` est un cluster Kubernetes √† n≈ìud unique. Au lieu d'avoir plusieurs serveurs comme dans le diagramme ci-dessus, `minikube` n'en a qu'un seul qui agit √† la fois comme serveur principal et comme n≈ìud.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/minikube-1.svg)

Chaque serveur dans un cluster Kubernetes obtient un r√¥le. Il y a deux r√¥les possibles :

* **control-plane** ‚Äî Prend la plupart des d√©cisions n√©cessaires et agit comme une sorte de cerveau de l'ensemble du cluster. Cela peut √™tre un seul serveur ou un groupe de serveurs dans des projets plus importants.
* **node** ‚Äî Responsable de l'ex√©cution des charges de travail. Ces serveurs sont g√©n√©ralement micro-g√©r√©s par le plan de contr√¥le et ex√©cutent diverses t√¢ches en suivant les instructions fournies. 

Chaque serveur de votre cluster aura un ensemble s√©lectionn√© de composants. Le nombre et le type de ces composants peuvent varier en fonction du r√¥le qu'un serveur a dans votre cluster. Cela signifie que les n≈ìuds n'ont pas tous les composants que le plan de contr√¥le poss√®de.

Dans les sous-sections √† venir, vous aurez un aper√ßu plus d√©taill√© des composants individuels qui constituent un cluster Kubernetes.

### Composants du plan de contr√¥le

Le plan de contr√¥le dans un cluster Kubernetes se compose de **cinq** composants. Ils sont les suivants :

1. **kube-api-server** : Cela agit comme l'entr√©e du plan de contr√¥le Kubernetes, responsable de la validation et du traitement des requ√™tes livr√©es en utilisant des biblioth√®ques clientes comme le programme `kubectl`.
2. **etcd** : Il s'agit d'un magasin cl√©-valeur distribu√© qui agit comme la seule source de v√©rit√© sur votre cluster. Il contient les donn√©es de configuration et les informations sur l'√©tat du cluster. [etcd](https://etcd.io/) est un projet open-source et est d√©velopp√© par les gens derri√®re Red Hat. Le code source du projet est h√©berg√© sur le d√©p√¥t GitHub [etcd-io/etcd](https://github.com/etcd-io/etcd).
3. **kube-controller-manager** : Les contr√¥leurs dans Kubernetes sont responsables du contr√¥le de l'√©tat du cluster. Lorsque vous laissez Kubernetes savoir ce que vous voulez dans votre cluster, les contr√¥leurs s'assurent que votre demande est satisfaite. Le `kube-controller-manager` est l'ensemble des processus de contr√¥leur regroup√©s en un seul processus.
4. **kube-scheduler** : L'attribution de t√¢ches √† un certain n≈ìud en tenant compte de ses ressources disponibles et des exigences de la t√¢che est connue sous le nom de planification. Le composant `kube-scheduler` effectue la t√¢che de planification dans Kubernetes en s'assurant qu'aucun des serveurs du cluster n'est surcharg√©.
5. **cloud-controller-manager** : Dans un environnement cloud r√©el, ce composant vous permet de connecter votre cluster √† l'API de votre fournisseur cloud ([GKE](https://cloud.google.com/kubernetes-engine)/[EKS](https://aws.amazon.com/eks/)). De cette mani√®re, les composants qui interagissent avec cette plateforme cloud restent isol√©s des composants qui interagissent uniquement avec votre cluster. Dans un cluster local comme `minikube`, ce composant n'existe pas.

### Composants des n≈ìuds

Compar√© au plan de contr√¥le, les n≈ìuds ont un tr√®s petit nombre de composants. Ces composants sont les suivants :

1. **kubelet** : Ce service agit comme la passerelle entre le plan de contr√¥le et chacun des n≈ìuds d'un cluster. Toutes les instructions du plan de contr√¥le vers les n≈ìuds passent par ce service. Il interagit √©galement avec le magasin `etcd` pour maintenir les informations d'√©tat √† jour.
2. **kube-proxy** : Ce petit service s'ex√©cute sur chaque serveur de n≈ìud et maintient les r√®gles de r√©seau sur eux. Toute requ√™te r√©seau qui atteint un service √† l'int√©rieur de votre cluster passe par ce service.
3. **Container Runtime** : Kubernetes est un outil d'orchestration de conteneurs, donc il ex√©cute des applications dans des conteneurs. Cela signifie que chaque n≈ìud doit avoir un runtime de conteneur comme [Docker](https://www.docker.com/) ou [rkt](https://coreos.com/rkt/) ou [cri-o](https://cri-o.io/).

### Objets Kubernetes

Selon la [documentation](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/) de Kubernetes ‚Äî

> "Les objets sont des entit√©s persistantes dans le syst√®me Kubernetes. Kubernetes utilise ces entit√©s pour repr√©senter l'√©tat de votre cluster. Plus pr√©cis√©ment, ils peuvent d√©crire quelles applications conteneuris√©es sont en cours d'ex√©cution, les ressources disponibles pour elles et les politiques concernant leur comportement."

Lorsque vous cr√©ez un objet Kubernetes, vous indiquez effectivement au syst√®me Kubernetes que vous voulez que cet objet existe quoi qu'il arrive et le syst√®me Kubernetes travaillera constamment pour maintenir l'objet en cours d'ex√©cution.

### Pods

Selon la [documentation](https://kubernetes.io/docs/concepts/workloads/pods/) de Kubernetes ‚Äî

> "Les pods sont les plus petites unit√©s d√©ployables de calcul que vous pouvez cr√©er et g√©rer dans Kubernetes". 

Un pod encapsule g√©n√©ralement un ou plusieurs conteneurs qui sont √©troitement li√©s, partageant un cycle de vie et des ressources consommables.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/pods-1.svg)

Bien qu'un pod puisse h√©berger plus d'un conteneur, vous ne devriez pas simplement mettre des conteneurs dans un pod sans r√©fl√©chir. Les conteneurs dans un pod doivent √™tre si √©troitement li√©s qu'ils peuvent √™tre trait√©s comme une seule application. 

Par exemple, votre API back-end peut d√©pendre de la base de donn√©es, mais cela ne signifie pas que vous mettrez les deux dans le m√™me pod. Tout au long de cet article, vous ne verrez aucun pod qui a plus d'un conteneur en cours d'ex√©cution.

G√©n√©ralement, vous ne devriez pas g√©rer un pod directement. Au lieu de cela, vous devriez travailler avec des objets de niveau sup√©rieur qui peuvent vous offrir une bien meilleure gestion. Vous en apprendrez davantage sur ces objets de niveau sup√©rieur dans les sections suivantes.

### Services

Selon la [documentation](https://kubernetes.io/docs/concepts/services-networking/service/) de Kubernetes ‚Äî

> "Un service dans Kubernetes est un moyen abstrait d'exposer une application s'ex√©cutant sur un ensemble de pods en tant que service r√©seau".

Les pods Kubernetes sont √©ph√©m√®res par nature. Ils sont cr√©√©s et apr√®s un certain temps, lorsqu'ils sont d√©truits, ils ne sont pas recycl√©s. 

Au lieu de cela, de nouveaux pods identiques prennent la place des anciens. Certains objets Kubernetes de niveau sup√©rieur sont m√™me capables de cr√©er et de d√©truire des pods de mani√®re dynamique.

Une nouvelle adresse IP est attribu√©e √† chaque pod au moment de leur cr√©ation. Mais dans le cas d'un objet de niveau sup√©rieur qui peut cr√©er, d√©truire et regrouper un certain nombre de pods, l'ensemble des pods en cours d'ex√©cution √† un moment donn√© peut √™tre diff√©rent de l'ensemble des pods ex√©cutant cette application un moment plus tard.

Cela pose un probl√®me : si un ensemble de pods dans votre cluster d√©pend d'un autre ensemble de pods dans votre cluster, comment font-ils pour se trouver et se suivre mutuellement les adresses IP ?

La [documentation](https://kubernetes.io/docs/concepts/services-networking/service/) de Kubernetes dit ‚Äî

> "un Service est une abstraction qui d√©finit un ensemble logique de Pods et une politique par laquelle y acc√©der". 

Ce qui signifie essentiellement qu'un Service regroupe un certain nombre de pods qui remplissent la m√™me fonction et les pr√©sente comme une seule entit√©. 

De cette fa√ßon, la confusion de suivre plusieurs pods dispara√Æt car ce Service unique agit d√©sormais comme une sorte de communicateur pour tous.

Dans l'exemple `hello-kube`, vous avez cr√©√© un service de type `LoadBalancer` qui permet aux requ√™tes en provenance de l'ext√©rieur du cluster de se connecter aux pods en cours d'ex√©cution √† l'int√©rieur du cluster.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/load-balancer-3.svg)

Chaque fois que vous devez donner acc√®s √† un ou plusieurs pods √† une autre application ou √† quelque chose en dehors du cluster, vous devez cr√©er un service. 

Par exemple, si vous avez un ensemble de pods ex√©cutant des serveurs web qui doivent √™tre accessibles depuis Internet, un service fournira l'abstraction n√©cessaire.

### Le tableau complet

Maintenant que vous avez une compr√©hension appropri√©e des composants individuels de Kubernetes, voici une repr√©sentation visuelle de leur fonctionnement ensemble en coulisses :

![Image](https://www.freecodecamp.org/news/content/images/2020/08/components-of-kubernetes.png)
_https://kubernetes.io/docs/concepts/overview/components/_

Avant de me lancer dans l'explication des d√©tails individuels, jetez un coup d'≈ìil √† ce que la [documentation](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/) de Kubernetes a √† dire ‚Äî

> "Pour travailler avec des objets Kubernetes ‚Äî que ce soit pour les cr√©er, les modifier ou les supprimer ‚Äî vous devrez utiliser l'[API Kubernetes](https://kubernetes.io/docs/concepts/overview/kubernetes-api/). Lorsque vous utilisez l'interface de ligne de commande `kubectl`, le CLI effectue les appels n√©cessaires √† l'API Kubernetes pour vous."

La premi√®re commande que vous avez ex√©cut√©e √©tait la commande `run`. Elle √©tait la suivante :

```bash
kubectl run hello-kube --image=fhsinchy/hello-kube --port=80
```

La commande `run` est responsable de la cr√©ation d'un nouveau pod qui ex√©cute l'image donn√©e. Une fois que vous avez √©mis cette commande, les ensembles d'√©v√©nements suivants se produisent √† l'int√©rieur du cluster Kubernetes :

* Le composant `kube-api-server` re√ßoit la demande, la valide et la traite.
* Le `kube-api-server` communique ensuite avec le composant `kubelet` sur le n≈ìud et fournit les instructions n√©cessaires pour cr√©er le pod.
* Le composant `kubelet` commence alors √† travailler pour faire fonctionner le pod et maintient √©galement les informations d'√©tat √† jour dans le magasin `etcd`.

La syntaxe g√©n√©rique pour la commande `run` est la suivante :

```
kubectl run <nom du pod> --image=<nom de l'image> --port=<port √† exposer>
```

Vous pouvez ex√©cuter n'importe quelle image de conteneur valide √† l'int√©rieur d'un pod. L'image Docker [fhsinchy/hello-kube](https://hub.docker.com/r/fhsinchy/hello-kube) contient une application JavaScript tr√®s simple qui s'ex√©cute sur le port 80 √† l'int√©rieur du conteneur. L'option `--port=80` permet au pod d'exposer le port 80 depuis l'int√©rieur du conteneur.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/pods-2.svg)

Le pod nouvellement cr√©√© s'ex√©cute √† l'int√©rieur du cluster `minikube` et est inaccessible depuis l'ext√©rieur. Pour exposer le pod et le rendre accessible, la deuxi√®me commande que vous avez √©mise √©tait la suivante :

```
kubectl expose pod hello-kube --type=LoadBalancer --port=80
```

La commande `expose` est responsable de la cr√©ation d'un service Kubernetes de type `LoadBalancer` qui permet aux utilisateurs d'acc√©der √† l'application s'ex√©cutant √† l'int√©rieur du pod. 

Tout comme la commande `run`, l'ex√©cution de la commande `expose` passe par les m√™mes √©tapes √† l'int√©rieur du cluster. Mais au lieu d'un pod, le `kube-api-server` fournit les instructions n√©cessaires pour cr√©er un service dans ce cas au composant `kubelet`.

La syntaxe g√©n√©rique pour la commande `expose` est la suivante :

```
kubectl expose <type de ressource √† exposer> <nom de la ressource> --type=<type de service √† cr√©er> --port=<port √† exposer>
```

Le type d'objet peut √™tre n'importe quel type d'objet Kubernetes valide. Le nom doit correspondre au nom de l'objet que vous essayez d'exposer.

 `--type` indique le type de service que vous souhaitez. Il existe quatre types de services diff√©rents disponibles pour la mise en r√©seau interne ou externe. 

Enfin, le `--port` est le num√©ro de port que vous souhaitez exposer depuis le conteneur en cours d'ex√©cution.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/services-half.svg)

Une fois le service cr√©√©, la derni√®re pi√®ce du puzzle √©tait d'acc√©der √† l'application s'ex√©cutant √† l'int√©rieur du pod. Pour ce faire, la commande que vous avez ex√©cut√©e √©tait la suivante :

```
minikube service hello-kube
```

Contrairement aux pr√©c√©dentes, cette derni√®re commande ne va pas au `kube-api-server`. Elle communique plut√¥t avec le cluster local en utilisant le programme `minikube`. La commande `service` pour `minikube` retourne une URL compl√®te pour un service donn√©.

Lorsque vous avez cr√©√© le pod `hello-kube` avec l'option `--port=80`, vous avez instruct√© Kubernetes de laisser le pod exposer le port 80 depuis l'int√©rieur du conteneur, mais il n'√©tait pas accessible depuis l'ext√©rieur du cluster. 

Ensuite, lorsque vous avez cr√©√© le service `LoadBalancer` avec l'option `--port=80`, il a mapp√© le port 80 de ce conteneur √† un port arbitraire dans le syst√®me local, le rendant accessible depuis l'ext√©rieur du cluster.

Sur mon syst√®me, la commande `service` retourne l'URL `192.168.99.101:30848` pour le pod. L'IP dans cette URL est en fait l'IP de la machine virtuelle `minikube`. Vous pouvez v√©rifier cela en ex√©cutant la commande suivante :

```bash
minikube ip

# 192.168.99.101
```

Pour v√©rifier que le port `30848` pointe vers le port 80 √† l'int√©rieur du pod, vous pouvez ex√©cuter la commande suivante :

```bash
kubectl get service hello-kube

# NAME         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
# hello-kube   LoadBalancer   10.109.60.75   <pending>     80:30848/TCP   119s
```

Dans la colonne `PORT(S)`, vous pouvez voir que le port `80` est effectivement mapp√© au port `30484` sur le syst√®me local. Donc, au lieu d'ex√©cuter la commande `service`, vous pouvez simplement inspecter l'IP et le port, puis les mettre dans votre navigateur manuellement pour acc√©der √† l'application `hello-kube`.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/image-86.png)

Maintenant, l'√©tat final du cluster peut √™tre visualis√© comme suit :

![Image](https://www.freecodecamp.org/news/content/images/2020/08/services-1.svg)

Si vous venez de Docker, alors la signification de l'utilisation d'un service afin d'exposer un pod peut sembler un peu trop verbeuse pour vous √† l'instant. 

Mais √† mesure que vous avancerez dans les exemples qui traitent de plus d'un pod, vous commencerez √† appr√©cier tout ce que Kubernetes a √† offrir.

## Se d√©barrasser des ressources Kubernetes

Maintenant que vous savez comment cr√©er des ressources Kubernetes comme les pods et les Services, vous devez savoir comment vous en d√©barrasser. La seule fa√ßon de se d√©barrasser d'une ressource Kubernetes est de la supprimer.

Vous pouvez le faire en utilisant la commande `delete` pour `kubectl`. La syntaxe g√©n√©rique de la commande est la suivante :

```bash
kubectl delete <type de ressource> <nom de la ressource>
```

Pour supprimer un pod nomm√© `hello-kube`, la commande sera la suivante :

```bash
kubectl delete pod hello-kube

# pod "hello-kube" deleted
```

Et pour supprimer un service nomm√© `hello-kube`, la commande sera la suivante :

```bash
kubectl delete service hello-kube

# service "hello-kube" deleted
```

Ou si vous √™tes d'humeur destructive, vous pouvez supprimer tous les objets d'un type en une seule fois en utilisant l'option `--all` pour la commande `delete`. La syntaxe g√©n√©rique pour l'option est la suivante :

```bash
kubectl delete <type d'objet> --all
```

Ainsi, pour supprimer tous les pods et services, vous devez ex√©cuter `kubectl delete pod --all` et `kubectl delete service --all` respectivement.

## Approche de d√©ploiement d√©clarative

Pour √™tre honn√™te, l'exemple `hello-kube` que vous venez de voir dans la section pr√©c√©dente n'est pas une mani√®re id√©ale de proc√©der au d√©ploiement avec Kubernetes. 

L'approche que vous avez prise dans cette section est une **approche imp√©rative**, ce qui signifie que vous avez d√ª ex√©cuter chaque commande manuellement l'une apr√®s l'autre. Prendre une approche imp√©rative va √† l'encontre de tout l'int√©r√™t de Kubernetes.

Une approche id√©ale pour le d√©ploiement avec Kubernetes est l'**approche d√©clarative**. Dans celle-ci, vous, en tant que d√©veloppeur, faites savoir √† Kubernetes l'√©tat dans lequel vous souhaitez que vos serveurs soient, et Kubernetes trouve un moyen de le mettre en ≈ìuvre. 

Dans cette section, vous allez d√©ployer la m√™me application `hello-kube` avec une approche d√©clarative.

Si vous n'avez pas encore clon√© le d√©p√¥t de code li√© ci-dessus, alors allez-y et r√©cup√©rez-le maintenant. 

Une fois que vous l'avez, allez dans le r√©pertoire `hello-kube`. Ce r√©pertoire contient le code de l'application `hello-kube` ainsi que le `Dockerfile` pour construire l'image.

```bash
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ public
‚îî‚îÄ‚îÄ src

2 directories, 3 files
```

Le code JavaScript se trouve dans le dossier `src`, mais ce n'est pas ce qui vous int√©resse. Le fichier que vous devriez regarder est le `Dockerfile` car il peut vous donner un aper√ßu de la mani√®re dont vous devriez planifier votre d√©ploiement. Le contenu du `Dockerfile` est le suivant :

```dockerfile
FROM node as builder

WORKDIR /usr/app

COPY ./package.json ./
RUN npm install
COPY . .
RUN npm run build

EXPOSE 80

FROM nginx
COPY --from=builder /usr/app/dist /usr/share/nginx/html
```

Comme vous pouvez le voir, il s'agit d'un processus de [construction multi-√©tapes](https://www.freecodecamp.org/news/the-docker-handbook/#multi-staged-builds).

* La premi√®re √©tape utilise `node` comme image de base et compile l'application JavaScript en un ensemble de fichiers pr√™ts pour la production.
* La deuxi√®me √©tape copie les fichiers construits pendant la premi√®re √©tape et les colle √† l'int√©rieur de la racine des documents NGINX par d√©faut. √âtant donn√© que l'image de base pour la deuxi√®me phase est `nginx`, l'image r√©sultante sera une image `nginx` servant les fichiers construits pendant la premi√®re phase sur le port 80 (port par d√©faut pour nginx).

Maintenant, pour d√©ployer cette application sur Kubernetes, vous devrez trouver un moyen d'ex√©cuter l'image en tant que conteneur et de rendre le port 80 accessible depuis le monde ext√©rieur.

### √âcrire votre premier ensemble de configurations

Dans l'approche d√©clarative, au lieu d'√©mettre des commandes individuelles dans le terminal, vous √©crivez plut√¥t la configuration n√©cessaire dans un fichier YAML et vous la fournissez √† Kubernetes. 

Dans le r√©pertoire du projet `hello-kube`, cr√©ez un autre r√©pertoire nomm√© `k8s`. `k8s` est l'abr√©viation de k(ubernete = 8 caract√®res)s. 

Vous n'avez pas besoin de nommer le dossier de cette mani√®re, vous pouvez le nommer comme vous le souhaitez. 

Il n'est m√™me pas n√©cessaire de le garder dans le r√©pertoire du projet. Ces fichiers de configuration peuvent vivre n'importe o√π dans votre ordinateur, car ils n'ont aucun lien avec le code source du projet.

Maintenant, √† l'int√©rieur de ce r√©pertoire `k8s`, cr√©ez un nouveau fichier nomm√© `hello-kube-pod.yaml`. Je vais √©crire le code pour le fichier d'abord, puis je vais l'expliquer ligne par ligne. Le contenu de ce fichier est le suivant :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hello-kube-pod
  labels:
    component: web
spec:
  containers:
    - name: hello-kube
      image: fhsinchy/hello-kube
      ports:
        - containerPort: 80

```

Chaque fichier de configuration Kubernetes valide a quatre champs obligatoires. Ils sont les suivants :

* `apiVersion` : Quelle version de l'API Kubernetes vous utilisez pour cr√©er cet objet. Cette valeur peut changer en fonction du type d'objet que vous cr√©ez. Pour cr√©er un `Pod`, la version requise est `v1`.
* `kind` : Quel type d'objet vous voulez cr√©er. Les objets dans Kubernetes peuvent √™tre de nombreux types. Au fur et √† mesure que vous parcourez l'article, vous en apprendrez beaucoup sur eux, mais pour l'instant, comprenez simplement que vous cr√©ez un objet `Pod`.
* `metadata` : Donn√©es qui aident √† identifier de mani√®re unique l'objet. Sous ce champ, vous pouvez avoir des informations comme `name`, `labels`, `annotation`, etc. La cha√Æne `metadata.name` appara√Ætra sur le terminal et sera utilis√©e dans les commandes `kubectl`. La paire cl√©-valeur sous le champ `metadata.labels` n'a pas besoin d'√™tre `components: web`. Vous pouvez lui donner n'importe quelle √©tiquette comme `app: hello-kube`. Cette valeur sera utilis√©e comme s√©lecteur lors de la cr√©ation du service `LoadBalancer` tr√®s bient√¥t.
* `spec` : contient l'√©tat que vous souhaitez pour l'objet. Le sous-champ `spec.containers` contient des informations sur les conteneurs qui s'ex√©cuteront √† l'int√©rieur de ce `Pod`. La valeur `spec.containers.name` est ce que le runtime de conteneur √† l'int√©rieur du n≈ìud attribuera au conteneur nouvellement cr√©√©. Le `spec.containers.image` est l'image de conteneur √† utiliser pour cr√©er ce conteneur. Et le champ `spec.containers.ports` contient la configuration concernant les diff√©rentes configurations de ports. `containerPort: 80` indique que vous souhaitez exposer le port 80 depuis le conteneur.

Si vous √™tes sur un Raspberry Pi, utilisez `raed667/hello-kube` comme image au lieu de `fhsinchy/hello-kube`. Maintenant, pour fournir ce fichier de configuration √† Kubernetes, vous utiliserez la commande `apply`. La syntaxe g√©n√©rique de la commande est la suivante :

```
kubectl apply -f <fichier de configuration>
```

Pour fournir un fichier de configuration nomm√© `hello-kube-pod.yaml`, la commande sera la suivante :

```bash
kubectl apply -f hello-kube-pod.yaml

# pod/hello-kube-pod created
```

Pour vous assurer que le `Pod` est op√©rationnel, ex√©cutez la commande suivante :

```bash
kubectl get pod

# NAME         READY   STATUS    RESTARTS   AGE
# hello-kube   1/1     Running   0          3m3s
```

Vous devriez voir `Running` dans la colonne `STATUS`. Si vous voyez quelque chose comme `ContainerCreating`, attendez une minute ou deux et v√©rifiez √† nouveau.

Une fois que le `Pod` est op√©rationnel, il est temps pour vous d'√©crire le fichier de configuration pour le service `LoadBalancer`. 

Cr√©ez un autre fichier dans le r√©pertoire `k8s` appel√© `hello-kube-load-balancer-service.yaml` et mettez le code suivant dedans :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: hello-kube-load-balancer-service
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 80
  selector:
    component: web
```

Comme le fichier de configuration pr√©c√©dent, les champs `apiVersion`, `kind` et `metadata` servent le m√™me but ici. Comme vous pouvez le voir, il n'y a pas de champ `labels` √† l'int√©rieur de `metadata` ici. C'est parce qu'un service s√©lectionne d'autres objets en utilisant des `labels`, d'autres objets ne s√©lectionnent pas un service.

> Rappelez-vous, les services √©tablissent une politique d'acc√®s pour d'autres objets, d'autres objets n'√©tablissent pas une politique d'acc√®s pour un service.

√Ä l'int√©rieur du champ `spec`, vous pouvez voir un nouvel ensemble de valeurs. Contrairement √† un `Pod`, les services ont quatre types. Ce sont `ClusterIP`, `NodePort`, `LoadBalancer`, et `ExternalName`.

Dans cet exemple, vous utilisez le type `LoadBalancer`, qui est la mani√®re standard d'exposer un service √† l'ext√©rieur du cluster. Ce service vous donnera une adresse IP que vous pourrez ensuite utiliser pour vous connecter aux applications s'ex√©cutant √† l'int√©rieur de votre cluster.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/load-balancer-4.svg)

Le type `LoadBalancer` n√©cessite deux valeurs de port pour fonctionner correctement. Sous le champ `ports`, la valeur `port` est pour acc√©der au pod lui-m√™me et sa valeur peut √™tre n'importe quoi. 

La valeur `targetPort` est celle de l'int√©rieur du conteneur et doit correspondre au port que vous souhaitez exposer depuis l'int√©rieur du conteneur. 

J'ai d√©j√† dit que l'application `hello-kube` s'ex√©cute sur le port 80 √† l'int√©rieur du conteneur. Vous avez m√™me expos√© ce port dans le fichier de configuration `Pod`, donc le `targetPort` sera `80`.

Le champ `selector` est utilis√© pour identifier les objets qui seront connect√©s √† ce service. La paire cl√©-valeur `component: web` doit correspondre √† la paire cl√©-valeur sous le champ `labels` dans le fichier de configuration `Pod`. Si vous avez utilis√© une autre paire cl√©-valeur comme `app: hello-kube` dans ce fichier de configuration, utilisez celle-ci √† la place.

Pour fournir ce fichier √† Kubernetes, vous utiliserez √† nouveau la commande `apply`. La commande pour fournir un fichier nomm√© `hello-kube-load-balancer-service.yaml` sera la suivante :

```bash
kubectl apply -f hello-kube-load-balancer-service.yaml

# service/hello-kube-load-balancer-service created
```

Pour vous assurer que l'√©quilibreur de charge a √©t√© cr√©√© avec succ√®s, ex√©cutez la commande suivante :

```bash
kubectl get service

# NAME                               TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
# hello-kube-load-balancer-service   LoadBalancer   10.107.231.120   <pending>     80:30848/TCP   7s
# kubernetes                         ClusterIP      10.96.0.1        <none>        443/TCP        21h
```

Assurez-vous de voir le nom `hello-kube-load-balancer-service` dans la liste. Maintenant que vous avez un pod en cours d'ex√©cution qui est expos√©, vous pouvez aller de l'avant et y acc√©der. Ex√©cutez la commande suivante pour ce faire :

```bash
minikube service hello-kube-load-balancer-service

# |-----------|----------------------------------|-------------|-----------------------------|
# | NAMESPACE |           NAME                   | TARGET PORT |             URL             |
# |-----------|----------------------------------|-------------|-----------------------------|
# | default   | hello-kube-load-balancer-service |          80 | http://192.168.99.101:30848 |
# |-----------|----------------------------------|-------------|-----------------------------|
# üéâ Opening service default/hello-kube-load-balancer in default browser...
```

Votre navigateur web par d√©faut devrait s'ouvrir automatiquement et vous devriez voir quelque chose comme ceci :

![Image](https://www.freecodecamp.org/news/content/images/2020/08/image-87.png)

Vous pouvez √©galement fournir les deux fichiers ensemble au lieu de les fournir individuellement. Pour ce faire, vous pouvez remplacer le nom du fichier par le nom du r√©pertoire comme suit :

```bash
kubectl apply -f k8s

# service/hello-kube-load-balancer-service created
# pod/hello-kube-pod created
```

Dans ce cas, assurez-vous que votre terminal est sur le r√©pertoire parent du r√©pertoire `k8s`. 

Si vous √™tes dans le r√©pertoire `k8s`, vous pouvez utiliser un point (`.`) pour faire r√©f√©rence au r√©pertoire courant. Lorsque vous appliquez des configurations en masse, il peut √™tre bon de se d√©barrasser des ressources cr√©√©es pr√©c√©demment. De cette fa√ßon, la possibilit√© de conflits devient beaucoup plus faible.

L'approche d√©clarative est l'approche id√©ale lorsque vous travaillez avec Kubernetes. Sauf pour quelques cas particuliers, que vous verrez √† la fin de l'article.

### Le tableau de bord Kubernetes

Dans une section pr√©c√©dente, vous avez utilis√© la commande `delete` pour vous d√©barrasser d'un objet Kubernetes. 

Dans cette section, cependant, j'ai pens√© que l'introduction du tableau de bord serait une excellente id√©e. Le tableau de bord Kubernetes est une interface graphique que vous pouvez utiliser pour g√©rer vos charges de travail, services, et plus encore.

Pour lancer le tableau de bord Kubernetes, ex√©cutez la commande suivante dans votre terminal :

```bash
minikube dashboard

# üéâ V√©rification de la sant√© du tableau de bord...
# üéâ Lancement du proxy...
# üéâ V√©rification de la sant√© du proxy...
# üéâ Ouverture de http://127.0.0.1:52393/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ dans votre navigateur par d√©faut...
```

Le tableau de bord devrait s'ouvrir automatiquement dans votre navigateur par d√©faut :

![Image](https://www.freecodecamp.org/news/content/images/2020/08/image-88.png)

L'interface est assez conviviale et vous √™tes libre de vous promener ici. Bien qu'il soit tout √† fait possible de cr√©er, g√©rer et supprimer des objets √† partir de cette interface, je vais utiliser le CLI pour le reste de cet article.

Ici, dans la liste _Pods_, vous pouvez utiliser le menu √† trois points sur le c√¥t√© droit pour _Supprimer_ le Pod. Vous pouvez faire de m√™me avec le service `LoadBalancer`. En fait, la liste _Services_ est commod√©ment plac√©e juste apr√®s la liste _Pods_. 

Vous pouvez fermer le tableau de bord en appuyant sur la combinaison de touches `Ctrl + C` ou en fermant la fen√™tre du terminal.

## Travailler avec des applications multi-conteneurs

Jusqu'√† pr√©sent, vous avez travaill√© avec des applications qui s'ex√©cutent dans un seul conteneur. 

Dans cette section, vous allez travailler avec une application compos√©e de deux conteneurs. Vous vous familiariserez √©galement avec `Deployment`, `ClusterIP`, `PersistentVolume`, `PersistentVolumeClaim` et quelques techniques de d√©bogage.

L'application avec laquelle vous allez travailler est une API de notes express simple avec une fonctionnalit√© CRUD compl√®te. L'application utilise [PostgreSQL](https://www.postgresql.org/) comme syst√®me de base de donn√©es. Vous allez donc non seulement d√©ployer l'application, mais aussi configurer la mise en r√©seau interne entre l'application et la base de donn√©es.

Le code de l'application se trouve dans le r√©pertoire `notes-api` √† l'int√©rieur du d√©p√¥t de projet.

```
.
‚îú‚îÄ‚îÄ api
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îî‚îÄ‚îÄ postgres

2 directories, 1 file
```

Le code source de l'application r√©side dans le r√©pertoire `api` et le r√©pertoire `postgres` contient un `Dockerfile` pour cr√©er l'image `postgres` personnalis√©e. Le fichier `docker-compose.yaml` contient la configuration n√©cessaire pour ex√©cuter l'application en utilisant `docker-compose`.

Tout comme avec le projet pr√©c√©dent, vous pouvez consulter le `Dockerfile` individuel pour chaque service afin de vous faire une id√©e de la mani√®re dont l'application s'ex√©cute √† l'int√©rieur du conteneur. 

Ou vous pouvez simplement inspecter le `docker-compose.yaml` et planifier votre d√©ploiement Kubernetes en utilisant celui-ci.

```yaml

version: "3.8"

services: 
    db:
        build:
            context: ./postgres
            dockerfile: Dockerfile.dev
        volumes: 
            - db-data:/var/lib/postgresql/data
        environment:
            POSTGRES_PASSWORD: 63eaQB9wtLqmNBpg
            POSTGRES_DB: notesdb
    api:
        build: 
            context: ./api
            dockerfile: Dockerfile.dev
        ports: 
            - 3000:3000
        volumes: 
            - /home/node/app/node_modules
            - ./api:/home/node/app
        environment: 
            DB_CONNECTION: pg
            DB_HOST: db
            DB_PORT: 5432
            DB_USER: postgres
            DB_DATABASE: notesdb
            DB_PASSWORD: 63eaQB9wtLqmNBpg

volumes:
    db-data:
        name: notes-db-dev-data
```

En regardant la d√©finition du service `api`, vous pouvez voir que l'application s'ex√©cute sur le port 3000 √† l'int√©rieur du conteneur. Elle n√©cessite √©galement un ensemble de variables d'environnement pour fonctionner correctement. 

Les volumes peuvent √™tre ignor√©s car ils √©taient n√©cessaires uniquement √† des fins de d√©veloppement et la configuration de construction est sp√©cifique √† Docker. Ainsi, les deux ensembles d'informations que vous pouvez reporter presque inchang√©s dans vos fichiers de configuration Kubernetes sont les suivants :

* Mappages de ports ‚Äî car vous devrez exposer le m√™me port depuis le conteneur.
* Variables d'environnement ‚Äî car ces variables seront les m√™mes dans tous les environnements (les valeurs vont changer, cependant).

Le service `db` est encore plus simple. Il n'a qu'un ensemble de variables d'environnement. Vous pouvez m√™me utiliser l'image officielle `postgres` au lieu d'une image personnalis√©e. 

Mais la seule raison pour une image personnalis√©e est si vous voulez que l'instance de la base de donn√©es soit livr√©e avec la table `notes` pr√©-cr√©√©e. 

Cette table est n√©cessaire pour l'application. Si vous regardez dans le r√©pertoire `postgres/docker-entrypoint-initdb.d`, vous verrez un fichier nomm√© `notes.sql` qui est utilis√© pour cr√©er la base de donn√©es lors de l'initialisation.

### Plan de d√©ploiement

Contrairement au projet pr√©c√©dent que vous avez d√©ploy√©, ce projet va √™tre un peu plus compliqu√©. 

Dans ce projet, vous allez cr√©er non pas une, mais trois instances de l'API de notes. Ces trois instances seront expos√©es √† l'ext√©rieur du cluster en utilisant un service `LoadBalancer`.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/notes-api-1.svg)

En plus de ces trois instances, il y aura une autre instance du syst√®me de base de donn√©es PostgreSQL. Les trois instances de l'application API de notes communiqueront avec cette instance de base de donn√©es en utilisant un service `ClusterIP`.

Le service `ClusterIP` est un autre type de service Kubernetes qui expose une application au sein de votre cluster. Cela signifie qu'aucun trafic externe ne peut atteindre l'application en utilisant un service `ClusterIP`.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/cluster-ip-2.svg)

Dans ce projet, la base de donn√©es doit √™tre accessible uniquement par l'API de notes, donc exposer le service de base de donn√©es au sein du cluster est un choix id√©al.

J'ai d√©j√† mentionn√© dans une section pr√©c√©dente que vous ne devriez pas cr√©er de pods directement. Donc dans ce projet, vous allez utiliser un `Deployment` au lieu d'un `Pod`.

### Contr√¥leurs de r√©plication, Replica Sets et Deployments

Selon la [documentation](https://kubernetes.io/docs/concepts/architecture/controller/) de Kubernetes ‚Äî

> "Dans Kubernetes, les contr√¥leurs sont des boucles de contr√¥le qui surveillent l'√©tat de votre cluster, puis effectuent ou demandent des changements l√† o√π c'est n√©cessaire. Chaque contr√¥leur essaie de rapprocher l'√©tat actuel du cluster de l'√©tat souhait√©. Une boucle de contr√¥le est une boucle non terminante qui r√©gule l'√©tat d'un syst√®me."

Un `ReplicationController`, comme son nom l'indique, permet de cr√©er facilement plusieurs r√©plicas. Une fois que le nombre souhait√© de r√©plicas est cr√©√©, le contr√¥leur s'assurera que l'√©tat reste ainsi.

Si apr√®s un certain temps vous d√©cidez de r√©duire le nombre de r√©plicas, alors le `ReplicationController` prendra des mesures imm√©diatement et se d√©barrasser des pods suppl√©mentaires. 

Sinon, si le nombre de r√©plicas devient inf√©rieur √† ce que vous vouliez (peut-√™tre que certains des pods ont plant√©), le `ReplicationController` en cr√©era de nouveaux pour correspondre √† l'√©tat souhait√©.

Aussi utiles qu'ils puissent vous sembler, le `ReplicationController` n'est plus la m√©thode recommand√©e pour cr√©er des r√©plicas de nos jours. Une nouvelle API appel√©e `ReplicaSet` a pris sa place.  

En plus du fait qu'un `ReplicaSet` peut vous offrir une gamme plus large d'options de s√©lection, `ReplicationController` et `ReplicaSet` sont plus ou moins la m√™me chose.

Avoir une gamme plus large d'options de s√©lecteur est bien, mais ce qui est encore mieux, c'est d'avoir plus de flexibilit√© en termes de d√©ploiement et de retour en arri√®re des mises √† jour. C'est l√† qu'intervient une autre API Kubernetes appel√©e `Deployment`.

Un `Deployment` est comme une extension de l'API `ReplicaSet` d√©j√† bien con√ßue. `Deployment` ne permet pas seulement de cr√©er des r√©plicas en un rien de temps, mais permet √©galement de publier des mises √† jour ou de revenir √† une fonction pr√©c√©dente avec seulement une ou deux commandes `kubectl`.

| ReplicationController                            | ReplicaSet                                  | Deployment                                                  |
|--------------------------------------------------|---------------------------------------------|-------------------------------------------------------------|
| Permet la cr√©ation facile de plusieurs pods      | Permet la cr√©ation facile de plusieurs pods | Permet la cr√©ation facile de plusieurs pods                 |
| La m√©thode originale de r√©plication dans Kubernetes | A des s√©lecteurs plus flexibles              | √âtend les ReplicaSets avec des mises √† jour et des retours en arri√®re faciles |

Dans ce projet, vous allez utiliser un `Deployment` pour maintenir les instances de l'application.

### Cr√©er votre premier d√©ploiement

Commen√ßons par √©crire le fichier de configuration pour le d√©ploiement de l'API de notes. Cr√©ez un r√©pertoire `k8s` √† l'int√©rieur du r√©pertoire du projet `notes-api`. 

√Ä l'int√©rieur de ce r√©pertoire, cr√©ez un fichier nomm√© `api-deployment.yaml` et mettez le contenu suivant dedans :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      component: api
  template:
    metadata:
      labels:
        component: api
    spec:
      containers:
        - name: api
          image: fhsinchy/notes-api
          ports:
            - containerPort: 3000

```

Dans ce fichier, les champs `apiVersion`, `kind`, `metadata` et `spec` servent le m√™me but que dans le projet pr√©c√©dent. Les changements notables dans ce fichier par rapport au pr√©c√©dent sont les suivants :

* Pour cr√©er un Pod, la `apiVersion` requise √©tait `v1`. Mais pour cr√©er un Deployment, la version requise est `apps/v1`. Les versions de l'API Kubernetes peuvent √™tre un peu d√©routantes √† certains moments, mais √† mesure que vous continuez √† travailler avec Kubernetes, vous vous y habituerez. Vous pouvez √©galement consulter la documentation officielle pour des exemples de fichiers YAML √† utiliser comme r√©f√©rence. Le type est `Deployment`, ce qui est assez explicite.
* `spec.replicas` d√©finit le nombre de r√©plicas en cours d'ex√©cution. D√©finir cette valeur √† `3` signifie que vous faites savoir √† Kubernetes que vous voulez trois instances de votre application en cours d'ex√©cution √† tout moment.
* `spec.selector` est l'endroit o√π vous faites savoir au `Deployment` quels pods contr√¥ler. J'ai d√©j√† mentionn√© qu'un `Deployment` est une extension de `ReplicaSet` et peut contr√¥ler un ensemble d'objets Kubernetes. D√©finir `selector.matchLabels` √† `component: api` signifie que ce `Deployment` contr√¥lera les pods qui ont une √©tiquette de `component: api`. Cette ligne indique √† Kubernetes que vous voulez que ce `Deployment` contr√¥le tous les pods ayant l'√©tiquette `component: api`.
* `spec.template` est le mod√®le pour configurer les pods. Il est presque identique au fichier de configuration pr√©c√©dent.

Si vous √™tes sur un Raspberry Pi, utilisez `raed667/notes-api` au lieu de `fhsinchy/notes-api` comme image. Maintenant, pour voir cette configuration en action, appliquez le fichier comme dans le projet pr√©c√©dent :

```bash
kubectl apply -f api-deployment.yaml

# deployment.apps/api-deployment created
```

Pour vous assurer que le `Deployment` a √©t√© cr√©√©, ex√©cutez la commande suivante :

```bash
kubectl get deployment

# NAME             READY   UP-TO-DATE   AVAILABLE   AGE
# api-deployment   0/3     3            0           2m7s
```

Si vous regardez la colonne `READY`, vous verrez `0/3`. Cela signifie que les pods n'ont pas encore √©t√© cr√©√©s. Attendez quelques minutes et essayez √† nouveau.

```bash
kubectl get deployment

# NAME             READY   UP-TO-DATE   AVAILABLE   AGE
# api-deployment   0/3     3            0           28m
```

Comme vous pouvez le voir, j'ai attendu pr√®s d'une demi-heure et toujours aucun des pods n'est pr√™t. L'API elle-m√™me ne fait que quelques centaines de kilo-octets. Un d√©ploiement de cette taille n'aurait pas d√ª prendre autant de temps. Ce qui signifie qu'il y a un probl√®me et que nous devons le r√©soudre.

### Inspecter les ressources Kubernetes

Avant de pouvoir r√©soudre un probl√®me, vous devez d'abord en trouver l'origine. Un bon point de d√©part est la commande `get`.

Vous connaissez d√©j√† la commande `get` qui imprime un tableau contenant des informations importantes sur une ou plusieurs ressources Kubernetes. La syntaxe g√©n√©rique de la commande est la suivante :

```
kubectl get <type de ressource> <nom de la ressource>
```

Pour ex√©cuter la commande `get` sur votre `api-deployment`, ex√©cutez la ligne de code suivante dans votre terminal :

```bash
kubectl get deployment api-deployment

# NAME             READY   UP-TO-DATE   AVAILABLE   AGE
# api-deployment   0/3     3            0           15m
```

Vous pouvez omettre le nom `api-deployment` pour obtenir une liste de tous les d√©ploiements disponibles. Vous pouvez √©galement ex√©cuter la commande `get` sur un fichier de configuration. 

Si vous souhaitez obtenir des informations sur les d√©ploiements d√©crits dans le fichier `api-deployment.yaml`, la commande doit √™tre la suivante :

```bash
kubectl get -f api-deployment

# NAME             READY   UP-TO-DATE   AVAILABLE   AGE
# api-deployment   0/3     3            0           18m
```

Par d√©faut, la commande `get` affiche une tr√®s petite quantit√© d'informations. Vous pouvez en obtenir plus en utilisant l'option `-o`. 

L'option `-o` d√©finit le format de sortie pour la commande `get`. Vous pouvez utiliser le format de sortie `wide` pour voir plus de d√©tails.

```bash
kubectl get -f api-deployment.yaml

# NAME             READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES               SELECTOR
# api-deployment   0/3     3            0           19m   api          fhsinchy/notes-api   component=api
```

Comme vous pouvez le voir, la liste contient maintenant plus d'informations qu'avant. Vous pouvez en apprendre davantage sur les options de la commande `get` dans la documentation officielle [docs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#get).

L'ex√©cution de `get` sur le `Deployment` ne donne rien d'int√©ressant, pour √™tre honn√™te. Dans de tels cas, vous devez descendre au niveau des ressources de bas niveau. 

Jetez un coup d'≈ìil √† la liste des pods et voyez si vous pouvez trouver quelque chose d'int√©ressant l√†-bas :

```bash
kubectl get pod

# NAME                             READY   STATUS             RESTARTS   AGE
# api-deployment-d59f9c884-88j45   0/1     CrashLoopBackOff   10         30m
# api-deployment-d59f9c884-96hfr   0/1     CrashLoopBackOff   10         30m
# api-deployment-d59f9c884-pzdxg   0/1     CrashLoopBackOff   10         30m
```

C'est int√©ressant. Tous les pods ont un `STATUS` de `CrashLoopBackOff`, ce qui est nouveau. Auparavant, vous n'aviez vu que les statuts `ContainerCreating` et `Running`. Vous pouvez voir `Error` √† la place de `CrashLoopBackOff` √©galement. 

En regardant la colonne `RESTARTS`, vous pouvez voir que les pods ont d√©j√† √©t√© red√©marr√©s 10 fois. Cela signifie que pour une raison quelconque, les pods √©chouent au d√©marrage.

Maintenant, pour obtenir une vue plus d√©taill√©e de l'un des pods, vous pouvez utiliser une autre commande appel√©e `describe`. Elle est tr√®s similaire √† la commande `get`. La syntaxe g√©n√©rique de la commande est la suivante :

```
kubectl get <type de ressource> <nom de la ressource>
```

Pour obtenir les d√©tails du pod `api-deployment-d59f9c884-88j45`, vous pouvez ex√©cuter la commande suivante :

```bash
kubectl describe pod api-deployment-d59f9c884-88j45

# Name:         api-deployment-d59f9c884-88j45
# Namespace:    default
# Priority:     0
# Node:         minikube/172.28.80.217
# Start Time:   Sun, 09 Aug 2020 16:01:28 +0600
# Labels:       component=api
#               pod-template-hash=d59f9c884
# Annotations:  <none>
# Status:       Running
# IP:           172.17.0.4
# IPs:
#   IP:           172.17.0.4
# Controlled By:  ReplicaSet/api-deployment-d59f9c884
# Containers:
#  api:
#     Container ID:   docker://d2bc15bda9bf4e6d08f7ca8ff5d3c8593655f5f398cf8bdd18b71da8807930c1
#     Image:          fhsinchy/notes-api
#     Image ID:       docker-pullable://fhsinchy/notes-api@sha256:4c715c7ce3ad3693c002fad5e7e7b70d5c20794a15dbfa27945376af3f3bb78c
#     Port:           3000/TCP
#     Host Port:      0/TCP
#     State:          Waiting
#       Reason:       CrashLoopBackOff
#     Last State:     Terminated
#       Reason:       Error
#       Exit Code:    1
#       Started:      Sun, 09 Aug 2020 16:13:12 +0600
#       Finished:     Sun, 09 Aug 2020 16:13:12 +0600
#     Ready:          False
#     Restart Count:  10
#     Environment:    <none>
#     Mounts:
#      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gqfr4 (ro)
# Conditions:
#   Type              Status
#   Initialized       True
#   Ready             False
#   ContainersReady   False
#   PodScheduled      True
# Volumes:
#   default-token-gqfr4:
#     Type:        Secret (a volume populated by a Secret)
#     SecretName:  default-token-gqfr4
#     Optional:    false
# QoS Class:       BestEffort
# Node-Selectors:  <none>
# Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
#                  node.kubernetes.io/unreachable:NoExecute for 300s
# Events:
#   Type     Reason     Age                         From               Message
#   ----     ------     ----                        ----               -------
#   Normal   Scheduled  <unknown>                   default-scheduler  Successfully assigned default/api-deployment-d59f9c884-88j45 to minikube
#   Normal   Pulled     2m40s (x4 over 3m47s)       kubelet, minikube  Successfully pulled image "fhsinchy/notes-api"
#   Normal   Created    2m40s (x4 over 3m47s)       kubelet, minikube  Created container api
#   Normal   Started    2m40s (x4 over 3m47s)       kubelet, minikube  Started container api
#   Normal   Pulling    107s (x5 over 3m56s)        kubelet, minikube  Pulling image "fhsinchy/notes-api"
#   Warning  BackOff    <invalid> (x44 over 3m32s)  kubelet, minikube  Back-off restarting failed container
```

La partie la plus int√©ressante dans tout ce mur de texte est la section `Events`. Jetez un coup d'≈ìil plus attentif :

```
Events:
  Type     Reason     Age                         From               Message
  ----     ------     ----                        ----               -------
  Normal   Scheduled  <unknown>                   default-scheduler  Successfully assigned default/api-deployment-d59f9c884-88j45 to minikube
  Normal   Pulled     2m40s (x4 over 3m47s)       kubelet, minikube  Successfully pulled image "fhsinchy/notes-api"
  Normal   Created    2m40s (x4 over 3m47s)       kubelet, minikube  Created container api
  Normal   Started    2m40s (x4 over 3m47s)       kubelet, minikube  Started container api
  Normal   Pulling    107s (x5 over 3m56s)        kubelet, minikube  Pulling image "fhsinchy/notes-api"
  Warning  BackOff    <invalid> (x44 over 3m32s)  kubelet, minikube  Back-off restarting failed container
```

√Ä partir de ces √©v√©nements, vous pouvez voir que l'image du conteneur a √©t√© t√©l√©charg√©e avec succ√®s. Le conteneur a √©galement √©t√© cr√©√©, mais il est √©vident d'apr√®s le `Back-off restarting failed container` que le conteneur a √©chou√© au d√©marrage.

La commande describe est tr√®s similaire √† la commande `get` et a le m√™me type d'options. 

Vous pouvez omettre le nom `api-deployment-d59f9c884-88j45` pour obtenir des informations sur tous les pods disponibles. Ou vous pouvez √©galement utiliser l'option `-f` pour passer un fichier de configuration √† la commande. Visitez la documentation officielle [docs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#describe) pour en savoir plus.

Maintenant que vous savez qu'il y a un probl√®me avec le conteneur, vous devez descendre au niveau du conteneur et voir ce qui se passe l√†.

### Obtenir les logs des conteneurs depuis les pods

Il existe une autre commande `kubectl` appel√©e `logs` qui peut vous aider √† obtenir les logs des conteneurs √† l'int√©rieur d'un pod. La syntaxe g√©n√©rique de la commande est la suivante :

```
kubectl logs <pod>
```

Pour afficher les logs √† l'int√©rieur du pod `api-deployment-d59f9c884-88j45`, la commande doit √™tre la suivante :

```bash
kubectl logs api-deployment-d59f9c884-88j45

# > api@1.0.0 start /usr/app
# > cross-env NODE_ENV=production node bin/www

# /usr/app/node_modules/knex/lib/client.js:55
#     throw new Error(`knex: Required configuration option 'client' is missing.`);
    ^

# Error: knex: Required configuration option 'client' is missing.
#     at new Client (/usr/app/node_modules/knex/lib/client.js:55:11)
#     at Knex (/usr/app/node_modules/knex/lib/knex.js:53:28)
#     at Object.<anonymous> (/usr/app/services/knex.js:5:18)
#     at Module._compile (internal/modules/cjs/loader.js:1138:30)
#     at Object.Module._extensions..js (internal/modules/cjs/loader.js:1158:10)
#     at Module.load (internal/modules/cjs/loader.js:986:32)
#     at Function.Module._load (internal/modules/cjs/loader.js:879:14)
#     at Module.require (internal/modules/cjs/loader.js:1026:19)
#     at require (internal/modules/cjs/helpers.js:72:18)
#     at Object.<anonymous> (/usr/app/services/index.js:1:14)
# npm ERR! code ELIFECYCLE
# npm ERR! errno 1
# npm ERR! api@1.0.0 start: `cross-env NODE_ENV=production node bin/www`
# npm ERR! Exit status 1
# npm ERR!
# npm ERR! Failed at the api@1.0.0 start script.
# npm ERR! This is probably not a problem with npm. There is likely additional logging output above.

# npm ERR! A complete log of this run can be found in:
# npm ERR!     /root/.npm/_logs/2020-08-09T10_28_52_779Z-debug.log
```

C'est ce dont vous avez besoin pour d√©boguer le probl√®me. Il semble que la biblioth√®que [knex.js](http://knexjs.org/) manque une valeur requise, ce qui emp√™che l'application de d√©marrer. Vous pouvez en apprendre davantage sur la commande `logs` dans la documentation officielle [docs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs).

Cela se produit parce que vous manquez certaines variables d'environnement requises dans la d√©finition du d√©ploiement. 

Si vous jetez un autre coup d'≈ìil √† la d√©finition du service `api` √† l'int√©rieur du fichier `docker-compose.yaml`, vous devriez voir quelque chose comme ceci :

```yaml
    api:
        build: 
            context: ./api
            dockerfile: Dockerfile.dev
        ports: 
            - 3000:3000
        volumes: 
            - /home/node/app/node_modules
            - ./api:/home/node/app
        environment: 
            DB_CONNECTION: pg
            DB_HOST: db
            DB_PORT: 5432
            DB_USER: postgres
            DB_DATABASE: notesdb
            DB_PASSWORD: 63eaQB9wtLqmNBpg
```

Ces variables d'environnement sont requises pour que l'application communique avec la base de donn√©es. Donc, les ajouter √† la configuration du d√©ploiement devrait r√©soudre le probl√®me.

### Variables d'environnement

L'ajout de variables d'environnement √† un fichier de configuration Kubernetes est tr√®s simple. Ouvrez le fichier `api-deployment.yaml` et mettez √† jour son contenu pour qu'il ressemble √† ceci :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      component: api
  template:
    metadata:
      labels:
        component: api
    spec:
      containers:
        - name: api
          image: fhsinchy/notes-api
          ports:
            - containerPort: 3000
          
          # ce sont les variables d'environnement
          env:
            - name: DB_CONNECTION
              value: pg

```

Le champ `containers.env` contient toutes les variables d'environnement. Si vous regardez de pr√®s, vous verrez que je n'ai pas ajout√© toutes les variables d'environnement du fichier `docker-compose.yaml`. J'en ai ajout√© une seule. 

La `DB_CONNECTION` indique que l'application utilise une base de donn√©es PostgreSQL. L'ajout de cette seule variable devrait r√©soudre le probl√®me.

Maintenant, appliquez √† nouveau le fichier de configuration en ex√©cutant la commande suivante :

```bash
kubectl apply -f api-deployment.yaml

# deployment.apps/api-deployment configured
```

Contrairement aux autres fois, la sortie ici indique qu'une ressource a √©t√© `configur√©e`. C'est la beaut√© de Kubernetes. Vous pouvez simplement corriger les probl√®mes et r√©appliquer imm√©diatement le m√™me fichier de configuration.

Maintenant, utilisez la commande `get` une fois de plus pour vous assurer que tout fonctionne correctement.

```bash
kubectl get deployment

# NAME             READY   UP-TO-DATE   AVAILABLE   AGE
# api-deployment   3/3     3            3           68m

kubectl get pod

# NAME                              READY   STATUS    RESTARTS   AGE
# api-deployment-66cdd98546-l9x8q   1/1     Running   0          7m26s
# api-deployment-66cdd98546-mbfw9   1/1     Running   0          7m31s
# api-deployment-66cdd98546-pntxv   1/1     Running   0          7m21s
```

Les trois pods sont en cours d'ex√©cution et le `Deployment` fonctionne √©galement correctement.

### Cr√©er le d√©ploiement de la base de donn√©es

Maintenant que l'API est op√©rationnelle, il est temps d'√©crire la configuration pour l'instance de la base de donn√©es. 

Cr√©ez un autre fichier appel√© `postgres-deployment.yaml` √† l'int√©rieur du r√©pertoire `k8s` et mettez le contenu suivant dedans :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      component: postgres
  template:
    metadata:
      labels:
        component: postgres
    spec:
      containers:
        - name: postgres
          image: fhsinchy/notes-postgres
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_PASSWORD
              value: 63eaQB9wtLqmNBpg
            - name: POSTGRES_DB
              value: notesdb

```

Si vous √™tes sur un Raspberry Pi, utilisez `raed667/notes-postgres` au lieu de `fhsinchy/notes-postgres` comme image. La configuration elle-m√™me est tr√®s similaire √† la pr√©c√©dente. Je ne vais pas expliquer tout dans ce fichier ‚Äî esp√©rons que vous le comprenez par vous-m√™me avec les connaissances que vous avez acquises dans cet article jusqu'√† pr√©sent.

PostgreSQL s'ex√©cute sur le port 5432 par d√©faut, et la variable `POSTGRES_PASSWORD` est requise pour ex√©cuter le conteneur `postgres`. Ce mot de passe sera √©galement utilis√© pour se connecter √† cette base de donn√©es par l'API. 

La variable `POSTGRES_DB` est facultative. Mais en raison de la mani√®re dont ce projet a √©t√© structur√©, elle est n√©cessaire ici ‚Äî sinon l'initialisation √©chouera. 

Vous pouvez en apprendre davantage sur l'image Docker officielle [postgres](https://hub.docker.com/_/postgres) depuis leur page Docker Hub. Pour des raisons de simplicit√©, je garde le nombre de r√©plicas √† 1 dans ce projet.

Pour appliquer ce fichier, ex√©cutez la commande suivante :

```bash
kubectl apply -f postgres-deployment.yaml

# deployment.apps/postgres-deployment created
```

Utilisez la commande `get` pour vous assurer que le d√©ploiement et les pods sont en cours d'ex√©cution correctement :

```bash
kubectl get deployment

# NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
# postgres-deployment   1/1     1            1           13m

kubectl get pod

# NAME                                   READY   STATUS    RESTARTS   AGE
# postgres-deployment-76fcc75998-mwnb7   1/1     Running   0          13m
```

Bien que le d√©ploiement et les pods soient en cours d'ex√©cution correctement, il y a un gros probl√®me avec le d√©ploiement de la base de donn√©es.

Si vous avez d√©j√† travaill√© avec un syst√®me de base de donn√©es, vous savez peut-√™tre d√©j√† que les bases de donn√©es stockent les donn√©es dans le syst√®me de fichiers. Actuellement, le d√©ploiement de la base de donn√©es ressemble √† ceci :

![Image](https://www.freecodecamp.org/news/content/images/2020/08/postgres-1.svg)

Le conteneur `postgres` est encapsul√© par un pod. Toutes les donn√©es sauvegard√©es restent dans le syst√®me de fichiers interne du conteneur. 

Maintenant, si pour une raison quelconque, le conteneur plante ou si le pod encapsulant le conteneur tombe en panne, toutes les donn√©es persistantes √† l'int√©rieur du syst√®me de fichiers seront perdues.

Lors d'un crash, Kubernetes cr√©era un nouveau pod pour maintenir l'√©tat souhait√©, mais il n'existe aucun m√©canisme de transfert de donn√©es entre les deux pods.

Pour r√©soudre ce probl√®me, vous pouvez stocker les donn√©es dans un espace s√©par√© √† l'ext√©rieur du pod dans le cluster.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/volume.svg)

La gestion d'un tel stockage est un probl√®me distinct de la gestion des instances de calcul. Le sous-syst√®me `PersistentVolume` dans Kubernetes fournit une API pour les utilisateurs et les administrateurs qui abstrait les d√©tails de la mani√®re dont le stockage est fourni de la mani√®re dont il est consomm√©.

### Volumes persistants et Persistent Volume Claims

Selon la [documentation](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) de Kubernetes ‚Äî

> "Un `PersistentVolume` (PV) est une partie du stockage dans le cluster qui a √©t√© provisionn√©e par un administrateur ou provisionn√©e dynamiquement en utilisant une `StorageClass`. C'est une ressource dans le cluster, tout comme un n≈ìud est une ressource du cluster."

Ce qui signifie essentiellement qu'un `PersistentVolume` est un moyen de prendre une tranche de votre espace de stockage et de la r√©server pour un certain pod. Les volumes sont toujours consomm√©s par des pods et non par un objet de haut niveau comme un d√©ploiement. 

Si vous souhaitez utiliser un volume avec un d√©ploiement qui a plusieurs pods, vous devrez suivre quelques √©tapes suppl√©mentaires.

Cr√©ez un nouveau fichier appel√© `database-persistent-volume.yaml` √† l'int√©rieur du r√©pertoire `k8s` et mettez le contenu suivant dans ce fichier :

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: database-persistent-volume
spec:
  storageClassName: manual
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"

```

Les champs `apiVersion`, `kind` et `metadata` servent le m√™me but que tout autre fichier de configuration. Le champ `spec`, cependant, contient certains nouveaux champs.

* `spec.storageClassName` indique le nom de la classe pour ce volume. Supposons qu'un fournisseur de cloud ait trois types de stockage disponibles. Ceux-ci peuvent √™tre _lent_, _rapide_ et _tr√®s rapide_. Le type de stockage que vous obtenez du fournisseur d√©pendra du montant d'argent que vous payez. Si vous demandez un stockage tr√®s rapide, vous devrez payer plus. Ces diff√©rents types de stockage sont les classes. J'utilise `manual` comme exemple ici. Vous pouvez utiliser ce que vous voulez dans votre cluster local.
* `spec.capacity.storage` est la quantit√© de stockage que ce volume aura. Je lui donne 5 gigaoctets de stockage dans ce projet.
* `spec.accessModes` d√©finit le mode d'acc√®s pour le volume. Il existe trois modes d'acc√®s possibles. `ReadWriteOnce` signifie que le volume peut √™tre mont√© en lecture-√©criture par un seul n≈ìud. `ReadWriteMany` signifie que le volume peut √™tre mont√© en lecture-√©criture par plusieurs n≈ìuds. `ReadOnlyMany` signifie que le volume peut √™tre mont√© en lecture seule par plusieurs n≈ìuds.
* `spec.hostPath` est quelque chose de sp√©cifique au d√©veloppement. Il indique le r√©pertoire dans votre cluster local √† n≈ìud unique qui sera trait√© comme un volume persistant. `/mnt/data` signifie que les donn√©es sauvegard√©es dans ce volume persistant vivront √† l'int√©rieur du r√©pertoire `/mnt/data` dans le cluster.

Pour appliquer ce fichier, ex√©cutez la commande suivante :

```bash
kubectl apply -f database-persistent-volume.yaml

# persistentvolume/database-persistent-volume created
```

Maintenant, utilisez la commande `get` pour v√©rifier que le volume a √©t√© cr√©√© :

```bash
kubectl get persistentvolume

# NAME                         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
# database-persistent-volume   5Gi        RWO            Retain           Available           manual                  58s
```

Maintenant que le volume persistant a √©t√© cr√©√©, vous avez besoin d'un moyen pour permettre au pod postgres d'y acc√©der. C'est l√† qu'intervient un `PersistentVolumeClaim` (PVC). 

Un PersistentVolumeClaim est une demande de stockage par un pod. Supposons que dans un cluster, vous avez beaucoup de volumes. Cette revendication d√©finira les caract√©ristiques qu'un volume doit remplir pour pouvoir satisfaire les n√©cessit√©s d'un pod.

Un exemple concret peut √™tre que vous achetez un SSD dans un magasin. Vous allez au magasin et le vendeur vous montre les mod√®les suivants :

| Mod√®le 1                            | Mod√®le 2                                  | Mod√®le 3                                                  |
|--------------------------------------------------|---------------------------------------------|-------------------------------------------------------------|
| 128GB      | 256GB | 512GB                 |
| SATA | NVME              | SATA |

Maintenant, vous demandez un mod√®le qui a au moins 200 Go de capacit√© de stockage et est un lecteur NVME. 

Le premier a moins de 200 Go et est SATA, donc il ne correspond pas √† votre demande. Le troisi√®me a plus de 200 Go, mais n'est pas NVME. Le deuxi√®me, cependant, a plus de 200 Go et est √©galement un NVME. C'est donc celui que vous obtenez.

Les mod√®les de SSD que le vendeur vous a montr√©s sont √©quivalents aux volumes persistants et vos exigences sont √©quivalentes aux revendications de volumes persistants.

Cr√©ez un autre nouveau fichier appel√© `database-persistent-volume-claim.yaml` √† l'int√©rieur du r√©pertoire `k8s` et mettez le contenu suivant dans ce fichier :

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: database-persistent-volume-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
```

Encore une fois, les champs `apiVersion`, `kind` et `metadata` servent le m√™me but que tout autre fichier de configuration.

* `spec.storageClass` dans un fichier de configuration de revendication indique le type de stockage que cette revendication souhaite. Cela signifie que tout `PersistentVolume` qui a `spec.storageClass` d√©fini sur `manual` est adapt√© pour √™tre consomm√© par cette revendication. Si vous avez plusieurs volumes avec la classe `manual`, la revendication obtiendra l'un d'eux et si vous n'avez aucun volume avec la classe `manual` ‚Äî un volume sera provisionn√© dynamiquement.
* `spec.accessModes` d√©finit √† nouveau le mode d'acc√®s ici. Cela indique que cette revendication souhaite un stockage qui a un `accessMode` de `ReadWriteOnce`. Supposons que vous avez deux volumes avec la classe d√©finie sur `manual`. L'un d'eux a son `accessModes` d√©fini sur `ReadWriteOnce` et l'autre sur `ReadWriteMany`. Cette revendication obtiendra celui avec `ReadWriteOnce`.
* `resources.requests.storage` est la quantit√© de stockage que cette revendication souhaite. `2Gi` ne signifie pas que le volume donn√© doit avoir exactement 2 gigaoctets de capacit√© de stockage. Cela signifie qu'il doit avoir au moins 2 gigaoctets. J'esp√®re que vous vous souvenez que vous avez d√©fini la capacit√© du volume persistant √† 5 gigaoctets, ce qui est plus que 2 gigaoctets.

Pour appliquer ce fichier, ex√©cutez la commande suivante :

```bash
kubectl apply -f database-persistent-volume-claim.yaml

# persistentvolumeclaim/database-persistent-volume-claim created
```

Maintenant, utilisez la commande `get` pour v√©rifier que le volume a √©t√© cr√©√© :

```bash
kubectl get persistentvolumeclaim

# NAME                               STATUS   VOLUME                       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
# database-persistent-volume-claim   Bound    database-persistent-volume   5Gi        RWO            manual         37s
```

Regardez la colonne `VOLUME`. Cette revendication est li√©e au volume persistant `database-persistent-volume` que vous avez cr√©√© pr√©c√©demment. Regardez √©galement la colonne `CAPACITY`. Elle est de `5Gi`, car la revendication a demand√© un volume avec au moins 2 gigaoctets de capacit√© de stockage.

### Provisionnement dynamique des volumes persistants

Dans la sous-section pr√©c√©dente, vous avez cr√©√© un volume persistant puis cr√©√© une revendication. Mais, que se passe-t-il s'il n'y a aucun volume persistant provisionn√© au pr√©alable ?

Dans de tels cas, un volume persistant compatible avec la revendication sera provisionn√© automatiquement. 

Pour commencer cette d√©monstration, supprimez le volume persistant et la revendication de volume persistant pr√©c√©demment cr√©√©s avec les commandes suivantes :

```yaml
kubectl delete persistentvolumeclaim --all

# persistentvolumeclaim "database-persistent-volume-claim" deleted

kubectl delete persistentvolumeclaim --all

# persistentvolume "database-persistent-volume" deleted
```

Ouvrez le fichier `database-persistent-volume-claim.yaml` et mettez √† jour son contenu pour qu'il soit comme suit :

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: database-persistent-volume-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi

```

J'ai supprim√© le champ `spec.storageClass` du fichier. Maintenant, r√©appliquez le fichier `database-persistent-volume-claim.yaml` sans appliquer le fichier `database-persistent-volume.yaml` :

```yaml
kubectl apply -f database-persistent-volume-claim.yaml

# persistentvolumeclaim/database-persistent-volume-claim created
```

Maintenant, utilisez la commande `get` pour regarder les informations de la revendication :

```yaml
kubectl get persistentvolumeclaim

# NAME                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
# database-persistent-volume-claim   Bound    pvc-525ae8af-00d3-4cc7-ae47-866aa13dffd5   2Gi        RWO            standard       2s
```

Comme vous pouvez le voir, un volume avec le nom `pvc-525ae8af-00d3-4cc7-ae47-866aa13dffd5` et une capacit√© de stockage de `2Gi` a √©t√© provisionn√© et li√© √† la revendication de mani√®re dynamique. 

Vous pouvez utiliser un volume persistant provisionn√© de mani√®re statique ou dynamique pour le reste de ce projet. J'utiliserai un volume provisionn√© de mani√®re dynamique.

### Connecter les volumes avec les pods

Maintenant que vous avez cr√©√© un volume persistant et une revendication, il est temps de permettre au pod de base de donn√©es d'utiliser ce volume. 

Vous faites cela en connectant le pod √† la revendication de volume persistant que vous avez faite dans la sous-section pr√©c√©dente. Ouvrez le fichier `postgres-deployment.yaml` et mettez √† jour son contenu pour qu'il soit comme suit :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      component: postgres
  template:
    metadata:
      labels:
        component: postgres
    spec:
      # configuration du volume pour le pod
      volumes:
        - name: postgres-storage
          persistentVolumeClaim:
            claimName: database-persistent-volume-claim
      containers:
        - name: postgres
          image: fhsinchy/notes-postgres
          ports:
            - containerPort: 5432
          # configuration du montage du volume pour le conteneur
          volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
              subPath: postgres
          env:
            - name: POSTGRES_PASSWORD
              value: 63eaQB9wtLqmNBpg
            - name: POSTGRES_DB
              value: notesdb

```

J'ai ajout√© deux nouveaux champs dans ce fichier de configuration.

* Le champ `spec.volumes` contient les informations n√©cessaires pour que le pod trouve la revendication de volume persistant. `spec.volumes.name` peut √™tre n'importe quoi que vous voulez. `spec.volumes.persistentVolumeClaim.claimName` doit correspondre √† la valeur `metadata.name` du fichier `database-persistent-volume-claim.yaml`.
* `containers.volumeMounts` contient les informations n√©cessaires pour monter le volume √† l'int√©rieur du conteneur. `containers.volumeMounts.name` doit correspondre √† la valeur de `spec.volumes.name`. `containers.volumeMounts.mountPath` indique le r√©pertoire o√π ce volume sera mont√©. `/var/lib/postgresql/data` est le r√©pertoire de donn√©es par d√©faut pour PostgreSQL. `containers.volumeMounts.subPath` indique un r√©pertoire qui sera cr√©√© √† l'int√©rieur du volume. Supposons que vous utilisez le m√™me volume avec d'autres pods √©galement. Dans ce cas, vous pouvez mettre des donn√©es sp√©cifiques au pod √† l'int√©rieur d'un autre r√©pertoire √† l'int√©rieur de ce volume. Toutes les donn√©es sauvegard√©es √† l'int√©rieur du r√©pertoire `/var/lib/postgresql/data` iront √† l'int√©rieur d'un r√©pertoire `postgres` dans le volume.

Maintenant, r√©appliquez le fichier `postgres-deployment.yaml` en ex√©cutant la commande suivante :

```bash
kubectl apply -f postgres-deployment.yaml

# deployment.apps/postgres-deployment configured
```

Maintenant, vous avez un d√©ploiement de base de donn√©es appropri√© avec un risque beaucoup plus faible de perte de donn√©es. 

Une chose que je voudrais mentionner ici est que le d√©ploiement de la base de donn√©es dans ce projet n'a qu'un seul r√©plica. S'il y avait plus d'un r√©plica, les choses auraient √©t√© diff√©rentes. 

Plusieurs pods acc√©dant au m√™me volume sans qu'ils ne connaissent l'existence des autres peuvent entra√Æner des r√©sultats catastrophiques. Dans de tels cas, la cr√©ation de sous-r√©pertoires pour les pods √† l'int√©rieur de ce volume peut √™tre une bonne id√©e.

### Relier tout ensemble

Maintenant que vous avez √† la fois l'API et la base de donn√©es en cours d'ex√©cution, il est temps de terminer certaines affaires inachev√©es et de configurer la mise en r√©seau. 

Vous avez d√©j√† appris dans les sections pr√©c√©dentes que pour configurer la mise en r√©seau dans Kubernetes, vous utilisez des services. Avant de commencer √† √©crire les services, jetez un coup d'≈ìil au plan de mise en r√©seau que j'ai pour ce projet.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/notes-api-2.svg)

* La base de donn√©es ne sera expos√©e qu'√† l'int√©rieur du cluster en utilisant un service `ClusterIP`. Aucun trafic externe ne sera autoris√©.
* Le d√©ploiement de l'API, cependant, sera expos√© au monde ext√©rieur. Les utilisateurs communiqueront avec l'API et l'API communiquera avec la base de donn√©es.

Vous avez pr√©c√©demment travaill√© avec un service `LoadBalancer` qui expose une application au monde ext√©rieur. Le `ClusterIP`, en revanche, expose une application √† l'int√©rieur du cluster et n'autorise aucun trafic ext√©rieur.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/cluster-ip-3.svg)

√âtant donn√© que le service de base de donn√©es doit √™tre disponible uniquement √† l'int√©rieur du cluster, un service `ClusterIP` est le choix parfait pour ce sc√©nario. 

Cr√©ez un nouveau fichier appel√© `postgres-cluster-ip-service.yaml` √† l'int√©rieur du r√©pertoire `k8s` et mettez le contenu suivant dedans :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-cluster-ip-service
spec:
  type: ClusterIP
  selector:
    component: postgres
  ports:
    - port: 5432
      targetPort: 5432

```

Comme vous pouvez le voir, le fichier de configuration pour un `ClusterIP` est identique √† celui pour un `LoadBalancer`. La seule chose qui diff√®re est la valeur `spec.type`. 

Vous devriez √™tre capable d'interpr√©ter ce fichier sans aucun probl√®me maintenant. 5432 est le port par d√©faut sur lequel PostgreSQL s'ex√©cute. C'est pourquoi ce port doit √™tre expos√©.

Le fichier de configuration suivant est pour le service `LoadBalancer`, responsable de l'exposition de l'API au monde ext√©rieur. Cr√©ez un autre fichier appel√© `api-load-balancer-service.yaml` et mettez le contenu suivant dedans :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: api-load-balancer-service
spec:
  type: LoadBalancer
  ports:
    - port: 3000
      targetPort: 3000
  selector:
    component: api

```

Cette configuration est identique √† celle que vous avez √©crite dans une section pr√©c√©dente. L'API s'ex√©cute sur le port 3000 √† l'int√©rieur du conteneur et c'est pourquoi ce port doit √™tre expos√©.

La derni√®re chose √† faire est d'ajouter le reste des variables d'environnement au d√©ploiement de l'API. Donc, ouvrez le fichier `api-deployment.yaml` et mettez √† jour son contenu comme ceci :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      component: api
  template:
    metadata:
      labels:
        component: api
    spec:
      containers:
        - name: api
          image: fhsinchy/notes-api
          ports:
            - containerPort: 3000
          env:
            - name: DB_CONNECTION
              value: pg
            - name: DB_HOST
              value: postgres-cluster-ip-service
            - name: DB_PORT
              value: '5432'
            - name: DB_USER
              value: postgres
            - name: DB_DATABASE
              value: notesdb
            - name: DB_PASSWORD
              value: 63eaQB9wtLqmNBpg

```

Auparavant, il n'y avait que la variable `DB_CONNECTION` sous `spec.containers.env`. Les nouvelles variables sont les suivantes :

* `DB_HOST` indique l'adresse h√¥te pour le service de base de donn√©es. Dans un environnement non conteneuris√©, la valeur est g√©n√©ralement `127.0.0.1`. Mais dans un environnement Kubernetes, vous ne connaissez pas l'adresse IP du pod de la base de donn√©es. Vous utilisez donc simplement le nom du service qui expose la base de donn√©es.
* `DB_PORT` est le port expos√© par le service de base de donn√©es, qui est 5432.
* `DB_USER` est l'utilisateur pour se connecter √† la base de donn√©es. `postgres` est le nom d'utilisateur par d√©faut.
* `DB_DATABASE` est la base de donn√©es √† laquelle l'API se connectera. Cela doit correspondre √† la valeur `spec.containers.env.DB_DATABASE` du fichier `postgres-deployment.yaml`.
* `DB_PASSWORD` est le mot de passe pour se connecter √† la base de donn√©es. Cela doit correspondre √† la valeur `spec.containers.env.DB_PASSWORD` du fichier `postgres-deployment.yaml`.

Avec cela fait, vous √™tes maintenant pr√™t √† tester l'API. Avant de le faire, je vous sugg√®re d'appliquer tous les fichiers de configuration une fois de plus en ex√©cutant la commande suivante :

```bash
kubectl apply -f k8s

# deployment.apps/api-deployment created
# service/api-load-balancer-service created
# persistentvolumeclaim/database-persistent-volume-claim created
# service/postgres-cluster-ip-service created
# deployment.apps/postgres-deployment created
```

Si vous rencontrez des erreurs, supprimez simplement toutes les ressources et r√©appliquez les fichiers. Les services, les volumes persistants et les revendications de volumes persistants doivent √™tre cr√©√©s instantan√©ment. 

Utilisez la commande `get` pour vous assurer que les d√©ploiements sont tous op√©rationnels :

```bash
kubectl get deployment

# NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
# api-deployment        3/3     3            3           106s
# postgres-deployment   1/1     1            1           106s
```

Comme vous pouvez le voir dans la colonne `READY`, tous les pods sont op√©rationnels. Pour acc√©der √† l'API, utilisez la commande `service` pour `minikube`.

```bash
minikube service api-load-balancer-service

# |-----------|---------------------------|-------------|-----------------------------|
# | NAMESPACE |           NAME            | TARGET PORT |             URL             |
# |-----------|---------------------------|-------------|-----------------------------|
# | default   | api-load-balancer-service |        3000 | http://172.19.186.112:31546 |
# |-----------|---------------------------|-------------|-----------------------------|
# üéâ Opening service default/api-load-balancer-service in default browser...
```

L'API devrait s'ouvrir automatiquement dans votre navigateur par d√©faut :

![Image](https://www.freecodecamp.org/news/content/images/2020/08/image-93.png)

C'est la r√©ponse par d√©faut pour l'API. Vous pouvez √©galement utiliser [`http://172.19.186.112:31546/`](http://172.19.186.112:31546/) avec un outil de test d'API comme [Insomnia](https://insomnia.rest/) ou [Postman](https://www.postman.com/) pour tester l'API. L'API a une fonctionnalit√© CRUD compl√®te. 

Vous pouvez voir les tests qui accompagnent le code source de l'API comme documentation. Il suffit d'ouvrir le fichier `api/tests/e2e/api/routes/notes.test.js`. Vous devriez √™tre capable de comprendre le fichier sans trop de difficult√© si vous avez de l'exp√©rience avec JavaScript et [express](https://expressjs.com/).

## Travailler avec les contr√¥leurs d'entr√©e

Jusqu'√† pr√©sent dans cet article, vous avez utilis√© `ClusterIP` pour exposer une application au sein du cluster et `LoadBalancer` pour exposer une application √† l'ext√©rieur du cluster.

Bien que j'aie cit√© `LoadBalancer` comme le type de service standard pour exposer une application √† l'ext√©rieur du cluster, il pr√©sente certains inconv√©nients. 

Lorsque vous utilisez des services `LoadBalancer` pour exposer des applications dans un environnement cloud, vous devrez payer pour chaque service expos√© individuellement, ce qui peut √™tre co√ªteux dans le cas de grands projets.

Il existe un autre type de service appel√© `NodePort` qui peut √™tre utilis√© comme alternative aux services de type `LoadBalancer`.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/node-port-2.svg)

`NodePort` ouvre un port sp√©cifique sur tous les n≈ìuds de votre cluster et g√®re tout le trafic qui passe par ce port ouvert. 

Comme vous le savez d√©j√†, les services regroupent un certain nombre de pods et contr√¥lent la mani√®re dont ils peuvent √™tre accessibles. Ainsi, toute requ√™te qui atteint le service via le port expos√© aboutira dans le pod correct.

Un exemple de fichier de configuration pour cr√©er un `NodePort` peut √™tre le suivant :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: hello-kube-node-port
spec:
  type: NodePort
  ports:
    - port: 8080
      targetPort: 8080
      nodePort: 31515
  selector:
    component: web
```

Le champ `spec.ports.nodePort` ici doit avoir une valeur comprise entre 30000 et 32767. Cette plage est en dehors des ports bien connus g√©n√©ralement utilis√©s par divers services mais est √©galement inhabituelle. Je veux dire, combien de fois voyez-vous un port avec autant de chiffres ?

> Vous pouvez essayer de remplacer les services `LoadBalancer` que vous avez cr√©√©s dans les sections pr√©c√©dentes par un service `NodePort`. Cela ne devrait pas √™tre difficile et peut √™tre trait√© comme un test pour ce que vous avez appris jusqu'√† pr√©sent.

Pour r√©soudre les probl√®mes que j'ai mentionn√©s, l'API `Ingress` a √©t√© cr√©√©e. Pour √™tre tr√®s clair, `Ingress` n'est en fait pas un type de service. Au lieu de cela, il se place devant plusieurs services et agit comme une sorte de routeur.

Un `IngressController` est n√©cessaire pour travailler avec les ressources `Ingress` dans votre cluster. Une liste des contr√¥leurs d'entr√©e disponibles peut √™tre trouv√©e dans la [documentation](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) de Kubernetes.

### Configuration du contr√¥leur d'entr√©e NGINX

Dans cet exemple, vous allez √©tendre l'API de notes en ajoutant un front-end. Et au lieu d'utiliser un service comme `LoadBalancer` ou `NodePort`, vous allez utiliser `Ingress` pour exposer l'application. 

Le contr√¥leur que vous allez utiliser est le [NGINX Ingress Controller](https://github.com/kubernetes/ingress-nginx/blob/master/README.md) car [NGINX](https://www.nginx.com/) sera utilis√© pour router les requ√™tes vers diff√©rents services ici. Le NGINX Ingress Controller facilite grandement le travail avec les configurations NGINX dans un cluster Kubernetes.

Le code du projet se trouve dans le r√©pertoire `fullstack-notes-application`.

```
.
‚îú‚îÄ‚îÄ api
‚îú‚îÄ‚îÄ client
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îú‚îÄ‚îÄ k8s
‚îÇ   ‚îú‚îÄ‚îÄ api-deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ database-persistent-volume-claim.yaml
‚îÇ   ‚îú‚îÄ‚îÄ postgres-cluster-ip-service.yaml
‚îÇ   ‚îî‚îÄ‚îÄ postgres-deployment.yaml
‚îú‚îÄ‚îÄ nginx
‚îî‚îÄ‚îÄ postgres

5 directories, 1 file
```

Vous verrez un r√©pertoire `k8s` l√†. Il contient tous les fichiers de configuration que vous avez √©crits dans la derni√®re sous-section, √† l'exception du fichier `api-load-balancer-service.yaml`. 

La raison en est que, dans ce projet, l'ancien service `LoadBalancer` sera remplac√© par un `Ingress`. De plus, au lieu d'exposer l'API, vous allez exposer l'application front-end au monde.

Avant de commencer √† √©crire les nouveaux fichiers de configuration, jetez un coup d'≈ìil √† la mani√®re dont les choses vont fonctionner en coulisses. 

![Image](https://www.freecodecamp.org/news/content/images/2020/08/fullstack-1.svg)

Un utilisateur visite l'application front-end et soumet les donn√©es n√©cessaires. L'application front-end transmet ensuite les donn√©es soumises √† l'API back-end. 

L'API persiste ensuite les donn√©es dans la base de donn√©es et les renvoie √©galement √† l'application front-end. Ensuite, le routage des requ√™tes est r√©alis√© en utilisant NGINX. 

Vous pouvez consulter le fichier `nginx/production.conf` pour comprendre comment ce routage a √©t√© configur√©.

Maintenant, la mise en r√©seau n√©cessaire pour que cela se produise est la suivante :

![Image](https://www.freecodecamp.org/news/content/images/2020/08/ingress.svg)

Ce diagramme peut √™tre expliqu√© comme suit :

* L'`Ingress` agira comme le point d'entr√©e et le routeur pour cette application. Il s'agit d'un `Ingress` de type `NGINX`, donc le port sera le port nginx par d√©faut, qui est 80.
* Chaque requ√™te qui arrive √† `/` sera rout√©e vers l'application front-end (le service de gauche). Donc si l'URL de cette application est `https://kube-notes.test`, alors toute requ√™te venant √† `https://kube-notes.test/foo` ou `https://kube-notes.test/bar` sera g√©r√©e par l'application front-end.
* Chaque requ√™te qui arrive √† `/api` sera rout√©e vers l'API back-end (le service de droite). Donc si l'URL est √† nouveau `https://kube-notes.test`, alors toute requ√™te venant √† `https://kube-notes.test/api/foo` ou `https://kube-notes.test/api/bar` sera g√©r√©e par l'API back-end.

Il √©tait tout √† fait possible de configurer le service `Ingress` pour qu'il fonctionne avec des sous-domaines au lieu de chemins comme celui-ci, mais j'ai choisi l'approche bas√©e sur les chemins car c'est ainsi que mon application est con√ßue.

Dans cette sous-section, vous devrez √©crire quatre nouveaux fichiers de configuration. 

* Configuration `ClusterIP` pour le d√©ploiement de l'API.
* Configuration `Deployment` pour l'application front-end.
* Configuration `ClusterIP` pour l'application front-end.
* Configuration `Ingress` pour le routage.

Je vais passer rapidement les trois premiers fichiers sans passer beaucoup de temps √† les expliquer. 

Le premier est la configuration `api-cluster-ip-service.yaml` et le contenu du fichier est le suivant :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: api-cluster-ip-service
spec:
  type: ClusterIP
  selector:
    component: api
  ports:
    - port: 3000
      targetPort: 3000
```

Bien que dans la sous-section pr√©c√©dente vous ayez expos√© l'API directement au monde ext√©rieur, dans celle-ci, vous allez laisser l'`Ingress` faire le gros du travail tout en exposant l'API en interne en utilisant un bon vieux service `ClusterIP`. 

La configuration elle-m√™me devrait √™tre assez explicite √† ce stade, donc je ne vais pas passer de temps √† l'expliquer.

Ensuite, cr√©ez un fichier nomm√© `client-deployment.yaml` responsable de l'ex√©cution de l'application front-end. Le contenu du fichier est le suivant :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: client-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      component: client
  template:
    metadata:
      labels:
        component: client
    spec:
      containers:
        - name: client
          image: fhsinchy/notes-client
          ports:
            - containerPort: 8080
          env:
            - name: VUE_APP_API_URL
              value: /api
```

Il est presque identique au fichier `api-deployment.yaml` et j'esp√®re que vous serez capable d'interpr√©ter ce fichier de configuration par vous-m√™me. 

La variable d'environnement `VUE_APP_API_URL` ici indique le chemin vers lequel les requ√™tes API doivent √™tre transf√©r√©es. Ces requ√™tes transf√©r√©es seront √† leur tour g√©r√©es par l'`Ingress`.

Pour exposer cette application client en interne, un autre service `ClusterIP` est n√©cessaire. Cr√©ez un nouveau fichier appel√© `client-cluster-ip-service.yaml` et mettez le contenu suivant dedans :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: client-cluster-ip-service
spec:
  type: ClusterIP
  selector:
    component: client
  ports:
    - port: 8080
      targetPort: 8080

```

Tout ce que cela fait est d'exposer le port 8080 √† l'int√©rieur du cluster sur lequel l'application front-end s'ex√©cute par d√©faut.

Maintenant que les anciennes configurations ennuyeuses sont termin√©es, la configuration suivante est le fichier `ingress-service.yaml` et le contenu du fichier est le suivant :

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-service
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - http:
        paths:
        - pathType: Prefix
          path: "/?(.*)"
          backend:
            service:
              name: client-cluster-ip-service
              port: 
                number: 8080
        -  pathType: Prefix
           path: "/api/?(.*)"
           backend:
             service:
              name: api-cluster-ip-service
              port:
                number: 3000

```

Ce fichier de configuration peut sembler assez inhabituel pour vous, mais il est en fait assez simple.

* L'API `Ingress` est encore en phase b√™ta, donc la `apiVersion` est `extensions/v1`.
* Les champs `kind` et `metadata.name` servent le m√™me but que n'importe laquelle des configurations que vous avez √©crites pr√©c√©demment.
* `metadata.annotations` peut contenir des informations concernant la configuration `Ingress`. Le `kubernetes.io/ingress.class: nginx` indique que l'objet `Ingress` doit √™tre contr√¥l√© par le contr√¥leur `ingress-nginx`. `nginx.ingress.kubernetes.io/rewrite-target` indique que vous souhaitez [r√©√©crire](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#rewrite) la cible de l'URL √† certains endroits.
* `spec.rules.http.paths` contient la configuration concernant les routages de chemins individuels que vous avez pr√©c√©demment vus √† l'int√©rieur du fichier `nginx/production.conf`. Le `paths.pathType` correspond au type de chemin. Par d√©faut, il est `Prefix` dans NGINX. Le champ `paths.path` indique le chemin qui doit √™tre rout√©. `backend.serviceName` est le service vers lequel le chemin mentionn√© ci-dessus doit √™tre rout√© et `backend.servicePort` est le port cible √† l'int√©rieur de ce service.
* `/?(.*)` et `/api/?(.*)` sont des regex simples qui signifient que la partie `?(.*)` sera rout√©e vers les services d√©sign√©s.

La mani√®re dont vous configurez les r√©√©critures peut changer de temps en temps, donc consulter la documentation officielle [docs](https://kubernetes.github.io/ingress-nginx/examples/rewrite/) serait une bonne id√©e.

Avant d'appliquer les nouvelles configurations, vous devrez activer l'addon `ingress` pour `minikube` en utilisant la commande `addons`. La syntaxe g√©n√©rique est la suivante :

```
minikube addons <option> <nom de l'addon>
```

Pour activer l'addon `ingress`, ex√©cutez la commande suivante :

```bash
minikube addons enable ingress

# üéâ V√©rification de l'addon ingress...
# üéâ L'addon 'ingress' est activ√©
```

Vous pouvez utiliser l'option `disable` pour la commande `addon` pour d√©sactiver n'importe quel addon. Vous pouvez en apprendre davantage sur la commande `addon` dans la documentation officielle [docs](https://minikube.sigs.k8s.io/docs/commands/addons/).

Une fois que l'addon a √©t√© activ√©, vous pouvez appliquer les fichiers de configuration. Je vous sugg√®re de supprimer toutes les ressources (services, d√©ploiements et revendications de volumes persistants) avant d'appliquer les nouvelles.

```bash
kubectl delete ingress --all

# ingress.extensions "ingress-service" deleted

kubectl delete service --all

# service "api-cluster-ip-service" deleted
# service "client-cluster-ip-service" deleted
# service "kubernetes" deleted
# service "postgres-cluster-ip-service" deleted

kubectl delete deployment --all

# deployment.apps "api-deployment" deleted
# deployment.apps "client-deployment" deleted
# deployment.apps "postgres-deployment" deleted

kubectl delete persistentvolumeclaim --all

# persistentvolumeclaim "database-persistent-volume-claim" deleted

kubectl apply -f k8s

# service/api-cluster-ip-service created
# deployment.apps/api-deployment created
# service/client-cluster-ip-service created
# deployment.apps/client-deployment created
# persistentvolumeclaim/database-persistent-volume-claim created
# ingress.extensions/ingress-service created
# service/postgres-cluster-ip-service created
# deployment.apps/postgres-deployment created
```

Attendez que toutes les ressources aient √©t√© cr√©√©es. Vous pouvez utiliser la commande `get` pour vous en assurer. Une fois qu'elles sont toutes en cours d'ex√©cution, vous pouvez acc√©der √† l'application √† l'adresse IP du cluster `minikube`. Pour obtenir l'IP, vous pouvez ex√©cuter la commande suivante :

```bash
minikube ip

# 172.17.0.2
```

Vous pouvez √©galement obtenir cette adresse IP en inspectant l'`Ingress` :

```bash
kubectl get ingress

# NAME              CLASS    HOSTS   ADDRESS      PORTS   AGE
# ingress-service   <none>   *       172.17.0.2   80      2m33s
```

Comme vous pouvez le voir, l'IP et le port sont visibles sous les colonnes `ADDRESS` et `PORTS`. En acc√©dant √† `127.17.0.2:80`, vous devriez atterrir directement sur l'application de notes.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/image-84.png)

Vous pouvez effectuer des op√©rations CRUD simples dans cette application. Le port 80 est le port par d√©faut pour NGINX, donc vous n'avez pas besoin d'√©crire le num√©ro de port dans l'URL. 

Vous pouvez faire beaucoup de choses avec ce contr√¥leur d'entr√©e si vous savez comment configurer NGINX. Apr√®s tout, c'est √† cela que sert ce contr√¥leur ‚Äî stocker les configurations NGINX sur une `ConfigMap` Kubernetes, que vous apprendrez dans la prochaine sous-section.

### Secrets et Config Maps dans Kubernetes

Jusqu'√† pr√©sent dans vos d√©ploiements, vous avez stock√© des informations sensibles telles que `POSTGRES_PASSWORD` en texte brut, ce qui n'est pas une tr√®s bonne id√©e. 

Pour stocker de telles valeurs dans votre cluster, vous pouvez utiliser un `Secret`, qui est une m√©thode beaucoup plus s√©curis√©e pour stocker des mots de passe, des jetons, etc.

> L'√©tape suivante peut ne pas fonctionner de la m√™me mani√®re dans la ligne de commande Windows. Vous pouvez utiliser [git](https://git-scm.com/) bash ou [cmder](https://cmder.net/) pour la t√¢che.

Pour stocker des informations dans un `Secret`, vous devez d'abord passer vos donn√©es par base64. Si le mot de passe en texte brut est `63eaQB9wtLqmNBpg`, ex√©cutez la commande suivante pour obtenir une version encod√©e en base64 :

```bash
echo -n "63eaQB9wtLqmNBpg" | base64

# NjNlYVFCOXd0THFtTkJwZw==
```

Cette √©tape n'est pas optionnelle, vous devez passer la cha√Æne de texte brut par base64. Maintenant, cr√©ez un fichier nomm√© `postgres-secret.yaml` √† l'int√©rieur du r√©pertoire `k8s` et mettez le contenu suivant dedans :

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
data:
  password: NjNlYVFCOXd0THFtTkJwZw==
```

Les champs `apiVersion`, `kind` et `metadata` sont assez explicites. Le champ `data` contient le secret r√©el. 

Comme vous pouvez le voir, j'ai cr√©√© une paire cl√©-valeur o√π la cl√© est `password` et la valeur est `NjNlYVFCOXd0THFtTkJwZw==`. Vous utiliserez la valeur `metadata.name` pour identifier ce `Secret` dans d'autres fichiers de configuration et la cl√© pour acc√©der √† la valeur du mot de passe.

Maintenant, pour utiliser ce secret √† l'int√©rieur de votre configuration de base de donn√©es, mettez √† jour le fichier `postgres-deployment.yaml` comme suit :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      component: postgres
  template:
    metadata:
      labels:
        component: postgres
    spec:
      volumes:
        - name: postgres-storage
          persistentVolumeClaim:
            claimName: database-persistent-volume-claim
      containers:
        - name: postgres
          image: fhsinchy/notes-postgres
          ports:
            - containerPort: 5432
          volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
              subPath: postgres
          env:
          	# ne mettant plus le mot de passe directement
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password
            - name: POSTGRES_DB
              value: notesdb

```

Comme vous pouvez le voir, l'ensemble du fichier est le m√™me sauf le champ `spec.template.spec.continers.env`. 

La variable d'environnement `name` utilis√©e pour stocker la valeur du mot de passe √©tait en texte brut auparavant. Mais maintenant, il y a un nouveau champ `valueFrom.secretKeyRef`. 

Le champ `name` ici fait r√©f√©rence au nom du `Secret` que vous avez cr√©√© il y a quelques instants, et la valeur `key` fait r√©f√©rence √† la cl√© de la paire cl√©-valeur dans ce fichier de configuration `Secret`. La valeur encod√©e sera d√©cod√©e en texte brut en interne par Kubernetes.

En plus de la configuration de la base de donn√©es, vous devrez √©galement mettre √† jour le fichier `api-deployment.yaml` comme suit :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      component: api
  template:
    metadata:
      labels:
        component: api
    spec:
      containers:
        - name: api
          image: fhsinchy/notes-api
          ports:
            - containerPort: 3000
          env:
            - name: DB_CONNECTION
              value: pg
            - name: DB_HOST
              value: postgres-cluster-ip-service
            - name: DB_PORT
              value: '5432'
            - name: DB_USER
              value: postgres
            - name: DB_DATABASE
              value: notesdb
              # ne mettant plus le mot de passe directement
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password

```

Maintenant, appliquez toutes ces nouvelles configurations en ex√©cutant la commande suivante :

```bash
kubectl apply -f k8s

# service/api-cluster-ip-service created
# deployment.apps/api-deployment created
# service/client-cluster-ip-service created
# deployment.apps/client-deployment created
# persistentvolumeclaim/database-persistent-volume-claim created
# secret/postgres-secret created
# ingress.extensions/ingress-service created
# service/postgres-cluster-ip-service created
# deployment.apps/postgres-deployment created
```

Selon l'√©tat de votre cluster, vous pouvez voir un ensemble de sortie diff√©rent.

> En cas de probl√®me, supprimez toutes les ressources Kubernetes et recr√©ez-les en appliquant les configurations.

Utilisez la commande `get` pour inspecter et vous assurer que tous les pods sont op√©rationnels. 

Maintenant, pour tester la nouvelle configuration, acc√©dez √† l'application de notes en utilisant l'IP de `minikube` et essayez de cr√©er de nouvelles notes. Pour obtenir l'IP, vous pouvez ex√©cuter la commande suivante :

```bash
minikube ip

# 172.17.0.2
```

En acc√©dant √† `127.17.0.2:80`, vous devriez atterrir directement sur l'application de notes.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/image-92.png)

Il existe une autre fa√ßon de cr√©er des secrets sans aucun fichier de configuration. Pour cr√©er le m√™me `Secret` en utilisant `kubectl`, ex√©cutez la commande suivante :

```bash
kubectl create secret generic postgres-secret --from-literal=password=63eaQB9wtLqmNBpg

# secret/postgres-secret created
```

C'est une approche plus pratique car vous pouvez sauter toute l'√©tape de codage en base64. Le secret dans ce cas sera cod√© automatiquement.

Un `ConfigMap` est similaire √† un `Secret` mais est destin√© √† √™tre utilis√© avec des informations non sensibles. 

Pour mettre toutes les autres variables d'environnement dans le d√©ploiement de l'API √† l'int√©rieur d'un `ConfigMap`, cr√©ez un nouveau fichier appel√© `api-config-map.yaml` √† l'int√©rieur du r√©pertoire `k8s` et mettez le contenu suivant dedans :

```yaml
apiVersion: v1 
kind: ConfigMap 
metadata:
  name: api-config-map 
data:
  DB_CONNECTION: pg
  DB_HOST: postgres-cluster-ip-service
  DB_PORT: '5432'
  DB_USER: postgres
  DB_DATABASE: notesdb

```

`apiVersion`, `kind` et `metadata` sont √† nouveau explicites. Le champ `data` peut contenir les variables d'environnement sous forme de paires cl√©-valeur. 

Contrairement au `Secret`, les cl√©s ici doivent correspondre exactement √† la cl√© requise par l'API. Ainsi, j'ai en quelque sorte copi√© les variables du fichier `api-deployment.yaml` et les ai coll√©es ici avec une l√©g√®re modification de la syntaxe.

Pour utiliser ce secret dans le d√©ploiement de l'API, ouvrez le fichier `api-deployment.yaml` et mettez √† jour son contenu comme suit :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      component: api
  template:
    metadata:
      labels:
        component: api
    spec:
      containers:
        - name: api
          image: fhsinchy/notes-api
          ports:
            - containerPort: 3000
          # ne mettant plus les variables d'environnement directement
          envFrom:
            - configMapRef:
                name: api-config-map
          env:
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password
          

```

L'ensemble du fichier est presque inchang√© sauf le champ `spec.template.spec.containers.env`. 

J'ai d√©plac√© les variables d'environnement vers le `ConfigMap`. `spec.template.spec.containers.envFrom` est utilis√© pour obtenir des donn√©es √† partir d'un `ConfigMap`. `configMapRef.name` ici indique le `ConfigMap` √† partir duquel les variables d'environnement seront extraites.

Maintenant, appliquez toutes ces nouvelles configurations en ex√©cutant la commande suivante :

```bash
kubectl apply -f k8s

# service/api-cluster-ip-service created
# configmap/api-config-map created
# deployment.apps/api-deployment created
# service/client-cluster-ip-service created
# deployment.apps/client-deployment created
# persistentvolumeclaim/database-persistent-volume-claim created
# ingress.extensions/ingress-service configured
# service/postgres-cluster-ip-service created
# deployment.apps/postgres-deployment created
# secret/postgres-secret created
```

Selon l'√©tat de votre cluster, vous pouvez voir un ensemble de sortie diff√©rent.

> En cas de probl√®me, supprimez toutes les ressources Kubernetes et recr√©ez-les en appliquant les configurations.

Une fois que vous vous √™tes assur√© que les pods sont op√©rationnels en utilisant la commande `get`, acc√©dez √† l'application de notes en utilisant l'IP de `minikube` et essayez de cr√©er de nouvelles notes. 

Pour obtenir l'IP, vous pouvez ex√©cuter la commande suivante :

```bash
minikube ip

# 172.17.0.2
```

En acc√©dant √† `127.17.0.2:80`, vous devriez atterrir directement sur l'application de notes.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/image-92.png)

`Secret` et `ConfigMap` ont encore quelques tours dans leur manche que je ne vais pas aborder pour l'instant. Mais si vous √™tes curieux, vous pouvez consulter la documentation officielle [docs](https://kubectl.docs.kubernetes.io/pages/app_management/secrets_and_configmaps.html).

### Effectuer des d√©ploiements de mises √† jour dans Kubernetes

Maintenant que vous avez d√©ploy√© avec succ√®s une application compos√©e de plusieurs conteneurs sur Kubernetes, il est temps d'apprendre √† effectuer des mises √† jour.

Aussi magique que Kubernetes puisse vous sembler, la mise √† jour d'un conteneur vers une version d'image plus r√©cente est un peu fastidieuse. Il existe plusieurs approches que les gens prennent souvent pour mettre √† jour un conteneur, mais je ne vais pas toutes les aborder.

Au lieu de cela, je vais passer directement √† l'approche que je prends le plus souvent pour mettre √† jour mes conteneurs. Si vous ouvrez le fichier `client-deployment.yaml` et regardez le champ `spec.template.spec.containers`, vous trouverez quelque chose qui ressemble √† ceci :

```yaml
containers:
    - name: client
      image: fhsinchy/notes-client
```

Comme vous pouvez le voir, dans le champ `image`, je n'ai pas utilis√© de tag d'image. Maintenant, si vous pensez qu'ajouter `:latest` √† la fin du nom de l'image garantira que le d√©ploiement tire toujours la derni√®re image disponible, vous vous trompez compl√®tement.

L'approche que j'utilise g√©n√©ralement est une approche imp√©rative. J'ai d√©j√† mentionn√© dans une section pr√©c√©dente que, dans quelques cas, utiliser une approche imp√©rative au lieu d'une approche d√©clarative est une bonne id√©e. Cr√©er un `Secret` ou mettre √† jour un conteneur est un tel cas.

La commande que vous pouvez utiliser pour effectuer la mise √† jour est la commande `set`, et la syntaxe g√©n√©rique est la suivante :

```
kubectl set image <type de ressource>/<nom de la ressource> <nom du conteneur>=<nom de l'image avec tag>
```

Le type de ressource est `deployment` et le nom de la ressource est `client-deployment`. Le nom du conteneur peut √™tre trouv√© sous le champ `containers` √† l'int√©rieur du fichier `client-deployment.yaml`, qui est `client` dans ce cas. 

J'ai d√©j√† construit une version de l'image `fhsinchy/notes-client` avec un tag `edge` que j'utiliserai pour mettre √† jour ce d√©ploiement.

Donc, la commande finale devrait √™tre la suivante :

```bash
kubectl set image deployment/client-deployment client=fhsinchy/notes-client:edge

# deployment.apps/client-deployment image updated
```

Le processus de mise √† jour peut prendre un certain temps, car Kubernetes va recr√©er tous les pods. Vous pouvez ex√©cuter la commande `get` pour savoir si tous les pods sont √† nouveau op√©rationnels. 

Une fois qu'ils ont tous √©t√© recr√©√©s, acc√©dez √† l'application de notes en utilisant l'IP de `minikube` et essayez de cr√©er de nouvelles notes. Pour obtenir l'IP, vous pouvez ex√©cuter la commande suivante :

```bash
minikube ip

# 172.17.0.2
```

En acc√©dant √† `127.17.0.2:80`, vous devriez atterrir directement sur l'application de notes.

![Image](https://www.freecodecamp.org/news/content/images/2020/08/image-92.png)

√âtant donn√© que je n'ai apport√© aucune modification r√©elle au code de l'application, tout restera le m√™me. Vous pouvez vous assurer que les pods utilisent la nouvelle image en utilisant la commande `describe`.

```bash
kubectl describe pod client-deployment-849bc58bcc-gz26b | grep 'Image'

# Image:          fhsinchy/notes-client:edge
# Image ID:       docker-pullable://fhsinchy/notes-client@sha256:58bce38c16376df0f6d1320554a56df772e30a568d251b007506fd3b5eb8d7c2
```

La commande `grep` est disponible sur Mac et Linux. Si vous √™tes sur Windows, utilisez git bash au lieu de la ligne de commande Windows. 

Bien que le processus de mise √† jour imp√©rative soit un peu fastidieux, il peut √™tre grandement simplifi√© en utilisant un bon flux de travail CI/CD.

### Combiner les configurations

Comme vous l'avez d√©j√† vu, le nombre de fichiers de configuration dans ce projet est assez √©lev√© malgr√© le fait qu'il ne contient que trois conteneurs.

Vous pouvez en fait combiner des fichiers de configuration comme suit :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: client-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      component: client
  template:
    metadata:
      labels:
        component: client
    spec:
      containers:
        - name: client
          image: fhsinchy/notes-client
          ports:
            - containerPort: 8080
          env:
            - name: VUE_APP_API_URL
              value: /api
              
---

apiVersion: v1
kind: Service
metadata:
  name: client-cluster-ip-service
spec:
  type: ClusterIP
  selector:
    component: client
  ports:
    - port: 8080
      targetPort: 8080

```

Comme vous pouvez le voir, j'ai combin√© le contenu des fichiers `client-deployment.yaml` et `client-cluster-ip-service.yaml` en utilisant un d√©limiteur (`---`). Bien que ce soit possible et puisse aider dans les projets o√π le nombre de conteneurs est tr√®s √©lev√©, je recommande de les garder s√©par√©s, propres et concis.

## D√©pannage

Dans cette section, je vais lister certains probl√®mes courants que vous pourriez rencontrer lors de votre utilisation de Kubernetes.

* Si vous √™tes sur Windows ou Mac et utilisez le pilote Docker pour `minikube`, le plugin `Ingress` ne fonctionnera pas.
* Si vous avez [Laravel Valet](https://laravel.com/docs/7.x/valet) en cours d'ex√©cution sur Mac et utilisez le pilote HyperKit pour `minikube`, il √©chouera √† se connecter √† Internet. D√©sactiver le service `dnsmasq` r√©soudra le probl√®me.
* Si vous avez un PC Ryzen (le mien est R5 1600) et ex√©cutez Windows 10, le pilote VirtualBox peut √©chouer √† d√©marrer en raison du manque de support pour la virtualisation imbriqu√©e. Vous devrez utiliser le pilote Hyper-V sur Windows 10 (Pro, Enterprise et Education). Pour les utilisateurs de l'√©dition Home, malheureusement, il n'y a pas d'option s√ªre sur ce mat√©riel.
* Si vous ex√©cutez Windows 10 (Pro, Enterprise et Education) avec le pilote Hyper-V pour `minikube`, la VM peut √©chouer √† d√©marrer avec un message concernant une m√©moire insuffisante. Ne paniquez pas, et ex√©cutez la commande `minikube start` une fois de plus pour d√©marrer correctement la VM.
* Si vous voyez certaines des commandes ex√©cut√©es dans cet article manquantes ou mal comport√©es dans la ligne de commande Windows, utilisez [git](https://git-scm.com/) bash ou [cmder](https://cmder.net/) √† la place.
* Si vous √™tes sur un Raspberry Pi, mes images ne fonctionneront pas. Utilisez les images construites par [Raed Chammam](https://github.com/RaedsLab). Les trois images peuvent √™tre trouv√©es sur son [profil Docker Hub](https://hub.docker.com/u/raed667). Les instructions concernant la construction d'images pour Raspberry Pi peuvent √™tre trouv√©es dans ce [probl√®me GitHub](https://github.com/fhsinchy/kubernetes-handbook-projects/issues/2#issue-899658948).

Je vous sugg√©rerais d'installer une bonne distribution Linux sur votre syst√®me et d'utiliser le pilote Docker pour `minikube`. C'est de loin la configuration la plus rapide et la plus fiable.

## Conclusion

Je tiens √† vous remercier du fond du c≈ìur pour le temps que vous avez pass√© √† lire cet article. J'esp√®re que vous avez appr√©ci√© votre temps et que vous avez appris toutes les bases de Kubernetes.

En plus de celui-ci, j'ai √©crit des manuels complets sur d'autres sujets compliqu√©s disponibles gratuitement sur [freeCodeCamp](https://www.freecodecamp.org/news/author/farhanhasin/).

Ces manuels font partie de ma mission de simplifier les technologies difficiles √† comprendre pour tout le monde. Chacun de ces manuels prend beaucoup de temps et d'efforts √† √©crire.

Si vous avez appr√©ci√© mon √©criture et souhaitez me motiver, envisagez de laisser des √©toiles sur [GitHub](https://github.com/fhsinchy/) et de m'endosser pour des comp√©tences pertinentes sur [LinkedIn](https://www.linkedin.com/in/farhanhasin/). J'accepte √©galement les parrainages, vous pouvez donc envisager de [m'offrir un caf√©](https://www.buymeacoffee.com/farhanhasin) si vous le souhaitez.

Je suis toujours ouvert aux suggestions et aux discussions sur [Twitter](https://twitter.com/frhnhsin) ou [LinkedIn](https://www.linkedin.com/in/farhanhasin/). Envoyez-moi des messages directs.

Enfin, envisagez de partager les ressources avec les autres, car

> Partager les connaissances est l'acte le plus fondamental d'amiti√©. Parce que c'est une fa√ßon de donner quelque chose sans perdre quelque chose. ‚Äî Richard Stallman

Jusqu'au prochain, restez en s√©curit√© et continuez √† apprendre.