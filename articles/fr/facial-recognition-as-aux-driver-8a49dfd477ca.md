---
title: 'De la réalité augmentée à la détection des émotions : comment les caméras
  sont devenues le meilleur outil pour décrypter le monde'
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2017-09-14T11:59:53.000Z'
originalURL: https://freecodecamp.org/news/facial-recognition-as-aux-driver-8a49dfd477ca
coverImage: https://cdn-media-1.freecodecamp.org/images/1*SaZceHMxVeG7iDRWLeDn6A.jpeg
tags:
- name: Apple
  slug: apple
- name: Augmented Reality
  slug: augmented-reality
- name: Design
  slug: design
- name: facial recognition
  slug: facial-recognition
- name: UX
  slug: ux
seo_title: 'De la réalité augmentée à la détection des émotions : comment les caméras
  sont devenues le meilleur outil pour décrypter le monde'
seo_desc: 'By Avi Ashkenazi

  The camera is finally on stage to help solve user experience (UX) design, technology,
  and communication issues.

  Years after the Kinect was trashed and Google Glass failed, there is now new hope.
  The impressive technological array tha...'
---

Par Avi Ashkenazi

La caméra est enfin sous les projecteurs pour aider à résoudre les problèmes de conception de l'expérience utilisateur (UX), de technologie et de communication.

Des années après que le [Kinect](https://en.wikipedia.org/wiki/Kinect) ait été abandonné et que [Google Glass](https://en.wikipedia.org/wiki/Google_Glass) ait échoué, il y a maintenant un nouvel espoir. Le impressionnant ensemble technologique qu'[Apple](https://en.wikipedia.org/wiki/Apple_Inc.) a miniaturisé, passant de [PrimeSense](https://en.wikipedia.org/wiki/PrimeSense) à l'[iPhone X](https://www.apple.com/ca/iphone-x/?afid=p238%7CskwMTeFkg-dc_mtid_20925xpb40345_pcrid_220916805480_&cid=wwa-ca-kwgo-iphone-slid-), est le début des interactions dépendantes des émotions.

Ce n'est pas nouveau. C'est commercialisé et donne aux développeurs accès à des informations indispensables.

![Image](https://cdn-media-1.freecodecamp.org/images/FUI8GMHXrVH8BJMvlFPAGRxPfeU17rUJUyiE)

Récemment, Mark Zuckerberg a mentionné que beaucoup des efforts de Facebook se concentreront sur la caméra et son environnement. [Snapchat](https://www.snapchat.com/l/en-gb/) s'est défini comme une entreprise de caméras. Apple et Google investissent également massivement dans les caméras. **La caméra a un pouvoir énorme que nous n'avons pas encore exploité. Elle a le pouvoir de détecter les émotions.**

### Les entrées doivent être faciles, naturelles et sans effort

![Image](https://cdn-media-1.freecodecamp.org/images/fJKL3RDfpjFK19X2W8Lthc1tFEadL9iqN9d0)

Lorsque Facebook a introduit pour la première fois les emojis comme une réaction améliorée au _J'aime_, j'ai réalisé qu'ils étaient sur quelque chose. Facebook a récemment ajouté cinq émotions qui aident Facebook à mieux comprendre les réactions émotionnelles de ses utilisateurs à son contenu. Je soutiens que les emojis sont une forme glorifiée de la même chose, mais qui fonctionne mieux que tout le reste.

Dans le passé, Facebook n'avait que le bouton _J'aime_ tandis que [YouTube](https://www.youtube.com/) avait les boutons _J'aime_ et _Je n'aime pas_. Mais ceux-ci ne suffisent pas pour suivre les émotions et ne fournissent pas beaucoup de valeur aux chercheurs et aux annonceurs. La plupart des gens expriment leurs émotions dans les commentaires, et pourtant il y a plus de _J'aime_ que de commentaires.

Les commentaires sont basés sur du texte ou même présentés avec une image, ce qui est plus difficile à analyser. C'est parce qu'il y a de nombreuses connexions contextuelles que l'algorithme doit deviner. Par exemple, à quel point la personne qui réagit à une publication est-elle familière avec la personne qui l'a publiée, et vice versa ? Comment la personne est-elle connectée au sujet spécifié ?

Y a-t-il un sous-texte, de l'argot ou quelque chose lié à l'expérience de la personne ? Est-ce une conversation qui se poursuit depuis le passé ? Facebook a fait un travail merveilleux en gardant la conversation positive. Facebook a empêché le bouton _Je n'aime pas_ de prendre le dessus, ce qui aurait pu décourager les gens de créer et de partager du contenu. Facebook l'a gardé positivement agréable.

Maintenant, je comparerais Facebook à un forum glorifié. Les utilisateurs peuvent répondre aux commentaires ou aux emojis. Les utilisateurs peuvent _aimer_ un _J'aime_ ?. Pourtant, il est encore très difficile de savoir ce que les gens ressentent. La plupart des gens qui lisent ne laissent pas de commentaire. Que ressentent les lecteurs lorsqu'ils lisent une publication alors ?

### L'ancienne expérience utilisateur pour les caméras

Que faites-vous avec une caméra ? Prendre des photos et des vidéos, et c'est à peu près tout. Il y a eu un énorme développement dans les applications de caméra. Il y a de nombreuses fonctionnalités qui sont liées à l'environnement du cas d'utilisation principal, par exemple la plage dynamique élevée (HDR), le ralenti, le mode portrait, etc.

![Image](https://cdn-media-1.freecodecamp.org/images/V6KJupNV99PBW08gLaLSxYH6xtXFCnpWWgce)
_[source de l'image](https://twitter.com/lukew/status/522056776477200384" rel="noopener" target="_blank" title=")_

Sur la base de l'énorme nombre de photos générées par les utilisateurs, une nouvelle vague de galeries intelligentes, de traitement de photos et d'applications de métadonnées a été créée.

![Image](https://cdn-media-1.freecodecamp.org/images/sSbmUt0PEag0Eo7G97k7vHdZK4TNAHhbMluu)
_Photographie de l'App Store Mac_

Cependant, récemment, l'accent a changé. Il est maintenant sur la caméra intégrée à la vie, qui est une combinaison des traits les plus forts et des meilleurs cas d'utilisation pour les téléphones mobiles. La prochaine génération de caméras sera entièrement intégrée dans nos vies et pourrait remplacer toutes les icônes d'entrée dans une application de messagerie (microphone, caméra et localisation).

Les caméras sont l'un des trois éléments qui ont été développés de manière constante à un rythme vertigineux. L'écran et le processeur sont les deux autres. Chaque téléphone qui est sorti a repoussé les limites, et ce, année après année. Les améliorations apportées aux caméras concernent leurs mégapixels, la stabilisation des mouvements, l'ouverture, la vitesse et, comme mentionné ci-dessus, les applications.

Évaluons l'évolution de quelques produits créés par des entreprises.

![Image](https://cdn-media-1.freecodecamp.org/images/fXI99nmPeAu7Jt9PyGqcTBV81iSH8QuzzUPr)
_Ce n'est qu'un aperçu de la mise à niveau des mégapixels (MP), sans inclure les doubles caméras, le flash, etc. Il y a eu de nombreux changements logiciels._

Une grande partie du développement s'est concentrée sur la caméra à l'arrière du téléphone car, au moins au début, on pensait que la caméra avant n'était utile que pour les appels vidéo. Cependant, la culture du selfie et Snapchat ont changé cela. Les masques de Snapchat, qui ont ensuite été copiés par tout le monde, sont un énorme succès. Les masques faciaux ne sont pas nouveaux. Google les a introduits il y a un moment, mais Snapchat a été efficace pour développer l'utilisation des masques.

### Points forts de la mémoire

En décembre 2009, Google a introduit [Google Goggles](https://en.wikipedia.org/wiki/Google_Goggles). C'était la première fois que les utilisateurs pouvaient utiliser leur téléphone pour obtenir des informations sur l'environnement qui les entoure. Les informations concernaient principalement des monuments au début.

![Image](https://cdn-media-1.freecodecamp.org/images/oHjMlhSBeYeeT0W2rh-NBhguf30uQ19daYyK)

En novembre 2011, [Samsung Nexus](https://en.wikipedia.org/wiki/Galaxy_Nexus) a introduit la reconnaissance faciale comme moyen de déverrouiller les téléphones. Comme beaucoup de choses faites pour la première fois, ce n'était pas très bon et a ensuite été abandonné.

![Image](https://cdn-media-1.freecodecamp.org/images/lA5hJhW2P9uz0i9H4qU0W6mCGjdpgETQk8XL)
_Samsung (Google) Nexus_

En février 2013, Google a lancé [Google Glass](https://en.wikipedia.org/wiki/Google_Glass). Il avait plus de cas d'utilisation car il pouvait recevoir des entrées non seulement de la caméra mais aussi d'autres éléments comme la voix. Le Google Glass était toujours là, mais il n'a pas réussi à gagner en traction car il était trop cher, avait l'air peu fashionable et a déclenché un contrecoup du public. Il n'était tout simplement pas prêt pour le grand public.

![Image](https://cdn-media-1.freecodecamp.org/images/Mi6TZll-7VCgLIMC1i0fmEVytAwQkAio4gh2)
_Google Glass 1_

Jusqu'à présent, les appareils n'ont que des informations limitées à leur disposition. Les informations qu'ils ont sont audiovisuelles avec GPS et données historiques. Mais cela a limité Google Glass à afficher les informations sur le petit écran positionné près de l'œil des utilisateurs. L'écran empêchait les utilisateurs de regarder autre chose. Mettre cette technologie sur un téléphone pour un usage externe n'est pas seulement une limitation technologique mais aussi physique.

Lorsque vous vous concentrez sur votre téléphone, vous ne pouvez pas voir autre chose. Votre champ de vision est limité. Cela est similaire au champ de vision dans les principes d'expérience utilisateur pour la réalité virtuelle (VR). C'est pourquoi il y a des villes qui ont créé des itinéraires pour les personnes qui utilisent leur téléphone en marchant et ont installé des feux de circulation qui aident les gens à marcher et à texter. Un concept comme [Microsoft HoloLens](https://en.wikipedia.org/wiki/Microsoft_HoloLens) est plus aligné avec l'environnement spatial et peut réellement aider les utilisateurs pendant qu'ils se déplacent et utilisent leur téléphone, plutôt que d'absorber leur attention et de les mettre en danger.

![Image](https://cdn-media-1.freecodecamp.org/images/lJJIwGw9jsElIfX8CLQldwFEGo7kmaxzIS92)
_[Kinect](https://en.wikipedia.org/wiki/Kinect" rel="noopener" target="_blank" title=") acheté par Apple Inc._

![Image](https://cdn-media-1.freecodecamp.org/images/UYGuaa5wqLz7fTpakaH8PkurU8lC-WLy9KOB)
_Microsoft HoloLens_

En juillet 2014, Amazon a introduit le [Fire Phone](https://en.wikipedia.org/wiki/Fire_Phone). Il comportait quatre caméras à l'avant du téléphone. C'était une percée, même si cela n'a pas réussi. Les quatre caméras frontales étaient utilisées pour le défilement et créaient des effets 3D basés sur l'accéléromètre et le regard des utilisateurs. C'était la première fois qu'un téléphone utilisait ses caméras avant comme méthode d'entrée des utilisateurs.

![Image](https://cdn-media-1.freecodecamp.org/images/mviXdikNehcnYX6pczcR5YZh5Sq42EXowt8w)
_Le Fire Phone_

En août 2016, le [Note 7 de Samsung](https://en.wikipedia.org/wiki/Samsung_Galaxy_Note_7) a été lancé. Il permettait aux utilisateurs de déverrouiller leur téléphone avec un scan de l'iris. Samsung a ressuscité une technologie de reconnaissance faciale qui était restée sur l'étagère pendant six ans. Malheureusement, regarder le tutoriel peut être déconcertant. Samsung n'a pas fait beaucoup de tests d'expérience utilisateur pour cette fonctionnalité.

Il est dérangeant de tenir ce gros téléphone et de le mettre à un angle de 90° par rapport à votre visage. Ce n'est pas quelque chose que quiconque devrait faire en marchant dans la rue. Cela peut bien fonctionner pour les femmes saoudiennes qui couvrent leur visage, mais en raison de défauts de fabrication, de nombreux téléphones Note 7 ont surchauffé, pris feu ou explosé. Le concept de scan de l'iris a été mis en attente pendant une autre année complète jusqu'à la sortie du [Note 8](http://www.samsung.com/ca/smartphones/galaxy-note8/).

![Image](https://cdn-media-1.freecodecamp.org/images/ARJMQFIYO3SSPkWqvyTdLAhYYCzAb7zMJclk)
_De la keynote de Samsung_

Mais à cette époque, personne n'a mentionné le scan de l'iris. Le Note 8 a mentionné que le scan de l'iris est une autre façon de déverrouiller un téléphone en conjonction avec le capteur d'empreintes digitales. C'est probablement parce que le téléphone n'était pas assez bon ou que Samsung n'a pas été en mesure de prendre une décision (similaire à la sortie du [Galaxy 6](https://en.wikipedia.org/wiki/Samsung_Galaxy_S6) et [6 Edge](http://www.samsung.com/global/galaxy/galaxys6/galaxy-s6-edge/)). Pour qu'un produit réussisse, il doit avoir plusieurs fonctions, sinon il risque d'être oublié.

Google a fait une pause, puis en juillet 2017 a sorti la deuxième version de Google Glass en tant que produit business-to-business. Les cas d'utilisation sont devenus plus spécifiques pour certaines industries.

![Image](https://cdn-media-1.freecodecamp.org/images/as8KODosoGmf0DLqu3BxyagxqR0Z4TrsKO9r)
_Google Glass 2_

Maintenant, Google est sur le point de sortir le [Google Lens](https://en.wikipedia.org/wiki/Google_Lens) pour amener le cas d'utilisation initial de Goggles au présent. C'est l'effort de Google pour apprendre à utiliser les informations visuelles avec un contexte supplémentaire et pour déterminer quel produit développer ensuite. Il semble que Google se dirige vers une caméra que les utilisateurs peuvent porter.

![Image](https://cdn-media-1.freecodecamp.org/images/dLR9bU8pa3T27s1RZIW-WBMFs3ckufaR-lD5)
_Application Google Lens_

Il y a d'autres entreprises qui explorent également les entrées visuelles. Par exemple, [Pinterest](https://www.pinterest.ca/) voit une énorme demande pour sa lentille de recherche visuelle, que ses utilisateurs utilisent pour rechercher des articles à acheter et pour aider les gens à curater des produits et services en ligne.

![Image](https://cdn-media-1.freecodecamp.org/images/xbxqsL71dpEQ6ntKGhbFCnc26uaRkqU5oqsb)
_Recherche visuelle Pinterest_

Les [lunettes de Snapchat](https://www.spectacles.com/) permettent aux utilisateurs d'enregistrer facilement des vidéos courtes (même si le processus de téléchargement est fastidieux).

![Image](https://cdn-media-1.freecodecamp.org/images/XOSBAIIZMPeZueZqGGjjFr-TnVbrvYqZF9tr)
_Lunettes de Snap_

Maintenant, la reconnaissance faciale est également sur le Note 8 et le [Galaxy 8](http://www.samsung.com/ca/smartphones/galaxy-note8/), mais cette fonctionnalité ne se déroule pas aussi bien que prévu.

![Image](https://cdn-media-1.freecodecamp.org/images/gF2-zI1OCAi6Tk19SkfDzu3ToUSOGWKX4ko2)
_Reconnaissance faciale Galaxy S8_

Pour voir une démonstration de reconnaissance faciale, cliquez [ici](https://twitter.com/MelTajon/status/904058526061830144/video/1).

Apple est lent à adopter de nouvelles technologies par rapport à ses concurrents. Mais d'un autre côté, Apple commercialise ses produits, par exemple, l'Apple [Watch](https://www.apple.com/ca/apple-watch-series-1/). La Watch était tout à fait sur la reconnaissance faciale et l'écran fini. Il n'y a pas de meilleure façon de faire utiliser cette fonctionnalité aux gens qu'en supprimant toutes les autres options (comme le [Touch ID](https://en.wikipedia.org/wiki/Touch_ID)). Il n'est pas surprenant qu'Apple ait fait cela l'année dernière avec l'audio sans fil (Apple a supprimé la prise casque) et le [USB C](https://en.wikipedia.org/wiki/USB-C) sur le [MacBook Pro](https://en.wikipedia.org/wiki/MacBook_Pro) (en supprimant tout le reste).

![Image](https://cdn-media-1.freecodecamp.org/images/R6-NvHnItPrCBGOhUvgAFI0UooTXbri8ptfL)

Il y a une raison beaucoup plus grande pour laquelle Apple a choisi cette technologie à ce moment-là. Cela a à voir avec ses efforts en matière de réalité augmentée.

![Image](https://cdn-media-1.freecodecamp.org/images/M2rXsHl5mhJykSZHXfG0gAftwt7zbxUmTlDT)

Face ID présente des défis qui incluent la reconnaissance des utilisateurs qui portent le [Niqāb](https://en.wikipedia.org/wiki/Niq%C4%81b) (voiles faciaux), des utilisateurs qui ont subi une chirurgie plastique et des utilisateurs qui changent physiquement parce qu'ils grandissent. Mais le tableau plus large ici est beaucoup plus intéressant. C'est la première fois que les utilisateurs peuvent faire quelque chose qu'ils font naturellement sans effort, tout en recevant des données qui sont significatives pour l'avenir de la technologie. Je crois qu'un écran qui peut lire les empreintes digitales est meilleur. Il semble que Samsung se dirige dans cette direction (bien que des rumeurs disent qu'Apple a essayé et échoué).

![Image](https://cdn-media-1.freecodecamp.org/images/95Zs7P65WrAdId1giWUCRpwfRWntXHeJComE)

### Alors, où cela nous mène-t-il ? Quel est l'objectif ?

Dans le passé, les entreprises utilisaient des lunettes et des appareils spéciaux pour effectuer des tests utilisateurs. La seule sortie qu'ils pouvaient donner était des [cartes thermiques](https://en.wikipedia.org/wiki/Heat_map). Pourtant, ils n'étaient pas en mesure de documenter ce sur quoi les utilisateurs se concentraient ou leurs émotions et réactions.

![Image](https://cdn-media-1.freecodecamp.org/images/Fhb83q3pVYAtpQm2Raq73j3YP6tzlmDmtqIJ)
_Lunettes Tobii Pro — en est un exemple_

Sur la base des tendances technologiques, il semble que l'avenir implique la réalité augmentée et la réalité virtuelle. Mais à mon avis, cela inclut l'audio, le son 3D et les entrées visuelles combinées. Ce serait une expérience merveilleuse, qui permettrait aux utilisateurs de regarder n'importe quoi, n'importe où, et d'obtenir des informations en même temps.

Et si nous étions capables de savoir où les utilisateurs regardent et sur quoi ils se concentrent ? Depuis des années, c'est quelque chose que les professionnels du marketing et de la conception ont essayé de capturer et d'analyser. Qu'est-ce qui peut faire cela mieux que l'ensemble de réseaux qu'un appareil comme l'[iPhone X](https://www.apple.com/ca/iphone-x/) a comme point de départ ? Plus tard, cela a évolué vers des lunettes qui peuvent voir sur quoi l'utilisateur se concentre.

### **Les réactions sont puissantes et addictives**

Les réactions aident les gens à converser et augmentent la rétention et l'engagement. Certaines applications offrent des réactions aux publications sous forme de messages qui peuvent être envoyés à des amis. Il y a des vidéos amusantes sur YouTube qui montrent les réactions des gens qui regardent des vidéos. Il y a même une émission de télévision, appelée [Gogglebox](https://en.wikipedia.org/wiki/Gogglebox), qui est dédiée à montrer des gens regardant la télévision.

Lors de [IO Google](https://events.google.com/io/), le festival annuel des développeurs, Google a ouvert l'option de payer les créateurs pour leur plateforme. C'est comme ce que le site brillant [Patreon](https://en.wikipedia.org/wiki/Patreon) fait, mais de manière beaucoup plus dominante. Une façon qui vous aide à vous démarquer de la foule et à attirer l'attention du créateur est [SuperChat](https://www.youtube.com/watch?v=b9szyPvMDTk).

[Dans le projet étudiant de Chris Harrison en 2009](ttps://www.youtube.com/watch?time_continue=88&v=PDI8eYIASf0), Harrison a créé un clavier qui avait des touches sensibles à la pression. Selon la force avec laquelle les utilisateurs tapent, le clavier lit les émotions des utilisateurs et détermine s'ils sont en colère ou excités. Les lettres deviennent également plus grandes en conséquence. Maintenant, imaginez le combiner avec une caméra qui voit les expressions faciales des utilisateurs pendant qu'ils tapent, car les gens ont tendance à exprimer leurs émotions pendant qu'ils tapent un message.

### **À quoi ressemblerait une telle UX**

Considérez l'appariement d'une télécommande et d'un point central en réalité virtuelle. Le centre est notre focus, mais nous avons également un point de focus secondaire, qui est là où les points de la télécommande sont. Cependant, ce type d'expérience utilisateur ne peut pas fonctionner en réalité augmentée. Pour tirer parti de la réalité augmentée, qui est un nouveau focus pour Apple, le focus de l'utilisateur doit être connu.

![Image](https://cdn-media-1.freecodecamp.org/images/zXxAcez4iyKwXaOLzwLMZwiHoDqe9zJDlRzW)
_[source de l'illustration](https://blog.kickpush.co/beyond-reality-first-steps-into-the-unknown-cbb19f039e51" rel="noopener" target="_blank" title=")_

Ce qui a commencé comme [ARKit](https://developer.apple.com/arkit/) et le [ARCore SDK de Google](https://venturebeat.com/2017/08/29/google-launches-arcore-sdk-in-preview-ar-on-android-phones-no-extra-hardware-required/), les deux seront l'avenir du développement. Cela est dû à la sortie et à l'entrée incroyables que les deux peuvent obtenir des caméras avant et arrière **combinées**. Cela permettra une plus grande concentration sur l'entrée.

### **Une vision plus critique des développements futurs**

![Image](https://cdn-media-1.freecodecamp.org/images/k8lObTtpncpJAnbB97YSSeYVYkKiV2TYZ8yw)

Alors qu'Apple a ouvert la voie à la reconnaissance faciale et déclenché des [Animoji](https://www.theverge.com/2017/9/12/16290210/new-iphone-emoji-animated-animoji-apple-ios-11-update) réactionnaires, cela va devenir intéressant lorsque d'autres organisations commenceront à mettre en œuvre [Face ID](https://en.wikipedia.org/wiki/Face_ID). Actuellement, cela se manifeste de manière basique et inoffensive, mais l'objectif reste d'obtenir plus d'informations. Des informations qui seront utilisées pour suivre et vendre des produits. Cela nous permettra également d'en apprendre davantage sur nous-mêmes et de recueillir nos données émotionnelles.

![Image](https://cdn-media-1.freecodecamp.org/images/2Nn-xjcAIB6HZ8zRuxDvV4TpCHKKhfwnSk6X)
_Animoji_

Il est important de dire que la caméra avant ne vient pas seule. C'est le résultat attendu de l'achat de [PrimeSense](https://en.wikipedia.org/wiki/PrimeSense) par Apple. Le réseau de technologies orientées vers l'avant comprend une caméra infrarouge, un capteur de profondeur, etc. (Je pense qu'ils pourraient bien faire avec un capteur de chaleur aussi.) Ce n'est pas que quelqu'un gardera des vidéos de nos visages en utilisant le téléphone, mais plutôt qu'il y aura un scraper qui documentera toutes les informations sur nos émotions.

![Image](https://cdn-media-1.freecodecamp.org/images/rky60ojrRD-nPMHs9J9EmbHmIx3aBmNK2DLp)
_Ne peut pas être trompé par un masque — de la keynote d'Apple_

![Image](https://cdn-media-1.freecodecamp.org/images/D4T1EvmyV9cvV08Ca3MJW2FUsxc3C7rYAaSZ)
_Ou assez drôle_

### **Résumé**

C'est excitant que la réalité augmentée ait des algorithmes qui peuvent lire les visages. Il y a de nombreux livres qui parlent de la façon d'identifier les réactions faciales, mais maintenant c'est le moment pour la technologie de le faire. Ce sera merveilleux pour de nombreuses raisons. Par exemple, les robots peuvent maintenant voir comment nous nous sentons et réagissons, ou avec des lunettes, nous pouvons obtenir plus de contexte sur ce dont nous avons besoin qu'ils fassent. En ce qui concerne les ordinateurs, il est préférable de regarder une combinaison d'éléments car cela aide la machine à mieux vous comprendre.

Les choses que vous pouvez faire si vous avez les informations sur ce que l'utilisateur se concentre sont infinies. C'est le rêve de toute personne qui travaille avec la technologie.

Cet article de blog a été initialement montré [ici](https://superavi.com/facial-recognition-as-ux-driver-from-ar-to-emotion-detection-how-the-camera-turned-to-be-the-best-tool-to-decipher-the-world/).