---
title: Comment r√©diger des prompts efficaces pour les agents IA en utilisant Langbase
subtitle: ''
author: Maham Codes
co_authors: []
series: null
date: '2025-03-19T15:25:14.209Z'
originalURL: https://freecodecamp.org/news/how-to-write-effective-prompts-for-ai-agents-using-langbase
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1742397395773/51f66f8f-56a4-418e-a59e-141de64be2b2.png
tags:
- name: llm
  slug: llm
- name: AI
  slug: ai
- name: ai agents
  slug: ai-agents
- name: '#PromptEngineering'
  slug: promptengineering
seo_title: Comment r√©diger des prompts efficaces pour les agents IA en utilisant Langbase
seo_desc: 'Prompt engineering isn‚Äôt just a skill these days ‚Äì it gives you an important
  competitive edge in your development.

  In 2025, the difference between AI agents that work and those that don‚Äôt comes down
  to how well they‚Äôre prompted. Whether you‚Äôre a deve...'
---

L'ing√©nierie des prompts n'est pas seulement une comp√©tence de nos jours ‚Äì elle vous donne un avantage concurrentiel important dans votre d√©veloppement.

En 2025, la diff√©rence entre les agents IA qui fonctionnent et ceux qui ne fonctionnent pas r√©side dans la qualit√© de leurs prompts. Que vous soyez d√©veloppeur, chef de produit ou simplement en train de construire avec l'IA, devenir tr√®s bon en ing√©nierie des prompts vous rendra significativement plus efficace.

Langbase vous permet de cr√©er des prompts haute performance et de d√©ployer des agents IA serverless optimis√©s pour les derniers mod√®les. Dans cet article, nous allons d√©composer des conseils et astuces pour vous aider √† concevoir des prompts efficaces. Nous examinerons √©galement quelques techniques avanc√©es d'ing√©nierie des prompts pour construire des agents serverless, et comment ajuster les param√®tres des LLM pour obtenir les meilleurs r√©sultats.

### Pr√©requis

Pour tirer le meilleur parti de cet article, vous aurez besoin de :

* Un compte Langbase ‚Äì [Inscription](http://langbase.com/signup) si vous ne l'avez pas d√©j√†.

* Des connaissances de base sur les [LLM](https://www.freecodecamp.org/news/how-to-start-building-projects-with-llms/), les [agents IA](https://www.freecodecamp.org/news/how-ai-agents-can-supercharge-language-models-handbook/), et le [RAG (retrieval-augmented generation)](https://www.freecodecamp.org/news/learn-rag-fundamentals-and-advanced-techniques/).

### **Voici ce que je vais couvrir :**

* [Fondamentaux de l'ing√©nierie des prompts](#heading-fondamentaux-de-lingenierie-des-prompts)

* [Conseils et astuces pour une conception efficace des prompts](#heading-conseils-et-astuces-pour-une-conception-efficace-des-prompts)

* [Prompts des agents Langbase Pipe](#heading-prompts-des-agents-langbase-pipe)

* [Comment concevoir les prompts de votre agent IA](#heading-comment-concevoir-les-prompts-de-votre-agent-ia)

* [Affiner les param√®tres de r√©ponse du mod√®le LLM](#heading-affiner-les-parametres-de-reponse-du-modele-llm)

Commen√ßons !

## Fondamentaux de l'ing√©nierie des prompts

Un prompt indique √† l'IA ce qu'elle doit faire‚Äîil d√©finit le contexte, guide la r√©ponse et fa√ßonne la conversation. L'ing√©nierie des prompts consiste √† concevoir des prompts qui rendent les agents IA r√©ellement utiles dans des applications r√©elles.

Voici comment r√©diger de bons prompts :

### 1. D√©finissez clairement votre objectif

Avant de r√©diger un prompt, soyez clair sur ce que vous voulez accomplir‚Äîcomme planifier la logique avant d'√©crire du code. Consid√©rez si des entr√©es dynamiques sont n√©cessaires et comment elles seront g√©r√©es. D√©finissez le format de sortie id√©al, qu'il s'agisse de JSON, XML ou de texte brut. D√©terminez si le mod√®le n√©cessite un contexte suppl√©mentaire ou si ses donn√©es d'entra√Ænement suffisent.

Fixez des contraintes sur la longueur, la structure ou le ton de la r√©ponse. Ajustez les param√®tres du LLM si n√©cessaire pour am√©liorer le contr√¥le. Plus vos objectifs sont pr√©cis, meilleurs sont les r√©sultats. Et n'oubliez pas, l'ing√©nierie efficace des prompts est souvent un effort d'√©quipe.

Voici un exemple de prompt pour vous aider. Si vous construisez un bot de support client, votre objectif devrait ressembler √† ceci :

*"G√©n√©rez des r√©ponses concises et polies en texte brut, en puisant les d√©tails dans la base de connaissances de l'entreprise."*

* D√©finissez le format de sortie (JSON, XML, texte brut).

* *Exemple : "R√©pondez au format JSON avec les champs 'answer' et 'source'."*

* D√©cidez si un contexte suppl√©mentaire est n√©cessaire.

* *Exemple : "Utilisez ce document comme r√©f√©rence : [URL]."*

* Fixez des contraintes sur la longueur, le ton ou la structure.

* *Exemple : "Limitez la r√©ponse √† 50 mots, utilisez un ton amical."*

### 2. Exp√©rimentez sans rel√¢che

Les LLM ne sont pas parfaits, et l'ing√©nierie des prompts non plus. Testez tout. Essayez diff√©rents formats, ajustez les param√®tres et fournissez des exemples. Les mod√®les d'IA varient en capacit√©‚Äîaffiner les prompts par it√©ration est le seul moyen de garantir des sorties fiables.

Supposons que votre IA ne donne pas de r√©ponses utiles. Vous pourriez :

* **Reformuler le prompt** : Au lieu de *"Expliquez ce sujet,"* essayez *"R√©sumez ceci en un paragraphe avec les points cl√©s √† retenir."*

* **Ajouter des contraintes** : *"Limitez la r√©ponse √† trois points."*

* **Donner des exemples** : *"Par exemple, si on vous demande √† propos de Python, r√©pondez comme ceci : 'Python est un langage polyvalent utilis√© pour l'IA, le d√©veloppement web et l'automatisation.'"*

### 3. Traitez les LLM comme des machines, pas comme des humains

Les LLM ne pensent pas. Ils suivent des instructions‚Äîpr√©cis√©ment. L'ambigu√Øt√© les confond. Trop expliquer peut √™tre aussi mauvais que ne pas assez expliquer. Et n'oubliez pas : les LLM g√©n√©reront une r√©ponse, m√™me si elle est fausse. Vous devez g√©rer ce risque.

Voici une comparaison entre des prompts sur-expliqu√©s et sous-expliqu√©s :

**Sur-expliqu√© :** *"Pouvez-vous, si possible, fournir une explication tr√®s d√©taill√©e mais concise sur le fonctionnement des r√©seaux de neurones, mais pas trop technique, et essayer d'√™tre engageant, mais aussi garder cela court ?"*  
**Meilleur prompt :** *"Expliquez les r√©seaux de neurones en termes simples, en moins de 100 mots, avec une analogie."*

**Sous-expliqu√© :** *"Parlez-moi des r√©seaux de neurones."*  
**Meilleur prompt :** *"D√©crivez les r√©seaux de neurones en deux phrases avec un exemple."*

## Conseils et astuces pour une conception efficace des prompts

Voici quelques conseils et astuces pour vous aider √† concevoir efficacement les prompts de votre IA et de vos agents :

* **Soyez sp√©cifique** ‚Äì Les prompts vagues conduisent √† de mauvaises sorties. D√©finissez le format, le ton et le niveau de d√©tail que vous souhaitez. Si n√©cessaire, divisez les t√¢ches complexes en √©tapes plus petites et encha√Ænez vos prompts.

* **Contr√¥lez la longueur de la r√©ponse** ‚Äì Si vous avez besoin d'une r√©ponse concise, sp√©cifiez la limite de mots ou de caract√®res. Par exemple : *"R√©sumez ceci en 50 mots."*

* **Fournissez du contexte** ‚Äì Les LLM ne savent pas tout. Si le mod√®le a besoin de connaissances sp√©cifiques, incluez-les dans votre prompt. Pour un contexte dynamique, utilisez une approche bas√©e sur le RAG pour injecter des informations pertinentes √† la demande.

* **Utilisez un raisonnement √©tape par √©tape** ‚Äì Si une t√¢che n√©cessite un raisonnement logique, instruisez explicitement le mod√®le : *"R√©fl√©chissez √©tape par √©tape avant de r√©pondre."* Cela am√©liore la pr√©cision.

* **S√©parez les instructions du contexte** ‚Äì Les prompts longs peuvent devenir d√©sordonn√©s. Commencez par des instructions claires, puis s√©parez les informations suppl√©mentaires.

* **Dites-lui quoi faire, pas quoi √©viter** ‚Äì Au lieu de dire : *"Ne pas expliquer la r√©ponse,"* dites : *"Ne sortez que la r√©ponse finale."* Les instructions positives fonctionnent mieux.

* **Fixez des contraintes** ‚Äì D√©finissez des limites sur le ton, la longueur ou la complexit√©. Exemple : *"√âcrivez dans un ton professionnel, en moins de 3 phrases."*

* **Attribuez un r√¥le** ‚Äì Les LLM performant mieux avec une persona d√©finie. Commencez par : *"Vous √™tes un expert en X,"* par exemple, pour guider le comportement du mod√®le.

* **Utilisez des exemples** ‚Äì Si la pr√©cision compte, montrez au mod√®le ce que vous attendez. Des techniques comme le few-shot et le chain-of-thought (CoT) aident √† am√©liorer le raisonnement complexe.

## Prompts des agents Langbase Pipe

Les agents IA ne sont pas simplement des chatbots‚Äîils raisonnent, planifient et agissent en fonction des entr√©es des utilisateurs. Contrairement aux simples requ√™tes LLM, les agents IA fonctionnent de mani√®re autonome, prenant des d√©cisions et interagissant avec des outils externes pour accomplir des t√¢ches.

Les [**Langbase Pipe Agents**](https://langbase.com/docs/pipe/quickstart) sont des agents IA serverless avec des API unifi√©es pour chaque LLM. Ils permettent aux d√©veloppeurs de d√©finir des prompts structur√©s pour contr√¥ler le comportement des agents sur diff√©rents mod√®les. Dans cette section, vous apprendrez √† structurer les prompts efficacement en cr√©ant un Pipe d'agent IA pour obtenir des r√©ponses fiables et utiles.

### Les trois prompts cl√©s dans les agents Langbase Pipe

Pour que les agents IA fonctionnent efficacement, vous avez besoin de trois types de prompts :

1. **Prompt syst√®me** : D√©finit le r√¥le, le ton et les directives du mod√®le LLM avant de traiter l'entr√©e de l'utilisateur.

2. **Prompt utilisateur** : L'entr√©e donn√©e par l'utilisateur pour demander une r√©ponse au mod√®le.

3. **Prompt assistant IA** : La r√©ponse g√©n√©r√©e par le mod√®le bas√©e sur l'entr√©e de l'utilisateur.

Pour apprendre √† utiliser ces trois prompts avec l'interface utilisateur en utilisant le studio IA Langbase, vous trouverez des instructions claires et concises dans ce [guide](https://langbase.com/docs/features/prompt). Il explique exactement o√π aller/quoi cliquer pour √©crire ces prompts.

Apprenons √† cr√©er un Pipe d'agent IA en utilisant le studio IA Langbase :

### √âtape 1 : Cr√©er un agent Pipe

Apr√®s vous √™tre connect√© √† votre compte Langbase, vous pouvez toujours aller sur [`pipe.new`](http://pipe.new) pour cr√©er un nouveau Pipe.

1. Donnez un nom √† votre Pipe. Appelons-le `Agent de support IA`.

2. Cliquez sur le bouton `[Cr√©er un Pipe]`. Et voil√†, vous avez cr√©√© votre premier Pipe.

![Cr√©ation d'un nouveau Pipe](https://raw.githubusercontent.com/LangbaseInc/docs-images/refs/heads/main/guides/build-performant-rag/create-pipe-dark.jpg align="left")

### √âtape 2 : Utiliser un mod√®le LLM

Si vous avez configur√© des cl√©s API LLM dans votre profil, le Pipe les utilisera automatiquement. Sinon, cliquez simplement sur le bouton des cl√©s API LLM ou rendez-vous dans les Param√®tres pour ajouter des cl√©s API LLM au niveau du Pipe.

Ajoutons maintenant une cl√© API de fournisseur LLM.

1. Cliquez sur le bouton des cl√©s LLM. Cela ouvrira un panneau lat√©ral.

2. S√©lectionnez les cl√©s au niveau du Pipe. Choisissez n'importe quel LLM. Par exemple, vous pouvez utiliser `OpenAI` (pour GPT) ou l'un des 250+ **mod√®les support√©s** sur Langbase.

3. Cliquez sur le bouton OpenAI `[AJOUTER UNE CL√â]`, ajoutez votre cl√© API LLM. Dans chaque modal de cl√©, vous trouverez un lien `Obtenez une nouvelle cl√© ici`, cliquez dessus pour cr√©er une nouvelle cl√© API sur le site web de n'importe quel fournisseur d'API.

![Ajouter une cl√© API LLM](https://raw.githubusercontent.com/LangbaseInc/docs-images/refs/heads/main/pipe/quickstart/updated/llm-keys-dark.jpg align="left")

### √âtape 3 : Construire votre Pipe : Configurer le mod√®le LLM

Commen√ßons √† construire notre Pipe. Retournez √† l'onglet `Pipe` et suivez ces √©tapes :

1. Cliquez sur le bouton `gpt-4o-mini` pour s√©lectionner et configurer le mod√®le LLM pour votre Pipe.

2. Par d√©faut, OpenAI `gpt-4o-mini` est s√©lectionn√©. Vous pouvez √©galement choisir n'importe quel mod√®le LLM.

3. Choisissez l'un des **pr√©r√©glages** pr√©configur√©s pour votre mod√®le.

4. Vous pouvez √©galement modifier l'un des param√®tres du mod√®le. En savoir plus avec l'ic√¥ne, √† c√¥t√© du nom du param√®tre.

![Construire votre Pipe : Configuration du mod√®le LLM](https://raw.githubusercontent.com/LangbaseInc/docs-images/refs/heads/main/pipe/quickstart/updated/model-dark.jpg align="left")

### √âtape 4 : Construire votre Pipe : Configurer les m√©tadonn√©es du Pipe

Utilisez la section Meta pour configurer le fonctionnement de votre Pipe `Agent de support IA`. Il existe plusieurs fa√ßons de le configurer.

Pour commencer, vous pouvez d√©finir le format de sortie du Pipe en JSON. Vous pouvez √©galement activer le mode mod√©ration pour filtrer le contenu inappropri√©, comme l'exige OpenAI.

Ensuite, vous pouvez activer ou d√©sactiver le mode streaming, et d√©sactiver le stockage des messages (prompt d'entr√©e et compl√©tion g√©n√©r√©e) pour les donn√©es sensibles comme les emails.

![Construire votre Pipe : Configuration des param√®tres meta du Pipe](https://raw.githubusercontent.com/LangbaseInc/docs-images/refs/heads/main/pipe/quickstart/updated/meta-dark.jpg align="left")

### √âtape 5 : Concevoir un prompt

Maintenant que vous avez configur√© votre mod√®le LLM et les m√©tadonn√©es du Pipe, il est temps de concevoir votre prompt.

**Prompt : Instructions syst√®me**

Ajoutons un message d'instruction syst√®me √† cet agent. Vous pouvez ajouter ceci : `Vous √™tes un assistant IA utile. Vous aiderez les utilisateurs avec leurs questions sur {{company}}. Assurez-vous toujours de fournir des informations pr√©cises et concises.`

![Conception de prompt : Instructions syst√®me](https://raw.githubusercontent.com/LangbaseInc/docs-images/refs/heads/main/pipe/quickstart/updated/system-prompt-dark.jpg align="left")

**Prompt : Message utilisateur**

Ajoutons maintenant un message utilisateur. Cliquez sur le bouton `UTILISATEUR` pour ajouter un nouveau message. Vous pouvez ajouter ceci : `Comment demander l'API de paiement ?`.

![Conception de prompt : Ajout d'un message utilisateur](https://raw.githubusercontent.com/LangbaseInc/docs-images/refs/heads/main/pipe/quickstart/updated/user-prompt-dark.jpg align="left")

**Prompt : Variables**

Tout texte √©crit entre doubles accolades `{{}}` devient une variable. Une section variables affichera toutes vos cl√©s et valeurs de variables.

Puisque vous avez ajout√© une variable `{{company}}`, vous pouvez la voir appara√Ætre dans les variables. Maintenant, vous √©valuez la valeur de la variable company comme `ACME`. Ce Pipe remplacera d√©sormais `{{company}}` par sa valeur dans tous les messages.

‚ú® Les variables vous permettent d'utiliser le m√™me Pipe avec diff√©rentes donn√©es.

![Conception de prompt : Ajout de variables](https://raw.githubusercontent.com/LangbaseInc/docs-images/refs/heads/main/pipe/quickstart/updated/variables-dark.jpg align="left")

**Prompt en tant que code**

Nous n'√©crivons pas de code ici, mais si vous deviez √©crire ce prompt en code, cela ressemblerait √† ceci :

1. Le prompt est un tableau `messages`. √Ä l'int√©rieur se trouvent des objets `message`.

2. Chaque objet `message` se compose g√©n√©ralement de deux propri√©t√©s :

   1. `role` soit "system", "user", ou "assistant".

   2. `content` que vous envoyez ou que vous attendez √™tre g√©n√©r√© par l'IA LLM.

```bash
// Exemple de prompt :
{
	messages: [
		{ role: 'system', content: 'Vous √™tes un assistant utile.' },
		{ role: 'user', content: 'Comment demander l\'API de paiement ?' },
		{ role: 'assistant', content: 'Bien s√ªr, voici...' }
	];
}
```

Si vous utilisez le **Langbase SDK** pour construire des agents IA Pipe serverless afin de d√©finir ces trois prompts, vous devez envoyer le contenu du prompt dans le tableau d'objets `messages` comme suit :

```bash
interface Message {
	role: 'user' | 'assistant' | 'system'| 'tool';
	content: string | null;
	name?: string;
	tool_call_id?: string;
	tool_calls?: ToolCall[];
}
```

Vous pouvez en savoir plus sur la cr√©ation d'un agent Pipe en utilisant le Langbase SDK [ici.](https://langbase.com/docs/sdk/pipe/create)

Maintenant que vous savez comment cr√©er un agent Pipe et ses prompts, discutons de quelques techniques efficaces pour concevoir les prompts de vos agents IA Pipe, qui se r√©v√®leront utiles pour une grande majorit√© de LLM.

## Comment concevoir les prompts de votre agent IA

### 1. Apprentissage few-shot

Le few-shot prompting am√©liore la capacit√© d'un agent IA √† g√©n√©rer des r√©ponses pr√©cises en lui fournissant quelques exemples avant de lui demander d'effectuer une t√¢che. Au lieu de s'appuyer uniquement sur des connaissances pr√©-entra√Æn√©es, le mod√®le apprend √† partir d'interactions √©chantillons, l'aidant √† g√©n√©raliser des motifs et √† r√©duire les erreurs.

Par exemple, dans un agent IA de support client, montrer des exemples de demandes de remboursement et de r√©ponses de d√©pannage permet au mod√®le de d√©duire comment g√©rer efficacement des requ√™tes similaires.

```bash
Vous √™tes un agent IA de support client. Utilisez les exemples ci-dessous pour comprendre comment r√©pondre.

Exemple 1 :
Client : "Je veux un remboursement pour ma commande."
IA : "Notre politique de remboursement permet les retours dans les 30 jours. Veuillez fournir votre num√©ro de commande, et je vous aiderai davantage."

Exemple 2 :
Client : "Mon produit ne fonctionne pas. Que dois-je faire ?"
IA : "Je suis d√©sol√© d'apprendre cela ! Pouvez-vous d√©crire le probl√®me ? En attendant, vous pouvez consulter notre guide de d√©pannage [lien]."

Maintenant, r√©pondez √† la requ√™te suivante :
Client : "J'ai re√ßu le mauvais article. Que dois-je faire ?"
```

### 2. Prompting augment√© par la m√©moire (bas√© sur RAG)

Le prompting augment√© par la m√©moire (bas√© sur RAG) am√©liore les r√©ponses de l'IA en r√©cup√©rant des donn√©es externes pertinentes au lieu de s'appuyer uniquement sur des connaissances pr√©-entra√Æn√©es. Cette approche est particuli√®rement utile lorsqu'il s'agit d'informations dynamiques ou sp√©cifiques √† un domaine.

En utilisant Langbase, vous pouvez cr√©er des agents de m√©moire pour cela. Les agents de m√©moire Langbase sont une API de recherche de contexte g√©r√©e pour les d√©veloppeurs. Ils constituent une solution de m√©moire √† long terme utile qui peut acqu√©rir, traiter, conserver et r√©cup√©rer ult√©rieurement des informations. Les agents de m√©moire combinent le stockage vectoriel, le RAG (Retrieval-Augmented Generation) et l'acc√®s √† Internet pour vous aider √† construire des fonctionnalit√©s et produits IA puissants.

En incorporant Langbase avec un syst√®me de g√©n√©ration augment√©e par r√©cup√©ration (RAG), la m√©moire est utilis√©e avec un agent Pipe pour r√©cup√©rer des donn√©es pertinentes pour les requ√™tes.

Le processus implique :

* Cr√©er des embeddings de requ√™te.

* R√©cup√©rer les donn√©es correspondantes de la m√©moire.

* Augmenter la requ√™te avec ces donn√©es de 3 √† 20 chunks.

* Les utiliser pour g√©n√©rer des r√©ponses pr√©cises et contextuelles.

### Prompt RAG

Lorsque une m√©moire est attach√©e √† un agent Pipe, par d√©faut un prompt RAG appara√Æt qui est aliment√© au LLM pour utiliser la m√©moire. Le prompt par d√©faut fonctionne bien dans la plupart des cas, mais vous pouvez personnaliser le prompt en fonction de votre cas d'utilisation.

![Prompt RAG](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcxZ-2ydsO9AoNFxGGLnLXalLgm6s4mr6TvJPGclnCGf9vIC1FM-nibFRUQLUfKLZMLq-HSUWF-gOkWY9JO5kdiVsCgW8ZCxS_Z5sL7bWVSVlKTq77qt8Q5d4rlqpooPlR4ARyAjQ?key=Q-k-bEK6x7APjPtDCPcYSPKJ align="left")

Vous pouvez apprendre √† construire un RAG en suivant ce guide [pas √† pas.](https://langbase.com/docs/guides/rag)

### 3. Prompting par cha√Æne de pens√©e (CoT)

Le prompting CoT aide les agents IA √† d√©composer des probl√®mes complexes en √©tapes logiques avant de r√©pondre. Au lieu de sauter aux conclusions, le mod√®le est guid√© pour raisonner √† travers le probl√®me de mani√®re syst√©matique.

Cette technique de prompting est id√©ale lorsque vous avez besoin du "comment" derri√®re la r√©ponse. Elle est particuli√®rement utile pour les t√¢ches n√©cessitant un raisonnement en plusieurs √©tapes, comme le d√©bogage de code.

Par exemple, un agent IA de codage peut analyser un probl√®me avec le prompt suivant :

```bash
Analysez le message d'erreur suivant et identifiez les causes possibles. Ensuite, d√©composez les √©tapes de d√©bogage pour corriger le probl√®me.
```

Cette approche conduit √† des r√©ponses plus pr√©cises et fiables en encourageant un raisonnement plus approfondi plut√¥t qu'en g√©n√©rant une r√©ponse h√¢tive.

### 4. Prompting bas√© sur les r√¥les

Le prompting bas√© sur les r√¥les aide les agents IA √† g√©n√©rer des r√©ponses plus pr√©cises et contextuelles en leur attribuant une identit√© sp√©cifique. Au lieu de fournir des r√©ponses g√©n√©riques, le mod√®le adopte les caract√©ristiques d'un expert du domaine, conduisant √† une meilleure pr√©cision et pertinence.

Par exemple, dans un agent IA de cybers√©curit√©, d√©finir son r√¥le comme expert en s√©curit√© garantit que ses r√©ponses privil√©gient l'√©valuation des risques et les meilleures pratiques. Un exemple de prompt pourrait √™tre :

```bash
Vous √™tes un expert en cybers√©curit√©. Identifiez les vuln√©rabilit√©s dans le code donn√© et sugg√©rez des correctifs.
```

Cette approche restreint la concentration du mod√®le LLM, l'aidant √† analyser les menaces plus efficacement plut√¥t que d'offrir des conseils larges et non structur√©s.

### 5. Prompting ReACT (Raisonnement + Action)

Cela permet aux agents IA de prendre des d√©cisions en alternant entre le raisonnement logique et les actions dans le monde r√©el. Au lieu de g√©n√©rer des r√©ponses statiques, le mod√®le interagit dynamiquement avec des outils, des API ou des bases de donn√©es pour r√©cup√©rer et traiter des informations.

Par exemple, un agent IA d'assistant personnel r√©servant des vols pourrait utiliser un prompt comme :

```bash
V√©rifiez la disponibilit√© des vols pour [destination] le [date]. Si aucun vol n'est trouv√©, sugg√©rez des dates alternatives.
```

Cette approche garantit que l'agent ne g√©n√®re pas de r√©sultats erron√©s‚Äîil r√©cup√®re des donn√©es r√©elles, les √©value et ajuste ses actions en cons√©quence, rendant les r√©ponses plus fiables et ancr√©es dans des r√©sultats r√©els. Cette technique int√®gre le raisonnement avec la prise de d√©cision en temps r√©el dans les agents. Elle est parfaite pour la r√©solution de probl√®mes dynamique et instantan√©e.

### 6. Prompts de s√©curit√©

Le studio IA Langbase dispose d'une section distincte qui vous permet de d√©finir des prompts de s√©curit√© √† l'int√©rieur d'un agent Pipe. Par exemple, ne pas r√©pondre aux questions en dehors du contexte donn√©.

L'un de ses cas d'utilisation peut √™tre de s'assurer que le LLM ne fournit aucune information sensible dans sa r√©ponse √† partir du contexte fourni.

![Prompts de s√©curit√©](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc2pQ9a1xgRcow-Pit-Ay1O9W7G49DLx7jkFHlwtNLTCOu1PAUnO55IoBgW2dc0F1A6lo2FSTOSe_Lq020YcsNjBSrIhIBn4msooMh2nw0MMAYq6LHRaw0jSNMaMNjHoixcjIwDmg?key=Q-k-bEK6x7APjPtDCPcYSPKJ align="left")

Apprenez √† d√©finir des instructions de s√©curit√© pour tout LLM √† l'int√©rieur d'un Pipe [ici.](https://langbase.com/docs/features/safety)

## Comment affiner les param√®tres de r√©ponse du LLM

Maintenant que vous connaissez les techniques pour concevoir des prompts solides pour vos agents Pipe, approfondissons cela en ajustant des param√®tres de mod√®le comme la temp√©rature, les tokens maximum, top_p, et d'autres pour affiner la mani√®re dont le mod√®le r√©pond aux requ√™tes des utilisateurs.

Voici les param√®tres LLM que vous pouvez ajuster pour construire des agents Pipe efficaces :

* **Pr√©cis** : Ajust√© pour des r√©ponses pr√©cises et exactes.

* **√âquilibr√©** : √âquilibre entre exactitude et cr√©ativit√©.

* **Cr√©atif** : Privil√©gie la cr√©ativit√© et la diversit√© dans les r√©ponses g√©n√©r√©es.

* **Personnalis√©** : Vous permet de configurer manuellement les param√®tres de r√©ponse.

* **Mode_JSON** : Garantit que le mod√®le produira toujours un JSON valide.

* **Temp√©rature** : Contr√¥le la cr√©ativit√© du LLM avec les sorties.

* **Max_tokens** : Sp√©cifie le nombre maximum de tokens qui peuvent √™tre g√©n√©r√©s dans la sortie.

* **P√©nalit√© de fr√©quence** : Emp√™che le mod√®le de r√©p√©ter un mot qui a √©t√© utilis√© trop r√©cemment/trop souvent.

* **P√©nalit√© de pr√©sence** : Emp√™che le mod√®le de r√©p√©ter un mot.

* **Top_p** : G√©n√®re des tokens jusqu'√† ce que la probabilit√© cumulative d√©passe le seuil choisi.

![Affiner les param√®tres de r√©ponse du mod√®le LLM](https://lh7-rt.googleusercontent.com/docsz/AD_4nXe2h29LWOzyHlHJ5fzPDIBpkJhHcsZYUXvtaBki-IYWEWrE0wrTGDFL-_VIcBsqqtjvb8OTr7_FHayhMlfdE5PXyJfFvqgSuSBPfh5QEU_AllCQg1MNsoYe-DZ-04g4zmxPE_PwyQ?key=Q-k-bEK6x7APjPtDCPcYSPKJ align="left")

## Conclusion

La construction d'agents IA serverless efficaces devient plus facile si vous utilisez ces techniques d'ing√©nierie des prompts. Vous pouvez l'essayer en cr√©ant votre propre agent Pipe en visitant [pipe.new](http://pipe.new).

Merci d'avoir lu !

Connectez-vous avec moi par üëã:

* En vous abonnant √† ma cha√Æne [YouTube](https://www.youtube.com/@AIwithMahamCodes). Si vous souhaitez apprendre sur l'IA et les agents.

* En vous abonnant √† ma newsletter gratuite ["The Agentic Engineer"](https://mahamcodes.substack.com/) o√π je partage toutes les derni√®res nouvelles/tendances/emplois sur l'IA et les agents, et bien plus encore.

* Suivez-moi sur [X (Twitter)](https://x.com/MahamDev).