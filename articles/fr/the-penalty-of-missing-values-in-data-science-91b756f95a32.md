---
title: La p√©nalit√© des valeurs manquantes en Data Science
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2019-04-04T15:46:44.000Z'
originalURL: https://freecodecamp.org/news/the-penalty-of-missing-values-in-data-science-91b756f95a32
coverImage: https://cdn-media-1.freecodecamp.org/images/1*an9j2v3NKxvhghxoRxF9nw.jpeg
tags:
- name: Data Science
  slug: data-science
- name: Machine Learning
  slug: machine-learning
- name: General Programming
  slug: programming
- name: Python
  slug: python
- name: 'tech '
  slug: tech
seo_title: La p√©nalit√© des valeurs manquantes en Data Science
seo_desc: 'By Tanveer Sayyed

  And using a ‚Äúsoft‚Äù method to impute the same.

  This post focuses more on a conceptual level rather than coding skills and is divided
  into two parts. Part-I describes the problems with missing values and when and why
  should we use mea...'
---

Par Tanveer Sayyed

#### Et l'utilisation d'une m√©thode "souple" pour imputer les m√™mes.

Cet article se concentre davantage sur un niveau conceptuel plut√¥t que sur des comp√©tences en codage et est divis√© en deux parties. La partie I d√©crit les probl√®mes li√©s aux valeurs manquantes et quand et pourquoi nous devrions utiliser la moyenne/m√©diane/mode. La partie II **r√©pudie** la partie I et argue pourquoi nous ne devrions utiliser aucune d'entre elles et utiliser plut√¥t la m√©thode _souple_ ‚Äî repr√©sentation al√©atoire mais proportionnelle.

![Image](https://cdn-media-1.freecodecamp.org/images/1*an9j2v3NKxvhghxoRxF9nw.jpeg)
_Photo par [Pexels](https://www.pexels.com/@rakicevic-nenad-233369?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels" rel="noopener" target="_blank" title="">Rakicevic Nenad</a> de <a href="https://www.pexels.com/photo/silhouette-photo-of-man-throw-paper-plane-1262304/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels" rel="noopener" target="_blank" title=")_

### Partie I : Pourquoi supprimons-nous les valeurs manquantes ? Quand utiliser la moyenne, la m√©diane, le mode ? Et pourquoi ?

Le probl√®me avec les donn√©es manquantes est qu'il n'existe pas de m√©thode fixe pour les traiter, et le probl√®me est universel. Les valeurs manquantes affectent nos performances et notre capacit√© pr√©dictive. Elles ont le potentiel de changer tous nos param√®tres statistiques. La mani√®re dont elles interagissent avec les valeurs aberrantes affecte √† nouveau nos statistiques. Les conclusions peuvent ainsi √™tre trompeuses.

Les diff√©rentes valeurs manquantes peuvent √™tre :

1. NaN  
2. None  
3.  
4. "Null"  
5. "missing"  
6. "not available"  
7. "NA"

Alors que les quatre derni√®res sont des valeurs de cha√Æne, pandas identifie par d√©faut NaN (aucun nombre assign√©) et None. Cependant, les deux ne sont pas identiques ; l'extrait de code ci-dessous montre pourquoi.

Le probl√®me est que si nous ne supprimons pas les NaN, nous sommes alors en double p√©ril. Premi√®rement, nous souffrons d√©j√† de la perte de donn√©es r√©elles et deuxi√®mement, si elles ne sont pas trait√©es avec soin, les NaN commencent √† "d√©vorer" nos donn√©es r√©elles et pourraient se propager dans tout l'ensemble de donn√©es au fur et √† mesure que nous avan√ßons. Instancions deux s√©ries et voyons.

Maintenant, voyons ce qui se passe lorsque nous effectuons certaines op√©rations sur ces listes.

Nous pouvons voir comment les donn√©es r√©elles (entiers 1, 2) ont √©t√© perdues lors de l'ex√©cution des op√©rations (Sortie : 21, 22). Une autre chose √† noter est les r√©sultats conflictuels dans la m√©thode Python int√©gr√©e et la m√©thode de s√©rie en raison de la pr√©sence de NaN (Sortie : 23, 24).

Maintenant, cr√©ons un data-frame qui contient toutes les valeurs manquantes √©nonc√©es ci-dessus ainsi qu'une valeur de garbage ('#$%'). Nous supprimerons les valeurs manquantes en manipulant cet ensemble de donn√©es jouet minuscule.

Le data-frame a une ligne compl√®te (i1) et une colonne compl√®te (c2) remplies uniquement de NaN. D'autres identifiants de valeurs manquantes ont √©galement √©t√© d√©lib√©r√©ment dispers√©s.

Ci-dessus, nous pouvons voir que le dernier terme dans c3 devrait √™tre "True" (pour "not available"). Pour que ce soit le cas, nous devons relire le data-frame. Cette fois, nous allons [_forcer_](https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b) pandas √† identifier "missing"/"not available"/"NA" comme des NaN.

Toutes les valeurs manquantes ont √©t√© identifi√©es, devons-nous nous en d√©barrasser en les supprimant compl√®tement ?

Il semble que nous ayons perdu toutes les valeurs ! La m√©thode _.dropna()_ supprime la ligne compl√®te (index) m√™me si une seule valeur est manquante. Par cons√©quent, _.dropna()_ a le prix de perdre des donn√©es qui peuvent √™tre pr√©cieuses.

#### Comment proc√©der ?

On pourrait pr√©sumer d'imputer toutes les valeurs manquantes avec z√©ro. Mais il y a un probl√®me fondamental avec cette approche : _la saintet√©/v√©racit√© de nos donn√©es est perdue, car dans le monde r√©el, une valeur manquante peut prendre n'importe quelle valeur. Mais nous la for√ßons √† ne prendre qu'une seule valeur rigide, c'est-√†-dire 0._

Le [_document officiel_](https://scikit-learn.org/stable/modules/impute.html) de sklearn mentionne : (l'accent est de moi)

>  infer them(missing values) from the **known** part of the data.

Alors, que devrions-nous faire maintenant ? Une meilleure option est d'utiliser la **moyenne** car elle est au moins un meilleur "repr√©sentant" d'une caract√©ristique que z√©ro. Pourquoi ? Parce que pour les caract√©ristiques continues/num√©riques, peu importe combien de fois nous ajoutons la moyenne, elle reste conserv√©e. Voici comment :

> Trois nombres ‚Äî 2, 6, 7 ‚Äî ont une moyenne = (2 + 6 + 7)/3 = 5

> En supposant que cette liste a un nombre infini de valeurs manquantes, rempla√ßons-les par la moyenne : ‚Äî 2, 6, 7, 5, 5, 5, 5‚Ä¶.. La moyenne restera 5 peu importe combien de fois nous l'ajoutons !

Mais il y a des probl√®mes avec la moyenne. Premi√®rement, elle est fortement influenc√©e par les valeurs aberrantes, moyenne(2 + 6 + 7 + **55**) = 17,5 ! Deuxi√®mement, bien qu'elle "repr√©sente" une caract√©ristique, elle est aussi la _pire_ pour refl√©ter la tendance centrale d'une donn√©e normale (voir les puces ci-dessous **b & c** [respectivement pour les donn√©es asym√©triques √† droite et √† gauche]).

![Image](https://cdn-media-1.freecodecamp.org/images/1*1xILJ73AYrAibYZrbpobbg.png)
_[commons.wikimedia](https://commons.wikimedia.org/wiki/File:Measures_of_Central_Tendency.png" rel="noopener" target="_blank" title=")_

Comme nous pouvons clairement l'observer dans les puces b & c, le mode refl√®te le mieux la tendance centrale. Le **mode** est la valeur la plus fr√©quente dans notre ensemble de donn√©es. Mais lorsqu'il s'agit de donn√©es continues, le mode peut cr√©er des _ambigu√Øt√©s_. Il peut y avoir plus d'un mode ou (rarement) aucun du tout si aucune des valeurs n'est r√©p√©t√©e. Le mode est ainsi utilis√© pour imputer les valeurs manquantes dans les colonnes qui sont **cat√©gorielles** par nature.

Apr√®s le mode, c'est la m√©diane qui refl√®te le mieux la tendance centrale. Ce qui implique que pour les donn√©es **continues**, l'utilisation de la m√©diane [_est meilleure_](https://creativemaths.net/blog/median/) que la moyenne ! La **m√©diane** est le score m√©dian des points de donn√©es lorsqu'ils sont dispos√©s dans l'ordre. Et contrairement √† la moyenne, la m√©diane n'est pas influenc√©e par les valeurs aberrantes de l'ensemble de donn√©es ‚Äî la m√©diane des nombres d√©j√† dispos√©s (2, 6, 7, **55**) est 6,5 !

> Donc, pour les donn√©es cat√©gorielles, l'utilisation du mode a plus de sens et pour les donn√©es continues, la m√©diane. Alors pourquoi utilisons-nous encore la moyenne pour les donn√©es continues ?

#### H√©ritage

Auparavant, dans un monde sans ordinateurs, il √©tait plus facile de calculer la moyenne que la m√©diane. √Ä cette √©poque, cela avait d√©finitivement du sens car r√©organiser manuellement des milliers d'entr√©es chaque fois que l'ensemble de donn√©es est mis √† jour, puis trouver la m√©diane √©tait effectivement une t√¢che fastidieuse. Mais devrions-nous continuer cet h√©ritage, alors que nous avons le pouvoir de calcul √† port√©e de main, aujourd'hui ? **Non**, cela impliquerait une sous-utilisation de notre potentiel.

Mais encore une fois, la rigidit√© reste, car nous utilisons toujours une _seule_ valeur ‚Äî moyenne/m√©diane/mode. Nous en discuterons davantage dans la section suivante. Pour l'instant, rempla√ßons les valeurs par la moyenne (dans c0), la m√©diane (dans c1) et le mode (dans c3). Avant cela, traitons la valeur de garbage '#$%' √† ('i2', 'c3').

Les valeurs respectives sont :

Nous utiliserons 3 m√©thodes diff√©rentes pour remplacer les NaN.

Il semble que nous devrons supprimer compl√®tement la colonne c2 car elle ne contient aucune donn√©e. _Notez_ qu'au d√©but, une ligne et une colonne √©taient compl√®tement remplies de NaN, mais nous n'avons pu manipuler avec succ√®s que les lignes et non les colonnes. Suppression de c2.

Nous nous sommes enfin d√©barrass√©s de toutes les valeurs manquantes !

### Partie II : Remplacement al√©atoire mais proportionnel (RBPR)

![Image](https://cdn-media-1.freecodecamp.org/images/1*QCpGBZ7v5RdL_oi7ChteSw.jpeg)
_Photo par [Pexels](https://www.pexels.com/@rakicevic-nenad-233369?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels" rel="noopener" target="_blank" title="">Rakicevic Nenad</a> de <a href="https://www.pexels.com/photo/silhouette-of-person-holding-glass-mason-jar-1274260/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels" rel="noopener" target="_blank" title=")_

Les m√©thodes ci-dessus, _je pense_, peuvent √™tre d√©crites comme des **approches d'imputation rigides**, car elles acceptent rigidement une seule valeur. Maintenant, concentrons-nous sur une approche d'imputation "souple". Souple car elle utilise des **probabilit√©s**. Ici, nous ne sommes pas _forc√©s_ de choisir une seule valeur. Nous remplacerons les NaN _al√©atoirement dans un ratio qui est "proportionnel" √† la population sans NaN_ (la proportion est calcul√©e en utilisant des probabilit√©s mais avec une touche d'al√©atoire).

Une explication avec un exemple serait meilleure. Supposons une liste ayant 15 √©l√©ments avec _un tiers de donn√©es manquantes_ :

[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, **_NaN, NaN, NaN, NaN, NaN_**] ‚Äî ‚Äî ‚Äî (_original_)

Maintenant, observez dans la liste _originale_ qu'il y a des ensembles de **4** uns, **4** deux, **2** trois, et **5** NaN. Ainsi, les uns et les deux sont en **majorit√©** tandis que les trois sont en **minorit√©**. Maintenant, commen√ßons par calculer les probabilit√©s et les valeurs attendues.

* _prob_(1 se produisant dans les NaN)   
= (nombre de 1s)/(population sans NaN)  
= 4/10  
= 2/5
* Valeur attendue/comptage de 1  
= (prob) * (nombre total de NaN)  
= (2 / 5) * (5)  
= **2**

De m√™me, la valeur attendue de prob(2 se produisant dans les NaN) est **2** et prob(3 se produisant dans les NaN) est **1** (Notez que **2+2+1=5**, est √©gal au nombre de NaN). Ainsi, notre liste ressemblera maintenant √† ceci :

[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, **_1, 1, 2, 2, 3_**] ‚Äî ‚Äî ‚Äî (_remplac√©_par_proportion_)

Le ratio des uns, des deux et des trois rempla√ßant les NaN est ainsi **2 : 2 : 1**. C'est-√†-dire que lorsque nous avons 'rien', il est **tr√®s probable** que les 'uns' et les 'deux' forment la majeure partie de celui-ci plut√¥t que les 'trois', au lieu d'une seule moyenne/mode/m√©diane rigide.

Si nous imputons simplement les NaN par la moyenne (1,8), alors notre liste ressemble √† :

[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, **_1.8, 1.8, 1.8, 1.8, 1.8_**] ‚Äî ‚Äî ‚Äî (_remplac√©_par_moyenne_)

Tra√ßons ces trois listes et tirons des **conclusions** de celles-ci :

![Image](https://cdn-media-1.freecodecamp.org/images/1*07_PPCvgCODpm6kC1AQQyQ.png)
_[Code de la bo√Æte √† moustaches (NaN-12.py)](https://gist.github.com/Vernal-Inertia/e8d95749416b2df6b8f63ee124b7b73b" rel="noopener" target="_blank" title=")_

**Premi√®rement**, la liste avec remplacement proportionnel a une bien meilleure _distribution des donn√©es_ que celle remplac√©e par la moyenne. **Deuxi√®mement**, observez comment la moyenne affecte la distribution avec '3'_(une minorit√©) :_ elle n'√©tait √† l'origine _pas_ une valeur aberrante, soudainement devenue telle dans le graphique-2 mais a retrouv√© son statut d'origine dans le graphique-3. Cela montre que la distribution du graphique-3 est moins [_biais√©e_](https://towardsdatascience.com/is-your-machine-learning-model-biased-94f9ee176b67). **Troisi√®mement**, cette approche est √©galement plus √©quitable, elle a donn√© √† '3'_(la minorit√©)_ une "chance" dans les valeurs manquantes qu'elle n'aurait _jamais_ eue autrement. La **quatri√®me** beaut√© de cette approche est que nous avons toujours r√©ussi √† conserver la moyenne !

**Cinqui√®me**, la distribution (bas√©e sur la probabilit√©) garantit, sans aucun doute, que les chances de cette m√©thode √† surajuster un mod√®le sont d√©finitivement moindres que l'imputation avec l'approche _rigide_. **Sixi√®me**, si les NaN sont remplac√©s "al√©atoirement", alors en appliquant un peu de logique, nous pouvons facilement calculer qu'il y a : 5!/(2!*2!*1!) = 30, arrangements diff√©rents (permutations) possibles :

‚Ä¶ 1, 1, 2, 2, 3],  
 ‚Ä¶ 1, 1, 2, 3, 2],  
 ‚Ä¶ 1, 1, 3, 2, 2],  
 ‚Ä¶ 1, 3, 1, 2, 2],  
 ‚Ä¶ 3, 1, 1, 2, 2] et 25 de plus !

Pour rendre ce dynamisme encore plus clair et intuitif, voyez ce gif avec 4 NaN. Chaque couleur repr√©sente une valeur NaN diff√©rente.

![Image](https://cdn-media-1.freecodecamp.org/images/1*VJ4AX9GIyXmc013vlkDFCg.gif)

Remarquez comment diff√©rents arrangements g√©n√®rent diff√©rentes _interactions_ entre les colonnes chaque fois que nous ex√©cutons le code. Par cons√©quent, nous ne g√©n√©rons pas de 'nouvelles' donn√©es ici, car nous utilisons simplement de mani√®re ing√©nieuse les donn√©es d√©j√† disponibles. Nous g√©n√©rons simplement de nouvelles interactions. _Et ces interactions fluctuantes sont la vraie **p√©nalit√© des NaN**._

#### Code :

Maintenant, codons ce concept et bornons-le. Le code pour traiter les caract√©ristiques num√©riques peut √™tre trouv√© [**_ici_**](https://gist.github.com/Vernal-Inertia/e49fc188e25d76f86df8a19874439b91) et pour les caract√©ristiques cat√©gorielles [**_ici_**](https://gist.github.com/Vernal-Inertia/0a56f6b8b5aa5b6b175522dfc188b34f). (J'√©vite d√©lib√©r√©ment d'afficher le code ici car l'accent est mis sur le concept, et cela rendrait inutilement l'article long. Si vous trouvez le code utile et que vous √™tes [algorithmiquement] _avide_ de l'optimiser davantage, je serais _ravi_ que vous le fassiez). Comment utiliser le code ?

```
random.seed = 0 np.random.seed = 0# important pour que les r√©sultats soient reproductibles
```

```
# Le df_original est exempt d'impuret√©s (par exemple, pas de '$' ou ',' dans le champ de prix   # et df_original.dtypes sont tous d√©finis.
```

```
1. df = df_original.copy()2. Appelez la fonction CountAll() donn√©e dans le code3. liste cat√©gorielle = [tous les noms de colonnes cat√©gorielles dans df]4. liste num√©rique = [tous les noms de colonnes num√©riques dans df]5. ex√©cutez une boucle for pour remplir les NaN √† travers la liste num√©rique, en utilisant la fonction Fill_NaNs_Numeric()6. ex√©cutez une boucle for pour remplir les NaN √† travers la liste cat√©gorielle, en utilisant la fonction Fill_NaNs_Catigorical()7. effectuez une division train-test et v√©rifiez la pr√©cision (ne sp√©cifiez pas le random_state)
```

```
(Apr√®s l'√©tape 7, nous avons besoin d'un peu de r√©glage de l'imputation. En veillant √† ce que les √©tapes 1-7 soient dans une seule cellule, ex√©cutez-la manuellement 15-20 fois pour avoir une id√©e de la 'plage' de pr√©cisions car elle continuera √† fluctuer en raison de l'al√©atoire. L'√©tape 7 aide √† obtenir une estimation des limites des pr√©cisions et nous aide √† r√©duire √† la "meilleure pr√©cision")
```

```
8.("sautez" cette √©tape si df est extr√™mement grand) ex√©cutez une boucle while conditionn√©e √† nouveau √† travers 1 √† 7 cette fois pour obtenir directement notre pr√©cision souhait√©e (r√©gl√©e).
```

```
(On peut vouloir √©crire et sauvegarder ce 'df' mis √† jour pour une utilisation future afin de s'√©pargner de r√©p√©ter ce processus).
```

[**_Ici_**](https://gist.github.com/Vernal-Inertia/bf2e75e23ea0a508bbebfeadb0aafabe) se trouve un exemple complet, avec toutes les √©tapes mentionn√©es, sur le c√©l√®bre ensemble de donn√©es Iris inclus dans la biblioth√®que sklearn. 20 % des valeurs de chaque colonne, y compris la cible, ont √©t√© supprim√©es al√©atoirement. Ensuite, les NaN de cet ensemble de donn√©es sont imput√©s en utilisant _cette_ approche. √Ä l'√©tape 7, il est facilement identifiable qu'apr√®s imputation, nous pouvons _ajuster_ notre _rappel_ √† au moins ‚â• 0,7 pour **"chaque" classe de la plante iris**, et c'est la condition √† l'√©tape 8. Apr√®s plusieurs ex√©cutions, quelques rapports sont les suivants :

![Image](https://cdn-media-1.freecodecamp.org/images/1*ytcUE1dbxPdPUBi6kvz_fA.png)
_Imputation souple sur l'ensemble de donn√©es Iris_

Ensuite, pour une deuxi√®me confirmation, nous tra√ßons les courbes PR apr√®s r√©glage, cette fois avec un RandomForestClassifier (n_estimators= 100). [les _classes_ sont {0 :'setosa', 1: 'versicolor', 2: 'virginica'}].

![Image](https://cdn-media-1.freecodecamp.org/images/1*7dI54GxjKeZsBJr45OPjDA.png)
_**Mesure de la qualit√© du RBPR par l'aire sous la courbe**_

Ces chiffres semblent corrects. Maintenant, tournons notre attention vers l'imputation _rigide_. _L'une_ des nombreux rapports de classification est montr√© ci-dessous : [observez les 1 (√† discuter bient√¥t) dans la _pr√©cision_ et le _rappel_ ainsi que le **d√©s√©quilibre de classe** dans le _support_]

```
               precision  recall   f1-score   support
```

```
setosa           1.00      0.52      0.68        25versicolor       0.45      1.00      0.62         9virginica        0.67      0.73      0.70        11
```

#### La loi des grands nombres

Maintenant, utilisons la _loi des grands nombres_ en utilisant DecisionTreeClassifier pour effectuer 500 it√©rations, chacune avec un ensemble diff√©rent de valeurs supprim√©es al√©atoirement, sur le m√™me ensemble de donn√©es Iris **sans r√©gler** les imputations ; c'est-√†-dire, nous sautons l'√©tape de r√©glage pour obtenir "d√©lib√©r√©ment" les scores _souples_ les **pires**. Le code est [_ici_](https://gist.github.com/Vernal-Inertia/45cfda9c4fe06243d70a6a5b66b55b7e). Les comparaisons finales en termes de scores de pr√©cision et de rappel, pour l'imputation rigide et souple, sont les suivantes :

![Image](https://cdn-media-1.freecodecamp.org/images/1*D-P5ZuIpPbckBmsVkkx46A.png)
_**RAPPELS**_

![Image](https://cdn-media-1.freecodecamp.org/images/1*K3ZYGoCcngDYgbqmt3kSFg.png)
_**PR√âCISIONS**_

La _pr√©cision_ et le _rappel_ sont pratiques principalement lorsque nous observons un d√©s√©quilibre de classe. Bien qu'initialement, nous avions une cible bien √©quilibr√©e, l'imputation **rigide** avec le mode l'a rendue d√©s√©quilibr√©e. Observez un grand nombre de _rappels_ rigides et de _pr√©cisions_ rigides ayant une valeur = 1. Ici, l'utilisation du mot "surajust√©" serait incorrecte car ce sont des scores de test et non d'entra√Ænement. La bonne fa√ßon de le dire serait : le mod√®le proph√©tique _rigide_ savait d√©j√† quoi pr√©dire, ou l'utilisation du mode a assur√© que les deux scores d√©passent.

Maintenant, observez les scores **souples**. Malgr√© l'absence de _r√©glage_ ainsi que beaucoup moins de valeurs √©tant = 1, les scores _souples_ sont _toujours_ capables de **rattraper/converger** avec les scores _rigides_ (sauf dans deux cas ‚Äî _rappel-versicolor_ et _pr√©cision-setosa_ ‚Äî pour des raisons √©videntes o√π un grand nombre de 1 proph√©tiques tirent de force la moyenne vers le haut). Observez √©galement le _rappel-souple-setosa_ (malgr√© la pr√©sence de nombreux 1 dans le _homologue rigide_), et la _pr√©cision-souple-versicolor_ augment√©e. La derni√®re chose √† noter est la r√©duction globale de la variation et de l'√©cart-type dans l'approche _souple_.

Pour r√©f√©rence, les scores f1 et les scores de pr√©cision sont : (observez √† nouveau la variation r√©duite et l'√©cart-type dans l'approche _souple_)

![Image](https://cdn-media-1.freecodecamp.org/images/1*vdMlSimy9p3L1C351cdu2A.png)
_**SCORE F1**_

![Image](https://cdn-media-1.freecodecamp.org/images/1*AVzJm2GtmGnmhgb6uFlGNQ.png)
_**SCORES DE PR√âCISION**_

> Ainsi, nous pouvons observer qu'√† long terme, m√™me **sans** r√©glage de l'imputation souple, nous avons obtenu des r√©sultats qui correspondent √† la performance de la strat√©gie d'imputation rigide. Ainsi, **apr√®s** r√©glage, nous pouvons obtenir des r√©sultats encore meilleurs.

### Conclusion

Pourquoi faisons-nous cela ? La seule raison est d'am√©liorer nos chances de [_faire face √† l'incertitude_](https://www.technologyreview.com/s/612764/giving-algorithms-a-sense-of-uncertainty-could-make-them-more-ethical/). _Nous ne nous p√©nalisons jamais pour les valeurs manquantes !_ Chaque fois que nous trouvons une valeur manquante, nous ancrons simplement notre navire au 'milieu' de la mer en supposant faussement que notre ancre a r√©ussi √† sonder la tranch√©e la plus profonde des "incertitudes". La tentative ici est de garder le navire en navigation en employant les ressources disponibles √† port√©e de main ‚Äî la vitesse et la direction du vent, la localisation des √©toiles, l'√©nergie des vagues et des mar√©es, etc. pour obtenir la meilleure 'diversification' de la prise, pour un meilleur retour.

![Image](https://cdn-media-1.freecodecamp.org/images/1*TmIQgJZka4OWKUqECJvi5Q.jpeg)
_Photo par Simon Matzinger de Pexels_

(Si vous identifiez quelque chose de faux/incorrect, veuillez r√©pondre. La critique est la bienvenue).