---
title: Comment j'ai con√ßu un compagnon IA Makaton avec Gemini Nano et l'API Gemini
subtitle: ''
author: OMOTAYO OMOYEMI
co_authors: []
series: null
date: '2025-11-07T16:33:07.595Z'
originalURL: https://freecodecamp.org/news/how-i-built-a-makaton-ai-companion-using-gemini-nano-and-the-gemini-api
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1762533154134/e2209ade-6971-464b-aeef-f05abd0a30d7.png
tags:
- name: geminiAPI
  slug: geminiapi
- name: Computer Vision
  slug: computer-vision
- name: nlp
  slug: nlp
- name: gemini-nano
  slug: gemini-nano
seo_title: Comment j'ai con√ßu un compagnon IA Makaton avec Gemini Nano et l'API Gemini
seo_desc: 'When I started my research on AI systems that could translate Makaton (a
  sign and symbol language designed to support speech and communication), I wanted
  to bridge a gap in accessibility for learners with speech or language difficulties.

  Over time, t...'
---

Lorsque j'ai commenc√© mes recherches sur les syst√®mes d'IA capables de traduire le Makaton (un langage de signes et de symboles con√ßu pour soutenir la parole et la communication), je voulais combler un manque d'accessibilit√© pour les apprenants ayant des difficult√©s d'√©locution ou de langage.

Au fil du temps, cet int√©r√™t acad√©mique a √©volu√© vers un prototype fonctionnel qui combine l'IA sur l'appareil et l'IA dans le cloud pour d√©crire des images et les traduire en significations anglaises. L'id√©e √©tait simple : je voulais construire une application web l√©g√®re qui reconnaisse les gestes ou les symboles Makaton et fournisse instantan√©ment une interpr√©tation en anglais.

Dans cet article, je vais vous expliquer comment j'ai construit mon compagnon IA Makaton, une application web d'une seule page aliment√©e par Gemini Nano (sur l'appareil) et l'API Gemini (cloud). Vous verrez comment cela fonctionne, comment j'ai r√©solu des probl√®mes courants comme le CORS et les erreurs de mod√®le d'API, et comment ce petit projet est devenu une √©tape de mon parcours vers l'IA pour l'accessibilit√©.

√Ä la fin de cet article, vous serez capable de :

* Comprendre le concept de base du Makaton et pourquoi il est important pour l'accessibilit√© et l'√©ducation inclusive.
    
* Apprendre √† combiner l'IA sur l'appareil (Gemini Nano) et l'IA bas√©e sur le cloud (API Gemini) dans un seul projet web.
    
* Construire une application web fonctionnelle aliment√©e par l'IA capable de d√©crire des images et de les mapper √† des significations anglaises pr√©d√©finies.
    
* D√©couvrir comment g√©rer les erreurs courantes telles que les probl√®mes d'endpoint de mod√®le, les cl√©s API manquantes et les restrictions CORS lors de l'utilisation d'API d'IA g√©n√©rative.
    
* Apprendre √† stocker les cl√©s API localement pour la confidentialit√© des utilisateurs √† l'aide de `localStorage`.
    
* Utiliser la synth√®se vocale du navigateur pour convertir les significations anglaises g√©n√©r√©es par l'IA en sortie vocale.
    

## Table des mati√®res

* [Outils et Stack technique](#heading-outils-et-stack-technique)
    
* [Construire l'application √©tape par √©tape](#heading-construire-l-application-etape-par-etape)
    
* [Comment r√©soudre les probl√®mes courants](#heading-comment-resoudre-les-problemes-courants)
    
* [D√©mo : Le compagnon IA Makaton en action](#heading-demo-le-compagnon-ia-makaton-en-action)
    
* [R√©flexions plus larges](#heading-reflexions-plus-larges)
    
* [Conclusion](#heading-conclusion)
    

## Outils et Stack technique

Pour construire le compagnon IA Makaton, je voulais quelque chose de l√©ger, rapide √† prototyper et facile √† ex√©cuter pour n'importe qui sans d√©pendances compliqu√©es. J'ai choisi une Stack technique web classique en mettant l'accent sur l'accessibilit√© et la transparence.

Voici ce que j'ai utilis√© :

### Frontend

* **HTML + CSS + JavaScript (Vanilla) :** Pas de Frameworks, juste du code propre et compr√©hensible que tout d√©butant peut suivre.
    
* Une seule page `index.html` g√®re l'interface de t√©l√©chargement, l'affichage des r√©sultats et la logique d'IA.
    

### Composants d'IA

* **Gemini Nano** s'ex√©cute localement dans Chrome Canary. Ce mod√®le sur l'appareil permet aux utilisateurs de g√©n√©rer du texte court sans appeler l'API cloud.
    
* **API Gemini (Cloud)** utilis√©e comme solution de repli lorsque l'IA sur l'appareil n'est pas disponible ou lorsqu'une analyse d'image est requise.
    
    * Mod√®les test√©s : `gemini-1.5-flash` et `gemini-pro-vision`.
        
    * La logique de repli garantit que l'application v√©rifie plusieurs endpoints de mod√®le si l'un d'eux renvoie une erreur 404.
        

### Stockage local

* La cl√© API Gemini est stock√©e en toute s√©curit√© dans le `localStorage` du navigateur, de sorte qu'elle ne quitte jamais l'ordinateur de l'utilisateur.
    

### API SpeechSynthesis du navigateur

* Convertit la signification anglaise traduite en audio parl√© en un clic.
    

### Logique de mapping

* Un petit dictionnaire personnalis√© (`mapping.js`) lie les descriptions g√©n√©r√©es par l'IA aux significations Makaton probables. Par exemple : `{ keywords: ["open hand", "raised hand", "wave"], meaning: "Hello / Stop" }`.
    

### Serveur local

* L'application est servie localement √† l'aide du serveur HTTP int√©gr√© de Python pour √©viter les probl√®mes de CORS :
    
    `python -m http.server 8080`
    

Ouvrez ensuite `http://localhost:8080` dans Chrome Canary.

## Construire l'application √©tape par √©tape

Plongeons maintenant dans le fonctionnement interne du compagnon IA Makaton. Ce projet suit un flux simple mais efficace : T√©l√©charger une image ‚Üí D√©crire (IA) ‚Üí Mapper √† la signification ‚Üí Parler ou Copier le r√©sultat.

Nous allons passer en revue chaque partie √©tape par √©tape.

### 1\. Configuration du dossier du projet

Vous n'avez pas besoin d'une configuration complexe. Cr√©ez simplement un nouveau dossier et ajoutez ces fichiers :

```plaintext
makaton-ai-companion/
‚îÇ
‚îú‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ styles.css
‚îú‚îÄ‚îÄ app.js
‚îî‚îÄ‚îÄ lib/
    ‚îú‚îÄ‚îÄ mapping.js
    ‚îî‚îÄ‚îÄ ai.js
```

Si vous pr√©f√©rez une version pr√™te √† l'emploi, vous pouvez tout servir √† partir d'un fichier zip (je partagerai un lien GitHub √† la fin).

### 2\. Cr√©ation de la structure HTML de base

Votre fichier `index.html` d√©finit l'interface o√π les utilisateurs t√©l√©chargent une image, cliquent sur *Describe* et visualisent les r√©sultats.

```html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Makaton AI Companion</title>
  <link rel="stylesheet" href="styles.css"/>
</head>
<body>
  <header class="app-header">
    <h1>üß© Makaton AI Companion</h1>
    <button id="btnSettings" class="btn secondary">Settings</button>
  </header>

  <main class="container">
    <section class="card">
      <h2>1) Upload an image (Makaton sign/symbol)</h2>
      <label for="file">
        Choose an image file
        <input id="file" type="file" accept="image/*" title="Select an image file"/>
      </label>
      <div id="preview" class="preview hidden"></div>
      <p id="status" class="status"></p>
      <div class="actions">
        <button id="btnDescribe" class="btn">Describe (Cloud or Nano)</button>
        <button id="btnType" class="btn ghost">Type a description instead</button>
      </div>
      <div id="typedBox" class="typed hidden">
        <textarea id="typed" rows="3" placeholder="Describe what you see..."></textarea>
        <button id="btnUseTyped" class="btn">Use this description</button>
      </div>
    </section>

    <section class="card">
      <h2>2) AI Output</h2>
      <div class="grid">
        <div>
          <h3>Image Description</h3>
          <div id="output" class="output"></div>
        </div>
        <div>
          <h3>English Meaning (Mapped)</h3>
          <div id="meaning" class="meaning"></div>
          <div class="actions">
            <button id="btnSpeak" class="btn ghost" disabled>üîä Speak</button>
            <button id="btnCopy" class="btn ghost" disabled>üìã Copy</button>
          </div>
        </div>
      </div>
    </section>
  </main>

  <dialog id="settings">
    <form method="dialog" class="settings-form">
      <h2>Settings</h2>
      <label>Gemini API key (optional)<input id="apiKey" type="password" placeholder="AIza..."/></label>
      <div class="settings-actions">
        <button id="btnSaveKey" type="submit" class="btn">Save</button>
        <button id="btnCloseSettings" type="button" class="btn secondary">Close</button>
      </div>
      <div id="apiStatus" class="api-status"></div>
    </form>
  </dialog>

  <script type="module" src="lib/mapping.js"></script>
  <script type="module" src="lib/ai.js"></script>
  <script type="module" src="app.js"></script>
</body>
</html>
```

Cette interface est intentionnellement minimale : pas de Frameworks, pas d'outils de build, juste du HTML clair.

### 3\. Mapping des descriptions aux significations Makaton

Le fichier `mapping.js` contient un dictionnaire simple bas√© sur des mots-cl√©s. Lorsque l'IA d√©crit une image (comme *"a raised open hand"*), l'application recherche des mots-cl√©s qui correspondent √† des signes Makaton connus.

```javascript
// lib/mapping.js

export const MAKATON_GLOSSES = [
  { keywords: ["open hand", "raised hand", "wave", "hand up"], meaning: "Hello / Stop" },
  { keywords: ["eat", "food", "spoon", "hand to mouth"], meaning: "Eat" },
  { keywords: ["drink", "cup", "glass", "bottle"], meaning: "Drink" },
  { keywords: ["home", "house", "roof"], meaning: "Home" },
  { keywords: ["sleep", "bed", "eyes closed"], meaning: "Sleep" },
  { keywords: ["book", "reading", "pages"], meaning: "Book / Read" },
  // Added so your current screenshot maps correctly:
  { keywords: ["help", "assist", "thumb on palm", "hand over hand", "assisting"], meaning: "Help" },
];

export function mapDescriptionToMeaning(desc) {
  if (!desc) return "";
  const d = desc.toLowerCase();
  for (const entry of MAKATON_GLOSSES) {
    if (entry.keywords.some(k => d.includes(k))) return entry.meaning;
  }
  if (d.includes("hand")) return "Gesture / Hand sign (clarify)";
  return "No direct mapping found.";
}
```

C'est simple mais suffisamment efficace pour simuler une v√©ritable traduction de symbole en langage √† des fins de d√©monstration.

### 4\. Ajout de la logique d'IA Gemini

Le fichier `ai.js` se connecte √† Gemini Nano (sur l'appareil) ou √† l'API Gemini (cloud). Si Nano n'est pas disponible, l'application se replie sur le mod√®le cloud. Et si cela √©choue, elle permet aux utilisateurs de saisir manuellement une description.

```javascript
// lib/ai.js ‚Äî dynamic model discovery (try-all version)

// --- On-device availability (Gemini Nano) ---
export async function checkAvailability() {
  const res = { nanoTextPossible: false };
  try {
    const canCreate = self.ai?.canCreateTextSession || self.ai?.languageModel?.canCreate;
    if (typeof canCreate === "function") {
      const ok = await (self.ai.canCreateTextSession?.() || self.ai.languageModel.canCreate?.());
      res.nanoTextPossible = ok === "readily" || ok === "after-download" || ok === true;
    }
  } catch {}
  return res;
}

export async function createNanoTextSession() {
  if (self.ai?.createTextSession) return await self.ai.createTextSession();
  if (self.ai?.languageModel?.create) return await self.ai.languageModel.create();
  throw new Error("Gemini Nano text session not available");
}

// --- Cloud: dynamically discover models for this key ---
async function listModels(key) {
  const url = "https://generativelanguage.googleapis.com/v1/models?key=" + encodeURIComponent(key);
  const r = await fetch(url);
  if (!r.ok) throw new Error("ListModels failed: " + (await r.text()));
  const j = await r.json();
  return (j.models || []).map(m => m.name).filter(Boolean);
}

function rankModels(names) {
  // Prefer Gemini 1.5 (multimodal), then flash variants, then anything with vision/pro.
  return names
    .filter(n => n.startsWith("models/"))              // ignore tunedModels, etc.
    .filter(n => !n.includes("experimental"))          // skip experimental
    .sort((a, b) => score(b) - score(a));

  function score(n) {
    let s = 0;
    if (n.includes("1.5")) s += 10;
    if (n.includes("flash")) s += 8;
    if (n.includes("pro-vision")) s += 7;
    if (n.includes("pro")) s += 6;
    if (n.includes("vision")) s += 5;
    if (n.includes("latest")) s += 2;
    return s;
  }
}

async function tryGenerateForModels(imageDataUrl, key, models, mimeType) {
  const base64 = imageDataUrl.split(",")[1];
  const body = {
    contents: [{
      parts: [
        { text: "Describe this image briefly in one sentence focusing on the main gesture or symbol." },
        { inline_data: { mime_type: mimeType || "image/png", data: base64 } }
      ]
    }]
  };
  let lastErr = "";
  for (const model of models) {
    const endpoint = "https://generativelanguage.googleapis.com/v1/" + model + ":generateContent?key=" + encodeURIComponent(key);
    try {
      const r = await fetch(endpoint, { method: "POST", headers: { "Content-Type": "application/json" }, body: JSON.stringify(body)});
      if (!r.ok) { lastErr = await r.text().catch(()=>String(r.status)); continue; }
      const j = await r.json();
      const text = j?.candidates?.[0]?.content?.parts?.map(p=>p.text).join(" ").trim();
      if (text) return text;
      lastErr = "Empty response from " + model;
    } catch (e) {
      lastErr = String(e?.message || e);
    }
  }
  throw new Error("All discovered models failed. Last error: " + lastErr);
}

export async function describeImageWithGemini(imageDataUrl, apiKey, mimeType = "image/png") {
  if (!apiKey) throw new Error("No API key provided");

  const models = await listModels(apiKey);
  if (!models.length) throw new Error("No models returned for this key. Ensure Generative Language API is enabled and T&Cs accepted in AI Studio.");

  const ranked = rankModels(models);
  if (!ranked.length) throw new Error("No usable model names returned (models/*).");

  return await tryGenerateForModels(imageDataUrl, apiKey, ranked, mimeType);
}

// --- Key storage (local only) ---
const KEY = "makaton_demo_gemini_key";
export function saveApiKey(k) { localStorage.setItem(KEY, k || ""); }
export function loadApiKey() { return localStorage.getItem(KEY) || ""; }
```

Remarque : Ce syst√®me de tentative est essentiel car de nombreux utilisateurs rencontrent des erreurs de mod√®le 404 en raison de l'indisponibilit√© de certaines versions de Gemini dans chaque compte.

### 5\. La logique principale (app.js)

Ce script lie tout ensemble : t√©l√©chargement de fichier, appel √† l'IA, mapping de signification et affichage des r√©sultats.

```javascript

import { mapDescriptionToMeaning } from './lib/mapping.js';
import { checkAvailability, createNanoTextSession, describeImageWithGemini, saveApiKey, loadApiKey } from './lib/ai.js';

document.addEventListener('DOMContentLoaded', () => {
  console.log('[Makaton] DOM ready');

  const $ = (s) => document.querySelector(s);

  // Elements
  const fileInput   = $('#file');
  const preview     = $('#preview');
  const meaningEl   = $('#meaning');
  const outputEl    = $('#output');
  const btnDescribe = $('#btnDescribe');
  const btnType     = $('#btnType');
  const typedBox    = $('#typedBox');
  const typed       = $('#typed');
  const btnUseTyped = $('#btnUseTyped');
  const btnSpeak    = $('#btnSpeak');
  const btnCopy     = $('#btnCopy');
  const statusEl    = $('#status');

  const settings        = $('#settings');
  const btnSettings     = $('#btnSettings');
  const btnCloseSettings= $('#btnCloseSettings');
  const btnSaveKey      = $('#btnSaveKey');
  const apiKeyInput     = $('#apiKey');
  const apiStatus       = $('#apiStatus');

  let currentImageDataUrl = null;
  let currentImageMime    = "image/png";

  // Sanity logs
  console.log('[Makaton] Elements:', {
    fileInput: !!fileInput, preview: !!preview, outputEl: !!outputEl,
    meaningEl: !!meaningEl, btnDescribe: !!btnDescribe, statusEl: !!statusEl
  });

  // Init API key
  if (apiKeyInput) apiKeyInput.value = loadApiKey() || "";

  // --- Helpers ---
  function setStatus(text) {
    if (statusEl) statusEl.textContent = text || '';
    console.log('[Makaton][Status]', text);
  }
  function clearOutputs() {
    if (outputEl) outputEl.textContent = '';
    if (meaningEl) meaningEl.textContent = '';
    if (btnSpeak) btnSpeak.disabled = true;
    if (btnCopy)  btnCopy.disabled  = true;
  }
  function setOutput(desc) {
    if (outputEl) outputEl.textContent = desc || '';
    const meaning = mapDescriptionToMeaning(desc || '');
    if (meaningEl) meaningEl.textContent = meaning;
    if (btnSpeak) btnSpeak.disabled = !meaning || meaning.includes('No direct mapping');
    if (btnCopy)  btnCopy.disabled  = !meaning;
    setStatus('Done.');
  }
  function fileToDataURL(file) {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload  = () => resolve(reader.result);
      reader.onerror = (e) => reject(e);
      reader.readAsDataURL(file);
    });
  }
  function handleFiles(files) {
    const file = files?.[0];
    if (!file) { setStatus('No file selected.'); return; }
    currentImageMime = file.type || "image/png";
    fileToDataURL(file)
      .then((dataUrl) => {
        currentImageDataUrl = dataUrl;
        if (preview) {
          preview.innerHTML = `<img alt="preview" src="${dataUrl}" />`;
          preview.classList.remove('hidden');
        }
        setStatus('Image loaded. Click "Describe" to continue.');
      })
      .catch((err) => {
        console.error('[Makaton] fileToDataURL error', err);
        setStatus('Could not read the image.');
      });
  }

  // --- File input change ---
  if (fileInput) {
    fileInput.addEventListener('change', (e) => {
      console.log('[Makaton] file input change');
      handleFiles(e.target.files);
    });
  } else {
    console.warn('[Makaton] #file input not found in DOM.');
  }

  // --- Drag & drop support on preview area ---
  if (preview) {
    preview.addEventListener('dragover', (e) => { e.preventDefault(); preview.classList.add('drag'); });
    preview.addEventListener('dragleave', () => preview.classList.remove('drag'));
    preview.addEventListener('drop', (e) => {
      e.preventDefault();
      preview.classList.remove('drag');
      console.log('[Makaton] drop');
      handleFiles(e.dataTransfer?.files);
    });
  }

  // --- Describe click ---
  if (btnDescribe) {
    btnDescribe.addEventListener('click', async () => {
      console.log('[Makaton] Describe clicked');
      if (!currentImageDataUrl) { setStatus('Please upload an image first.'); return; }
      clearOutputs();
      setStatus('Checking on-device AI availability‚Ä¶');

      const avail = await checkAvailability().catch(() => ({ nanoTextPossible: false }));
      try {
        const apiKey = loadApiKey();
        if (apiKey) {
          setStatus('Using Gemini cloud for image description‚Ä¶');
          const desc = await describeImageWithGemini(currentImageDataUrl, apiKey, currentImageMime);
          setOutput(desc);
          return;
        }
        if (avail.nanoTextPossible) {
          setStatus('No API key found. Using on-device AI (text) for best guess‚Ä¶');
          const session = await createNanoTextSession();
          const desc = await session.prompt('Given an image is uploaded by the user (not directly visible to you), infer a likely one-sentence description of a common Makaton sign or symbol a teacher might upload. Keep it generic and safe.');
          setOutput(desc);
          return;
        }
        setStatus('No AI available. Please type a brief description.');
        if (typedBox) typedBox.classList.remove('hidden');
      } catch (err) {
        console.error('[Makaton] Describe error', err);
        setStatus('Description failed: ' + (err?.message || err));
        if (typedBox) typedBox.classList.remove('hidden');
      }
    });
  } else {
    console.warn('[Makaton] Describe button not found.');
  }

  // --- Manual typing flow ---
  if (btnType) {
    btnType.addEventListener('click', () => {
      if (typedBox) typedBox.classList.remove('hidden');
      if (typed) typed.focus();
    });
  }
  if (btnUseTyped) {
    btnUseTyped.addEventListener('click', () => {
      const text = (typed?.value || '').trim();
      if (!text) { setStatus('Type a description first.'); return; }
      setOutput(text);
    });
  }

  // --- Utilities ---
  if (btnSpeak) {
    btnSpeak.addEventListener('click', () => {
      const text = meaningEl?.textContent?.trim();
      if (!text) return;
      const u = new SpeechSynthesisUtterance(text);
      speechSynthesis.cancel();
      speechSynthesis.speak(u);
    });
  }
  if (btnCopy) {
    btnCopy.addEventListener('click', async () => {
      const text = meaningEl?.textContent?.trim();
      if (!text) return;
      try {
        await navigator.clipboard.writeText(text);
        setStatus('Copied meaning to clipboard.');
      } catch {
        setStatus('Copy failed.');
      }
    });
  }

  // --- Settings modal ---
  if (btnSettings && settings) btnSettings.addEventListener('click', () => settings.showModal());
  if (btnCloseSettings && settings) btnCloseSettings.addEventListener('click', () => settings.close());
  if (btnSaveKey) {
    btnSaveKey.addEventListener('click', (e) => {
      e.preventDefault();
      const k = apiKeyInput?.value?.trim() || "";
      saveApiKey(k);
      if (apiStatus) apiStatus.textContent = k ? "API key saved locally. Try Describe again." : "Cleared API key. You can still use on-device or typed mode.";
    });
  }

  // First status
  setStatus('Ready. Upload an image to begin.');
});
```

D√©composons les sections principales du script `app.js` pour le compagnon IA Makaton, car il s'y passe beaucoup de choses :

1. **Importations et configuration initiale :**
    
    * Le script importe des fonctions de `mapping.js` et `ai.js` pour g√©rer le mapping des descriptions aux significations et les interactions avec l'IA.
        
    * Il configure des √©couteurs d'√©v√©nements pour le moment o√π le contenu du DOM est enti√®rement charg√©, garantissant que tous les √©l√©ments sont pr√™ts pour l'interaction.
        
2. **S√©lection des √©l√©ments :**
    
    * Il utilise une fonction utilitaire `$` pour s√©lectionner les √©l√©ments du DOM par leurs s√©lecteurs CSS. Cela inclut les entr√©es de fichiers, les boutons et les zones d'affichage pour les aper√ßus d'images et les r√©sultats.
        
3. **Sanity logs :**
    
    * Il enregistre la pr√©sence des √©l√©ments cl√©s dans la console √† des fins de d√©bogage, s'assurant que tous les √©l√©ments n√©cessaires sont trouv√©s dans le DOM.
        
4. **Initialisation de la cl√© API :**
    
    * Il charge toute cl√© API enregistr√©e depuis le stockage local et la d√©finit dans le champ de saisie pour le confort de l'utilisateur.
        
5. **Fonctions utilitaires :**
    
    * `setStatus` : Met √† jour le message de statut affich√© √† l'utilisateur.
        
    * `clearOutputs` : Efface les zones d'affichage des r√©sultats et des significations et d√©sactive les boutons pour parler et copier.
        
    * `setOutput` : Affiche la description g√©n√©r√©e par l'IA et la mappe √† une signification Makaton, en activant les boutons si une signification valide est trouv√©e.
        
    * `fileToDataURL` : Convertit un fichier t√©l√©charg√© en une Data URL pour l'aper√ßu et le traitement de l'image.
        
    * `handleFiles` : G√®re la s√©lection de fichiers, la mise √† jour de l'aper√ßu et la d√©finition de la Data URL de l'image actuelle.
        
6. **Gestion du changement d'entr√©e de fichier :**
    
    * Il √©coute les changements dans l'entr√©e de fichier, traite le fichier s√©lectionn√© et met √† jour la zone d'aper√ßu.
        
7. **Support du Drag & Drop :**
    
    * Il ajoute une fonctionnalit√© de glisser-d√©poser √† la zone d'aper√ßu, permettant aux utilisateurs de faire glisser des fichiers directement sur l'application pour traitement.
        
8. **Clic sur le bouton Describe :**
    
    * Il g√®re l'√©v√©nement de clic sur le bouton "Describe", v√©rifiant la pr√©sence d'une image t√©l√©charg√©e et tentant de la d√©crire √† l'aide de l'API Gemini ou de l'IA sur l'appareil.
        
    * Si aucune IA n'est disponible, il invite l'utilisateur √† saisir manuellement une description.
        
9. **Flux de saisie manuelle :**
    
    * Il permet aux utilisateurs de saisir manuellement une description si le traitement par l'IA est indisponible ou √©choue, mettant √† jour le r√©sultat avec le texte saisi.
        
10. **Utilitaires :**
    
    * `btnSpeak` : Utilise l'API SpeechSynthesis du navigateur pour lire √† haute voix la signification mapp√©e.
        
    * `btnCopy` : Copie la signification mapp√©e dans le presse-papiers pour un partage facile.
        
11. **Modal de param√®tres :**
    
    * Il g√®re le modal de param√®tres pour saisir et enregistrer la cl√© API, fournissant un retour sur le statut de la cl√©.
        
12. **Statut initial :**
    
    * Il d√©finit le message de statut initial pour guider l'utilisateur √† t√©l√©charger une image pour commencer le processus.
        

Ce script lie efficacement l'interface utilisateur, la gestion des fichiers, le traitement de l'IA et l'affichage des r√©sultats, offrant une exp√©rience fluide pour traduire les signes Makaton en significations anglaises.

#### Comment la vision et le langage collaborent ici

En travaillant sur ce projet, j'ai commenc√© √† appr√©cier comment la vision par ordinateur et la compr√©hension du langage se compl√®tent dans des syst√®mes multimodaux comme celui-ci.

* Le mod√®le de vision (Gemini ou Nano) interpr√®te *ce qu'il voit* comme les formes de mains, les gestes ou la disposition et transforme ce contexte visuel en langage descriptif.
    
* La logique de mapping du langage interpr√®te ensuite ces mots, d√©duit l'intention et trouve la correspondance s√©mantique la plus proche (par exemple, ¬´ help ¬ª, ¬´ friend ¬ª, ¬´ eat ¬ª).
    
* C'est une collaboration entre deux formes de compr√©hension (*perceptive* et *s√©mantique*) qui permettent ensemble √† l'IA de combler le foss√© entre le geste et la signification.
    

Cette prise de conscience a remodel√© ma fa√ßon de penser l'accessibilit√© : les meilleures technologies d'assistance √©mergent souvent non pas de mod√®les plus intelligents seuls, mais de l'interaction entre des modalit√©s comme voir, d√©crire et raisonner en contexte.

### 6\. Optionnel ‚Äî Parler et Copier

Pour rendre l'application plus accessible, j'ai ajout√© une sortie vocale et un bouton de copie rapide :

```javascript
btnSpeak.addEventListener('click', () => {
  const text = meaningEl.textContent.trim();
  if (text) speechSynthesis.speak(new SpeechSynthesisUtterance(text));
});

btnCopy.addEventListener('click', async () => {
  const text = meaningEl.textContent.trim();
  if (text) await navigator.clipboard.writeText(text);
});
```

Cela donne aux utilisateurs un retour √† la fois visuel et auditif, particuli√®rement utile pour les apprenants ou les √©ducateurs.

## Comment r√©soudre les probl√®mes courants

Aucun projet d'IA ou d'int√©gration web ne se d√©roule sans accroc la premi√®re fois ‚Äì et c'est normal. Voici une analyse des principaux probl√®mes auxquels j'ai √©t√© confront√© lors de la construction du compagnon IA Makaton, comment je les ai diagnostiqu√©s et comment j'ai r√©solu chacun d'eux.

Ces le√ßons aideront quiconque tente d'int√©grer les API Gemini, l'IA sur l'appareil ou des applications web locales sans backend complet.

### 1\. L'erreur ‚ÄúCORS‚Äù lors de l'ex√©cution avec `file://`

Lorsque j'ai ouvert mon `index.html` directement depuis mon explorateur de fichiers, Chrome a renvoy√© plusieurs erreurs de politique CORS :

```python
Access to script at 'file:///lib/ai.js' from origin 'null' has been blocked by CORS policy.
```

Au d√©but, cela semblait d√©routant, mais la raison est simple : les navigateurs modernes bloquent les modules JavaScript (`import/export`) lors de l'ex√©cution √† partir de chemins `file://` pour des raisons de s√©curit√©.

‚úÖ **Solution :** J'ai r√©alis√© que je devais servir les fichiers via **HTTP**, et non depuis le syst√®me de fichiers. J'ai donc lanc√© un serveur web local rapide √† l'aide de Python :

```python
python -m http.server 8080
```

Puis j'ai ouvert :

```python
http://localhost:8080/index.html
```

Cette seule √©tape a corrig√© toutes les erreurs CORS et a permis √† mes modules de se charger correctement.

### 2\. ‚ÄúModel Not Found‚Äù (404) de l'API Gemini

Le d√©fi suivant est venu de l'API Gemini. M√™me si j'avais une cl√© API valide, ma console affichait cette erreur :

```python
"models/gemini-1.5-flash" is not found for API version v1beta, or is not supported for generateContent.
```

Il s'av√®re que les endpoints d'API de Google peuvent varier l√©g√®rement en fonction de la configuration de votre projet et des autorisations de votre cl√©.

‚úÖ **Solution :** J'ai r√©√©crit mon script `lib/ai.js` pour essayer automatiquement **plusieurs endpoints de mod√®le Gemini** jusqu'√† ce qu'il en trouve un qui fonctionne. Quelque chose comme ceci :

```python
const GEMINI_IMAGE_ENDPOINTS = [
  "https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent",
  "https://generativelanguage.googleapis.com/v1/models/gemini-1.5-pro:generateContent",
  "https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash-latest:generateContent",
];
```

Et je l'ai envelopp√© dans une boucle qui s'arr√™te d√®s qu'un endpoint r√©ussit.

Plus tard, je l'ai encore am√©lior√© en listant dynamiquement les mod√®les disponibles √† l'aide de  
`https://generativelanguage.googleapis.com/v1/models?key=VOTRE_CLE` et en essayant automatiquement ceux qui supportent la g√©n√©ration d'images.

Cette approche de d√©couverte dynamique a corrig√© les erreurs 404 de mani√®re permanente.

### **3\. Empaqueter une version locale √† fichier unique**

Une fois que tout a fonctionn√©, je voulais une version que d'autres pourraient tester facilement sans installer Node.js ou ex√©cuter des outils de build.

‚úÖ **Solution :** J'ai regroup√© le projet dans un simple fichier zip contenant :

```python
index.html
app.js
lib/ai.js
lib/mapping.js
styles.css
```

De cette fa√ßon, n'importe qui peut simplement d√©zipper et ex√©cuter :

```python
python -m http.server 8080
```

et ouvrir `localhost:8080`.

Tout s'ex√©cute localement dans le navigateur, aucun code c√¥t√© serveur n'est requis. Cela le rend √©galement parfait pour les d√©mos, les salles de classe, etc.

### 4\. D√©bogage des erreurs d'importation de script dans la console

Un autre probl√®me subtil est apparu lorsque j'ai remarqu√© ce message rouge :

```python
The requested module './lib/mapping.js' does not provide an export named 'mapDescriptionToMeaning'
```

Cette ligne m'a dit exactement ce qui n'allait pas : mes noms de fonctions d'importation et d'exportation ne correspondaient pas. La correction a √©t√© simple :

```python
// app.js
import { mapDescriptionToMeaning } from './lib/mapping.js';
```

Et ensuite s'assurer que le fichier de mapping l'exportait :

```python
// mapping.js
export function mapDescriptionToMeaning(desc) { ... }
```

Apr√®s cela, toutes les pi√®ces se sont connect√©es sans probl√®me.

L'utilisation de la console du navigateur **comme tableau de bord de d√©bogage** s'est av√©r√©e √™tre l'outil le plus puissant de tous. Chaque correction a commenc√© par la lecture et le raisonnement sur ces lignes d'erreur rouges.

## D√©mo : Le compagnon IA Makaton en action

Voyons le compagnon IA Makaton en action et comprenons ce qui se passe sous le capot.

### √âtape 1 : Ex√©cuter l'application localement

Une fois que vous avez t√©l√©charg√© ou clon√© le dossier du projet, ouvrez votre terminal dans ce r√©pertoire et d√©marrez un serveur de d√©veloppement local : `python -m http.server 8080`. Ouvrez ensuite votre navigateur et visitez : `http://localhost:8080/index.html`

Vous devriez voir l'interface du compagnon IA Makaton :

![Interface principale de l'application compagnon IA Makaton](https://github.com/tayo4christ/makaton-ai-companion/blob/9cc834fa75f6dcd39866c538ed42255f9006bb51/assets/app-interface.jpg?raw=true align="left")

### √âtape 2 : Obtenir votre cl√© API Gemini

Pour activer la description d'image bas√©e sur le cloud, vous aurez besoin d'une [**cl√© API Gemini**](https://aistudio.google.com/welcome?utm_source=PMAX&utm_medium=display&utm_campaign=FY25-global-DR-pmax-1710442&utm_content=pmax&gclsrc=aw.ds&gad_source=1&gad_campaignid=21521981511&gbraid=0AAAAACn9t66nbeHlpP_VYvpWIrX7IJGEW&gclid=EAIaIQobChMIqf-KiIHbkAMV1ZFQBh0KHA8wEAAYASAAEgKLA_D_BwE) de Google AI Studio.

**Voici comment en g√©n√©rer une :**

1. Visitez : `https://aistudio.google.com/welcome`
    
2. Cliquez sur **‚ÄúCreate API key‚Äù** et liez-la √† votre projet Google Cloud (ou cr√©ez-en un nouveau).
    
3. Copiez la cl√©, elle ressemblera √† ceci : `AIzaSyA...XXXXXXXXXXXX`
    
4. Ouvrez le compagnon IA Makaton dans votre navigateur et cliquez sur le bouton **Settings** (en haut √† gauche).
    
5. Collez votre cl√© dans la zone de saisie et cliquez sur **Save**.
    

![Configuration de la cl√© API OpenAI dans l'interface de l'application](https://github.com/tayo4christ/makaton-ai-companion/blob/9cc834fa75f6dcd39866c538ed42255f9006bb51/assets/api-key-setting.jpg?raw=true align="left")

Vous verrez un message de confirmation comme celui-ci :

> *‚ÄúAPI key saved locally. Try Describe again.‚Äù*

Cela signifie que votre cl√© est stock√©e en toute s√©curit√© dans le localStorage et n'est accessible que depuis votre navigateur.

### √âtape 3 : Activer Gemini Nano pour l'IA sur l'appareil

Si vous utilisez [**Chrome Canary**,](https://www.google.com/intl/en_uk/chrome/canary/) vous pouvez ex√©cuter Gemini Nano localement sans acc√®s √† Internet. Cela permet au compagnon IA Makaton de g√©n√©rer du texte m√™me lorsque la cl√© API n'est pas d√©finie.

#### T√©l√©charger et installer Chrome Canary :

Visitez la page de t√©l√©chargement officielle de Chrome Canary et installez-le sur votre syst√®me Windows ou macOS. Chrome Canary est une version sp√©ciale de Chrome con√ßue pour les d√©veloppeurs et les premiers adoptants, offrant les derni√®res fonctionnalit√©s et mises √† jour.

#### Activer Gemini Nano :

Ouvrez Chrome Canary et tapez `chrome://flags/#prompt-api-for-gemini-nano` dans la barre d'adresse.

Localisez le flag "Prompt API for Gemini Nano" dans la liste. R√©glez ce flag sur **Enabled**. Cette action permet √† Chrome Canary de prendre en charge le mod√®le Gemini Nano pour le traitement de l'IA sur l'appareil.

Apr√®s avoir activ√© le flag, relancez Chrome Canary pour appliquer les modifications.

#### T√©l√©charger le mod√®le Gemini Nano :

Ouvrez un nouvel onglet dans Chrome Canary et entrez `chrome://components` dans la barre d'adresse.

Faites d√©filer vers le bas pour trouver le composant **‚ÄúOptimization Guide‚Äù**. Cliquez sur **Check for update**. Cette action lancera le t√©l√©chargement du mod√®le Gemini Nano, n√©cessaire pour ex√©cuter des t√¢ches d'IA localement sans connexion Internet.

#### V√©rifier l'installation :

Une fois le mod√®le Gemini Nano install√©, l'application compagnon IA Makaton le d√©tectera automatiquement. Vous devriez voir un message indiquant que l'application utilise l'IA sur l'appareil : *‚ÄúNo API key found. Using on-device AI (text) for best guess‚Ä¶‚Äù*

Cette confirmation signifie que l'application peut d√©sormais g√©n√©rer des descriptions textuelles √† l'aide du mod√®le Gemini Nano sans avoir besoin d'une cl√© API ou d'un acc√®s Internet.

En suivant ces √©tapes d√©taill√©es, vous vous assurez que le mod√®le Gemini Nano est correctement configur√© et pr√™t √† √™tre utilis√© pour le traitement de l'IA sur l'appareil dans le compagnon IA Makaton.

### √âtape 4 : T√©l√©charger un signe ou un symbole Makaton

Cliquez sur **Choose File** pour t√©l√©charger n'importe quelle image Makaton (par exemple, le signe ¬´ help ¬ª), puis appuyez sur **Describe (Cloud or Nano)**. Vous verrez imm√©diatement des logs de console confirmant que l'application fonctionne correctement et se connecte √† l'API Gemini :

![Sortie de la console montrant les logs de traduction en temps r√©el](https://github.com/tayo4christ/makaton-ai-companion/blob/9cc834fa75f6dcd39866c538ed42255f9006bb51/assets/console.jpg?raw=true align="left")

### √âtape 5 : Description par l'IA et mapping

Voici ce qui se passe ensuite :

1. L'image est lue et encod√©e en Base64.
    
2. L'API Gemini (cloud ou sur l'appareil) g√©n√®re une courte description visuelle.
    
3. La description est transmise √† la fonction `mapDescriptionToMeaning()`.
    
4. Si les mots-cl√©s correspondent √† une entr√©e dans le dictionnaire `MAKATON_GLOSSES`, l'application affiche la signification anglaise correspondante.
    
5. Enfin, les utilisateurs peuvent cliquer sur **Speak** ou **Copy** pour entendre ou r√©utiliser la traduction.
    

Exemples de r√©sultats :

**Lorsqu'aucun mapping n'est trouv√© :**  
La description de l'IA est pr√©cise mais ne correspond pas encore √† un mot-cl√© Makaton connu.

![D√©monstration incorrecte montrant le mod√®le interpr√©tant mal un signe](https://github.com/tayo4christ/makaton-ai-companion/blob/9cc834fa75f6dcd39866c538ed42255f9006bb51/assets/Incorrect-demonstration.jpg?raw=true align="left")

**Apr√®s la mise √† jour de la liste de mapping :**  
L'ajout de nouveaux mots-cl√©s comme `"help"`, `"assist"` ou `"hand over hand"` permet une traduction correcte.

![D√©monstration correcte o√π l'IA reconna√Æt avec pr√©cision le signe Makaton](https://github.com/tayo4christ/makaton-ai-companion/blob/9cc834fa75f6dcd39866c538ed42255f9006bb51/assets/correct-demonstration.jpg?raw=true align="left")

### Pourquoi c'est important

Cela d√©montre comment des outils accessibles, assist√©s par l'IA, peuvent soutenir la communication pour les personnes qui d√©pendent du Makaton. M√™me lorsqu'un geste n'est pas reconnu, le syst√®me fournit une sortie structur√©e et permet aux utilisateurs ou aux √©ducateurs d'√©largir la liste de mapping, rendant l'outil plus intelligent au fil du temps.

## R√©flexions plus larges

La construction de ce projet s'est av√©r√©e √™tre bien plus qu'un simple exercice de codage pour moi.  
C'√©tait une exp√©rience significative combinant l'accessibilit√©, le traitement du langage naturel (NLP) et la vision par ordinateur. Ces trois domaines, lorsqu'ils sont r√©unis, peuvent cr√©er un v√©ritable impact social.

En y travaillant, j'ai commenc√© √† comprendre comment la vision par ordinateur et la compr√©hension du langage se compl√®tent en pratique. Le mod√®le de vision per√ßoit le monde en identifiant des formes, des gestes et des motifs spatiaux, tandis que le mod√®le de langage interpr√®te ce que ces visuels signifient en termes humains.  
Dans ce projet, le syst√®me d'intelligence artificielle voit d'abord le signe Makaton, puis le d√©crit, et enfin le mappe √† un mot anglais qui porte une intention et une signification.

Cette interaction entre perception et s√©mantique est ce qui rend l'intelligence artificielle multimodale si puissante. Il ne s'agit pas seulement de reconna√Ætre une image ou de g√©n√©rer du texte ; il s'agit de construire des syst√®mes qui connectent la compr√©hension √† travers diff√©rentes formes d'information pour rendre la technologie plus inclusive et centr√©e sur l'humain.

Cette prise de conscience a chang√© ma fa√ßon de concevoir la technologie d'accessibilit√©. La v√©ritable innovation ne passe pas seulement par des mod√®les plus intelligents, mais par l'harmonie entre voir et comprendre, entre ce qu'un syst√®me d'intelligence artificielle observe et comment il communique cette observation pour aider les gens.

### L'accessibilit√© rencontre l'IA

Travailler sur ce projet m'a rappel√© que l'accessibilit√© n'est pas seulement une question de conformit√© ou de dispositifs d'assistance. C'est aussi une question d'inclusion. Un syst√®me d'IA simple capable de d√©crire un geste de la main ou un symbole en temps r√©el peut autonomiser les enseignants, les parents et les √©l√®ves qui communiquent √† l'aide du Makaton ou de syst√®mes similaires.

En mappant les descriptions g√©n√©r√©es par l'IA √† des phrases significatives, l'application d√©montre comment l'IA peut soutenir l'√©ducation inclusive, m√™me √† petite √©chelle. Elle comble le foss√© de communication entre les apprenants verbaux et non verbaux, ce que les syst√®mes de traduction traditionnels n√©gligent souvent.

### Int√©gration du NLP et de la vision par ordinateur

Sur le plan technique, ce projet m'a montr√© √† quel point la vision par ordinateur et la compr√©hension du langage se compl√®tent naturellement. Les mod√®les multimodaux de l'API Gemini ont pu analyser une image et produire des phrases coh√©rentes en langage naturel, ce que les anciennes API ne pouvaient pas faire sans encha√Æner plusieurs outils.

En injectant ce r√©sultat dans une fonction de mapping NLP l√©g√®re, j'ai pu simuler un traducteur de symbole en langage √† un stade tr√®s pr√©coce, ce qui est au c≈ìur de mon int√©r√™t de recherche plus large pour la traduction automatique du Makaton vers l'anglais.

### Pourquoi l'IA locale (Gemini Nano) est importante

Bien que les mod√®les cloud soient puissants, l'exp√©rimentation de Gemini Nano a r√©v√©l√© quelque chose de passionnant :  
l'IA sur l'appareil peut rendre les outils d'accessibilit√© plus rapides, plus s√ªrs et plus priv√©s.

Dans les salles de classe ou les s√©ances de th√©rapie, on ne peut souvent pas compter sur des connexions Internet stables ou partager des donn√©es sensibles sur les √©l√®ves. L'ex√©cution de l'inf√©rence localement signifie que les gestes ou les images de symboles des apprenants ne quittent jamais l'appareil, une √©tape cruciale vers une IA d'accessibilit√© pr√©servant la confidentialit√©.

Et comme Nano s'ex√©cute directement dans Chrome Canary, cela montre comment l'IA s'int√®gre au niveau du navigateur, abaissant les barri√®res pour les enseignants et les d√©veloppeurs afin de construire des solutions inclusives sans avoir besoin d'une infrastructure massive.

### Perspectives d'avenir

Ce prototype n'est qu'un point de d√©part. Les futures it√©rations pourraient int√©grer la reconnaissance des gestes directement √† partir de l'entr√©e de la cam√©ra, prendre en charge plusieurs ensembles de symboles, ou m√™me apprendre des retours des utilisateurs pour enrichir automatiquement le dictionnaire.

Plus important encore, cela renforce une conviction centrale dans mon parcours de recherche et d'enseignement :

**L'innovation en mati√®re d'accessibilit√© ne n√©cessite pas de syst√®mes massifs. Elle commence par la curiosit√©, l'empathie et quelques lignes de code cibl√©es.**

## Conclusion

La construction du compagnon IA Makaton a √©t√© l'un des projets les plus gratifiants de mon parcours dans l'IA ‚Äì pas seulement parce qu'il a fonctionn√©, mais parce qu'il a prouv√© √† quel point l'innovation peut √™tre accessible.

Avec juste un navigateur, quelques lignes de JavaScript et la bonne API, j'ai pu combiner la vision par ordinateur, la compr√©hension du langage et la conception de l'accessibilit√© dans un syst√®me fonctionnel qui traduit les symboles en significations. C'est un petit pas vers un avenir o√π n'importe qui, ind√©pendamment de ses capacit√©s d'√©locution ou de langage, peut √™tre compris gr√¢ce √† la technologie.

Le projet a √©galement renforc√© quelque chose de profond√©ment personnel pour moi en tant que chercheur et √©ducateur : l'IA pour l'accessibilit√© n'a pas besoin d'√™tre complexe, co√ªteuse ou centralis√©e. Elle peut √™tre l√©g√®re, ouverte et construite avec empathie par quiconque est pr√™t √† apprendre et √† exp√©rimenter.

### Rejoignez la conversation

Si ce projet vous inspire, j'aimerais voir vos propres exp√©riences et am√©liorations. Pouvez-vous le rendre compatible avec les gestes en direct via webcam ? Pourriez-vous l'adapter √† d'autres syst√®mes de symboles, comme le PECS ou la BSL ?

Partagez vos id√©es dans les commentaires ou taguez-moi si vous publiez votre propre version. Ensemble, nous pouvons transformer un petit prototype en un outil d'accessibilit√© pilot√© par la communaut√© et continuer √† explorer comment l'IA peut donner une voix √† plus de personnes.

Code source complet sur GitHub : [Makaton-ai-companion](https://github.com/tayo4christ/makaton-ai-companion)