---
title: Comment entra√Æner les tokenizers BPE, WordPiece et Unigram √† partir de z√©ro
  en utilisant Hugging Face
subtitle: ''
author: Harshit Tyagi
co_authors: []
series: null
date: '2021-10-18T22:27:40.000Z'
originalURL: https://freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face
coverImage: https://www.freecodecamp.org/news/content/images/2021/10/tok_hf.png
tags:
- name: algorithms
  slug: algorithms
- name: Machine Learning
  slug: machine-learning
- name: natural language processing
  slug: natural-language-processing
- name: nlp
  slug: nlp
seo_title: Comment entra√Æner les tokenizers BPE, WordPiece et Unigram √† partir de
  z√©ro en utilisant Hugging Face
seo_desc: 'If you''ve had some experience with NLP, you probably know that tokenization
  is at the helm of any NLP pipeline.

  Tokenization is often regarded as a subfield of NLP but it has its own story of
  evolution. And now it underpins many state-of-the-art NLP ...'
---

Si vous avez d√©j√† une certaine exp√©rience en NLP, vous savez probablement que la tokenization est au c≈ìur de tout pipeline NLP.

La tokenization est souvent consid√©r√©e comme un sous-domaine du NLP, mais elle a sa propre [histoire d'√©volution](https://dswharshit.substack.com/p/the-evolution-of-tokenization-byte). Et maintenant, elle sous-tend de nombreux mod√®les NLP de pointe.

Cet article traite de l'entra√Ænement des tokenizers √† partir de z√©ro en utilisant le **package de tokenizers de Hugging Face**.

Avant de passer √† la partie amusante de l'entra√Ænement et de la comparaison des diff√©rents tokenizers, je veux vous donner un bref r√©sum√© des principales diff√©rences entre les algorithmes.

La principale diff√©rence r√©side dans le **choix des paires de caract√®res** √† fusionner et **la politique de fusion** que chacun de ces algorithmes utilise pour g√©n√©rer l'ensemble final de tokens.

## Algorithme BPE ‚Äì un mod√®le bas√© sur la fr√©quence

Le Byte Pair Encoding utilise la fr√©quence des motifs de sous-mots pour les pr√©s√©lectionner en vue de la fusion.

L'inconv√©nient d'utiliser la fr√©quence comme facteur principal est que vous pouvez obtenir des encodages finaux ambigus qui pourraient ne pas √™tre utiles pour le nouveau texte d'entr√©e.

Mais il offre encore la possibilit√© de s'am√©liorer en termes de g√©n√©ration de tokens non ambigus.

## Algorithme Unigram ‚Äì un mod√®le bas√© sur la probabilit√©

Ensuite, nous avons le mod√®le Unigram qui aborde le probl√®me de fusion en calculant la probabilit√© de chaque combinaison de sous-mots plut√¥t qu'en choisissant le motif le plus fr√©quent.

Il calcule la probabilit√© de chaque token de sous-mot et le supprime ensuite en fonction d'une fonction de perte expliqu√©e dans [cet article de recherche](https://arxiv.org/pdf/1804.10959.pdf).

Sur la base d'un certain seuil de la valeur de perte, vous pouvez ensuite d√©clencher le mod√®le pour supprimer les 20-30 % inf√©rieurs des tokens de sous-mots.

Unigram est un algorithme enti√®rement probabiliste qui choisit √† la fois les paires de caract√®res et la d√©cision finale de fusion (ou non) √† chaque it√©ration en fonction de la probabilit√©.

## Algorithme WordPiece

Avec la sortie de BERT en 2018, un nouvel algorithme de tokenization de sous-mots appel√© WordPiece est apparu, qui peut √™tre consid√©r√© comme un interm√©diaire entre les algorithmes BPE et Unigram.

WordPiece est √©galement un algorithme glouton qui utilise la probabilit√© plut√¥t que la fr√©quence de comptage pour fusionner la meilleure paire √† chaque it√©ration, mais le choix des caract√®res √† apparier est bas√© sur la fr√©quence de comptage.

Ainsi, il est similaire √† BPE en termes de choix des caract√®res √† apparier et similaire √† Unigram en termes de choix de la meilleure paire √† fusionner.

Les diff√©rences algorithmiques √©tant couvertes, j'ai essay√© de mettre en ≈ìuvre chacun de ces algorithmes (pas √† partir de z√©ro) pour comparer la sortie g√©n√©r√©e par chacun d'eux.

## Comment entra√Æner les algorithmes BPE, Unigram et WordPiece

Maintenant, afin d'avoir une comparaison impartiale des sorties, je ne voulais pas utiliser des algorithmes pr√©-entra√Æn√©s, car cela introduirait la taille, la qualit√© et le contenu du jeu de donn√©es dans l'√©quation.

Une solution pourrait √™tre de coder ces algorithmes √† partir de z√©ro en utilisant les articles de recherche, puis de les tester. C'est une bonne approche pour vraiment comprendre le fonctionnement de chaque algorithme, mais vous pourriez passer des semaines √† le faire.

J'ai plut√¥t utilis√© le **package de tokenizers de Hugging Face** qui offre l'impl√©mentation de tous les tokenizers les plus utilis√©s aujourd'hui. Il m'a √©galement permis d'entra√Æner ces mod√®les √† partir de z√©ro sur mon choix de jeu de donn√©es, puis de tokenizer la cha√Æne d'entr√©e de mon choix.

### Comment entra√Æner les jeux de donn√©es

J'ai choisi deux jeux de donn√©es diff√©rents pour entra√Æner ces mod√®les. L'un est un livre gratuit de Gutenberg qui sert de petit jeu de donn√©es, et l'autre est le [wikitext-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) qui contient 516 Mo de texte.

Dans le Colab, vous pouvez d'abord t√©l√©charger les jeux de donn√©es et les d√©compresser (si n√©cessaire) :

```javascript
!wget http://www.gutenberg.org/cache/epub/16457/pg16457.txt
```

```javascript
!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip
```

```javascript
!unzip wikitext-103-raw-v1.zip
```

### Importer les mod√®les et entra√Æneurs requis

En parcourant la documentation, vous trouverez que l'API principale du package est la classe `Tokenizer`.

Vous pouvez ensuite instancier n'importe quel tokenizer avec le mod√®le de votre choix (BPE/Unigram/WordPiece).

Ici, j'ai import√© la classe principale, tous les mod√®les que je voulais tester, et leurs entra√Æneurs, car je veux entra√Æner ces mod√®les √† partir de z√©ro.

```javascript
## importation du tokenizer et de l'entra√Æneur de sous-mots BPE
from tokenizers import Tokenizer
from tokenizers.models import BPE, Unigram, WordLevel, WordPiece
from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \
                                WordPieceTrainer, UnigramTrainer

## un pr√©-tokenizer pour segmenter le texte en mots
from tokenizers.pre_tokenizers import Whitespace
```

### Comment automatiser l'entra√Ænement et la tokenization

Puisque je dois effectuer des processus quelque peu similaires pour trois mod√®les diff√©rents, j'ai divis√© les processus en 3 fonctions. Je n'aurai besoin d'appeler ces fonctions pour chaque mod√®le et mon travail sera termin√©.

Alors, quelles sont ces fonctions ?

#### √âtape 1 - Pr√©parer le tokenizer

La pr√©paration du tokenizer n√©cessite d'instancier la classe Tokenizer avec un mod√®le de notre choix. Mais puisque nous avons quatre mod√®les (j'ai √©galement ajout√© un algorithme simple de niveau mot) √† tester, nous allons √©crire des cas if/else pour instancier le tokenizer avec le bon mod√®le.

Pour entra√Æner le tokenizer instanci√© sur les petits et grands jeux de donn√©es, nous devrons √©galement instancier un entra√Æneur, dans notre cas, ce seront [`BpeTrainer`](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.trainers.BpeTrainer)`, WordLevelTrainer, WordPieceTrainer, et UnigramTrainer.`

L'instanciation et l'entra√Ænement n√©cessiteront de sp√©cifier certains tokens sp√©ciaux. Ce sont des tokens pour les mots inconnus et d'autres tokens sp√©ciaux que nous devrons utiliser plus tard pour ajouter √† notre vocabulaire.

Vous pouvez √©galement sp√©cifier d'autres arguments d'entra√Ænement, comme la taille du vocabulaire ou la fr√©quence minimale ici.

```javascript
unk_token = "<UNK>"  # token pour les mots inconnus
spl_tokens = ["<UNK>", "<SEP>", "<MASK>", "<CLS>"]  # tokens sp√©ciaux

def prepare_tokenizer_trainer(alg):
    """
    Pr√©pare le tokenizer et l'entra√Æneur avec des tokens inconnus et sp√©ciaux.
    """
    if alg == 'BPE':
        tokenizer = Tokenizer(BPE(unk_token = unk_token))
        trainer = BpeTrainer(special_tokens = spl_tokens)
    elif alg == 'UNI':
        tokenizer = Tokenizer(Unigram())
        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens)
    elif alg == 'WPC':
        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))
        trainer = WordPieceTrainer(special_tokens = spl_tokens)
    else:
        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))
        trainer = WordLevelTrainer(special_tokens = spl_tokens)
    
    tokenizer.pre_tokenizer = Whitespace()
    return tokenizer, trainer
```

Nous aurons √©galement besoin d'ajouter un pr√©-tokenizer pour diviser notre entr√©e en mots, car sans pr√©-tokenizer, nous pourrions obtenir des tokens qui chevauchent plusieurs mots : par exemple, nous pourrions obtenir un token `"there is"` puisque ces deux mots apparaissent souvent c√¥te √† c√¥te.

> *L'utilisation d'un pr√©-tokenizer garantira qu'aucun token n'est plus grand qu'un mot retourn√© par le pr√©-tokenizer.*

Cette fonction retournera le tokenizer et son objet entra√Æneur que nous pouvons utiliser pour entra√Æner le mod√®le sur un jeu de donn√©es.

Ici, nous utilisons le m√™me pr√©-tokenizer (`Whitespace`) pour tous les mod√®les. Vous pouvez choisir de le tester avec [d'autres](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers).

#### √âtape 2 - Entra√Æner le tokenizer

Apr√®s avoir pr√©par√© les tokenizers et les entra√Æneurs, nous pouvons commencer le processus d'entra√Ænement.

Voici une fonction qui prendra les fichiers sur lesquels nous avons l'intention d'entra√Æner notre tokenizer ainsi que l'identifiant de l'algorithme.

* `'WLV'` - Algorithme de niveau mot

* `'WPC'` - Algorithme WordPiece

* `'BPE'` - Byte Pair Encoding

* `'UNI'` - Unigram

```javascript
def train_tokenizer(files, alg='WLV'):
    """
    Prend les fichiers et entra√Æne le tokenizer.
    """
    tokenizer, trainer = prepare_tokenizer_trainer(alg)
    tokenizer.train(files, trainer) # entra√Ænement du tokenizer
    tokenizer.save("./tokenizer-trained.json")
    tokenizer = Tokenizer.from_file("./tokenizer-trained.json")
    return tokenizer
```

C'est la fonction principale que nous devrons appeler pour entra√Æner le tokenizer. Elle pr√©parera d'abord le tokenizer et l'entra√Æneur, puis commencera √† entra√Æner les tokenizers avec les fichiers fournis.

Apr√®s l'entra√Ænement, elle sauvegarde le mod√®le dans un fichier JSON, le charge √† partir du fichier et retourne le tokenizer entra√Æn√© pour commencer √† encoder la nouvelle entr√©e.

#### √âtape 3 - Tokenizer la cha√Æne d'entr√©e

La derni√®re √©tape consiste √† commencer √† encoder les nouvelles cha√Ænes d'entr√©e et √† comparer les tokens g√©n√©r√©s par chaque algorithme.

Ici, nous allons √©crire une boucle for imbriqu√©e pour entra√Æner chaque mod√®le d'abord sur le jeu de donn√©es plus petit, puis sur le jeu de donn√©es plus grand, et tokenizer la cha√Æne d'entr√©e √©galement.

**Cha√Æne d'entr√©e -** "This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!üòç"

```javascript
small_file = ['pg16457.txt']
large_files = [f"./wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]

for files in [small_file, large_files]:
    print(f"========Using vocabulary from {files}=======")
    for alg in ['WLV', 'BPE', 'UNI', 'WPC']:
        trained_tokenizer = train_tokenizer(files, alg)
        input_string = "This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!üòç"
        output = tokenize(input_string, trained_tokenizer)
        tokens_dict[alg] = output.tokens
        print("----", alg, "----")
        print(output.tokens, "->", len(output.tokens))
```

**Et voici la sortie :**

![Image](https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F43eb1a88-36a1-4343-be1e-ac65843e3837_1306x430.png align="left")

## Analyse de la sortie :

En regardant la sortie, vous verrez la diff√©rence dans la mani√®re dont les tokens ont √©t√© g√©n√©r√©s, ce qui a conduit √† un nombre diff√©rent de tokens g√©n√©r√©s.

* Un simple **algorithme de niveau mot** a cr√©√© 35 tokens, peu importe le jeu de donn√©es sur lequel il a √©t√© entra√Æn√©.

* L'algorithme **BPE** a cr√©√© 55 tokens lorsqu'il a √©t√© entra√Æn√© sur un jeu de donn√©es plus petit et 47 lorsqu'il a √©t√© entra√Æn√© sur un jeu de donn√©es plus grand. Cela montre qu'il a √©t√© capable de fusionner plus de paires de caract√®res lorsqu'il a √©t√© entra√Æn√© sur un jeu de donn√©es plus grand.

* Le **mod√®le Unigram** a cr√©√© un nombre similaire (68 et 67) de tokens avec les deux jeux de donn√©es. Mais vous pouvez voir la diff√©rence dans les tokens g√©n√©r√©s :

![Image](https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdf3d128-641c-4680-9b43-0e04a505d67c_428x43.png align="left")

Avec un jeu de donn√©es plus grand, la fusion s'est rapproch√©e de la g√©n√©ration de tokens mieux adapt√©s pour encoder les mots de la langue anglaise du monde r√©el que nous utilisons souvent.

![Image](https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feb49063b-8896-496e-acec-0dea60d6ea37_260x40.png align="left")

**WordPiece** a cr√©√© 52 tokens lorsqu'il a √©t√© entra√Æn√© sur un jeu de donn√©es plus petit et 48 lorsqu'il a √©t√© entra√Æn√© sur un jeu de donn√©es plus grand. Les tokens g√©n√©r√©s ont un double ## pour indiquer l'utilisation d'un token comme pr√©fixe/suffixe.

![Image](https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5225119-6158-45e7-83b1-26bf587791f3_391x45.png align="left")

Les trois algorithmes ont g√©n√©r√© des tokens de sous-mots meilleurs et moins bons lorsqu'ils ont √©t√© entra√Æn√©s sur un jeu de donn√©es plus grand.

## Comment comparer les tokens

Pour comparer les tokens, j'ai stock√© la sortie de chaque algorithme dans un dictionnaire et je vais le transformer en un dataframe pour mieux visualiser les diff√©rences entre les tokens.

Puisque le nombre de tokens g√©n√©r√©s par chaque mod√®le est diff√©rent, j'ai ajout√© un token pour rendre les donn√©es rectangulaires et les adapter √† un dataframe.

est essentiellement nan dans le dataframe.

```javascript
import pandas as pd

max_len = max(len(tokens_dict['UNI']), len(tokens_dict['WPC']), len(tokens_dict['BPE']))
diff_bpe = max_len - len(tokens_dict['BPE'])
diff_wpc = max_len - len(tokens_dict['WPC'])

tokens_dict['BPE'] = tokens_dict['BPE'] + ['<PAD>']*diff_bpe
tokens_dict['WPC'] = tokens_dict['WPC'] + ['<PAD>']*diff_wpc

del tokens_dict['WLV']

df = pd.DataFrame(tokens_dict)
df.head(10)
```

**Voici la sortie :**

![Image](https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F856ab4bc-7343-4114-9867-27e64a71d21a_306x474.png align="left")

Vous pouvez √©galement regarder la diff√©rence entre les tokens en utilisant des ensembles :

![Image](https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7be68b1a-d979-4688-94e9-b19219f2259d_370x692.png align="left")

![Image](https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8942a6e1-d1bc-4a6e-bbec-3473a87ef9ca_370x626.png align="left")

Pour consulter le code, rendez-vous sur ce [notebook Colab](https://colab.research.google.com/drive/10gwzRY55JqzgeEQOX6nwFs6bQ84-mB9f?usp=sharing).

## R√©flexions finales et prochaines √©tapes

Sur la base des types de tokens g√©n√©r√©s, WPC semble g√©n√©rer des tokens de sous-mots plus couramment trouv√©s dans la langue anglaise ‚Äì mais ne me tenez pas √† cette observation.

Ces algorithmes sont l√©g√®rement diff√©rents les uns des autres et font un travail quelque peu similaire de d√©veloppement d'un mod√®le NLP d√©cent. Mais une grande partie de la performance d√©pend de l'utilisation de votre mod√®le de langage, de la taille du vocabulaire, de la vitesse et d'autres facteurs.

Cela conclut notre examen des algorithmes de tokenization. La prochaine √©tape pour approfondir ce sujet est de comprendre ce que sont les embeddings, comment la tokenization joue un r√¥le vital dans la cr√©ation de ces embeddings et comment ils affectent les performances d'un mod√®le.

Une avanc√©e suppl√©mentaire de ces algorithmes est l'[algorithme SentencePiece](https://arxiv.org/pdf/1808.06226.pdf), qui est une approche globale du probl√®me de tokenization. Mais une grande partie de ce probl√®me est att√©nu√©e par HuggingFace, et encore mieux ‚Äì ils ont tous les algorithmes impl√©ment√©s dans un seul d√©p√¥t GitHub.

### R√©f√©rences et notes

Si vous avez des questions sur mon analyse ou sur mon travail dans cet article, je vous encourage vivement √† consulter ces ressources pour une compr√©hension pr√©cise du fonctionnement de chaque algorithme :

1. [Subword regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/pdf/1804.10959.pdf) par Taku Kudo

2. [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf) - Article de recherche qui discute des diff√©rentes techniques de segmentation bas√©es sur l'algorithme de compression BPE.

3. [Package de tokenizers de Hugging Face](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html).

### Me contacter

Si vous cherchez √† vous lancer dans le domaine de la science des donn√©es ou du ML, consultez mon cours sur les [**Fondamentaux de la science des donn√©es et du ML**](https://www.wiplane.com/p/foundations-for-data-science-ml).

Si vous souhaitez voir plus de contenu de ce type et que vous n'√™tes pas abonn√©, envisagez de vous abonner √† [ma newsletter](https://dswharshit.substack.com/).

Vous avez quelque chose √† ajouter ou √† sugg√©rer, vous pouvez me contacter via :

* [YouTube](https://www.youtube.com/channel/UCH-xwLTKQaABNs2QmGxK2bQ)

* [Twitter](https://twitter.com/dswharshit)

* [LinkedIn](https://www.linkedin.com/in/tyagiharshit/)