---
title: Qu'est-ce que les R√©seaux de Neurones Graphiques ? Comment les GNN fonctionnent,
  expliqu√© avec des exemples
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2022-02-01T16:50:35.000Z'
originalURL: https://freecodecamp.org/news/graph-neural-networks-explained-with-examples
coverImage: https://www.freecodecamp.org/news/content/images/2022/01/download-1.png
tags:
- name: Machine Learning
  slug: machine-learning
- name: MathJax
  slug: mathjax
- name: neural networks
  slug: neural-networks
seo_title: Qu'est-ce que les R√©seaux de Neurones Graphiques ? Comment les GNN fonctionnent,
  expliqu√© avec des exemples
seo_desc: 'By Rishit Dagli

  Graph Neural Networks are getting more and more popular and are being used extensively
  in a wide variety of projects.

  In this article, I help you get started and understand how graph neural networks
  work while also trying to address t...'
---

Par Rishit Dagli

Les R√©seaux de Neurones Graphiques deviennent de plus en plus populaires et sont utilis√©s de mani√®re extensive dans une grande vari√©t√© de projets.

Dans cet article, je vous aide √† commencer et √† comprendre comment les r√©seaux de neurones graphiques fonctionnent tout en essayant de r√©pondre √† la question "pourquoi" √† chaque √©tape. 

Enfin, nous examinerons √©galement la mise en ≈ìuvre de certaines des m√©thodes dont nous parlons dans cet article en code.

Et ne vous inqui√©tez pas ‚Äì vous n'aurez pas besoin de conna√Ætre beaucoup de math√©matiques pour comprendre ces concepts et apprendre √† les appliquer.

## Qu'est-ce qu'un graphe ?

Pour faire simple, un graphe est une collection de n≈ìuds et des ar√™tes entre les n≈ìuds. Dans le diagramme ci-dessous, les cercles blancs repr√©sentent les n≈ìuds, et ils sont connect√©s par des ar√™tes, les lignes de couleur rouge. 

Vous pourriez continuer √† ajouter des n≈ìuds et des ar√™tes au graphe. Vous pourriez √©galement ajouter des directions aux ar√™tes, ce qui en ferait un graphe dirig√©.

![Image](https://www.freecodecamp.org/news/content/images/2022/01/image-89.png)
_Une repr√©sentation simple d'un graphe_

Quelque chose de tr√®s pratique est la matrice d'adjacence, qui est une fa√ßon d'exprimer le graphe. Les valeurs de cette matrice \(A_{ij}\) sont d√©finies comme suit :

$$A_{ij} = \left\{\begin{array}{ c l }1 & \quad \textrm{si une ar√™te existe } j \rightarrow i \\  0  & \quad \textrm{si aucune ar√™te n'existe} \end{array} \right.$$

Une autre fa√ßon de repr√©senter la matrice d'adjacence consiste simplement √† inverser la direction, donc dans la m√™me √©quation \(A_{ij}\) sera 1 s'il y a une ar√™te \(i \rightarrow j\) √† la place. 

La derni√®re repr√©sentation est en fait ce que j'ai √©tudi√© √† l'√©cole. Mais souvent dans les articles de Machine Learning, vous trouverez la premi√®re notation utilis√©e ‚Äì donc pour cet article, nous allons nous en tenir √† la premi√®re repr√©sentation.

Il y a beaucoup de choses int√©ressantes que vous pourriez remarquer √† partir de la matrice d'adjacence. Tout d'abord, vous pourriez remarquer que si le graphe est non dirig√©, vous obtenez essentiellement une matrice sym√©trique et des propri√©t√©s plus int√©ressantes, surtout avec les valeurs propres de cette matrice. 

Une telle interpr√©tation qui serait utile dans ce contexte est de prendre les puissances de la matrice \((A^n)_{ij}\) qui nous donne le nombre de (directed ou undirected) walks de longueur \(n\) entre les n≈ìuds \(i\) et \(j\).

## Pourquoi travailler avec des donn√©es sous forme de graphes ?

Les graphes sont utilis√©s dans toutes sortes de sc√©narios courants et ont de nombreuses applications possibles. 

Probablement l'application la plus courante de la repr√©sentation des donn√©es par des graphes est l'utilisation de graphes mol√©culaires pour repr√©senter des structures chimiques. Ceux-ci ont aid√© √† pr√©dire les longueurs de liaison, les charges et de nouvelles mol√©cules.

Avec les graphes mol√©culaires, vous pouvez utiliser le Machine Learning pour pr√©dire si une mol√©cule est un m√©dicament puissant. 

Par exemple, vous pourriez entra√Æner un r√©seau de neurones graphiques pour pr√©dire si une mol√©cule inhibe certaines bact√©ries et l'entra√Æner sur une vari√©t√© de compos√©s dont vous connaissez les r√©sultats.

Ensuite, vous pourriez essentiellement appliquer votre mod√®le √† n'importe quelle mol√©cule et d√©couvrir qu'une mol√©cule pr√©c√©demment n√©glig√©e fonctionnerait en fait comme un excellent antibiotique. C'est ainsi que [Stokes et al.](https://www.sciencedirect.com/science/article/pii/S0092867420301021) dans leur article (2020) ont pr√©dit un nouvel antibiotique appel√© Halicin.

Un autre article int√©ressant de DeepMind ([ETA Prediction with Graph Neural Networks in Google Maps](https://arxiv.org/abs/2108.11482), 2021) a mod√©lis√© les cartes de transport sous forme de graphes et a ex√©cut√© un r√©seau de neurones graphiques pour am√©liorer la pr√©cision des ETAs jusqu'√† 50% dans Google Maps. 

Dans cet article, ils partitionnent les itin√©raires de voyage en super segments qui mod√©lisent une partie de l'itin√©raire. Cela leur a donn√© une structure de graphe sur laquelle ils ex√©cutent un r√©seau de neurones graphiques.

Il y a eu d'autres articles int√©ressants qui repr√©sentent des donn√©es naturellement pr√©sentes sous forme de graphes (r√©seaux sociaux, circuits √©lectriques, diagrammes de Feynman et plus) qui ont fait des d√©couvertes significatives √©galement. 

Et si vous y r√©fl√©chissez, un r√©seau de neurones standard peut √©galement √™tre repr√©sent√© sous forme de graphe üß†.

## Que pouvons-nous faire avec les R√©seaux de Neurones Graphiques ?

Commen√ßons d'abord par ce que nous pourrions vouloir faire avec notre r√©seau de neurones graphiques avant de comprendre comment nous pourrions le faire. 

Un type de sortie que nous pourrions vouloir de notre r√©seau de neurones graphiques est au niveau du graphe entier, pour avoir un seul vecteur de sortie. Vous pourriez relier ce type de sortie √† la pr√©diction de l'ETA ou √† la pr√©diction de l'√©nergie de liaison √† partir d'une structure mol√©culaire des exemples dont nous avons parl√©.

Un autre type de sortie que vous pourriez vouloir est les pr√©dictions au niveau des n≈ìuds ou des ar√™tes et obtenir un vecteur pour chaque n≈ìud ou ar√™te. Vous pourriez relier cela √† un exemple o√π vous devez _classer_ chaque n≈ìud dans la pr√©diction ou probablement pr√©dire l'angle de liaison pour toutes les liaisons donn√©es la structure mol√©culaire.

Vous pourriez √©galement √™tre int√©ress√© √† r√©pondre √† la question "O√π devrais-je placer une nouvelle ar√™te ou un nouveau n≈ìud" ou pr√©dire o√π une ar√™te ou un n≈ìud pourrait appara√Ætre. Nous pourrions non seulement obtenir cette pr√©diction √† partir du graphe, mais nous pourrions √©galement transformer d'autres donn√©es en un graphe.

![Image](https://www.freecodecamp.org/news/content/images/2022/01/image-90.png)
_D√©finir ce que nous voulons que notre GNN fasse_

Comme vous l'avez peut-√™tre devin√© avec le r√©seau de neurones graphiques, nous voulons d'abord g√©n√©rer un graphe de sortie ou des latents √† partir desquels nous pourrions ensuite travailler sur cette grande vari√©t√© de t√¢ches standard. 

Donc, essentiellement, ce que nous devons faire _√† partir du graphe latent_ (caract√©ristiques pour chaque n≈ìud repr√©sent√©es comme \(\vec{h_i}\)) pour les pr√©dictions au niveau du graphe est :

* d'abord trouver un moyen d'agr√©ger tous les vecteurs (comme simplement les additionner), et 
* puis cr√©er une fonction pour obtenir les pr√©dictions :

$$\vec{Z_G} = f(\sum_i \vec{h_i})$$

Et maintenant, il est assez simple de montrer √† un niveau √©lev√© ce que nous devons faire √† partir des latents pour obtenir nos sorties. 

Pour les sorties au niveau des n≈ìuds, nous aurions simplement un vecteur de n≈ìud pass√© dans notre fonction et obtenir les pr√©dictions pour ce n≈ìud :

$$\vec{Z_i} = f(\vec{h_i})$$

## Le probl√®me des entr√©es de taille variable

Maintenant que nous savons ce que nous pouvons faire avec les r√©seaux de neurones graphiques et pourquoi vous pourriez vouloir repr√©senter vos donn√©es sous forme de graphes, voyons comment nous pourrions proc√©der √† l'entra√Ænement sur des donn√©es graphiques.

Mais d'abord, nous avons un probl√®me entre les mains : les graphes sont essentiellement des entr√©es de taille variable. Dans un r√©seau de neurones standard, comme le montre la figure ci-dessous, la couche d'entr√©e (repr√©sent√©e dans la figure par \(x_i\)) a un nombre fixe de neurones. Dans ce r√©seau, vous ne pouvez pas soudainement appliquer le r√©seau √† une entr√©e de taille variable.

![Image](https://www.freecodecamp.org/news/content/images/2022/01/image-99.png)
_Pourquoi le r√©seau de neurones standard ne fonctionnera pas ?_

Mais si vous vous souvenez, vous pouvez appliquer des r√©seaux de neurones convolutionnels sur des entr√©es de taille variable. 

Mettons cela en termes d'un exemple : vous avez une convolution avec le nombre de filtres \(K=5\), l'√©tendue spatiale \(F=2\), le pas \(S=4\), et aucun remplissage z√©ro \(P=0\). Vous pouvez passer des entr√©es \((256 \times 256 \times 3)\) et obtenir des sorties \((64 \times 64 \times 5)\) (\(\left \lfloor{\frac{256-2+0}{4}+1}\right \rfloor\)) et vous pouvez √©galement passer des entr√©es \((96 \times 96 \times 6)\) et obtenir des sorties \((24 \times 24 \times 5)\) et ainsi de suite ‚Äì c'est essentiellement ind√©pendant de la taille. 

Cela nous fait nous demander si nous pouvons tirer quelque inspiration des r√©seaux de neurones convolutionnels.

Une autre fa√ßon vraiment int√©ressante de r√©soudre le probl√®me des tailles d'entr√©e variables qui s'inspire de la physique vient de l'article [Learning to Simulate Complex Physics with Graph Networks](https://arxiv.org/abs/2002.09405) de DeeepMind (2020). 

Commen√ßons par prendre quelques particules \(i\) et chacune de ces particules a une certaine localisation \(\vec{r_i}\) et une certaine vitesse \(\vec{v_i}\). Supposons que ces particules ont des ressorts entre elles pour nous aider √† comprendre les interactions.

Maintenant, ce syst√®me est, bien s√ªr, un graphe : vous pouvez prendre les particules comme des n≈ìuds et les ressorts comme des ar√™tes. Si vous vous souvenez de la physique simple du lyc√©e, \(force = masse \cdot acc√©l√©ration\) ‚Äì et, bien, quelle est une autre fa√ßon dans ce syst√®me de d√©signer la force totale agissant sur la particule ? C'est la somme des forces agissant sur toutes les particules voisines. 

Vous pouvez maintenant √©crire (\(e_{ij}\) repr√©sente les propri√©t√©s de l'ar√™te ou du ressort entre i et j) :

$$m\frac{\mathrm{d} \vec{v_i}}{\mathrm{d}t} = \sum_{j \in \textrm{ voisins de } i } \vec{F}(\vec{r_i}, \vec{r_j}, e_{ij})$$

Quelque chose que je voudrais attirer votre attention ici est que cette loi de force est toujours la m√™me. Peut-√™tre y a-t-il des diff√©rences dans les propri√©t√©s du ressort ou de l'ar√™te, mais vous pouvez toujours appliquer la m√™me loi. Vous pouvez avoir diff√©rents nombres de n≈ìuds et d'ar√™tes et vous pouvez toujours appliquer la m√™me √©quation de mouvement.

![Image](https://www.freecodecamp.org/news/content/images/2022/01/download-2.png)
_Visualisation des solutions pr√©sent√©es pour les entr√©es de taille variable_

Si vous regardez de pr√®s, les intuitions que nous avons discut√©es pour contourner le probl√®me des entr√©es fixes ont un aspect de similarit√© entre elles : il est assez clair en √©crivant que la deuxi√®me approche prend en compte les n≈ìuds et les ar√™tes voisins et cr√©e une fonction (ici la force) de ceux-ci. Je voulais souligner que la fa√ßon dont les r√©seaux de neurones convolutionnels fonctionnent n'est pas tr√®s diff√©rente.

## Comment apprendre √† partir des donn√©es dans un graphe

Maintenant que nous avons discut√© de ce qui pourrait nous inspirer pour cr√©er un r√©seau de neurones graphiques, essayons maintenant d'en construire un. Ici, nous verrons comment nous pouvons apprendre √† partir des donn√©es r√©sidant dans un graphe. 

Nous commencerons par parler du "**Neural Message Passing**" qui est _analogue_ aux filtres dans un r√©seau de neurones convolutionnel ou √† la force dont nous avons parl√© dans la section pr√©c√©dente.

Donc, supposons que nous avons un graphe avec 3 n≈ìuds (dirig√©s ou non dirig√©s). Comme vous l'avez peut-√™tre devin√©, nous avons une valeur correspondante pour chaque n≈ìud \(x_1\), \(x_2\) et \(x_3\). 

Tout comme n'importe quel r√©seau de neurones, notre objectif est de trouver un algorithme pour mettre √† jour ces valeurs de n≈ìuds, ce qui est analogue √† une couche dans le r√©seau de neurones graphiques. Et puis vous pouvez bien s√ªr continuer √† ajouter de telles couches.

Alors, comment faites-vous ces mises √† jour ? Une id√©e serait d'utiliser les ar√™tes dans notre graphe. Aux fins de cet article, supposons que parmi les 3 n≈ìuds, nous avons une ar√™te pointant de \(x_3 \rightarrow x_1\). Nous pouvons envoyer un message le long de cette ar√™te qui transportera une valeur qui sera calcul√©e par un r√©seau de neurones.

Pour ce cas, nous pouvons l'√©crire comme ci-dessous (et nous allons d√©composer ce que cela signifie aussi) :

$$\vec{m_{31}}=f_e(\vec{h_3}, \vec{h_1}, \vec{e_{31}})$$

Nous utiliserons nos m√™mes notations :

* \(m_31\) est le message pass√© du n≈ìud 3 au n≈ìud 1, 
* \(\vec{h_3}\) est la valeur que le n≈ìud 3 a, 
* \(\vec{e_{31}}\) est la valeur de l'ar√™te entre le n≈ìud 3 et le n≈ìud 1, et 
* \(f_e\) repr√©sente la fonction "quelque r√©seau de neurones" qui d√©pend de toutes ces valeurs, souvent appel√©e la fonction de message.

Et disons que nous avons √©galement une ar√™te de \(x_2 \rightarrow x_1\). Nous pouvons appliquer la m√™me expression que nous avons cr√©√©e ci-dessus, en rempla√ßant simplement les num√©ros de n≈ìuds. 

Si vous avez plus de n≈ìuds, vous voudrez faire cela pour chaque ar√™te pointant vers le n≈ìud 1. Et le moyen le plus simple de les accumuler est de les additionner simplement. Regardez de pr√®s et vous verrez que cela est vraiment similaire √† l'intuition des particules que nous avons discut√©e plus t√¥t !

Maintenant, vous avez une valeur agr√©g√©e des messages arrivant au n≈ìud 2, mais vous devez encore mettre √† jour ses poids. Nous utiliserons donc un autre r√©seau de neurones \(f_v\) souvent appel√© le r√©seau de mise √† jour. Il d√©pend de deux choses : votre valeur originale du n≈ìud 3 bien s√ªr et l'agr√©gat des messages que nous avions.

En mettant simplement ces √©l√©ments ensemble non seulement pour le n≈ìud 3 dans notre exemple mais pour n'importe quel n≈ìud dans n'importe quel graphe, nous pouvons l'√©crire comme :

$$\vec{h_i^{\prime}} = f_v(h_i, \sum_{j \in N_i} \vec{m_{ij}})$$

\(\vec{h_i^{\prime}}\) sont nos valeurs de n≈ìuds mises √† jour, et \(\vec{m_{ij}}\) sont les messages arrivant au n≈ìud \(i\) que nous avons calcul√©s plus t√¥t. 

Vous appliqueriez ensuite ces deux m√™mes r√©seaux de neurones \(f_e\) et \(f_v\) pour chacun des n≈ìuds composant le graphe. 

Une chose vraiment importante √† noter ici est que les deux r√©seaux de neurones o√π nous devons mettre √† jour nos valeurs de n≈ìuds fonctionnent sur des entr√©es de **taille fixe** comme un r√©seau de neurones standard. G√©n√©ralement, les deux r√©seaux de neurones dont nous avons parl√© \(f_e\) et \(f_v\) sont de petits MLP.

![Image](https://www.freecodecamp.org/news/content/images/2022/01/image-130.png)
_Visualisation des R√©seaux de Neurones √† Passage de Messages_

Plus t√¥t, nous avons parl√© des diff√©rents types de sorties qui nous int√©ressent d'obtenir de nos r√©seaux de neurones graphiques. Vous avez peut-√™tre d√©j√† remarqu√© que lors de l'entra√Ænement de notre mod√®le de la mani√®re dont nous en avons parl√©, nous serons en mesure de g√©n√©rer les pr√©dictions au niveau des n≈ìuds : un vecteur pour chaque n≈ìud. 

Pour effectuer la classification de graphes, nous voulons essayer d'agr√©ger toutes les valeurs de n≈ìuds que nous avons apr√®s avoir entra√Æn√© notre r√©seau. Nous utiliserons une couche de lecture ou de pooling (il est assez clair comment le nom vient). 

G√©n√©ralement, nous pouvons cr√©er une fonction \(f_r\) d√©pendant de l'ensemble des valeurs de n≈ìuds. Mais elle doit √©galement √™tre ind√©pendante de la permutation (ne doit pas d√©pendre de votre choix d'√©tiquetage des n≈ìuds), et elle devrait ressembler √† ceci :

$$y^{\prime} = f_r({x_i \vert i \in \textrm{ graphe} })$$

La mani√®re la plus simple de d√©finir une fonction de lecture serait de faire la somme de toutes les valeurs de n≈ìuds. Ensuite, trouver la moyenne, le maximum, ou le minimum, ou m√™me une combinaison de ces propri√©t√©s invariantes par permutation qui conviennent le mieux √† la situation. Votre \(f_r\), comme vous l'avez peut-√™tre devin√©, peut √©galement √™tre un r√©seau de neurones qui est souvent utilis√© en pratique.

Les id√©es et les intuitions dont nous venons de parler cr√©ent les Message Passing Neural Networks (MPNNs), l'un des r√©seaux de neurones graphiques les plus puissants, propos√© pour la premi√®re fois dans [Neural Message Passing for Quantum Chemistry](http://proceedings.mlr.press/v70/gilmer17a.html) (Gilmer et al. 2017).

### Comment changer les valeurs des ar√™tes

Il semble maintenant que nous avons effectivement cr√©√© un r√©seau de neurones graphiques g√©n√©ral. Mais vous pouvez voir que notre r√©seau de messages n√©cessite \(e_{ij}\), la propri√©t√© de l'ar√™te ‚Äì tout comme vous initialisez al√©atoirement les valeurs des n≈ìuds au d√©but. 

Mais tandis que les valeurs des n≈ìuds changent √† chaque √©tape, les valeurs des ar√™tes sont √©galement initialis√©es par vous ‚Äì mais elles ne sont pas chang√©es. Donc, nous devons essayer de g√©n√©raliser cela aussi, une extension de ce que nous venons de voir.

En comprenant comment fonctionnent les mises √† jour des n≈ìuds, je pense que vous pouvez tr√®s facilement appliquer quelque chose de similaire pour une fonction de mise √† jour des ar√™tes √©galement. 

\(U_{edge}\) est un autre r√©seau de neurones standard :

$$e_{ij}^{\prime} = U_{edge}(e_{ij}, x_i, x_j)$$

Quelque chose que vous pourriez √©galement faire avec ce cadre est que les sorties par \(U_{edge}\) sont d√©j√† des propri√©t√©s au niveau des ar√™tes ‚Äì alors pourquoi ne pas les utiliser simplement comme mon message ? Eh bien, vous pourriez faire cela aussi.

### Discussion sur les Message Passing Neural Networks

Les Message Passing Neural Networks (MPNN) sont les couches de r√©seaux de neurones graphiques les plus g√©n√©rales. Mais cela n√©cessite le stockage et la manipulation des messages des ar√™tes ainsi que des caract√©ristiques des n≈ìuds. 

Cela peut devenir un peu probl√©matique en termes de m√©moire et de repr√©sentation. Donc, parfois, ceux-ci souffrent de probl√®mes de scalabilit√© et, en pratique, sont applicables √† des graphes de petite taille. 

Comme le dit Petar Veliƒçkoviƒá, "les MPNN sont les MLP du domaine des graphes". Nous examinerons quelques extensions des MPNN ainsi que comment impl√©menter un MPNN en code.

Vous pouvez assez facilement appliquer exactement ce dont nous avons parl√© dans PyTorch ou TensorFlow ‚Äì mais essayez de le faire et vous verrez que cela fait exploser la m√©moire.

Habituellement, ce que nous faisons avec les r√©seaux de neurones standards est de travailler sur des lots de donn√©es. Donc, vous passez g√©n√©ralement un tableau d'entr√©e de forme [taille du lot, # de neurones d'entr√©e] au r√©seau de neurones pour le faire fonctionner efficacement. 

Maintenant, notre nombre de neurones d'entr√©e ici n'est pas le m√™me que mis en √©vidence pr√©c√©demment, et oui, les r√©seaux de neurones convolutionnels traitent des images de taille arbitraire. Mais lorsque vous pensez en termes de lots, vous avez besoin que toutes les images aient les m√™mes dimensions.

Il y a plusieurs choses que vous pourriez faire :

* Op√©rer avec un seul graphe √† la fois (bien s√ªr tr√®s inefficace)
* Vous pourriez √©galement agr√©ger vos graphes en un grand graphe et ne pas permettre aux messages de passer d'un des plus petits graphes √† un autre petit graphe. Cela introduirait des complications lors de la r√©alisation de pr√©dictions au niveau du graphe et vous devriez adapter votre fonction de lecture.
* Vous pourriez √©galement utiliser des Ragged Tensors qui sont des tenseurs de longueur variable : un excellent tutoriel peut √™tre trouv√© [ici](https://www.tensorflow.org/guide/ragged_tensor).
* Tirer √† nouveau l'inspiration des CNN : vous pourriez utiliser le remplissage de sorte que votre lot ait, par exemple, des graphes de diff√©rentes tailles. Donc, vous prenez simplement un graphe avec 7 n≈ìuds et d√©finissez les 3 n≈ìuds restants √† 0. Il en va de m√™me pour un graphe avec 8 n≈ìuds, d√©finissez les 2 n≈ìuds restants √† 0.

## Autres architectures GNN populaires

Dans cette section, je vais vous donner un aper√ßu de quelques autres couches de r√©seaux de neurones graphiques largement utilis√©es. 

Nous ne regarderons pas l'intuition derri√®re chacune de ces couches et comment chaque partie s'assemble dans la fonction de mise √† jour. Au lieu de cela, je vais simplement vous donner un aper√ßu de haut niveau de ces m√©thodes. Vous pourriez certainement lire les articles originaux pour obtenir une meilleure compr√©hension.

### Graph Convolutional Networks

L'une des architectures GNN les plus populaires est [Graph Convolutional Networks](https://arxiv.org/abs/1609.02907) (GCN) par Kipf et al. qui est essentiellement une m√©thode spectrale. 

Les m√©thodes spectrales fonctionnent avec la repr√©sentation d'un graphe dans le [domaine spectral](https://arxiv.org/abs/1312.6203). Spectral ici signifie que nous utiliserons les vecteurs propres du Laplacien. 

Les GCN sont bas√©s sur ChebNets qui proposent que la repr√©sentation des caract√©ristiques de tout vecteur ne devrait √™tre affect√©e que par son voisinage √† k-sauts. Nous calculerions notre convolution en utilisant les polyn√¥mes de Chebyshev.

Dans un GCN, cela est simplifi√© √† \(K=1\). Nous commencerons par d√©finir une matrice de degr√© (sommation ligne par ligne de la matrice d'adjacence) :

$$\tilde{D}_{ij}=\sum_j\tilde{A}_{ij}$$

La r√®gle de mise √† jour du r√©seau de convolution graphique apr√®s avoir utilis√© une normalisation sym√©trique peut √™tre √©crite o√π H est la matrice des caract√©ristiques et W est la matrice des poids entra√Ænables :

$$H^{\prime}=\sigma(\tilde{D}^{-1/2} \tilde{A}\tilde{D}^{-1/2} HW)$$

N≈ìud par n≈ìud, vous pouvez √©crire cela comme o√π \(N_i\) et \(N_j\) sont les tailles des voisinages des n≈ìuds :

$$\vec{h_i^{\prime}} = \sigma(\sum_{i \in N_j} \frac{1}{\sqrt{|N_i||N_j|}} W \vec{h_j^{\prime}} )$$

Bien s√ªr, avec GCN, vous n'avez plus de caract√©ristiques d'ar√™tes, et l'id√©e qu'un n≈ìud peut envoyer une valeur √† travers le graphe que nous avions avec MPNN dont nous avons discut√© plus t√¥t.

### Graph Attention Network

Rappelez-vous la r√®gle de mise √† jour n≈ìud par n≈ìud dans GCN que nous venons de voir ? \(\frac{1}{\sqrt{|N_i||N_j|}}\) est d√©riv√© de la matrice de degr√© du graphe. 

Dans [Graph Attention Network](https://arxiv.org/abs/1710.10903) (GAT) par Veliƒçkoviƒá et al., ce coefficient \(\alpha_{ij}\) est calcul√© implicitement. Donc pour une ar√™te particuli√®re, vous prenez les caract√©ristiques du n≈ìud √©metteur, du n≈ìud r√©cepteur, et des caract√©ristiques de l'ar√™te √©galement et vous les passez √† travers une fonction d'attention.

$$a_{ij}=a(\vec{h_i}, \vec{h_j}, \vec{e_{ij}})$$

\(a\) pourrait √™tre n'importe quel m√©canisme d'auto-attention apprenable et partag√© comme les transformers. Ceux-ci pourraient ensuite √™tre normalis√©s avec une fonction softmax √† travers le voisinage :

$$\alpha_{ij}=\frac{e^{a_{ij}}}{\sum_{k \in N_i} e^{a_{ik}}}$$

Cela constitue la r√®gle de mise √† jour du GAT. Les auteurs √©mettent l'hypoth√®se que cela pourrait √™tre significativement stabilis√© avec une auto-attention multi-t√™tes. Voici une visualisation par les auteurs de l'article montrant une √©tape du GAT.

![Image](https://www.freecodecamp.org/news/content/images/2022/01/image-131.png)
_Une seule √©tape du GAT_

Cette m√©thode est √©galement tr√®s scalable car elle devait calculer un _scalaire_ pour l'influence de la forme du n≈ìud i au n≈ìud j et noter un vecteur comme dans MPNN. Mais cela n'est probablement pas aussi g√©n√©ral que les MPNN, cependant.

## Impl√©mentation de code pour les R√©seaux de Neurones Graphiques

Avec plusieurs frameworks comme PyTorch Geometric, TF-GNN, Spektral (bas√© sur TensorFlow) et plus, il est en effet assez simple d'impl√©menter des r√©seaux de neurones graphiques. Nous verrons quelques exemples ici en commen√ßant par les MPNNs.

Voici comment vous cr√©ez un r√©seau de neurones √† passage de messages similaire √† celui de l'article original "Neural Message Passing for Quantum Chemistry" avec PyTorch Geometric :

```python
import torch.nn as nn
import torch.nn.functional as F
import torch_geometric.transforms as T
from torch_geometric.utils import normalized_cut
from torch_geometric.nn import NNConv, global_mean_pool, graclus, max_pool, max_pool_x


def normalized_cut_2d(edge_index, pos):
    row, col = edge_index
    edge_attr = torch.norm(pos[row] - pos[col], p=2, dim=1)
    return normalized_cut(edge_index, edge_attr, num_nodes=pos.size(0))


class Net(nn.Module):
    def __init__(self):
        super().__init__()
        nn1 = nn.Sequential(
            nn.Linear(2, 25), nn.ReLU(), nn.Linear(25, d.num_features * 32)
        )
        self.conv1 = NNConv(d.num_features, 32, nn1, aggr="mean")

        nn2 = nn.Sequential(nn.Linear(2, 25), nn.ReLU(), nn.Linear(25, 32 * 64))
        self.conv2 = NNConv(32, 64, nn2, aggr="mean")

        self.fc1 = torch.nn.Linear(64, 128)
        self.fc2 = torch.nn.Linear(128, d.num_classes)

    def forward(self, data):
        data.x = F.elu(self.conv1(data.x, data.edge_index, data.edge_attr))
        weight = normalized_cut_2d(data.edge_index, data.pos)
        cluster = graclus(data.edge_index, weight, data.x.size(0))
        data.edge_attr = None
        data = max_pool(cluster, data, transform=transform)

        data.x = F.elu(self.conv2(data.x, data.edge_index, data.edge_attr))
        weight = normalized_cut_2d(data.edge_index, data.pos)
        cluster = graclus(data.edge_index, weight, data.x.size(0))
        x, batch = max_pool_x(cluster, data.x, data.batch)

        x = global_mean_pool(x, batch)
        x = F.elu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        return F.log_softmax(self.fc2(x), dim=1)
```

Vous pouvez trouver un notebook Colab complet d√©montrant l'impl√©mentation [ici](https://colab.research.google.com/drive/11gtwzl_E4TWqEswwv5mZh4ZWHRz0b3PA?usp=sharing), et il est en effet assez lourd. Il est assez simple de l'impl√©menter dans TensorFlow √©galement, et vous pouvez trouver un tutoriel complet sur [Keras Examples ici](https://keras.io/examples/graph/mpnn-molecular-graphs).

L'impl√©mentation d'un GCN est √©galement assez simple avec PyTorch Geometric. Vous pouvez facilement l'impl√©menter avec TensorFlow √©galement, et vous pouvez trouver un notebook Colab complet [ici](https://colab.research.google.com/drive/1Dgs2rpYleGGTYg0ciCX792zGpfQrtp4p?usp=sharing).

```python
class Net(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GCNConv(dataset.num_features, 16, cached=True,
                             normalize=not args.use_gdc)
        self.conv2 = GCNConv(16, dataset.num_classes, cached=True,
                             normalize=not args.use_gdc)

    def forward(self):
        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr
        x = F.relu(self.conv1(x, edge_index, edge_weight))
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index, edge_weight)
        return F.log_softmax(x, dim=1)
```

Et maintenant, essayons d'impl√©menter un GAT. Vous pouvez trouver le notebook Colab complet [ici](https://colab.research.google.com/drive/1gzRJsRbUUVesxj5bxMz3zkapwdeTuR8F?usp=sharing).

```python
class Net(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()

        self.conv1 = GATConv(in_channels, 8, heads=8, dropout=0.6)
        # Sur le jeu de donn√©es Pubmed, utilisez heads=8 dans conv2.
        self.conv2 = GATConv(8 * 8, out_channels, heads=1, concat=False,
                             dropout=0.6)

    def forward(self, x, edge_index):
        x = F.dropout(x, p=0.6, training=self.training)
        x = F.elu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=-1)
```

## Conclusion

Merci d'√™tre rest√© avec moi jusqu'√† la fin. J'esp√®re que vous avez appris une ou deux choses sur les r√©seaux de neurones graphiques et que vous avez appr√©ci√© de lire comment ces intuitions pour les r√©seaux de neurones graphiques se forment en premier lieu.

Si vous avez appris quelque chose de nouveau ou appr√©ci√© la lecture de cet article, veuillez le partager afin que d'autres puissent le voir. En attendant, √† la prochaine publication !

Enfin, pour le lecteur motiv√©, parmi d'autres, je vous encourage √©galement √† lire l'article original "The Graph Neural Network Model" o√π le GNN a √©t√© propos√© pour la premi√®re fois, car il est vraiment int√©ressant. Une archive en acc√®s libre de l'article peut √™tre trouv√©e [ici](https://persagen.com/files/misc/scarselli2009graph.pdf). Cet article s'inspire √©galement de [Theoretical Foundations of Graph Neural Networks](https://www.youtube.com/watch?v=uF53xsT7mjc) et [CS224W](http://web.stanford.edu/class/cs224w/index.html) que je vous sugg√®re de consulter.

Vous pouvez √©galement me trouver sur Twitter [@rishit_dagli](https://twitter.com/rishit_dagli), o√π je tweete sur le machine learning et un peu sur Android.