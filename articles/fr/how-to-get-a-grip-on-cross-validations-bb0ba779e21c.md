---
title: Comment ma√Ætriser la validation crois√©e en apprentissage automatique
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2018-12-17T21:00:34.000Z'
originalURL: https://freecodecamp.org/news/how-to-get-a-grip-on-cross-validations-bb0ba779e21c
coverImage: https://cdn-media-1.freecodecamp.org/images/1*_gg1Te-7SJfk9E2D-mORfw.png
tags:
- name: AI
  slug: ai
- name: Data Science
  slug: data-science
- name: Machine Learning
  slug: machine-learning
- name: General Programming
  slug: programming
- name: 'tech '
  slug: tech
seo_title: Comment ma√Ætriser la validation crois√©e en apprentissage automatique
seo_desc: 'By Shruti Tanwar

  Lately, I‚Äôve had the chance to be involved in building a product that aims at accelerating
  ML/AI (Machine Learning / Artificial Intelligence) adoption for businesses. In the
  process of developing such an exciting product, I learned a...'
---

Par Shruti Tanwar

R√©cemment, j'ai eu l'opportunit√© de participer √† la cr√©ation d'un produit visant √† acc√©l√©rer l'adoption de l'IA/ML (Intelligence Artificielle / Apprentissage Automatique) pour les entreprises. Au cours du d√©veloppement de ce produit passionnant, j'ai appris une ou deux choses en chemin.

Et bien que l'IA/ML soit un domaine trop vaste pour √™tre couvert dans un seul article, je saisis cette occasion pour mettre en lumi√®re l'un des concepts qui vous aidera √† construire un mod√®le pr√©dictif r√©silient. Un mod√®le capable de performer de mani√®re fiable dans le monde r√©el et de se comporter de mani√®re "√©quitable" sur des donn√©es invisibles.

Vous ne pouvez jamais √™tre s√ªr √† 100 % du comportement de votre mod√®le d'apprentissage automatique. Il y a toujours place √† l'am√©lioration, au progr√®s ou √† un certain ajustement üí°. Se contenter d'ajuster le mod√®le √† vos donn√©es d'entra√Ænement et esp√©rer qu'il performe avec pr√©cision dans le monde r√©el serait un mauvais choix. Certains facteurs qui peuvent garantir ou au moins assurer une performance raisonnable doivent √™tre consid√©r√©s avant de d√©ployer le mod√®le en production.

Vous devez vous assurer que votre mod√®le comprend les diff√©rents motifs dans vos donn√©es ‚Äî qu'il n'est pas sous-ajust√© ou sur-ajust√© ‚Äî et que le biais et la variance du mod√®le sont faibles.

La ¬´ **Validation Crois√©e** ¬ª ‚úÖ est la technique qui vous aide √† valider la performance de votre mod√®le. C'est une m√©thode statistique utilis√©e pour estimer la comp√©tence des mod√®les d'apprentissage automatique. Voici une d√©finition pour vous :

> La **[validation crois√©e](https://en.wikipedia.org/wiki/Cross-validation_(statistics))**, parfois appel√©e **estimation par rotation** ou **test hors √©chantillon**, est l'une des diverses techniques similaires de [validation de mod√®le](https://en.wikipedia.org/wiki/Model_validation) pour √©valuer comment les r√©sultats d'une analyse [statistique](https://en.wikipedia.org/wiki/Statistics) se g√©n√©raliseront √† un ensemble de donn√©es ind√©pendant. Elle est principalement utilis√©e dans des contextes o√π l'objectif est la pr√©diction, et o√π l'on souhaite estimer avec quelle [pr√©cision](https://en.wikipedia.org/wiki/Accuracy) un [mod√®le pr√©dictif](https://en.wikipedia.org/wiki/Predictive_modelling) performera en pratique.

En termes extr√™mement simples, la mise en ≈ìuvre pratique du jargon ci-dessus serait la suivante :

Lors de l'entra√Ænement d'un mod√®le, une partie des donn√©es est retir√©e avant le d√©but de l'entra√Ænement.   
Une fois l'entra√Ænement termin√©, les donn√©es qui ont √©t√© retir√©es sont utilis√©es pour tester la performance du mod√®le appris et ajuster les param√®tres afin d'am√©liorer la performance finale du mod√®le.

C'est l'id√©e fondamentale pour toute la gamme de m√©thodes d'√©valuation appel√©es _validation crois√©e_.

![Image](https://cdn-media-1.freecodecamp.org/images/augTyKVuV5uvIJKNnqUf3oR1K5n7E8DaqirO)
_Cycle de vie de la validation et de l'ajustement du mod√®le._

Avant de discuter des techniques de validation, examinons rapidement deux termes utilis√©s ci-dessus : sur-ajustement et sous-ajustement. Qu'est-ce exactement que le sous-ajustement et le sur-ajustement des mod√®les et comment cela affecte-t-il la performance d'un mod√®le avec des donn√©es r√©elles ?

Nous pouvons le comprendre facilement √† travers le graphique suivant.

![Image](https://cdn-media-1.freecodecamp.org/images/hW2fMyk1PB4dhjyv9TzSj4Yt4K5bkJoUj284)

Un mod√®le est dit **sous-ajust√©** (biais √©lev√©) lorsqu'il performe mal sur les donn√©es d'entra√Ænement. Comme nous pouvons le voir sur le graphique de gauche, la ligne ne couvre pas la plupart des points de donn√©es sur le graphique, ce qui signifie qu'elle n'a pas pu capturer la relation entre l'entr√©e (disons `X`) et la sortie √† pr√©dire (disons `Y`).

Un mod√®le **sur-ajust√©** (variance √©lev√©e), en revanche, performe bien sur les donn√©es d'entra√Ænement mais ne performe pas bien sur les donn√©es d'√©valuation. Dans un tel cas, le mod√®le m√©morise les donn√©es qu'il a vues au lieu d'apprendre et n'est pas capable de g√©n√©raliser √† des donn√©es invisibles.

Le graphique de droite repr√©sente le cas du sur-ajustement. Nous voyons que la ligne pr√©dite couvre tous les points de donn√©es du graphique. Bien que cela puisse sembler indiquer que le mod√®le devrait fonctionner encore mieux, malheureusement, cela est loin de la v√©rit√© pratique. La ligne pr√©dite couvrant tous les points, y compris le bruit et les valeurs aberrantes, produit des r√©sultats m√©diocres en raison de sa complexit√©.

Passons maintenant aux diff√©rents types de techniques de validation crois√©e.

#### **M√©thode de retenue (Holdout Method)**

Le type le plus simple de validation crois√©e. Ici, l'ensemble de donn√©es est s√©par√© en deux ensembles, appel√©s ensemble d'entra√Ænement et ensemble de test. Le mod√®le est ajust√© uniquement sur l'ensemble d'entra√Ænement. Ensuite, les pr√©dictions sont faites pour les donn√©es de l'ensemble de test (que le mod√®le n'a jamais vues auparavant). Les erreurs qu'il commet sont agr√©g√©es pour donner l'erreur moyenne absolue de l'ensemble de test, qui est utilis√©e pour √©valuer le mod√®le.

![Image](https://cdn-media-1.freecodecamp.org/images/-r5fjgSUHFROLDzfsbmMmToDXFusQ3hESc1W)
_Validation crois√©e par retenue_

Ce type d'√©valuation d√©pend dans une certaine mesure des points de donn√©es qui se retrouvent dans l'ensemble d'entra√Ænement et de ceux qui se retrouvent dans l'ensemble de test, et peut donc affecter l'√©valuation en fonction de la mani√®re dont la division est faite.

#### **Validation crois√©e K-fold (K-fold cross-validation)**

L'une des techniques de validation les plus populaires est la validation crois√©e K-fold. Cela est d√ª √† sa simplicit√© qui produit g√©n√©ralement une estimation moins biais√©e ou moins optimiste de la comp√©tence du mod√®le que d'autres m√©thodes, telles qu'une simple division entra√Ænement/test.

Ici, l'ensemble de donn√©es est divis√© en `k` sous-ensembles, et la m√©thode de retenue est r√©p√©t√©e `k` fois. Chaque fois, l'un des `k` sous-ensembles est utilis√© comme ensemble de test et les autres `k-1` sous-ensembles constituent l'ensemble d'entra√Ænement. Ensuite, l'erreur moyenne est calcul√©e sur tous les `k` essais.

La proc√©dure g√©n√©rale est la suivante :

1. M√©langer al√©atoirement l'ensemble de donn√©es et le diviser en `k` groupes
2. Prendre un groupe comme ensemble de retenue ou de test et les groupes restants comme ensemble de donn√©es d'entra√Ænement.
3. Ajuster un mod√®le sur l'ensemble d'entra√Ænement et l'√©valuer sur l'ensemble de test.
4. Conserver le score d'√©valuation et jeter le mod√®le.
5. R√©sumer la comp√©tence du mod√®le en utilisant l'√©chantillon des scores d'√©valuation du mod√®le.

![Image](https://cdn-media-1.freecodecamp.org/images/mXGtXkZXwGrF4FsktIsKTftJyfZDILM4xpHD)
_Validation crois√©e K-fold_

L'avantage de cette m√©thode sur les autres est qu'il importe peu de la mani√®re dont les donn√©es sont divis√©es. Chaque point de donn√©es se retrouvera dans un ensemble de test exactement une fois et se retrouvera dans l'ensemble d'entra√Ænement exactement `k-1` fois. √Ä mesure que `k` augmente, nous observons une diminution de la variance de l'estimation r√©sultante.

Un inconv√©nient de cette m√©thode peut √™tre le calcul requis pendant l'entra√Ænement. L'algorithme d'entra√Ænement doit √™tre relanc√© √† partir de z√©ro `k` fois, ce qui signifie qu'il faut `k` fois plus de calcul pour faire une √©valuation.

#### **Validation crois√©e leave-one-out (Leave-one-out cross-validation)**

Leave-one-out est une sorte de cousine de la validation crois√©e K-fold o√π `k` devient √©gal √† `n`, le nombre total de points de donn√©es dans l'ensemble. C'est essentiellement une version extr√™me logique de la validation K-fold. Comment cela fonctionne en pratique est en laissant exactement un point de donn√©es √† chaque it√©ration et en utilisant ce point de donn√©es pour faire la pr√©diction.

L'approximateur de fonction est entra√Æn√© sur toutes les donn√©es, exactement `n` fois, sauf pour un point et une pr√©diction est faite pour ce point. Comme avant, l'erreur moyenne est calcul√©e et utilis√©e pour √©valuer le mod√®le.

Nous y voil√† et appelons cela une conclusion. J'esp√®re que vous avez appr√©ci√© la lecture autant que j'ai appr√©ci√© la cr√©ation. ‚ù§Ô∏è Faites-moi savoir vos pens√©es ?, commentaires ? ou conseils ? dans les commentaires ci-dessous.  
Et pendant que vous y √™tes, pourquoi ne pas aller voir ce que je construis avec mon √©quipe, chez [skyl.ai](https://skyl.ai/) et engager une conversation avec moi ou partager vos commentaires. Sant√©.