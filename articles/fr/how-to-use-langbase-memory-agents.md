---
title: Comment utiliser les agents de m√©moire Langbase pour transformer n'importe
  quel LLM en IA conversationnelle pour vos documents
subtitle: ''
author: Maham Codes
co_authors: []
series: null
date: '2025-01-17T21:17:39.296Z'
originalURL: https://freecodecamp.org/news/how-to-use-langbase-memory-agents
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1737148633610/45e0af50-6026-4953-8e1a-953a7d5b6df6.png
tags:
- name: llm
  slug: llm
- name: ai agents
  slug: ai-agents
seo_title: Comment utiliser les agents de m√©moire Langbase pour transformer n'importe
  quel LLM en IA conversationnelle pour vos documents
seo_desc: It‚Äôs 2025, and Large Language Models (LLMs) still can‚Äôt access your private
  data. Ask them something personal, and they‚Äôll either guess or give you the wrong
  answer. That‚Äôs the limitation‚Äîthey‚Äôre trained on public information and don‚Äôt have
  access to...
---

Nous sommes en 2025, et les grands mod√®les de langage (LLM) ne peuvent toujours pas acc√©der √† vos donn√©es priv√©es. Posez-leur une question personnelle, et ils devineront ou vous donneront la mauvaise r√©ponse. C'est la limitation : ils sont form√©s sur des informations publiques et n'ont pas acc√®s √† votre contexte priv√©.

Les agents de m√©moire r√©solvent ce probl√®me en reliant de mani√®re s√©curis√©e vos donn√©es priv√©es √† n'importe quel LLM en temps r√©el. Dans ce tutoriel, je vais vous montrer comment transformer un LLM en une IA conversationnelle qui discute avec vos documents personnels en utilisant les agents de m√©moire Langbase.

### Voici ce que nous allons couvrir :

1. [Qu'est-ce que les agents de m√©moire ?](#heading-quest-ce-que-les-agents-de-memoire)
    
2. [S√©curiser vos donn√©es avec les agents de m√©moire](#heading-securiser-vos-donnees-avec-les-agents-de-memoire)
    
3. [Cas d'utilisation des agents de m√©moire](#heading-cas-dutilisation-des-agents-de-memoire)
    
4. [Pr√©requis](#heading-prerequis)
    
5. [√âtape 1 : Cr√©er un r√©pertoire et initialiser npm](#heading-etape-1-creer-un-repertoire-et-initialiser-npm)
    
6. [√âtape 2 : Cr√©er un agent Pipe](#heading-etape-2-creer-un-agent-pipe)
    
7. [√âtape 3 : Ajouter un fichier .env](#heading-etape-3-ajouter-un-fichier-env)
    
8. [√âtape 4 : Cr√©er un agent de m√©moire](#heading-etape-4-creer-un-agent-de-memoire)
    
9. [√âtape 5 : Ajouter des documents √† l'agent de m√©moire](#heading-etape-5-ajouter-des-documents-a-lagent-de-memoire)
    
10. [√âtape 6 : G√©n√©rer des embeddings de m√©moire](#heading-etape-6-generer-des-embeddings-de-memoire)
    
    * [Qu'est-ce que les embeddings de m√©moire ?](#heading-quest-ce-que-les-embeddings-de-memoire)
        
    * [Pourquoi avez-vous besoin d'embeddings de m√©moire ?](#heading-pourquoi-avez-vous-besoin-dembeddings-de-memoire)
        
    * [Comment g√©n√©rer des embeddings](#heading-comment-generer-des-embeddings)
        
11. [√âtape 7 : Int√©grer la m√©moire dans l'agent Pipe](#heading-etape-7-integrer-la-memoire-dans-lagent-pipe)
    
12. [√âtape 8 : Int√©grer l'agent de m√©moire dans Node.js](#heading-etape-8-integrer-lagent-de-memoire-dans-nodejs)
    
13. [√âtape 9 : D√©marrer le serveur BaseAI](#heading-etape-9-demarrer-le-serveur-baseai)
    
14. [√âtape 10 : Ex√©cuter l'agent de m√©moire](#heading-etape-10-executer-lagent-de-memoire)
    
15. [Le r√©sultat](#heading-le-resultat)
    

## Qu'est-ce que les agents de m√©moire ?

La m√©moire est ce qui rend les interactions significatives. C'est ainsi que les syst√®mes peuvent se souvenir de ce qui s'est pass√© auparavant, un aspect cl√© pour construire des agents IA v√©ritablement intelligents.

Voici le probl√®me : les LLM peuvent sembler humains, mais ils n'ont pas de m√©moire int√©gr√©e. Ils sont **stateless par conception**. Pour les rendre utiles pour des t√¢ches r√©elles, vous devez ajouter de la m√©moire. C'est l√† que les agents de m√©moire interviennent.

Les [agents de m√©moire Langbase](https://langbase.com/docs/memory) (solution de m√©moire √† long terme) sont con√ßus pour **acqu√©rir, traiter, retenir et r√©cup√©rer** des informations de mani√®re transparente. Ils attachent dynamiquement des donn√©es priv√©es √† n'importe quel LLM, permettant des r√©ponses contextuelles en temps r√©el et r√©duisant les hallucinations.

Ces agents combinent le stockage vectoriel, la g√©n√©ration augment√©e par r√©cup√©ration (RAG), et l'acc√®s √† Internet pour cr√©er une API de recherche de contexte puissante et g√©r√©e. Les d√©veloppeurs peuvent les utiliser pour construire des applications IA plus intelligentes et plus capables.

Dans une configuration RAG, la m√©moire - lorsqu'elle est connect√©e directement √† un [agent Pipe Langbase](https://langbase.com/docs/pipe) - devient un **agent de m√©moire**. Ce couplage donne au LLM la capacit√© de r√©cup√©rer des donn√©es pertinentes et de fournir des r√©ponses pr√©cises et contextuellement exactes, r√©pondant aux limitations des LLM lorsqu'il s'agit de g√©rer des donn√©es priv√©es.

<div data-node-type="callout">
<div data-node-type="callout-emoji">üí°</div>
<div data-node-type="callout-text">Pipe est un agent IA serverless. Il dispose d'une m√©moire agentique et d'outils.</div>
</div>

Voici une repr√©sentation diagramme de l'ensemble du processus :

![Diagramme architectural du flux de travail des agents de m√©moire](https://cdn.hashnode.com/res/hashnode/image/upload/v1736776247313/6d66a33b-bf82-4a8e-96d7-1d8a6382b863.png align="center")

## S√©curiser vos donn√©es avec les agents de m√©moire

Les agents de m√©moire priorisent la s√©curit√© des donn√©es en gardant les informations priv√©es isol√©es et trait√©es localement ou dans des environnements s√©curis√©s. Les donn√©es utilis√©es pour cr√©er des embeddings de m√©moire ne sont pas envoy√©es √† des serveurs externes sauf si explicitement configur√©, garantissant que les informations sensibles restent prot√©g√©es.

De plus, l'acc√®s au syst√®me de m√©moire est strictement contr√¥l√© par des cl√©s API et des permissions, emp√™chant tout acc√®s non autoris√©. Cette configuration am√©liore non seulement les capacit√©s de l'IA, mais maintient √©galement la confiance des utilisateurs en prot√©geant leurs donn√©es.

## Cas d'utilisation des agents de m√©moire

Voici quelques applications pratiques de ces agents :

* **Support client** : Fournir une assistance personnalis√©e et contextuelle en rappelant l'historique des interactions.
    
* **Recherche de documents** : Permettre une recherche s√©mantique rapide dans de grandes bases de donn√©es, manuels ou FAQ.
    
* **Assistance de code** : Fournir une documentation sp√©cifique au projet et des conseils de d√©bogage pour les d√©veloppeurs.
    
* **Gestion des connaissances** : Centraliser et r√©cup√©rer des informations internes pour les √©quipes de mani√®re efficace.
    
* **√âducation et formation** : Fournir aux √©tudiants ou employ√©s des mat√©riaux de formation personnalis√©s, suivre les progr√®s et r√©pondre aux questions bas√©es sur les ressources stock√©es.
    
* **Sant√©** : R√©cup√©rer de mani√®re s√©curis√©e les dossiers des patients ou l'historique m√©dical pour un support pr√©cis.
    
* **Flux de travail collaboratifs** : Suivre l'historique des projets et s'int√©grer avec des outils pour l'alignement des √©quipes.
    
* **Conformit√© l√©gale** : R√©f√©rencer des directives pour garantir des d√©cisions pr√©cises et conformes aux r√©glementations.
    

Les nombreux cas d'utilisation rendus possibles par les agents de m√©moire ouvrent de nouvelles possibilit√©s et changent ce que l'**Intelligence G√©n√©rale Artificielle (AGI)** peut faire.

## Pr√©requis

Avant de commencer √† cr√©er un agent de m√©moire qui discute avec vos documents, vous devrez avoir la configuration et les outils suivants pr√™ts √† l'emploi.

Dans ce tutoriel, j'utiliserai la pile technologique suivante :

* [BaseAI](http://baseai.dev) ‚Äî le framework web pour construire des agents IA localement.
    
* [Langbase](http://langbase.com) ‚Äî la plateforme pour construire et d√©ployer vos agents IA serverless.
    
* [OpenAI](https://openai.com/) ‚Äî pour obtenir la cl√© LLM pour le mod√®le pr√©f√©r√©.
    

Vous devrez √©galement :

* Vous inscrire sur Langbase pour obtenir acc√®s √† la cl√© API.
    
* Vous inscrire sur OpenAI pour g√©n√©rer la cl√© LLM pour le mod√®le que vous souhaitez utiliser (pour cette d√©monstration, j'utiliserai GPT-4o mini).
    

Commen√ßons !

## √âtape 1 : Cr√©er un r√©pertoire et initialiser npm

Pour commencer √† cr√©er un agent de m√©moire qui discute avec vos documents, vous devez cr√©er un r√©pertoire dans votre machine locale et installer toutes les d√©pendances de d√©veloppement pertinentes. Vous pouvez faire cela en naviguant vers celui-ci et en ex√©cutant la commande suivante dans le terminal :

```bash
mkdir my-project
npm init -y
npm install dotenv
```

Cette commande cr√©era un fichier `package.json` dans votre r√©pertoire de projet avec des valeurs par d√©faut. Elle installera √©galement le package `dotenv` pour lire les variables d'environnement depuis le fichier `.env`.

## √âtape 2 : Cr√©er un agent Pipe

Ensuite, nous allons cr√©er un agent pipe. Les pipes sont diff√©rents des autres agents, car ils sont des agents IA serverless avec des outils agentiques qui peuvent fonctionner avec n'importe quel langage ou framework. Ils sont facilement d√©ployables, et avec une seule API, ils vous permettent de connecter 100+ LLM √† n'importe quelle donn√©e pour construire n'importe quel flux de travail d'API de d√©veloppeur.

Pour cr√©er votre pipe d'agent IA, naviguez vers votre r√©pertoire de projet. Ex√©cutez la commande suivante :

```bash
npx baseai@latest pipe
```

Lors de l'ex√©cution, vous verrez les invites suivantes :

```bash
BaseAI n'est pas install√© mais est requis pour fonctionner. Souhaitez-vous l'installer ? Oui/Non
Nom du pipe ?  pipe-with-memory
Description du pipe ? Pipe attach√© √† une m√©moire
Statut du pipe ? Public/Priv√©
Prompt syst√®me ? Vous √™tes un assistant IA utile
```

Une fois que vous avez termin√© avec le nom, la description et le statut du pipe de l'agent IA, tout sera configur√© automatiquement pour vous. Votre pipe sera cr√©√© avec succ√®s √† `/baseai/pipes/pipe-with-memory.ts`.

## √âtape 3 : Ajouter un fichier .env

Cr√©ez un fichier `.env` dans le r√©pertoire racine de votre projet et ajoutez-y la cl√© API [OpenAI](https://platform.openai.com/api-keys) et Langbase. Vous pouvez acc√©der √† votre cl√© API Langbase depuis [ici](https://langbase.com/docs/api-reference/api-keys).

## √âtape 4 : Cr√©er un agent de m√©moire

Ensuite, nous allons cr√©er une m√©moire puis l'attacher au Pipe pour en faire un agent de m√©moire. Pour cela, ex√©cutez cette commande dans votre terminal :

```bash
npx baseai@latest memory
```

Lors de l'ex√©cution de cette commande, vous verrez les invites suivantes :

```bash
Nom de la m√©moire ?  chat-with-docs-agent
Description de la m√©moire ? FAQs docs
Souhaitez-vous cr√©er une m√©moire √† partir du d√©p√¥t git actuel du projet ? Oui/Non
```

Apr√®s cela, tout sera configur√© automatiquement pour vous et vous pourrez acc√©der √† votre m√©moire cr√©√©e avec succ√®s √† `/baseai/memory/chat-with-docs-agent.ts`.

## √âtape 5 : Ajouter des documents √† l'agent de m√©moire

√Ä l'int√©rieur de `/baseai/memory/chat-with-docs-agent.ts`, vous verrez un autre dossier appel√© documents. C'est l√† que vous stockerez les fichiers que vous souhaitez que votre agent IA acc√®de. Pour cette d√©monstration, j'enregistrerai la page FAQs de Pipe sous forme de fichier `.pdf` ou `.txt`. Ensuite, je le convertirai en fichier markdown et le placerai dans le r√©pertoire `baseai/memory/chat-with-docs/documents`.

Cette √©tape garantit que l'agent de m√©moire peut **traiter et r√©cup√©rer** des informations √† partir de vos documents, rendant l'agent IA capable de r√©pondre aux requ√™tes en fonction du contenu que vous fournissez.

## √âtape 6 : G√©n√©rer des embeddings de m√©moire

Maintenant que vous avez ajout√© des documents √† la m√©moire, l'√©tape suivante consiste √† g√©n√©rer des embeddings de m√©moire. Mais d'abord, que sont exactement les embeddings et pourquoi sont-ils essentiels ?

### Qu'est-ce que les embeddings de m√©moire ?

Les embeddings sont des repr√©sentations num√©riques de vos documents qui permettent √† l'IA de comprendre le contexte et les relations entre les mots, les phrases et les sentences. Pensez aux embeddings comme un moyen de traduire vos documents dans une langue que l'IA peut traiter pour la recherche s√©mantique et la r√©cup√©ration.

### Pourquoi avez-vous besoin d'embeddings de m√©moire ?

Sans embeddings, l'agent IA ne pourrait pas faire correspondre les requ√™tes des utilisateurs avec le contenu pertinent de vos documents. En g√©n√©rant des embeddings, vous cr√©ez essentiellement un index recherchable qui permet des r√©ponses pr√©cises et efficaces de l'agent de m√©moire.

### Comment g√©n√©rer des embeddings

Pour g√©n√©rer des embeddings pour vos documents, ex√©cutez la commande suivante dans votre terminal :

```bash
npx baseai@latest embed -m chat-with-docs-agent
```

Votre m√©moire est maintenant pr√™te √† √™tre connect√©e √† un Pipe (agent de m√©moire), permettant √† votre agent IA de r√©cup√©rer des r√©ponses pr√©cises et contextuelles √† partir de vos documents.

## √âtape 7 : Int√©grer la m√©moire dans l'agent Pipe

Ensuite, vous devez attacher la m√©moire que vous avez cr√©√©e √† votre agent Pipe pour en faire un agent de m√©moire. Pour cela, allez dans `/baseai/pipes/pipe-with-memory.ts`. Voici √† quoi cela ressemblera pour le moment :

```typescript
import { PipeI } from '@baseai/core';

const pipePipeWithMemory = (): PipeI => ({
	apiKey: process.env.LANGBASE_API_KEY!, // Remplacez par votre cl√© API https://langbase.com/docs/api-reference/api-keys
	name: 'pipe-with-memory',
	description: 'Pipe attach√© √† une m√©moire',
	status: 'public',
	model: 'openai:gpt-4o-mini',
	stream: true,
	json: false,
	store: true,
	moderate: true,
	top_p: 1,
	max_tokens: 1000,
	temperature: 0.7,
	presence_penalty: 1,
	frequency_penalty: 1,
	stop: [],
	tool_choice: 'auto',
	parallel_tool_calls: false,
	messages: [
		{ role: 'system', content: `Vous √™tes un assistant IA utile.` }],
	variables: [],
    memory: [],
    tools: []
});

export default pipePipeWithMemory;
```

Maintenant, int√©grez la m√©moire dans le pipe en l'important en haut et en l'appelant en tant que fonction dans le tableau `memory`. Voici √† quoi ressemblera le code apr√®s avoir fait tout cela :

```typescript
import { PipeI } from '@baseai/core';
import chatWithDocsAgentMemory from '../memory/chat-with-docs-agent';

const pipePipeWithMemory = (): PipeI => ({
	apiKey: process.env.LANGBASE_API_KEY!, // Remplacez par votre cl√© API https://langbase.com/docs/api-reference/api-keys
	name: 'pipe-with-memory',
	description: 'Pipe attach√© √† une m√©moire',
	status: 'public',
	model: 'openai:gpt-4o-mini',
	stream: true,
	json: false,
	store: true,
	moderate: true,
	top_p: 1,
	max_tokens: 1000,
	temperature: 0.7,
	presence_penalty: 1,
	frequency_penalty: 1,
	stop: [],
	tool_choice: 'auto',
	parallel_tool_calls: false,
	messages: [
		{ role: 'system', content: `Vous √™tes un assistant IA utile.` }],
	variables: [],
    memory: [chatWithDocsAgentMemory()],
    tools: []
});

export default pipePipeWithMemory;
```

## √âtape 8 : Int√©grer l'agent de m√©moire dans Node.js

Maintenant, nous allons int√©grer l'agent de m√©moire que vous avez cr√©√© dans le projet Node.js pour construire une interface de ligne de commande (CLI) interactive pour le document attach√©. Ce projet Node.js servira de base pour tester et interagir avec l'agent de m√©moire (au d√©but du tutoriel, nous avons configur√© un projet Node.js en initialisant npm).

Maintenant, cr√©ez un fichier `index.ts` :

```bash
touch index.ts
```

Dans ce fichier TypeScript, importez l'agent pipe que vous avez cr√©√©. Nous utiliserons le primitif pipe de `@baseai/core` pour ex√©cuter le pipe.

Ajoutez le code suivant au fichier `index.ts` :

```typescript
import 'dotenv/config';
import { Pipe } from '@baseai/core';
import inquirer from 'inquirer';
import ora from 'ora';
import chalk from 'chalk';
import pipePipeWithMemory from './baseai/pipes/pipe-with-memory';

const pipe = new Pipe(pipePipeWithMemory());

async function main() {

   const initialSpinner = ora('Conversation avec l\'agent de m√©moire...').start();
   try {
       const { completion: calculatorTool} = await pipe.run({
           messages: [{ role: 'user', content: 'Bonjour' }],
       });
       initialSpinner.stop();
       console.log(chalk.cyan('R√©ponse de l\'agent g√©n√©rateur de rapports...'));
       console.log(calculatorTool);
   } catch (error) {
       initialSpinner.stop();
       console.error(chalk.red('Erreur lors du traitement de la requ√™te initiale :'), error);
   }


   while (true) {
       const { userMsg } = await inquirer.prompt([
           {
               type: 'input',
               name: 'userMsg',
               message: chalk.blue('Entrez votre requ√™te (ou tapez "exit" pour quitter) :'),
           },
       ]);


       if (userMsg.toLowerCase() === 'exit') {
           console.log(chalk.green('Au revoir !'));
           break;
       }


       const spinner = ora('Traitement de votre requ√™te...').start();


       try {
           const { completion: reportAgentResponse } = await pipe.run({
               messages: [{ role: 'user', content: userMsg }],
           });


           spinner.stop();
           console.log(chalk.cyan('Agent :'));
           console.log(reportAgentResponse);
       } catch (error) {
           spinner.stop();
           console.error(chalk.red('Erreur lors du traitement de votre requ√™te :'), error);
       }
   }
}

main();
```

Ce code cr√©e une CLI interactive pour discuter avec un agent IA, en utilisant un pipe de la biblioth√®que `@baseai/core` pour traiter les entr√©es de l'utilisateur. Voici ce qui se passe :

* Il importe les biblioth√®ques n√©cessaires telles que `dotenv` pour la configuration de l'environnement, `inquirer` pour les entr√©es de l'utilisateur, `ora` pour les spinners de chargement, et `chalk` pour les sorties color√©es. Assurez-vous d'installer d'abord ces biblioth√®ques en utilisant cette commande dans votre terminal : `npm install ora inquirer`.
    
* Un objet pipe est cr√©√© √† partir de la biblioth√®que BaseAI en utilisant une m√©moire pr√©d√©finie appel√©e `pipe-with-memory`.
    

Dans la fonction `main()` :

* Un spinner commence pendant qu'une conversation initiale avec l'agent IA est initi√©e avec le message 'Bonjour'.
    
* La r√©ponse de l'IA est affich√©e.
    
* Une boucle s'ex√©cute pour demander continuellement √† l'utilisateur une entr√©e et envoyer des requ√™tes √† l'agent IA.
    
* Les r√©ponses de l'IA sont affich√©es, et le processus continue jusqu'√† ce que l'utilisateur tape "exit".
    

## √âtape 9 : D√©marrer le serveur BaseAI

Pour ex√©cuter l'agent de m√©moire localement, vous devez d'abord d√©marrer le serveur BaseAI. Ex√©cutez la commande suivante dans votre terminal :

```bash
npx baseai@latest dev
```

## √âtape 10 : Ex√©cuter l'agent de m√©moire

Ex√©cutez le fichier index.ts en utilisant la commande suivante :

```bash
npx tsx index.ts
```

## Le r√©sultat

Dans votre terminal, vous serez invit√© √† "Entrez votre requ√™te". Par exemple, demandons : "Qu'est-ce qu'un pipe sur Langbase ?" Et il nous donnera la r√©ponse avec les sources/citations correctes √©galement.

Avec cette configuration, nous avons construit un agent "Discuter avec votre document" qui utilise la puissance des LLM et des agents de m√©moire Langbase pour surmonter les limitations des LLM, garantissant des r√©ponses pr√©cises sans hallucinations sur les donn√©es priv√©es.

Voici une d√©monstration du r√©sultat final :

%[https://youtu.be/v2Iev-q3kuc] 

Merci d'avoir lu !