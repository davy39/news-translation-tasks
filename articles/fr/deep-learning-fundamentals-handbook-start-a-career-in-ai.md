---
title: Manuel des Fondamentaux de l'Apprentissage Profond ‚Äì Ce que Vous Devez Savoir
  pour D√©marrer Votre Carri√®re en IA
subtitle: ''
author: Tatev Aslanyan
co_authors: []
series: null
date: '2024-02-16T23:47:10.000Z'
originalURL: https://freecodecamp.org/news/deep-learning-fundamentals-handbook-start-a-career-in-ai
coverImage: https://www.freecodecamp.org/news/content/images/2024/02/The-Deep-Learning-Fundamentals-Handbook-Cover-Version-3--1-.png
tags:
- name: Artificial Intelligence
  slug: artificial-intelligence
- name: Deep Learning
  slug: deep-learning
- name: MathJax
  slug: mathjax
seo_title: Manuel des Fondamentaux de l'Apprentissage Profond ‚Äì Ce que Vous Devez
  Savoir pour D√©marrer Votre Carri√®re en IA
seo_desc: 'If you want to get into the field of Artificial Intelligence (AI), one
  of the most in-demand career paths these days, you''ve come to the right place.

  Learning Deep Learning Fundamentals is your essential first step to learning about
  Computer Vision, ...'
---

Si vous souhaitez vous lancer dans le domaine de l'Intelligence Artificielle (IA), l'un des parcours professionnels les plus demand√©s ces jours-ci, vous √™tes au bon endroit.

Apprendre les Fondamentaux de l'Apprentissage Profond est votre premi√®re √©tape essentielle pour comprendre la Vision par Ordinateur, le Traitement du Langage Naturel (NLP), les Grands Mod√®les de Langage, l'univers cr√©atif de l'IA G√©n√©rative, et bien plus encore.

Si vous aspirez √† devenir Data Scientist, Chercheur en IA, Ing√©nieur en IA ou Chercheur en Machine Learning, ce guide est fait pour vous.

L'innovation en IA se produit rapidement. Que vous soyez d√©butant ou d√©j√† impliqu√© dans le Machine Learning, vous devriez continuer √† consolider votre base de connaissances et apprendre les fondamentaux de l'Apprentissage Profond.

Consid√©rez ce manuel comme votre feuille de route personnelle pour naviguer dans le paysage de l'IA. Que vous soyez un passionn√© en herbe curieux de savoir comment l'IA transforme notre monde, un √©tudiant visant √† construire une carri√®re dans la tech, ou un professionnel cherchant √† se reconvertir dans ce domaine passionnant, il vous sera utile.

Ce guide peut vous aider √† :

* Apprendre tous les Fondamentaux de l'Apprentissage Profond en un seul endroit √† partir de z√©ro

* Rafra√Æchir votre m√©moire sur tous les fondamentaux de l'Apprentissage Profond

* Vous pr√©parer pour vos prochains entretiens en IA.

## Table des Mati√®res

1. [Chapitre 1 : Qu'est-ce que l'Apprentissage Profond ?](#heading-chapter-1-quest-ce-que-lapprentissage-profond)

2. [Chapitre 2 : Fondements des R√©seaux de Neurones](#heading-chapter-2-fondements-des-reseaux-de-neurones)  
‚Äì Architecture des R√©seaux de Neurones  
‚Äì Fonctions d'Activation

3. [Chapitre 3 : Comment Entra√Æner les R√©seaux de Neurones](#chapter-4-comment-entrainer-les-reseaux-de-neurones)  
‚Äì Passe Avant - d√©rivation math√©matique  
‚Äì Passe Arri√®re - d√©rivation math√©matique

4. [Chapitre 4 : Algorithmes d'Optimisation en IA](#chapter-5-algorithmes-doptimisation-en-ia)  
‚Äì Descente de Gradient - avec Python  
‚Äì SGD - avec Python  
‚Äì SGD avec Momentum - avec Python  
‚Äì RMSProp - avec Python  
‚Äì Adam - avec Python  
‚Äì AdamW - avec Python

5. [Chapitre 5 : R√©gularisation et G√©n√©ralisation](#chapter-6-regularisation-et-generalisation)  
‚Äì Dropout  
‚Äì R√©gularisation Ridge (R√©gularisation L2)  
‚Äì R√©gularisation Lasso (R√©gularisation L1)  
‚Äì Normalisation par Lots

6. [Chapitre 6 : Probl√®me du Gradient √âvanescent](#chapter-7-probleme-du-gradient-evanescent)  
‚Äì Utiliser des fonctions d'activation appropri√©es  
‚Äì Utiliser l'Initialisation de Xavier ou He  
‚Äì Effectuer la Normalisation par Lots  
‚Äì Ajouter des Connexions R√©siduelles

7. [Chapitre 7 : Probl√®me du Gradient Explosif](#chapter-8-lutte-contre-les-gradients-explosifs)

8. [Chapitre 8 : Mod√©lisation de S√©quences avec les RNN et LSTM](#heading-chapter-8-modelisation-de-sequences-avec-les-rnn-et-lstm)  
‚Äì Architecture des R√©seaux de Neurones R√©currents (RNN)  
‚Äì Pseudocode des R√©seaux de Neurones R√©currents  
‚Äì Limites des R√©seaux de Neurones R√©currents  
‚Äì Architecture de la M√©moire √† Long et Court Terme (LSTM)

9. [Chapitre 9 : Pr√©paration aux Entretiens en Apprentissage Profond](#heading-chapter-9-preparation-aux-entretiens-en-apprentissage-profond)  
‚Äì Partie 1 : Cours d'Entretien en Apprentissage Profond [50 Q&R]  
‚Äì Partie 2 : Cours d'Entretien en Apprentissage Profond [100 Q&R]

## Pr√©requis

L'Apprentissage Profond est un domaine d'√©tude avanc√© dans les champs de l'Intelligence Artificielle et du Machine Learning. Pour bien comprendre les concepts discut√©s ici, il est essentiel que vous ayez une solide fondation dans plusieurs domaines cl√©s.

### 1. Bases du Machine Learning

Comprendre les principes fondamentaux du machine learning est crucial. Si vous n'√™tes pas encore familier avec ceux-ci, je vous recommande de consulter mon [Manuel des Fondamentaux du Machine Learning](https://www.freecodecamp.org/news/machine-learning-handbook/), o√π j'ai expos√© tous les travaux pr√©paratoires n√©cessaires. De plus, mon cours [Fondamentaux du Machine Learning](https://lunartech.ai/course-overview/) offre un enseignement complet sur ces principes.

### 2. Fondamentaux de la Statistique

La statistique joue un r√¥le vital dans la compr√©hension des motifs de donn√©es et des inf√©rences en machine learning. Pour ceux qui ont besoin de se rafra√Æchir la m√©moire sur ce sujet, mon cours [Fondamentaux de la Statistique](https://lunartech.ai/course-overview/) est une autre ressource o√π je couvre tous les concepts statistiques essentiels dont vous aurez besoin.

### 3. Alg√®bre Lin√©aire et Th√©orie Diff√©rentielle

Une [compr√©hension de haut niveau de l'alg√®bre lin√©aire](https://www.freecodecamp.org/news/linear-algebra-full-course/) et de la [th√©orie diff√©rentielle](https://en.wikipedia.org/wiki/Differential_(mathematics)) est √©galement importante. Nous couvrirons certains aspects, tels que les r√®gles de diff√©rentiation, dans ce manuel. Nous aborderons la multiplication de matrices, les op√©rations sur les matrices et les vecteurs, les concepts de normalisation et les bases de la th√©orie de la diff√©rentiation.

Mais je vous encourage √† renforcer votre compr√©hension dans ces domaines. Vous pouvez trouver plus de contenu sur freeCodeCamp en recherchant "Linear Algebra" comme ce cours "[Full Linear Algebra Course](https://youtu.be/LwCRRUa8yTU?si=DEeXlC9_d1Ct9eAF)".

Notez que si vous n'avez pas les pr√©requis tels que les Fondamentaux de la Statistique, du Machine Learning et des Math√©matiques, suivre ce manuel sera assez difficile. Nous utiliserons des concepts de tous ces domaines, y compris la moyenne, la variance, les r√®gles de la cha√Æne, la multiplication de matrices, les d√©riv√©es, et ainsi de suite. Alors, assurez-vous de les avoir pour tirer le meilleur parti de ce contenu.

### Exemple de R√©f√©rence ‚Äì Pr√©diction du Prix des Maisons

Tout au long de ce livre, nous utiliserons un exemple pratique pour illustrer et clarifier les concepts que vous apprenez. Nous explorerons cette id√©e de pr√©dire le prix d'une maison en fonction de ses caract√©ristiques. Cet exemple servira de point de r√©f√©rence pour rendre les concepts abstraits ou complexes plus concrets et plus faciles √† comprendre.

## Chapitre 1 : Qu'est-ce que l'Apprentissage Profond ?

L'Apprentissage Profond est une s√©rie d'algorithmes inspir√©s par la structure et la fonction du cerveau. L'Apprentissage Profond permet aux mod√®les quantitatifs compos√©s de plusieurs couches de traitement d'√©tudier la repr√©sentation des donn√©es avec plusieurs niveaux d'abstraction.

![Image](https://www.freecodecamp.org/news/content/images/2024/01/0-Q3PICBlib-932hhH.png align="left")

*Exploration des Couches de l'IA : De l'Intelligence Artificielle √† l'Apprentissage Profond. (Source de l'Image : [LunarTech.ai](https://lunartech.ai))*

L'Apprentissage Profond est une branche du Machine Learning, et il tente d'imiter le fonctionnement du cerveau humain et la prise de d√©cisions bas√©es sur des mod√®les √† base de r√©seaux de neurones.

En termes plus simples, l'Apprentissage Profond est une version plus avanc√©e et plus complexe du Machine Learning traditionnel. Les mod√®les d'Apprentissage Profond sont bas√©s sur des [R√©seaux de Neurones](https://www.freecodecamp.org/news/learn-machine-learning-and-neural-networks-without-frameworks/) et ils tentent d'imiter la fa√ßon dont les humains pensent et prennent des d√©cisions.

Le probl√®me avec les m√©thodes Statistiques ou ML traditionnelles est qu'elles sont bas√©es sur des r√®gles et des instructions sp√©cifiques. Ainsi, chaque fois que l'ensemble des hypoth√®ses du mod√®le ne sont pas satisfaites, le mod√®le peut avoir beaucoup de mal √† r√©soudre le probl√®me et √† effectuer des pr√©dictions. Il existe √©galement des types de probl√®mes tels que la reconnaissance d'images, et d'autres t√¢ches plus avanc√©es, qui ne peuvent pas √™tre r√©solus avec des mod√®les Statistiques ou de Machine Learning traditionnels.

C'est pr√©cis√©ment l√† que l'Apprentissage Profond intervient.

![Image](https://www.freecodecamp.org/news/content/images/2024/01/1-hx3DLumiQYwPGY1Ax_sGMA-copy.png align="left")

*Hi√©rarchie de l'IA : Navigation des Concepts G√©n√©raux de l'IA aux Mod√®les de Langage Sp√©cialis√©s (Source de l'Image : [Medium](https://medium.com/womenintechnology/ai-c3412c5aa0ac))*

### Applications de l'Apprentissage Profond

Voici quelques exemples o√π l'Apprentissage Profond est utilis√© dans diverses industries et applications :

#### Sant√©

* **Diagnostic et Pronostic des Maladies** : Les algorithmes d'apprentissage profond aident √† analyser les images m√©dicales comme les radiographies, les IRM et les scanners pour diagnostiquer des maladies telles que le cancer de mani√®re plus pr√©cise avec des mod√®les de vision par ordinateur. Ils le font beaucoup plus rapidement que les m√©thodes traditionnelles. Ils peuvent √©galement pr√©dire les r√©sultats des patients en analysant les motifs dans les donn√©es des patients.

* **D√©couverte et D√©veloppement de M√©dicaments** : Les mod√®les d'apprentissage profond aident √† identifier les candidats potentiels pour les m√©dicaments et √† acc√©l√©rer le processus de d√©veloppement des m√©dicaments, r√©duisant ainsi consid√©rablement le temps et les co√ªts.

#### Finance

* **Trading Algorithme** : Les mod√®les d'apprentissage profond sont utilis√©s pour pr√©dire les tendances du march√© boursier et automatiser les d√©cisions de trading, traitant de vastes quantit√©s de donn√©es financi√®res √† haute vitesse.

* **D√©tection de Fraude** : Les banques et les institutions financi√®res utilisent l'apprentissage profond pour d√©tecter les motifs inhabituels indicatifs d'activit√©s frauduleuses, am√©liorant ainsi la s√©curit√© et la confiance des clients.

#### Automobile et Transport

* **V√©hicules Autonomes** : Les voitures autonomes utilisent √©galement largement l'apprentissage profond pour interpr√©ter les donn√©es des capteurs, leur permettant de naviguer en toute s√©curit√© dans des environnements complexes, en utilisant la vision par ordinateur et d'autres m√©thodes.

* **Gestion du Trafic** : Les mod√®les d'IA analysent les motifs de trafic pour optimiser le flux de trafic et r√©duire la congestion dans les villes.

#### Vente au D√©tail et E-Commerce

* **Exp√©rience de Shopping Personnalis√©e** : Les algorithmes d'apprentissage profond aident dans la vente au d√©tail et le e-commerce √† analyser les donn√©es des clients et √† fournir des recommandations de produits personnalis√©es. Cela am√©liore l'exp√©rience utilisateur et stimule les ventes.

* **Optimisation de la Cha√Æne d'Approvisionnement** : Les mod√®les d'IA pr√©voient la demande, optimisent les stocks et am√©liorent les op√©rations logistiques, am√©liorant l'efficacit√© de la cha√Æne d'approvisionnement.

#### Divertissement et M√©dias

* **Recommandation de Contenu** : Des plateformes comme Netflix et Spotify utilisent l'apprentissage profond pour analyser les pr√©f√©rences des utilisateurs et l'historique de visionnage afin de recommander du contenu personnalis√©.

* **D√©veloppement de Jeux Vid√©o** : L'IA est utilis√©e pour cr√©er des environnements de jeu plus r√©alistes et interactifs, am√©liorant l'exp√©rience du joueur.

#### Technologie et Communications

* **Assistants Virtuels** : Siri, Alexa et autres assistants virtuels utilisent l'apprentissage profond pour le traitement du langage naturel et la reconnaissance vocale, les rendant plus r√©actifs et conviviaux.

* **Services de Traduction de Langue** : Des services comme Google Translate exploitent l'apprentissage profond pour une traduction de langue en temps r√©el et pr√©cise, brisant les barri√®res linguistiques.

#### Fabrication et Production

* **Maintenance Pr√©dictive** : Les mod√®les d'apprentissage profond pr√©disent quand les machines n√©cessitent une maintenance, r√©duisant les temps d'arr√™t et √©conomisant des co√ªts.

* **Contr√¥le de Qualit√©** : Les algorithmes d'IA inspectent et d√©tectent les d√©fauts dans les produits √† haute vitesse avec une plus grande pr√©cision que les inspecteurs humains.

#### Agriculture

* **Surveillance et Analyse des R√©coltes** : Les mod√®les d'IA analysent les images de drones et de satellites pour surveiller la sant√© des r√©coltes, optimiser les pratiques agricoles et pr√©dire les rendements.

#### S√©curit√© et Surveillance

* **Reconnaissance Faciale** : Utilis√©e pour am√©liorer les syst√®mes de s√©curit√©, les mod√®les d'apprentissage profond peuvent identifier avec pr√©cision les individus m√™me dans des environnements bond√©s.

* **D√©tection d'Anomalies** : Les algorithmes d'IA surveillent les images de s√©curit√© pour d√©tecter les activit√©s ou comportements inhabituels, aidant √† la pr√©vention de la criminalit√©.

#### Recherche et Acad√©mie

* **D√©couverte Scientifique** : L'apprentissage profond aide les chercheurs √† analyser des donn√©es complexes, conduisant √† des d√©couvertes dans des domaines comme l'astronomie, la physique et la biologie.

* **Outils √âducatifs** : Les syst√®mes de tutorat pilot√©s par l'IA fournissent des exp√©riences d'apprentissage personnalis√©es, s'adaptant aux besoins individuels des √©tudiants.

L'Apprentissage Profond a consid√©rablement affin√© l'√©tat de l'art de la reconnaissance vocale, de la reconnaissance d'objets, de la compr√©hension de la parole, de la traduction automatis√©e, de la reconnaissance d'images, et de nombreuses autres disciplines telles que la d√©couverte de m√©dicaments et la g√©nomique.

## Chapitre 2 : Fondements des R√©seaux de Neurones

Maintenant, parlons de certaines caract√©ristiques et fonctionnalit√©s cl√©s des R√©seaux de Neurones :

* **Structure en Couches** : Les mod√®les d'apprentissage profond, √† leur c≈ìur, se composent de plusieurs couches, chacune transformant les donn√©es d'entr√©e en repr√©sentations plus abstraites et composites.

* **Hi√©rarchie des Caract√©ristiques** : Les caract√©ristiques simples (comme les bords en reconnaissance d'image) se recombinent d'une couche √† l'autre, pour former des caract√©ristiques plus complexes (comme des objets ou des formes).

* **Apprentissage de Bout en Bout** : Les mod√®les d'apprentissage profond effectuent des t√¢ches √† partir de donn√©es brutes jusqu'aux cat√©gories ou d√©cisions finales, s'am√©liorant souvent avec la quantit√© de donn√©es fournies. Ainsi, les grandes donn√©es jouent un r√¥le cl√© pour l'Apprentissage Profond.

Voici les composants principaux des mod√®les d'Apprentissage Profond :

### Neurones

Ce sont les √©l√©ments de base des r√©seaux de neurones qui re√ßoivent des entr√©es et transmettent leur sortie √† la couche suivante apr√®s avoir appliqu√© une fonction d'activation (plus de d√©tails √† ce sujet dans les chapitres suivants).

### Poids et Biais

Param√®tres du r√©seau de neurones qui sont ajust√©s au cours du processus d'apprentissage pour aider le mod√®le √† faire des pr√©dictions pr√©cises. Ce sont les valeurs que l'algorithme d'optimisation doit optimiser en continu id√©alement en peu de temps pour atteindre le mod√®le le plus optimal et pr√©cis (par exemple, couramment r√©f√©renc√©es par w\_ij et b\_ij ).

**Terme de Biais** : En pratique, un terme de biais ( b ) est souvent ajout√© au produit somme des poids d'entr√©e avant d'appliquer la fonction d'activation. C'est un terme qui permet au neurone de d√©caler la fonction d'activation vers la gauche ou la droite, ce qui peut √™tre crucial pour apprendre des motifs complexes.

**Processus d'Apprentissage** : Les poids sont ajust√©s pendant la phase d'entra√Ænement du r√©seau. Gr√¢ce √† un processus impliquant souvent la descente de gradient, le r√©seau met √† jour de mani√®re it√©rative les poids pour minimiser la diff√©rence entre sa sortie et les valeurs cibles.

**Contexte d'Utilisation** : Ce neurone pourrait faire partie d'un r√©seau plus large, compos√© de plusieurs couches. Les r√©seaux de neurones sont utilis√©s pour r√©soudre une vaste gamme de probl√®mes, allant de la reconnaissance d'images et de la parole √† la pr√©diction des tendances du march√© boursier.

**Correction de la Notation Math√©matique** : L'√©quation fournie dans le texte utilise le symbole ( \\phi ), qui est inhabituel dans ce contexte. Typiquement, une simple sommation ( \\sum ) est utilis√©e pour d√©signer l'agr√©gation des entr√©es pond√©r√©es, suivie de la fonction d'activation ( f ), comme dans

$$f\left(\sum_{i=1}^{n} W_ix_i + b\right)$$

### Fonctions d'Activation

Les fonctions qui introduisent des propri√©t√©s non lin√©aires au r√©seau, lui permettant d'apprendre des motifs de donn√©es complexes. Gr√¢ce aux fonctions d'activation, au lieu d'agir comme si tous les signaux d'entr√©e ou unit√©s cach√©es √©taient √©galement importants, les fonctions d'activation aident √† transformer ces valeurs, ce qui r√©sulte en un mod√®le non lin√©aire beaucoup plus flexible plut√¥t qu'un mod√®le de type lin√©aire.

Chaque neurone dans une couche cach√©e transforme les entr√©es de la couche pr√©c√©dente avec une somme pond√©r√©e suivie d'une fonction d'activation non lin√©aire (c'est ce qui diff√©rencie votre r√©seau de neurones non lin√©aire flexible de la r√©gression lin√©aire commune). Les sorties de ces neurones sont ensuite transmises √† la couche suivante et √† la suivante, et ainsi de suite, jusqu'√† ce que la couche finale soit atteinte.

Nous discuterons des fonctions d'activation en d√©tail dans ce manuel, ainsi que des exemples des 4 fonctions d'activation les plus populaires pour rendre cela tr√®s clair car c'est un concept tr√®s important et une partie cruciale du processus d'apprentissage dans les r√©seaux de neurones.

Ce processus d'entr√©es passant par des couches cach√©es en utilisant la ou les fonctions d'activation et r√©sultant en une sortie est connu sous le nom de propagation avant.

### Architecture des R√©seaux de Neurones

Les r√©seaux de neurones ont g√©n√©ralement trois types de couches : les couches d'entr√©e, les couches cach√©es et les couches de sortie. Apprenons un peu plus sur chacune d'entre elles maintenant.

Nous utiliserons notre exemple de pr√©diction de prix de maison pour en apprendre davantage sur ces couches. Ci-dessous, vous pouvez voir la figure visualisant une architecture simple de r√©seau de neurones que nous allons d√©composer couche par couche.

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-106.png align="left")

*Architecture Simple de R√©seau de Neurones : Entr√©es, Poids et Sorties Expliqu√©s (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

### Couches d'entr√©e

Les couches d'entr√©e sont les couches initiales o√π se trouvent les donn√©es. Elles contiennent les caract√©ristiques que votre mod√®le prend en entr√©e pour ensuite entra√Æner votre mod√®le.

C'est l√† que le r√©seau de neurones re√ßoit ses donn√©es d'entr√©e. Chaque neurone dans la couche d'entr√©e de votre r√©seau de neurones repr√©sente une caract√©ristique des donn√©es d'entr√©e. Si vous avez deux caract√©ristiques, vous aurez deux couches d'entr√©e.

Ci-dessous se trouve la visualisation de l'architecture d'un R√©seau de Neurones Simple, avec N caract√©ristiques d'entr√©e (N signaux d'entr√©e) que vous pouvez voir dans la couche d'entr√©e. Vous pouvez √©galement voir la couche cach√©e unique avec 3 unit√©s cach√©es h1, h2, et h3 et la couche de sortie.

Commen√ßons par la Couche d'Entr√©e et comprenons ce que sont ces caract√©ristiques Z1, Z2, ..., Zn.

![Image](https://www.freecodecamp.org/news/content/images/2024/01/Screenshot-2024-01-31-at-10.57.32-AM.png align="left")

*Architecture Simple de R√©seau de Neurones Mettant en √âvidence les Couches d'Entr√©e (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Dans notre exemple d'utilisation de r√©seaux de neurones pour pr√©dire le prix d'une maison, la couche d'entr√©e prendra des caract√©ristiques de la maison telles que le nombre de chambres, l'√¢ge de la maison, la proximit√© de l'oc√©an, ou s'il y a une piscine, afin d'apprendre sur la maison. C'est ce qui sera donn√© √† la couche d'entr√©e du r√©seau de neurones. Chacune de ces caract√©ristiques sert de neurone d'entr√©e, fournissant au mod√®le des donn√©es essentielles.

Mais alors se pose la question de savoir combien chacune de ces caract√©ristiques devrait contribuer au processus d'apprentissage. Sont-elles toutes √©galement importantes, ou certaines sont-elles plus importantes et devraient contribuer davantage √† l'estimation du prix ?

La r√©ponse √† cette question r√©side dans ce que nous appelons les "poids" que nous avons d√©finis pr√©c√©demment ainsi que les facteurs de biais.

Dans la figure ci-dessus, chaque neurone obtient un poids w\_ij o√π i est l'indice du neurone d'entr√©e et j est l'indice de l'unit√© cach√©e √† laquelle ils contribuent dans la Couche Cach√©e. Ainsi, par exemple, w\_11, w\_12, w\_13 d√©crivent l'importance de la caract√©ristique 1 pour l'apprentissage sur la maison pour l'unit√© cach√©e h1, h2, et h3 respectivement.

Gardez √† l'esprit ces param√®tres de poids car ils sont l'une des parties les plus importantes d'un r√©seau de neurones. Ce sont les poids d'importance que le r√©seau de neurones mettra √† jour pendant le processus d'entra√Ænement, afin d'optimiser le processus d'apprentissage.

### Couches cach√©es

Les couches cach√©es sont la partie centrale de votre mod√®le o√π l'apprentissage se produit. Elles viennent juste apr√®s les Couches d'Entr√©e. Vous pouvez avoir d'une √† plusieurs couches cach√©es.

Simplifions ce concept en regardant notre r√©seau de neurones simple ainsi que notre exemple de prix de maison.

Ci-dessous, j'ai mis en √©vidence la Couche Cach√©e dans notre r√©seau de neurones simple dont l'architecture nous avons vue pr√©c√©demment, que vous pouvez consid√©rer comme une partie tr√®s importante de votre r√©seau de neurones pour extraire des motifs et des relations √† partir des donn√©es qui ne sont pas imm√©diatement apparents au premier regard.

![Image](https://www.freecodecamp.org/news/content/images/2024/01/Screenshot-2024-01-31-at-11.01.01-AM.png align="left")

*Architecture Simple de R√©seau de Neurones Mettant en √âvidence la Couche Cach√©e (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Dans notre exemple d'estimation du prix d'une maison avec un r√©seau de neurones, les couches cach√©es jouent un r√¥le crucial dans le traitement et l'interpr√©tation des informations re√ßues de la couche d'entr√©e, comme les caract√©ristiques de la maison que nous venons de mentionner ci-dessus.

Ces couches sont constitu√©es de neurones qui appliquent des poids et des biais aux caract√©ristiques d'entr√©e ‚Äì comme l'√¢ge de la maison, le nombre de chambres, la proximit√© de l'oc√©an et la pr√©sence d'une piscine ‚Äì pour extraire des motifs et des relations qui ne sont pas imm√©diatement apparents.

Dans ce contexte, les couches cach√©es peuvent apprendre des interd√©pendances complexes entre les caract√©ristiques de la maison, comme la mani√®re dont la combinaison d'un emplacement de choix, de l'√¢ge de la maison et des √©quipements modernes augmente consid√©rablement le prix de la maison.

Elles agissent comme le moteur de calcul du r√©seau de neurones, transformant les donn√©es brutes en informations qui conduisent √† une estimation pr√©cise de la valeur marchande d'une maison. Gr√¢ce √† l'entra√Ænement, les couches cach√©es ajustent ces poids et biais (param√®tres) pour minimiser les erreurs de pr√©diction du mod√®le, am√©liorant progressivement la pr√©cision du mod√®le dans l'estimation des prix des maisons.

Ces couches effectuent la majorit√© des calculs gr√¢ce √† leurs neurones interconnect√©s. Dans cet exemple simple, nous n'avons qu'une seule couche cach√©e et 3 unit√©s cach√©es (par exemple, un autre hyperparam√®tre √† optimiser pendant votre apprentissage en utilisant des techniques telles que [Random Search CV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) ou d'autres).

Mais dans les probl√®mes du monde r√©el, les r√©seaux de neurones sont beaucoup plus profonds et votre nombre de couches cach√©es, avec les param√®tres de poids et de biais, peut d√©passer des milliards avec de nombreuses couches cach√©es.

### Couche de sortie

Les couches de sortie sont le composant final d'un r√©seau de neurones ‚Äì la couche finale qui fournit la sortie du r√©seau de neurones apr√®s toutes les transformations en sortie pour une t√¢che sp√©cifique unique. Cette sortie peut √™tre une valeur unique (dans le cas de la r√©gression par exemple) ou un vecteur (comme dans les grands mod√®les de langage o√π nous produisons un vecteur de probabilit√©s, ou des embeddings).

Une couche de sortie peut √™tre une √©tiquette de classe pour un mod√®le de classification, une valeur num√©rique continue pour un mod√®le de r√©gression, ou m√™me un vecteur de nombres, selon la t√¢che.

Les couches cach√©es dans un r√©seau de neurones sont l'endroit o√π l'apprentissage r√©el se produit, o√π le r√©seau d'apprentissage profond apprend √† partir des donn√©es en extrayant et en transformant les caract√©ristiques fournies.

√Ä mesure que les donn√©es p√©n√®trent plus profond√©ment dans le r√©seau, les caract√©ristiques deviennent plus abstraites et plus composites, chaque couche s'appuyant sur la sortie/valeurs des couches pr√©c√©dentes. La profondeur et la largeur (nombre de neurones) des couches cach√©es sont des facteurs cl√©s dans la capacit√© du r√©seau √† apprendre des motifs complexes. Ci-dessous se trouve le diagramme que nous avons vu pr√©c√©demment montrant l'architecture des r√©seaux de neurones simples.

![Image](https://www.freecodecamp.org/news/content/images/2024/01/Screenshot-2024-01-31-at-11.27.39-AM.png align="left")

*Architecture Simple de R√©seau de Neurones Mettant en √âvidence la Sortie (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Dans notre exemple de pr√©diction de prix de maison, l'aboutissement du processus d'apprentissage est repr√©sent√© par la couche de sortie, qui repr√©sente notre objectif final : le prix pr√©dit de la maison.

Une fois les caract√©ristiques d'entr√©e ‚Äì comme le nombre de chambres, l'√¢ge de la maison, la distance √† l'oc√©an et la pr√©sence d'une piscine ‚Äì aliment√©es dans le r√©seau de neurones, elles traversent une ou plusieurs couches cach√©es du r√©seau de neurones. C'est au sein de ces couches cach√©es que le r√©seau de neurones d√©couvre des motifs complexes et des interconnexions dans les donn√©es.

Enfin, cette information trait√©e atteint la couche de sortie, o√π le mod√®le consolide toutes ses d√©couvertes et produit les r√©sultats ou pr√©dictions finaux, dans ce cas, le prix de la maison.

Ainsi, la couche de sortie consolide toutes les informations acquises. Ces transformations sont appliqu√©es tout au long des couches cach√©es pour produire une seule valeur : le prix pr√©dit de la maison (souvent appel√© Y^, prononc√© "Y hat").

Cette pr√©diction est l'estimation par le r√©seau de neurones de la valeur marchande de la maison, bas√©e sur sa compr√©hension apprise de la mani√®re dont diff√©rentes caract√©ristiques de la maison affectent le prix de la maison. Elle d√©montre la capacit√© du r√©seau √† synth√©tiser des donn√©es complexes en informations exploitables, dans ce cas, en produisant une pr√©diction de prix pr√©cise, gr√¢ce √† son mod√®le optimis√©.

### Fonctions d'activation

Les [fonctions d'activation](https://en.wikipedia.org/wiki/Activation_function) introduisent des propri√©t√©s non lin√©aires dans le mod√®le de r√©seau de neurones, ce qui permet au mod√®le d'apprendre des motifs plus complexes.

Sans non-lin√©arit√©, votre r√©seau profond se comporterait comme un [perceptron](https://www.freecodecamp.org/news/the-history-of-ai/#the-perceptron) √† une seule couche, qui ne peut apprendre que des [fonctions lin√©airement s√©parables](https://en.wikipedia.org/wiki/Linear_separability). Les fonctions d'activation d√©finissent comment les neurones doivent √™tre activ√©s ‚Äì d'o√π le nom de fonction d'activation.

Les fonctions d'activation servent de pont entre les signaux d'entr√©e re√ßus par le r√©seau et la sortie qu'il g√©n√®re. Ces fonctions d√©terminent comment la somme pond√©r√©e des neurones d'entr√©e ‚Äì chacun repr√©sentant une caract√©ristique sp√©cifique comme le nombre de chambres, l'√¢ge de la maison, la proximit√© de l'oc√©an et la pr√©sence d'une piscine ‚Äì doit √™tre transform√©e ou "activ√©e" pour contribuer au processus d'apprentissage du r√©seau.

Les fonctions d'activation sont une partie extr√™mement importante de l'entra√Ænement des r√©seaux de neurones. Lorsque le r√©seau est compos√© de couches cach√©es et de couches de sortie, vous devez choisir une fonction d'activation pour les deux (diff√©rentes fonctions d'activation peuvent √™tre utilis√©es dans diff√©rentes parties du mod√®le). Le choix de la fonction d'activation a un impact √©norme sur les performances et les capacit√©s du r√©seau de neurones.

Chacun des signaux entrants ou connexions est dynamiquement renforc√© ou affaibli en fonction de leur fr√©quence d'utilisation (c'est ainsi que nous apprenons de nouvelles id√©es et concepts). C'est la force de chaque connexion qui d√©termine la contribution de l'entr√©e √† la sortie des neurones.

Apr√®s avoir √©t√© pond√©r√©s par la force de leurs signaux respectifs, les entr√©es sont somm√©es ensemble dans le **corps cellulaire**. Cela est ensuite transform√© en un nouveau signal qui est transmis ou propag√© le long de l'*axone* des cellules et envoy√© √† d'autres neurones. Ce travail fonctionnel de la fonction d'activation peut √™tre repr√©sent√© math√©matiquement comme suit :

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-107.png align="left")

*Activation des Neurones : Transformation des Entr√©es Pond√©r√©es en Sorties (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Ici, nous avons les entr√©es x1, x2, ...xn et leurs poids correspondants w1, w2, ... wn, et nous les agr√©geons en une seule valeur de Y en utilisant la fonction d'activation f.

Cette figure est une version simplifi√©e d'un neurone au sein d'un r√©seau de neurones artificiels. Chaque entr√©e ( X\_i ) est associ√©e √† un poids correspondant ( W\_i ), et ces produits sont agr√©g√©s pour calculer la sortie ( Y ) du neurone. Le X\_i est la valeur d'entr√©e du signal i (comme le nombre de chambres de la maison, en tant que caract√©ristique d√©crivant la maison). Son poids d'importance par w\_i correspond √† chaque X\_i, donc la somme de toutes ces valeurs d'entr√©e pond√©r√©es peut √™tre exprim√©e comme suit :

$$\phi\left(\sum_{i=1}^{m} w_i x_i\right)$$

Dans cette √©quation, phi repr√©sente la fonction que nous utilisons pour joindre les signaux de diff√©rents neurones d'entr√©e en une seule valeur. Cette fonction est appel√©e la Fonction d'Activation.

Chaque synapse se voit attribuer un poids, une valeur d'importance. Ces poids et biais constituent la pierre angulaire de l'apprentissage des R√©seaux de Neurones. Ces poids et biais d√©terminent si les signaux sont transmis ou non, ou dans quelle mesure chaque signal est transmis.

Dans le contexte de la pr√©diction des prix des maisons, apr√®s que les caract√©ristiques d'entr√©e ont √©t√© pond√©r√©es selon leur pertinence apprise lors de l'entra√Ænement, la fonction d'activation entre en jeu. Elle prend cette somme pond√©r√©e des entr√©es et applique une op√©ration math√©matique sp√©cifique pour produire un score d'activation.

Ce score est une valeur unique qui repr√©sente efficacement les informations d'entr√©e agr√©g√©es. Il permet au r√©seau de prendre des d√©cisions ou des pr√©dictions complexes bas√©es sur les donn√©es d'entr√©e qu'il re√ßoit.

Essentiellement, les fonctions d'activation sont le m√©canisme par lequel les r√©seaux de neurones convertissent la somme pond√©r√©e d'une entr√©e en une sortie qui a du sens dans le contexte du probl√®me sp√©cifique √† r√©soudre (comme l'estimation du prix d'une maison ici). Elles permettent au r√©seau d'apprendre des relations non lin√©aires entre les caract√©ristiques et les r√©sultats, permettant la pr√©diction pr√©cise de la valeur marchande d'une maison √† partir de ses caract√©ristiques.

La fonction d'activation moderne par d√©faut ou la plus populaire pour les couches cach√©es est l'Unit√© Lin√©aire Rectifi√©e (ReLU) ou la fonction Softmax, principalement pour des raisons de pr√©cision et de performance. Pour la couche de sortie, la fonction d'activation est principalement choisie en fonction du format des pr√©dictions (probabilit√©, scalaire, etc.).

Chaque fois que vous envisagez une fonction d'activation, soyez conscient du **Probl√®me du Gradient √âvanescent** (nous reviendrons sur ce sujet plus tard). Cela se produit lorsque les gradients sont trop petits ou trop grands, ce qui peut rendre le processus d'apprentissage difficile.

Certaines fonctions d'activation comme la sigmo√Øde ou la tanh peuvent provoquer des gradients √©vanescents dans les r√©seaux profonds, tandis que certaines d'entre elles peuvent aider √† att√©nuer ce probl√®me.

Examinons maintenant quelques autres types de fonctions d'activation, et quand/comment elles sont utiles.

#### **Fonction d'Activation Lin√©aire**

Une Fonction d'Activation Lin√©aire peut √™tre exprim√©e comme suit :

$$f(z) = z$$

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-109.png align="left")

*Fonction d'Activation Lin√©aire (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Ce graphique montre une fonction d'activation lin√©aire pour un r√©seau de neurones, d√©finie par *f*(*z*)=*z*. O√π z est l'entr√©e (appel√©e Z-scores comme nous l'avons mentionn√© pr√©c√©demment) pour la fonction d'activation f( ). Cela signifie que la sortie est directement proportionnelle √† l'entr√©e.

Les Fonctions d'Activation Lin√©aires sont les fonctions d'activation les plus simples, et elles sont relativement faciles √† calculer. Mais elles ont une limitation importante : les NNs avec seulement des neurones lin√©aires peuvent √™tre exprim√©s comme un r√©seau sans couches cach√©es ‚Äì mais les couches cach√©es dans les NNs sont ce qui leur permet d'apprendre des caract√©ristiques importantes √† partir des signaux d'entr√©e.

Ainsi, afin d'apprendre des motifs complexes √† partir de probl√®mes complexes, nous avons besoin de Fonctions d'Activation plus avanc√©es plut√¥t que de Fonctions Lin√©aires.

Vous pouvez utiliser une fonction lin√©aire, par exemple, dans la derni√®re couche de sortie lorsque le r√©sultat brut est suffisant pour vous et que vous ne souhaitez aucune transformation. Mais 99% du temps, cette fonction d'activation est inutile en Apprentissage Profond.

#### **Fonction d'Activation Sigmo√Øde**

L'une des fonctions d'activation les plus populaires est la Fonction d'Activation Sigmo√Øde, qui peut √™tre exprim√©e comme suit :

$$f(z) = \frac{1}{1 + e^{-z}}$$

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-111.png align="left")

*Fonction d'Activation Sigmo√Øde (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Dans cette figure, la fonction d'activation sigmo√Øde est visualis√©e, qui est une courbe lisse en forme de S couramment utilis√©e dans les r√©seaux de neurones. Si vous √™tes familier avec la R√©gression Logistique, alors cette fonction vous semblera famili√®re √©galement. Cette fonction transforme toutes les valeurs d'entr√©e en valeurs dans la plage de (0,1) ce qui est tr√®s pratique lorsque vous voulez que le mod√®le fournisse une sortie sous forme de probabilit√©s ou d'un %.

En gros, lorsque le logit est tr√®s petit, la sortie d'un neurone logistique est tr√®s proche de 0. Lorsque le logit est tr√®s grand, la sortie du neurone logistique est plus proche de 1. Entre ces deux valeurs extr√™mes, le neurone prend une forme de S. Cette forme de S de la courbe aide √©galement √† diff√©rencier les sorties qui sont proches de 0 ou proches de 1, fournissant une fronti√®re de d√©cision claire.

Vous utiliserez souvent la Fonction d'Activation Sigmo√Øde dans la couche de sortie, car elle est id√©ale pour les cas o√π l'objectif est d'obtenir une valeur du mod√®le en sortie entre 0 et 1 (une probabilit√© par exemple). Donc, si vous avez un probl√®me de classification, envisagez d√©finitivement cette fonction d'activation.

Mais gardez √† l'esprit que cette activation est tr√®s intensive et qu'un grand nombre de neurones seront activ√©s. C'est aussi pourquoi, pour les unit√©s cach√©es, l'activation Sigmo√Øde n'est pas la meilleure option, car elle fixe les grandes valeurs aux limites de 0 et 1, provoquant rapidement la constance des param√®tres ‚Üí pas de gradients (utilis√©s pour mettre √† jour les poids et les facteurs de biais).

C'est le c√©l√®bre **Probl√®me du Gradient √âvanescent** (plus de d√©tails √† ce sujet dans les prochains chapitres). Cela entra√Æne l'incapacit√© du mod√®le √† apprendre avec pr√©cision √† partir des donn√©es et √† produire des pr√©dictions pr√©cises.

#### **ReLU (Unit√© Lin√©aire Rectifi√©e)**

Un type diff√©rent de relation non lin√©aire est d√©couvert lors de l'utilisation de l'**Unit√© Lin√©aire Rectifi√©e (ReLU)**. Cette fonction d'activation est moins stricte et fonctionne bien lorsque votre attention est port√©e sur les valeurs positives.

La fonction d'activation ReLU active les neurones qui ont des valeurs positives mais d√©sactive les valeurs n√©gatives, contrairement √† la fonction Sigmo√Øde qui active presque tous les neurones. Cette fonction d'activation peut √™tre exprim√©e comme suit :

$$f(z) = \begin{cases} 0 & \text{si } z < 0 \\ z & \text{si } z \geq 0 \end{cases}$$

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-114.png align="left")

*Fonction d'Activation ReLU (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Comme vous pouvez le voir ci-dessus √† partir de cette visualisation, la fonction d'activation ReLU n'active pas du tout les neurones d'entr√©e avec des valeurs n√©gatives (vous pouvez voir que pour les x qui sont n√©gatifs, la valeur correspondante de l'axe Y est 0). Alors que pour les entr√©es positives x, la fonction d'activation retourne la valeur exacte x (Y=X ligne lin√©aire comme vous le voyez sur la figure). Mais c'est toujours un bon choix par d√©faut pour les couches cach√©es. Elle est efficacement calculable et r√©duit la probabilit√© de gradients √©vanescents pendant l'entra√Ænement, surtout pour les r√©seaux profonds.

#### Fonction d'Activation Leaky ReLU

Alors que ReLU n'active pas les neurones d'entr√©e avec des valeurs n√©gatives, Leaky ReLU prend en compte ces valeurs d'entr√©e n√©gatives. Elle apprend √† partir de celles-ci, bien qu'avec un taux plus faible √©gal √† 0,01.

Cette fonction d'activation peut √™tre exprim√©e comme suit :

$$f(z) = \begin{cases} 0.01z & \text{si } z < 0 \\ z & \text{si } z \geq 0 \end{cases}$$

Ainsi, Leaky ReLU permet un petit gradient non nul lorsque la valeur d'entr√©e est satur√©e et non active.

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-116.png align="left")

*Fonction d'Activation Leaky ReLU (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Cette visualisation montre la fonction d'activation Leaky ReLU couramment utilis√©e dans les r√©seaux de neurones, en particulier pour les couches cach√©es et o√π les activations n√©gatives sont acceptables. Contrairement au ReLU standard, qui donne une sortie de z√©ro pour toute entr√©e n√©gative, Leaky ReLU permet une petite sortie non nulle pour les entr√©es n√©gatives.

Comme ReLU, Leaky ReLU est √©galement un bon choix par d√©faut pour les couches cach√©es. Elle est efficacement calculable et r√©duit la probabilit√© de gradients √©vanescents pendant l'entra√Ænement, en particulier pour les r√©seaux profonds avec plusieurs couches cach√©es. Nous parlerons davantage de ces fonctions d'activation et des pr√©c√©dentes lors de la discussion sur le Probl√®me du Gradient √âvanescent, et si vous souhaitez plus de d√©tails et que le concept soit expliqu√© dans un tutoriel, consultez les ressources ci-dessous.

### Fonction d'Activation Tangente Hyperbolique (Tanh)

La fonction d'activation tangente hyperbolique est souvent simplement appel√©e fonction **Tanh**. Elle est tr√®s similaire √† la fonction d'activation sigmo√Øde. Elle a m√™me la m√™me repr√©sentation en forme de S.

Cette fonction prend n'importe quelle valeur r√©elle comme valeur d'entr√©e et produit une valeur dans la plage -1 √† 1. Cette fonction d'activation peut √™tre exprim√©e comme suit :

$$f(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-118.png align="left")

*Fonction d'Activation Tanh (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

La figure montre la fonction d'activation tanh (tangente hyperbolique). Ainsi, cette fonction produit des valeurs allant de -1 √† 1, fournissant une sortie normalis√©e qui peut aider √† la convergence des r√©seaux de neurones pendant l'entra√Ænement. Elle est similaire √† la fonction sigmo√Øde mais est ajust√©e pour permettre des sorties n√©gatives, ce qui peut √™tre b√©n√©fique pour certains types de r√©seaux de neurones o√π la moyenne des sorties doit √™tre centr√©e autour de z√©ro.

Note - si vous souhaitez obtenir plus de d√©tails sur ces fonctions d'activation, consultez ce tutoriel o√π je couvre ce concept plus en d√©tail √† l'adresse ["Qu'est-ce qu'une Fonction d'Activation"](https://www.youtube.com/watch?v=03-0UdyzWg4) et ["Comment R√©soudre le Probl√®me du Gradient √âvanescent"](https://www.youtube.com/watch?v=HEeOBaFNXV4).

Encore une fois, la fonction d'activation par d√©faut ou la plus populaire pour les couches cach√©es est l'Unit√© Lin√©aire Rectifi√©e (ReLU) ou la fonction Softmax, principalement pour des raisons de pr√©cision/performance. Pour la couche de sortie, la fonction d'activation est principalement choisie en fonction du format des pr√©dictions (probabilit√©, scalaire, etc.).

## Chapitre 3 : Comment Entra√Æner les R√©seaux de Neurones

L'entra√Ænement des r√©seaux de neurones est un processus syst√©matique qui implique deux processus principaux, effectu√©s de mani√®re r√©p√©t√©e, appel√©s passes avant et arri√®re.

Tout d'abord, les donn√©es passent par la Passe Avant jusqu'√† la sortie. Ensuite, elle est suivie par une passe arri√®re. L'id√©e derri√®re ce processus est de parcourir le r√©seau √† plusieurs reprises pour ajuster les poids et minimiser les fonctions de perte ou de co√ªt.

Pour mieux comprendre, nous allons examiner un R√©seau de Neurones simple o√π nous avons 3 signaux d'entr√©e, et une seule couche cach√©e qui a 4 unit√©s cach√©es. Cela peut √™tre visualis√© comme suit :

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-125.png align="left")

*De la Couche d'Entr√©e √† travers les Couches Cach√©es jusqu'√† la Pr√©diction (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Ici, vous pouvez voir que nous avons 3 signaux d'entr√©e dans notre couche d'entr√©e, 1 couche cach√©e avec 4 unit√©s cach√©es, et 1 couche de sortie. Il s'agit d'un graphe de calcul visualisant ce r√©seau de neurones de base et la mani√®re dont l'information circule de la gauche, les entr√©es initiales, vers la droite, jusqu'√† la pr√©diction Y^ (Y hat), apr√®s √™tre pass√©e par plusieurs transformations.

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-126.png align="left")

*Propagation Avant et Arri√®re dans les R√©seaux de Neurones (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Maintenant, examinons cette figure qui montre l'id√©e g√©n√©rale du flux d'informations.

* Nous partons de l'entr√©e X (que nous d√©finissons par A\[0\] comme les activations initiales)

* Ensuite, √† chaque √©tape (index√©e par \[1\]) nous prenons la matrice des poids (W\[1\] et le vecteur de biais b\[1\]) et calculons les scores Z (Z\[1\])

* Ensuite, nous appliquons la fonction d'activation pour obtenir les scores d'activation (A\[1\]) au niveau \[1\]. Cela se produit √† l'√©tape de temps 1, qui dans notre exemple est la couche cach√©e 1.

Comme nous obtenons une seule couche, l'√©tape suivante est la couche de sortie, o√π les informations de la couche pr√©c√©dente (A\[1\]) sont utilis√©es pour calculer les nouveaux scores Z\[2\] en combinant l'entr√©e A\[1\] de la couche pr√©c√©dente et avec W\[2\] / b\[2\] de cette couche. Nous appliquons ensuite une autre couche d'activation (notre fonction d'activation de la couche de sortie) sur le Z\[2\] nouvellement calcul√© pour calculer le A\[2\].

Comme le A\[2\] est dans la couche de sortie, cela nous donne notre pr√©diction, Y\_hat. C'est la Passe Avant ou Propagation Avant.

Ensuite, vous pouvez voir dans la deuxi√®me partie de la figure, nous passons de Y\_hat √† tous ces termes qui sont en quelque sorte les m√™mes que dans la passe avant mais avec une diff√©rence cruciale : ils ont tous un **"d"** devant eux, qui fait r√©f√©rence √† la "d√©riv√©e".

Ainsi, apr√®s que Y\_hat est produit, nous obtenons nos pr√©dictions, et le r√©seau est capable de comparer le Y\_hat (valeurs pr√©dites de la variable de r√©ponse y, dans notre exemple le prix de la maison) aux vrais prix des maisons Y et d'obtenir la fonction de perte.

Si vous souhaitez en savoir plus sur les fonctions de perte, consultez [ici](https://en.wikipedia.org/wiki/Loss_function) ou ce [tutoriel](https://www.youtube.com/watch?v=1I-3Tdk2-Hg).

Ensuite, le r√©seau calcule la d√©riv√©e de la fonction de perte par rapport aux activations A et au score Z (dA et dZ). Ensuite, il utilise ceux-ci pour calculer les gradients/d√©riv√©es par rapport aux poids W et aux biais b (dW et db).

Cela se produit √©galement par couche et de mani√®re s√©quentielle, mais comme vous pouvez le voir √† partir de la fl√®che dans la figure ci-dessus, cette fois cela se produit √† l'envers de droite √† gauche contrairement √† la propagation avant.

C'est aussi pourquoi nous appelons ce processus r√©tropropagation. Les gradients de la couche 2 contribuent au calcul des gradients dans la couche 1, comme vous pouvez √©galement le voir √† partir du graphe.

### Passe Avant

La propagation avant est le processus d'alimentation des donn√©es d'entr√©e √† travers un r√©seau de neurones pour g√©n√©rer une sortie. Nous d√©finirons les donn√©es d'entr√©e par X qui contient 3 caract√©ristiques X1, X2, X3 qui peuvent √™tre d√©crites math√©matiquement comme suit :

zi=ùúèTxi+b  
ùüì  
y^i=ai=ùúπ(zi)  
ùüì  
l(ai,yi)

O√π dans ces √©quations nous passons de l'entr√©e x\_i dans notre r√©seau de neurones simple, au calcul de la perte.

D√©composons-les :

**√âtape 1 :** Chaque neurone dans les couches suivantes calcule une somme pond√©r√©e de ses entr√©es (x^i) plus un terme de biais b. Nous appelons cela un score z^i. Les entr√©es sont les sorties des neurones de la couche pr√©c√©dente, et les poids ainsi que le biais sont les param√®tres que le r√©seau de neurones vise √† apprendre et √† estimer.

**√âtape 2 :** Ensuite, en utilisant une fonction d'activation, que nous d√©signons par la lettre grecque delta, le r√©seau transforme les scores Z en une nouvelle valeur que nous d√©finissons par a^i. Notez que la valeur d'activation au passage initial lorsque nous sommes √† la couche initiale dans le r√©seau (couche 0) est √©gale √† x^i. C'est alors la valeur pr√©dite dans ce passage sp√©cifique.

Pour √™tre plus pr√©cis, compliquons un peu notre notation. Nous d√©finirons chaque score dans la premi√®re couche cach√©e, couche \[1\], par unit√© (car nous avons 4 unit√©s dans cette unit√© cach√©e) et g√©n√©raliserons cela par unit√© cach√©e *i* :

zi\[1\]=(ùúèi\[1\])Tx+(bi\[1\])Tforùü∂i=1,2,3,4  
ai\[1\]=ùúπ(zi\[1\])

R√©√©crivons maintenant cela en utilisant l'Alg√®bre Lin√©aire et sp√©cifiquement les op√©rations de matrices et de vecteurs :

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-132.png align="left")

*Op√©rations de Matrices dans les Calculs de R√©seaux de Neurones (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Cette image pr√©sente une mani√®re de repr√©senter les calculs dans une couche de r√©seau de neurones en utilisant des op√©rations de matrices de l'Alg√®bre Lin√©aire. Elle montre comment les calculs individuels pour chaque neurone dans une couche peuvent √™tre exprim√©s de mani√®re compacte et effectu√©s simultan√©ment en utilisant la multiplication de matrices et la sommation.

La matrice √©tiquet√©e W^\[1\] contient les poids appliqu√©s aux entr√©es pour chaque neurone dans la premi√®re couche cach√©e. Le vecteur X\[1\] est l'entr√©e de la couche. En multipliant la matrice des poids par le vecteur d'entr√©e puis en ajoutant le vecteur de biais b\[1\], nous obtenons le vecteur Z\[1\], que nous avons √©galement appel√© score Z pr√©c√©demment et qui repr√©sente la somme pond√©r√©e des entr√©es plus le biais pour chaque neurone.

Cette forme compacte nous permet d'utiliser des routines efficaces d'alg√®bre lin√©aire pour calculer les sorties de tous les neurones de la couche en une seule fois.

Cette approche est fondamentale dans les r√©seaux de neurones car elle permet le traitement des entr√©es √† travers plusieurs couches de mani√®re efficace, permettant aux r√©seaux de neurones de s'adapter √† un grand nombre de neurones et √† des architectures complexes.

Ainsi, nous passons du niveau unitaire √† la repr√©sentation des transformations dans nos r√©seaux de neurones simples en utilisant la multiplication de matrices et les sommations de l'Alg√®bre Lin√©aire.

#### Activation de la Premi√®re Couche

Maintenant, examinons cette √©quation qui montre l'id√©e g√©n√©rale du flux d'informations lorsque nous passons de l'entr√©e X\[1\] (que nous d√©finissons par A\[0\] comme les activations initiales) puis par √©tape (index√©e par \[1\]) nous prenons la matrice des poids (W\[1\] et le vecteur de biais b\[1\]) et calculons les scores Z (Z\[1\]). Ensuite, nous appliquons la fonction d'activation de la couche 1, g\[1\] pour obtenir les scores d'activation (A\[1\]) au niveau \[1\]. Cela se produit √† l'√©tape de temps 1, qui dans notre exemple est la couche cach√©e 1.

#### Activation de la Deuxi√®me Couche (Couche de Sortie)

Comme nous obtenons une seule couche, l'√©tape suivante est la couche de sortie, o√π les informations de la couche pr√©c√©dente (A\[1\]) sont utilis√©es pour calculer les nouveaux scores Z\[2\] en combinant l'entr√©e A\[1\] de la couche pr√©c√©dente et avec W\[2\] / b\[2\] de cette couche. Nous appliquons ensuite une autre fonction d'activation g\[2\] (notre fonction d'activation de la couche de sortie) sur le Z\[2\] nouvellement calcul√© pour calculer le A\[2\].

Apr√®s que la fonction d'activation a √©t√© appliqu√©e, elle peut ensuite √™tre aliment√©e dans la couche suivante du r√©seau s'il y en a une, ou directement dans la couche de sortie si c'est un r√©seau √† une seule couche cach√©e. Comme dans notre cas, la couche 2 est notre couche de sortie, nous sommes pr√™ts √† passer √† Y\_hat, nos pr√©dictions.

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-137.png align="left")

*Flux de Donn√©es S√©quentiel √† Travers les Couches du R√©seau de Neurones (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Cette image montre une mani√®re de repr√©senter les calculs dans une couche de r√©seau de neurones en utilisant des op√©rations de matrices. Elle montre comment les calculs individuels pour chaque neurone dans une couche de r√©seau de neurones peuvent √™tre exprim√©s de mani√®re compacte, effectu√©s simultan√©ment par multiplication de matrices et addition.

Ici, la matrice √©tiquet√©e W\[1\] contient les poids appliqu√©s aux entr√©es pour chaque neurone dans la premi√®re couche cach√©e. Le vecteur *X*\[1\] est l'entr√©e de cette couche. En multipliant la matrice des poids par le vecteur d'entr√©e puis en ajoutant le vecteur de biais b\[1\], nous obtenons le vecteur Z\[1\], qui repr√©sente la somme pond√©r√©e des entr√©es plus le biais pour chaque neurone.

Cette forme compacte nous permet d'utiliser des routines efficaces d'alg√®bre lin√©aire pour calculer les sorties de tous les neurones de la couche en une seule fois. Le vecteur r√©sultant Z\[1\] est ensuite pass√© √† travers une fonction d'activation (non montr√©e dans cette partie de l'image), qui effectue une transformation non lin√©aire sur chaque √©l√©ment, r√©sultant en la sortie finale de la couche.

Cette approche est fondamentale dans les r√©seaux de neurones car elle permet le traitement des entr√©es √† travers plusieurs couches de mani√®re efficace, permettant aux r√©seaux de neurones de s'adapter √† un grand nombre de neurones et √† des architectures complexes.

#### Calcul de la Fonction de Perte

Comme le A\[2\] est dans la couche de sortie, cela nous donne notre pr√©diction, Y\_hat. Apr√®s que Y\_hat est produit, nous avons nos pr√©dictions, et le r√©seau est capable de comparer le Y\_hat (valeurs pr√©dites de la variable de r√©ponse y, dans notre exemple le prix de la maison) aux vrais prix des maisons Y, et d'obtenir la fonction de perte *J.* La perte totale peut √™tre calcul√©e comme suit :

o√π log() est le logarithme utilis√© pour calculer cette fonction de perte.

### Passe Arri√®re

La r√©tropropagation est une partie cruciale du processus d'entra√Ænement d'un r√©seau de neurones. Combin√©e avec des algorithmes d'optimisation comme la Descente de Gradient (GD), la Descente de Gradient Stochastique (SGD), ou Adam, ils effectuent la Passe Arri√®re.

La r√©tropropagation est un algorithme efficace pour calculer le gradient de la fonction de co√ªt (perte) (J) par rapport √† chaque param√®tre (poids & biais) dans le r√©seau.

Ainsi, pour √™tre clair, la r√©tropropagation est le processus r√©el de calcul des gradients dans le mod√®le, et ensuite la Descente de Gradient est l'algorithme qui prend les gradients en entr√©e et met √† jour les param√®tres.

Lorsque nous calculons les gradients et les utilisons pour mettre √† jour les param√®tres dans le mod√®le, cela nous aide √† mettre √† jour les param√®tres et √† les diriger vers une direction plus correcte pour trouver l'optimum global afin de minimiser. Cela aide √† minimiser davantage la fonction de perte et √† am√©liorer la pr√©cision de pr√©diction du mod√®le.

√Ä chaque passe, apr√®s que la propagation avant est termin√©e, les gradients doivent √™tre obtenus. Ensuite, nous les utilisons pour obtenir les param√®tres du mod√®le, tels que les param√®tres de poids et de biais.

Regardons un exemple de calculs de gradients pour la r√©tropropagation dans un r√©seau de neurones que nous avons vu dans la Propagation Avant avec une seule couche cach√©e et 4 unit√©s cach√©es.

La r√©tropropagation commence toujours par la fin, alors visualisons-la pour vous aider √† comprendre ce processus :

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-139.png align="left")

*Processus de R√©tropropagation dans les R√©seaux de Neurones : Calcul des Gradients (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Dans cette figure, le r√©seau calcule la d√©riv√©e de la fonction de perte par rapport aux activations A et au score Z (dA et dZ). Il utilise ensuite ceux-ci pour calculer les gradients/d√©riv√©es par rapport aux poids W et aux biais b (dW et db). Cela se produit √©galement par couche et de mani√®re s√©quentielle, mais comme vous pouvez le voir √† partir de la fl√®che dans la figure, cette fois cela se produit √† l'envers de droite √† gauche contrairement √† la propagation avant.

C'est aussi pourquoi nous appelons ce processus r√©tropropagation. Les gradients de la couche 2 contribuent au calcul des gradients dans la couche 1 comme vous pouvez √©galement le voir √† partir du graphe.

Ainsi, l'id√©e est que nous calculons les gradients par rapport √† l'activation (dA\[2\]), puis par rapport √† la pr√©-activation (dZ\[2\]), et par rapport aux poids (dW\[2\]) et au biais (db\[2\]) de la couche de sortie, en supposant que nous avons une fonction de co√ªt J apr√®s avoir calcul√© le Y^. Assurez-vous de toujours mettre en cache les Z\[i\] car ils sont n√©cessaires dans ce processus.

Math√©matiquement, les gradients peuvent √™tre calcul√©s en utilisant les r√®gles de diff√©rentiation courantes, y compris l'obtention de la d√©riv√©e du logarithme, et en utilisant la **R√®gle de la Somme** et les **R√®gles de la Cha√Æne**. Le premier gradient dA\[2\] peut √™tre exprim√© comme suit :

Le gradient suivant que nous devons calculer est le gradient de la fonction de co√ªt par rapport √† Z\[2\], c'est-√†-dire dZ\[2\].

Nous savons ce qui suit :

A\[2\]=ùúπ(Z\[2\])ùü∞  
dJdA\[2\]=dA\[2\]dZ\[2\]ùü∞  
dA\[2\]dZ\[2\]=ùúπùü≤(Z\[2\])

Ainsi, A\[2\] = ùúπ(Z\[2\]), nous pouvons alors utiliser ces d√©riv√©es de la fonction sigmo√Øde ùúπ'(Z\[2\]) = ùúπ(Z\[2\]) \* (1 - ùúπ(Z\[2\])). Cela peut √™tre d√©riv√© math√©matiquement comme suit :

$$\begin{align*} \frac{dZ^{[2]}}{dJ} &= \frac{dJ}{dZ^{[2]}} \\ \downarrow \\ \frac{dZ^{[2]}}{dJ} &= \frac{dJ}{dA^{[2]}} \cdot \frac{dA^{[2]}}{dZ^{[2]}} \quad \text{en utilisant la r√®gle de la cha√Æne} \\ \downarrow \\ \frac{dZ^{[2]}}{dJ} &= dA^{[2]} \cdot \sigma'(Z^{[2]}) \\ \downarrow \\ \frac{dZ^{[2]}}{dJ} &= dA^{[2]} \cdot A^{[2]} \cdot (1 - A^{[2]}) \end{align*}$$

$$\begin{align*} \sigma(Z^{[2]}) &= \frac{1}{1 - e^{Z^{[2]}}} = (1 - e^{-Z^{[2]}})^{-1} \\ \downarrow \\ \sigma'(Z^{[2]}) &= \frac{d\sigma(Z^{[2]})}{dZ^{[2]}} \\ \downarrow \\ \sigma'(Z^{[2]}) &= -\frac{-1}{(1 - e^{Z^{[2]}})^2} \cdot (-1) \cdot e^{Z^{[2]}} \\ \downarrow \\ \sigma'(Z^{[2]}) &= \frac{1}{1 - e^{Z^{[2]}}} \cdot \frac{e^{Z^{[2]}}}{1 - e^{Z^{[2]}}} \\ \downarrow \\ \sigma'(Z^{[2]}) &= \sigma(Z^{[2]}) \cdot (1 - \sigma(Z^{[2]})) = A^{[2]} \cdot (1 - A^{[2]}) \end{align*}$$

Maintenant que nous savons le comment et le pourquoi derri√®re le calcul du gradient par rapport au score Z, nous pouvons calculer le gradient par rapport au poids W. Cela est tr√®s important pour la mise √† jour de la valeur du param√®tre de poids (par exemple, la direction).

$$\begin{align*} Z^{[2]} &= W^{[2]T} \cdot A^{[1]} + b^{[2]} \\ \downarrow \\ \frac{db^{[2]}}{dZ^{[2]}} &= \frac{dJ}{dZ^{[2]}} \cdot \frac{dZ^{[2]}}{db^{[2]}} \quad \text{en utilisant la r√®gle de la cha√Æne} \\ \downarrow \\ db^{[2]} &= dZ^{[2]} \cdot 1 + 0 \quad \text{en utilisant la r√®gle de la constante} \\ \downarrow \\ db^{[2]} &= dZ^{[2]} \end{align*}$$

Maintenant, dans cette √©tape, la seule chose restante est de calculer le gradient par rapport au biais, notre deuxi√®me param√®tre b, dans la couche cach√©e, couche 2.

$$\begin{align*} Z^{[2]} = W^{[2]T} \cdot A^{[1]} + b^{[2]} \\ \frac{db^{[2]}}{dJ} = \frac{dJ}{dZ^{[2]}} \cdot \frac{dZ^{[2]}}{db^{[2]}} \quad \text{en utilisant la r√®gle de la cha√Æne} \\ db^{[2]} = dZ^{[2]} \cdot 1 + 0 \quad \text{en utilisant la r√®gle de la constante} \\ db^{[2]} = dZ^{[2]} \end{align*}$$

Puisque b\[2\] est un terme de biais, sa d√©riv√©e est simplement la somme des gradients dZ\[2\] sur tous les exemples d'entra√Ænement (ce qui, dans une impl√©mentation vectoris√©e, est souvent fait en sommant dZ\[2\] sur les m observations).

Une fois la r√©tropropagation termin√©e, l'√©tape suivante est d'utiliser ces gradients comme entr√©e pour un algorithme d'optimisation comme GD, SGD, ou d'autres pour d√©terminer comment les param√®tres doivent √™tre mis √† jour.

Ainsi, nous sommes enfin pr√™ts √† mettre √† jour les param√®tres de Poids et de Biais du mod√®le dans cette passe.

Voici un exemple utilisant l'algorithme GD :

$$W^{[2]} = W^{[2]} - \eta \cdot dW^{[2]}$$

$$b^{[2]} = b^{[2]} - \eta \cdot db^{[2]}$$

Ici, le ùúá repr√©sente le param√®tre d'apprentissage en supposant l'algorithme d'optimisation GD simple (plus sur les algorithmes d'optimisation dans les chapitres suivants).

Dans la section suivante, nous entrerons dans plus de d√©tails sur la mani√®re dont vous pouvez utiliser divers algorithmes d'optimisation pour entra√Æner des mod√®les d'Apprentissage Profond.

## Chapitre 4 : Algorithmes d'Optimisation en IA

Une fois le gradient calcul√© via la r√©tropropagation, l'√©tape suivante consiste √† utiliser un algorithme d'optimisation pour ajuster les poids afin de minimiser la fonction de co√ªt.

Pour √™tre clair, l'algorithme d'optimisation prend en entr√©e les gradients calcul√©s et utilise ceux-ci pour mettre √† jour les param√®tres du mod√®le.

Ce sont les algorithmes d'optimisation les plus populaires utilis√©s lors de l'entra√Ænement des R√©seaux de Neurones :

* Descente de Gradient (GD)

* Descente de Gradient Stochastique (SGD)

* SGD avec Momentum

* RMSProp

* Optimiseur Adam

Conna√Ætre les fondamentaux des mod√®les d'Apprentissage Profond et apprendre √† entra√Æner ces mod√®les est d√©finitivement une grande partie de l'Apprentissage Profond. Si vous avez lu jusqu'ici et que les math√©matiques ne vous ont pas fatigu√©, f√©licitations ! Vous avez saisi certains sujets difficiles. Mais ce n'est qu'une partie du travail.

Pour utiliser votre mod√®le d'Apprentissage Profond pour r√©soudre des probl√®mes r√©els, vous devrez l'optimiser apr√®s avoir √©tabli sa base. C'est-√†-dire que vous devez optimiser l'ensemble des param√®tres dans votre mod√®le de Machine Learning pour trouver l'ensemble des param√®tres optimaux qui donnent le mod√®le le plus performant (toutes choses √©tant √©gales par ailleurs).

Ainsi, pour optimiser ou ajuster votre mod√®le de Machine Learning, vous devez effectuer une optimisation des hyperparam√®tres. En trouvant la combinaison optimale des valeurs des hyperparam√®tres, nous pouvons diminuer les erreurs que le mod√®le produit et construire le r√©seau de neurones le plus pr√©cis.

Un hyperparam√®tre d'un mod√®le est une constante dans le mod√®le. Il est externe au mod√®le, et sa valeur ne peut pas √™tre estim√©e √† partir des donn√©es (mais doit plut√¥t √™tre sp√©cifi√©e √† l'avance avant que le mod√®le ne soit entra√Æn√©). Par exemple, les param√®tres de poids et de biais dans un r√©seau de neurones sont des param√®tres que nous voulons optimiser.

NOTE : Comme les algorithmes d'optimisation sont utilis√©s dans tous les r√©seaux de neurones, j'ai pens√© qu'il serait utile de vous fournir le code Python que vous pouvez impl√©menter pour effectuer manuellement l'optimisation des r√©seaux de neurones.

Gardez simplement √† l'esprit que ce n'est pas ce que vous ferez en pratique, car il existe des biblioth√®ques √† cet effet. N√©anmoins, voir le code Python vous aidera √† comprendre le fonctionnement r√©el de ces algorithmes comme GD, SGD, SGD avec Momentum, Adam, AdamW beaucoup mieux.

Je vous fournirai les formules, les explications, ainsi que le code Python afin que vous puissiez voir le code Python derri√®re les fonctions r√©elles des biblioth√®ques qui impl√©mentent ces algorithmes d'optimisation.

### Descente de Gradient (GD)

L'algorithme de Descente de Gradient par Lots (souvent appel√© simplement Descente de Gradient ou GD), calcule le gradient de la Fonction de Perte **J(ùúá)** par rapport au param√®tre cible en utilisant l'ensemble des donn√©es d'entra√Ænement.

Nous faisons cela en pr√©disant d'abord les valeurs pour toutes les observations √† chaque it√©ration, et en les comparant √† la valeur donn√©e dans les donn√©es d'entra√Ænement.

Ces deux valeurs sont utilis√©es pour calculer le terme d'erreur de pr√©diction par observation qui est ensuite utilis√© pour mettre √† jour les param√®tres du mod√®le. Ce processus se poursuit jusqu'√† ce que le mod√®le converge.

Le gradient ou la premi√®re d√©riv√©e de la fonction de perte peut √™tre exprim√© comme suit :

$$\nabla_{\theta} J(\theta)$$

Ensuite, ce gradient est utilis√© pour mettre √† jour la valeur des it√©rations pr√©c√©dentes du param√®tre cible. C'est-√†-dire :

$$\theta = \theta - \eta \cdot \nabla_{\theta} J(\theta)$$

Dans cette √©quation :

* *ùúá* repr√©sente le(s) param√®tre(s) ou le(s) poids(s) d'un mod√®le que vous essayez d'optimiser. Dans de nombreux contextes, en particulier dans les r√©seaux de neurones, *ùúá* peut √™tre un vecteur contenant de nombreux poids individuels.

* *ùúá* est le taux d'apprentissage. C'est un hyperparam√®tre qui dicte la taille du pas √† chaque it√©ration tout en se d√©pla√ßant vers un minimum de la fonction de co√ªt. Un taux d'apprentissage plus petit peut rendre l'optimisation plus pr√©cise, mais pourrait √©galement ralentir le processus de convergence. Un taux d'apprentissage plus grand peut acc√©l√©rer la convergence, mais risque de d√©passer le minimum. Cela peut √™tre \[0,1\] mais est g√©n√©ralement un nombre entre (0,001 et 0,04)

* ùúµ\_J\_(*ùúá*) est le gradient de la fonction de co√ªt *J* par rapport au param√®tre ùúá. Il indique la direction et l'amplitude de l'augmentation la plus raide de *J*. En soustrayant cela de la valeur actuelle du param√®tre (multipli√©e par le taux d'apprentissage), nous ajustons *ùúá* dans la direction de la diminution la plus raide de *J*.

En termes de R√©seaux de Neurones, dans la section pr√©c√©dente nous avons vu l'utilisation de cette technique d'optimisation simple.

Il y a deux inconv√©nients majeurs √† la GD qui rendent cette technique d'optimisation peu populaire, surtout lorsqu'il s'agit de grands ensembles de donn√©es complexes.

Puisque dans chaque it√©ration l'ensemble des donn√©es d'entra√Ænement doit √™tre utilis√© et stock√©, le temps de calcul peut √™tre tr√®s long, ce qui entra√Æne un processus incroyablement lent. En plus de cela, le stockage de cette grande quantit√© de donn√©es entra√Æne des probl√®mes de m√©moire, rendant la GD lourde et lente en termes de calcul.

Vous pouvez en apprendre plus dans ce [Tutoriel d'Entretien sur la Descente de Gradient](https://youtu.be/rOI2GuwjJSY).

#### Descente de Gradient en Python

Regardons un exemple de l'utilisation de la Descente de Gradient en Python :

```css
def update_parameters_with_gd(parameters, grads, learning_rate):
    """
    Met √† jour les param√®tres en utilisant une r√®gle de mise √† jour simple de descente de gradient.
    
    Arguments :
    parameters -- dictionnaire python contenant vos param√®tres 
                  (par exemple, {"W1": W1, "b1": b1, "W2": W2, "b2": b2, ..., "WL": WL, "bL": bL})
    grads -- dictionnaire python contenant vos gradients pour mettre √† jour chaque param√®tre 
             (par exemple, {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2, ..., "dWL": dWL, "dbL": dbL})
    learning_rate -- le taux d'apprentissage, scalaire.
    
    Retourne :
    parameters -- dictionnaire python contenant vos param√®tres mis √† jour 
    """

    L = len(parameters) // 2 # nombre de couches dans les r√©seaux de neurones

    # R√®gle de mise √† jour pour chaque param√®tre
    for l in range(L):
        parameters["W" + str(l+1)] -= learning_rate * grads["dW" + str(l+1)]
        parameters["b" + str(l+1)] -= learning_rate * grads["db" + str(l+1)]
        
    return parameters
```

Il s'agit d'un extrait de code Python impl√©mentant l'algorithme de descente de gradient (GD) pour la mise √† jour des param√®tres dans un r√©seau de neurones qui prend ces trois arguments :

1. **parameters** : dictionnaire contenant les param√®tres actuels du r√©seau de neurones (par exemple, poids et biais pour chaque couche du r√©seau de neurones)

2. **grads** : dictionnaire contenant les gradients des param√®tres, calcul√©s pendant la r√©tropropagation

3. **learning\_rate** : valeur scalaire repr√©sentant le taux d'apprentissage, qui contr√¥le la taille du pas des mises √† jour des param√®tres.

Ce code parcourt les couches du r√©seau de neurones et met √† jour les poids (W) et les biais (b) pour chaque couche en utilisant la r√®gle de mise √† jour suivante pour chaque param√®tre :

Apr√®s avoir parcouru toutes les couches du r√©seau de neurones, il retourne les param√®tres mis √† jour. Ce processus aide le r√©seau de neurones √† apprendre et √† ajuster ses param√®tres pour minimiser la perte pendant l'entra√Ænement, am√©liorant finalement ses performances et r√©sultant en des pr√©dictions tr√®s pr√©cises.

### Descente de Gradient Stochastique (SGD)

La m√©thode de Descente de Gradient Stochastique (SGD), √©galement connue sous le nom de Descente de Gradient Incr√©mentale, est une approche it√©rative pour r√©soudre les probl√®mes d'optimisation avec une fonction objectif diff√©rentielle, exactement comme la GD.

Mais contrairement √† la GD, la SGD n'utilise pas l'ensemble du lot de donn√©es d'entra√Ænement pour mettre √† jour la valeur du param√®tre √† chaque it√©ration. La m√©thode SGD est souvent appel√©e approximation stochastique de la descente de gradient. Elle vise √† trouver les points extr√™mes ou z√©ro de la fonction stochastique contenant des param√®tres qui ne peuvent pas √™tre estim√©s directement.

La SGD minimise cette fonction de co√ªt en parcourant les donn√©es de l'ensemble de donn√©es d'entra√Ænement et en mettant √† jour les valeurs des param√®tres √† chaque it√©ration.

Dans la SGD, tous les param√®tres du mod√®le sont am√©lior√©s √† chaque √©tape d'it√©ration avec un seul √©chantillon d'entra√Ænement ou un mini-lot. Ainsi, au lieu de parcourir tous les √©chantillons d'entra√Ænement √† la fois pour modifier les param√®tres du mod√®le, l'algorithme SGD am√©liore les param√®tres en regardant un seul ensemble d'entra√Ænement **al√©atoirement** √©chantillonn√© (d'o√π le nom [**Stochastique**](https://www.merriam-webster.com/dictionary/stochastic), qui signifie "impliquant le hasard ou la probabilit√©").

Il ajuste les param√®tres dans la direction oppos√©e du gradient par un pas proportionnel au taux d'apprentissage. La mise √† jour √† l'√©tape de temps `t` peut √™tre donn√©e par la formule suivante :

$$\theta_{t+1} = \theta_t - \eta \nabla_{\theta} J(\theta_t)$$

Dans cette √©quation :

* *ùúá* repr√©sente le(s) param√®tre(s) ou le(s) poids(s) d'un mod√®le que vous essayez d'optimiser. Dans de nombreux contextes, en particulier dans les r√©seaux de neurones, *ùúá* peut √™tre un vecteur contenant de nombreux poids individuels.

* *ùúá* est le taux d'apprentissage. C'est un hyperparam√®tre qui dicte la taille du pas √† chaque it√©ration tout en se d√©pla√ßant vers un minimum de la fonction de co√ªt. Un taux d'apprentissage plus petit peut rendre l'optimisation plus pr√©cise mais pourrait √©galement ralentir le processus de convergence. Un taux d'apprentissage plus grand peut acc√©l√©rer la convergence mais risque de d√©passer le minimum.

* ùúµ\_J\_(*ùúát*) est le gradient de la fonction de co√ªt *J* par rapport au param√®tre ùúá pour une entr√©e donn√©e *x*(*i*) et sa sortie cible correspondante *y*(*i*) √† l'√©tape t. Il indique la direction et l'amplitude de l'augmentation la plus raide de *J*. En soustrayant cela de la valeur actuelle du param√®tre (multipli√©e par le taux d'apprentissage), nous ajustons *ùúá* dans la direction de la diminution la plus raide de *J*.

* *x*(*i*) repr√©sente le *i√®me* √©chantillon de donn√©es d'entr√©e de votre ensemble de donn√©es.

* *y*(*i*) est la vraie sortie cible pour le *i√®me* √©chantillon de donn√©es d'entr√©e.

Dans le contexte de la Descente de Gradient Stochastique (SGD), la r√®gle de mise √† jour s'applique aux √©chantillons de donn√©es individuels *x*(*i*) et *y*(*i*) plut√¥t qu'√† l'ensemble de donn√©es, ce qui serait le cas pour la Descente de Gradient par lots.

Cette seule √©tape am√©liore la vitesse du processus de recherche des minima globaux du probl√®me d'optimisation et c'est ce qui diff√©rencie la SGD de la GD. Ainsi, la SGD ajuste de mani√®re coh√©rente les param√®tres en tentant de se d√©placer dans la direction du minimum global de la fonction objectif.

Dans la SGD, tous les param√®tres du mod√®le sont am√©lior√©s √† chaque √©tape d'it√©ration avec un seul √©chantillon d'entra√Ænement. Ainsi, au lieu de parcourir tous les √©chantillons d'entra√Ænement √† la fois pour modifier les param√®tres du mod√®le, la SGD am√©liore les param√®tres en regardant un seul √©chantillon d'entra√Ænement.

Bien que la SGD aborde le probl√®me du temps de calcul lent de la GD, car elle s'adapte bien aux grandes donn√©es et √† la taille du mod√®le, elle est connue comme un "mauvais optimiseur" car elle est sujette √† trouver un optimum local au lieu d'un optimum global.

La SGD peut √™tre bruyante en raison de cette nature stochastique, car elle utilise des gradients calcul√©s √† partir d'un sous-ensemble des donn√©es (un mini-lot ou un point unique). Cela peut entra√Æner une variance dans les mises √† jour des param√®tres.

Pour plus de d√©tails sur la SGD, vous pouvez consulter ce [tutoriel](https://youtu.be/hqrI5OPtGOI).

### Exemple de SGD en Python

Maintenant, voyons comment l'impl√©menter en Python :

```python
def update_parameters_with_sgd(parameters, grads, learning_rate):
    """
    Met √† jour les param√®tres en utilisant SGD
    
    Arguments d'entr√©e :
    parameters -- dictionnaire contenant vos param√®tres (par exemple, poids, biais)
    grads -- dictionnaire contenant les gradients pour mettre √† jour chaque param√®tre
    learning_rate -- le taux d'apprentissage, scalaire.
    
    Sortie :
    parameters -- dictionnaire contenant vos param√®tres mis √† jour
    """
    
    for key in parameters:
        # R√®gle de mise √† jour pour chaque param√®tre
        parameters[key] = parameters[key] - learning_rate * grads['d' + key]
        
    return parameters
```

Voici ce qui se passe dans ce code :

* `parameters` est un dictionnaire qui contient les poids et les biais de votre r√©seau (par exemple, `parameters['W1']`, `parameters['b1']`, et ainsi de suite)

* `grads` contient les gradients des poids et des biais (par exemple, `grads['dW1']`, `grads['db1']`, et ainsi de suite).

* La fonction `initialize_velocity()` est utilis√©e pour cr√©er le dictionnaire de vitesse avant de commencer √† entra√Æner le r√©seau avec le momentum.

* La fonction `update_parameters_with_momentum()` utilise ensuite cette vitesse en conjonction avec les gradients pour mettre √† jour les param√®tres.

### **SGD avec Momentum**

Lorsque la fonction d'erreur est complexe et non convexe, au lieu de trouver l'optimum global, l'algorithme SGD se d√©place par erreur dans la direction de nombreux minima locaux.

Afin de r√©soudre ce probl√®me et d'am√©liorer davantage l'algorithme SGD, diverses m√©thodes ont √©t√© introduites. Une m√©thode populaire pour √©chapper √† un minimum local et se d√©placer dans la bonne direction d'un minimum global est le **SGD avec Momentum**.

L'objectif de la m√©thode SGD avec momentum est d'acc√©l√©rer les vecteurs de gradient dans la direction du minimum global, ce qui entra√Æne une convergence plus rapide.

L'id√©e derri√®re le momentum est que les param√®tres du mod√®le sont appris en utilisant les directions et les valeurs des ajustements de param√®tres pr√©c√©dents. De plus, les valeurs d'ajustement sont calcul√©es de mani√®re √† ce que les ajustements plus r√©cents soient pond√©r√©s plus lourdement (ils obtiennent des poids plus grands) par rapport aux ajustements tr√®s pr√©coces (ils obtiennent des poids plus petits).

En gros, le SGD avec momentum est con√ßu pour acc√©l√©rer la convergence du SGD et r√©duire ses oscillations. Il introduit donc un terme de vitesse, qui est une fraction de la mise √† jour pr√©c√©dente. Cette √©tape exacte aide l'optimiseur √† accumuler de la vitesse dans les directions avec des gradients persistants et coh√©rents, et √† amortir les mises √† jour dans les directions fluctuantes.

Les r√®gles de mise √† jour pour le momentum sont les suivantes, o√π vous devez d'abord calculer le gradient (comme avec le SGD simple) puis mettre √† jour la vitesse et le param√®tre theta.

$$v_{t+1} = \gamma v_t + \eta \nabla_{\theta} J(\theta_t)$$

$$\theta_{t+1} = \theta_t - v_{t+1}$$

Le momentum *ùúÉ* qui est typiquement une valeur entre 0,5 & 0,9, d√©termine combien des gradients pass√©s seront conserv√©s et utilis√©s dans la mise √† jour.

La raison de cette diff√©rence est que, avec la m√©thode SGD, nous ne d√©terminons pas la d√©riv√©e exacte de la fonction de perte, mais nous l'estimons sur un petit lot. Puisque le gradient est bruyant, il est probable qu'il ne se d√©place pas toujours dans la direction optimale.

Le momentum aide alors √† estimer ces d√©riv√©es plus pr√©cis√©ment, ce qui entra√Æne de meilleurs choix de direction lors du d√©placement vers le minimum global.

Une autre raison de la diff√©rence de performance entre le SGD classique et le SGD avec momentum r√©side dans la zone appel√©e Courbure Pathologique, √©galement appel√©e la **zone de ravin**.

La Courbure Pathologique ou la Zone de Ravin peut √™tre repr√©sent√©e par le graphique suivant. La ligne orange repr√©sente le chemin pris par la m√©thode bas√©e sur le gradient tandis que la ligne bleue fonc√©e repr√©sente le chemin id√©al vers la direction de fin du minimum global.

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-151.png align="left")

*Chemins d'Optimisation : Descente de Gradient vs. Trajectoire Id√©ale vers le Minimum Global*

Pour visualiser la diff√©rence entre le SGD et le SGD Momentum, regardons la figure suivante :

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-152.png align="left")

*Comparaison des Chemins de Descente de Gradient dans Diff√©rents Paysages d'Optimisation*

Sur le c√¥t√© gauche se trouve la m√©thode SGD sans Momentum. Sur le c√¥t√© droit se trouve le SGD avec Momentum. Le motif orange repr√©sente le chemin du gradient dans une recherche du minimum global. Comme vous pouvez le voir, dans la figure de gauche, nous avons plus de ces oscillations par rapport √† celle de droite, et c'est l'impact du Momentum, o√π nous acc√©l√©rons l'entra√Ænement et l'algorithme fait alors moins de ces mouvements.

L'id√©e derri√®re le momentum est que les param√®tres du mod√®le sont appris en utilisant les directions et les valeurs des ajustements de param√®tres pr√©c√©dents. De plus, les valeurs d'ajustement sont calcul√©es de mani√®re √† ce que les ajustements plus r√©cents soient pond√©r√©s plus lourdement (ils obtiennent des poids plus grands) par rapport aux ajustements tr√®s pr√©coces (ils obtiennent des poids plus petits).

#### Exemple de SGD avec Momentum en Python

Voyons √† quoi cela ressemble en code :

```python
def initialize_velocity(parameters):
    """
    Initialise la vitesse en tant que dictionnaire python avec :
                - cl√©s : "dW1", "db1", ..., "dWL", "dbL"
                - valeurs : tableaux numpy de z√©ros de la m√™me forme que les gradients/param√®tres correspondants.
    """
    L = len(parameters) // 2 # nombre de couches dans les r√©seaux de neurones
    v = {}
    
    for l in range(L):
        v["dW" + str(l+1)] = np.zeros_like(parameters["W" + str(l+1)])
        v["db" + str(l+1)] = np.zeros_like(parameters["b" + str(l+1)])
        
    return v

def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):
    """
    Met √† jour les param√®tres en utilisant le Momentum
    
    Arguments :
    parameters -- dictionnaire python contenant vos param√®tres
    grads -- dictionnaire python contenant vos gradients pour chaque param√®tre
    v -- dictionnaire python contenant la vitesse actuelle
    beta -- l'hyperparam√®tre de momentum, scalaire
    learning_rate -- le taux d'apprentissage, scalaire
    
    Retourne :
    parameters -- dictionnaire python contenant vos param√®tres mis √† jour 
    v -- dictionnaire python contenant vos vitesses mises √† jour
    """
    
    L = len(parameters) // 2 # nombre de couches dans les r√©seaux de neurones
    
    # Mise √† jour du momentum pour chaque param√®tre
    for l in range(L):
        # calculer les vitesses
        v["dW" + str(l+1)] = beta * v["dW" + str(l+1)] + (1 - beta) * grads["dW" + str(l+1)]
        v["db" + str(l+1)] = beta * v["db" + str(l+1)] + (1 - beta) * grads["db" + str(l+1)]
        # mettre √† jour les param√®tres
        parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * v["dW" + str(l+1)]
        parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * v["db" + str(l+1)]
        
    return parameters, v
```

Dans ce code, nous avons deux fonctions pour impl√©menter l'algorithme de descente de gradient bas√© sur le momentum (SGD avec momentum) :

1. **initialize\_velocity(parameters)** : Cette fonction initialise la vitesse pour chaque param√®tre dans le r√©seau de neurones. Elle prend les param√®tres actuels en entr√©e et retourne un dictionnaire (v) avec des cl√©s pour les gradients ("dW1", "db1", ..., "dWL", "dbL") et initialise les valeurs correspondantes en tant que tableaux numpy remplis de z√©ros.

2. **update\_parameters\_with\_momentum(parameters, grads, v, beta, learning\_rate)** : Cette fonction met √† jour les param√®tres en utilisant la technique d'optimisation Momentum. Elle prend les arguments suivants :

3. parameters : dictionnaire contenant les param√®tres actuels du r√©seau de neurones.

4. **grads** : dictionnaire contenant les gradients des param√®tres.

5. **v** : dictionnaire contenant les vitesses actuelles des param√®tres (initialis√© en utilisant la fonction **initialize\_velocity**).

6. **beta** : hyperparam√®tre de momentum, un scalaire qui contr√¥le l'influence des gradients pass√©s sur les mises √† jour.

7. **learning\_rate** : taux d'apprentissage, un scalaire contr√¥lant la taille du pas des mises √† jour des param√®tres.

√Ä l'int√©rieur de la fonction, elle parcourt les couches du r√©seau de neurones et effectue les √©tapes suivantes pour chaque param√®tre :

* Calcule la nouvelle vitesse en utilisant la formule de momentum.

* Met √† jour le param√®tre en utilisant la nouvelle vitesse et le taux d'apprentissage.

* Enfin, elle retourne les param√®tres mis √† jour et les vitesses.

### RMSProp

La Propagation de la Moyenne Quadratique, commun√©ment appel√©e RMSprop, est une m√©thode d'optimisation avec un taux d'apprentissage adaptatif. Elle a √©t√© propos√©e par Geoff Hinton dans son cours Coursera.

RMSprop ajuste le taux d'apprentissage pour chaque param√®tre en divisant le taux d'apprentissage pour un poids par une moyenne mobile des magnitudes des gradients r√©cents pour ce poids.

RMSprop peut √™tre d√©fini math√©matiquement comme suit :

$$v_t = \beta v_{t-1} + (1 - \beta) g_t^2$$

$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} \cdot g_t$$

* \_vt\_ùü∞ est la moyenne mobile des gradients au carr√©.

* *ùúÇ* est le taux de d√©croissance qui contr√¥le la moyenne mobile (g√©n√©ralement fix√© √† 0,9).

* *ùúá* est le taux d'apprentissage.

* *ùúπ* est un petit scalaire utilis√© pour √©viter la division par z√©ro (g√©n√©ralement autour de 10^-8).

* \_gt\_ùü∞ est le gradient √† l'√©tape de temps *t*, et \_ùúát\_ùü∞ est le vecteur de param√®tres √† l'√©tape de temps *t*.

L'algorithme calcule d'abord la moyenne mobile des gradients au carr√© (le hessien) pour chaque param√®tre : v\_t √† l'√©tape t.

Ensuite, il divise le taux d'apprentissage eta par la racine carr√©e de cette vitesse moyenne (division √©l√©ment par √©l√©ment si les param√®tres sont des vecteurs ou des matrices). Ensuite, il utilise cela dans la m√™me √©tape pour mettre √† jour les param√®tres.

### Exemple de RMSProp en Python

Voici un exemple de son fonctionnement en Python :

```python
def update_parameters_with_rmsprop(parameters, grads, s, learning_rate, beta, epsilon):
    """
    Met √† jour les param√®tres en utilisant RMSprop.
    
    Arguments :
    parameters -- dictionnaire python contenant vos param√®tres 
                    (par exemple, {"W1": W1, "b1": b1, "W2": W2, "b2": b2})
    grads -- dictionnaire python contenant vos gradients pour mettre √† jour chaque param√®tre 
                    (par exemple, {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2})
    s -- dictionnaire python contenant la moyenne mobile des gradients au carr√© 
                    (par exemple, {"dW1": s_dW1, "db1": s_db1, "dW2": s_dW2, "db2": s_db2})
    learning_rate -- le taux d'apprentissage, scalaire.
    beta -- l'hyperparam√®tre de momentum, scalaire.
    epsilon -- petit nombre pour √©viter la division par z√©ro, scalaire.
    
    Retourne :
    parameters -- dictionnaire python contenant vos param√®tres mis √† jour 
    s -- dictionnaire python contenant la moyenne mobile mise √† jour des gradients au carr√©
    """
    
    L = len(parameters) // 2 # nombre de couches dans les r√©seaux de neurones

    # R√®gle de mise √† jour pour chaque param√®tre
    for l in range(L):
        # Calculer la moyenne mobile des gradients au carr√©
        s["dW" + str(l+1)] = beta * s["dW" + str(l+1)] + (1 - beta) * np.square(grads["dW" + str(l+1)])
        s["db" + str(l+1)] = beta * s["db" + str(l+1)] + (1 - beta) * np.square(grads["db" + str(l+1)])
        
        # Mettre √† jour les param√®tres
        parameters["W" + str(l+1)] -= learning_rate * grads["dW" + str(l+1)] / (np.sqrt(s["dW" + str(l+1)]) + epsilon)
        parameters["b" + str(l+1)] -= learning_rate * grads["db" + str(l+1)] / (np.sqrt(s["db" + str(l+1)]) + epsilon)

    return parameters, s
```

Ce code d√©finit une fonction pour mettre √† jour les param√®tres d'un r√©seau de neurones en utilisant la technique d'optimisation RMSprop. Voici un r√©sum√© de la fonction :

* **update\_parameters\_with\_rmsprop(parameters, grads, s, learning\_rate, beta, epsilon)** : fonction met √† jour les param√®tres d'un r√©seau de neurones en utilisant RMSprop.

Elle prend les arguments suivants :

* **parameters** : dictionnaire contenant les param√®tres actuels du r√©seau de neurones.

* **grads** : dictionnaire contenant les gradients des param√®tres.

* **s** : dictionnaire contenant la moyenne mobile des gradients au carr√© pour chaque param√®tre.

* **learning\_rate** : taux d'apprentissage, un scalaire.

* **beta** : hyperparam√®tre de momentum, un scalaire.

* **epsilon** : Un petit nombre ajout√© pour √©viter la division par z√©ro, un scalaire.

√Ä l'int√©rieur de cette fonction, le code parcourt les couches du r√©seau de neurones et effectue les √©tapes suivantes pour chaque param√®tre :

* Calcule la moyenne mobile des gradients au carr√© pour les poids (W) et les biais (b) en utilisant la formule RMSprop.

* Met √† jour les param√®tres en utilisant les moyennes mobiles calcul√©es et le taux d'apprentissage, avec un terme epsilon suppl√©mentaire au d√©nominateur pour √©viter la division par z√©ro.

Enfin, le code retourne les param√®tres mis √† jour et la moyenne mobile mise √† jour des gradients au carr√© (s).

RMSprop est une technique d'optimisation qui adapte le taux d'apprentissage pour chaque param√®tre en fonction de l'historique des gradients au carr√©. Elle aide √† stabiliser et √† acc√©l√©rer l'entra√Ænement, en particulier lorsqu'on traite avec des gradients clairsem√©s ou bruyants.

### Optimiseur Adam

Une autre technique populaire pour am√©liorer la proc√©dure d'optimisation SGD est l'**Estimation Adaptative des Moments (Adam)** introduite par Kingma et Ba (2015). Adam combine essentiellement le momentum SGD avec RMSProp.

La principale diff√©rence par rapport au SGD avec momentum, qui utilise un seul taux d'apprentissage pour toutes les mises √† jour de param√®tres, est que l'algorithme Adam d√©finit diff√©rents taux d'apprentissage pour diff√©rents param√®tres.

L'algorithme calcule les taux d'apprentissage adaptatifs individuels pour chaque param√®tre en fonction des estimations des deux premiers moments des gradients (premi√®re et deuxi√®me d√©riv√©e de la fonction de perte).

Ainsi, chaque param√®tre a un taux d'apprentissage unique, qui est mis √† jour en utilisant la moyenne d√©croissante exponentielle des premiers moments (la moyenne) et des deuxi√®mes moments (la variance) des gradients.

En gros, Adam calcule des taux d'apprentissage adaptatifs individuels pour diff√©rents param√®tres √† partir des estimations des 1er et 2√®me moments des gradients.

Les r√®gles de mise √† jour pour l'optimiseur Adam peuvent √™tre exprim√©es comme suit :

1. Calculer les moyennes mobiles des gradients et des gradients au carr√©

2. Ajuster ces moyennes mobiles pour un facteur de biais

3. Utiliser ces moyennes mobiles pour mettre √† jour le taux d'apprentissage pour chaque param√®tre individuellement

Math√©matiquement, ces √©tapes sont repr√©sent√©es comme suit :

$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$

$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$

$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$

$$\hat{v}t = \frac{v_t}{1 - \beta_2^t}$$

$$\theta{t+1} = \theta_t - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

* \_mt\_ùü∞ et *vt* sont des estimations du premier moment (la moyenne) et du deuxi√®me moment (la variance non centr√©e) des gradients, respectivement.

* *m\_hat* et v\_hat sont des versions corrig√©es du biais de ces estimations.

* \_ùúÇ\_1ùü∞ et \_ùúÇ\_2ùü∞ sont les taux de d√©croissance exponentielle pour ces estimations de moments (g√©n√©ralement fix√©s √† 0,9 & 0,999, respectivement).

* *ùúÅ* est le taux d'apprentissage.

* *ùúπ* est un petit scalaire utilis√© pour √©viter la division par z√©ro (g√©n√©ralement autour de 10^(ùü∞8)).

### Exemple de Adam en Python

Voici un exemple de l'utilisation de Adam en Python :

```python
def initialize_adam(parameters) :
    """
    Initialise v et s en tant que deux dictionnaires python avec :
                - cl√©s : "dW1", "db1", ..., "dWL", "dbL"
                - valeurs : tableaux numpy de z√©ros de la m√™me forme que les gradients/param√®tres correspondants.
    """
    
    L = len(parameters) // 2 # nombre de couches dans les r√©seaux de neurones
    v = {}
    s = {}
    
    for l in range(L):
        v["dW" + str(l+1)] = np.zeros_like(parameters["W" + str(l+1)])
        v["db" + str(l+1)] = np.zeros_like(parameters["b" + str(l+1)])
        s["dW" + str(l+1)] = np.zeros_like(parameters["W" + str(l+1)])
        s["db" + str(l+1)] = np.zeros_like(parameters["b" + str(l+1)])
    
    return v, s

def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,
                                beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    Met √† jour les param√®tres en utilisant Adam
    
    Arguments :
    parameters -- dictionnaire python contenant vos param√®tres :
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- dictionnaire python contenant vos gradients pour chaque param√®tre :
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    v -- variable Adam, moyenne mobile du premier gradient, dictionnaire python
    s -- variable Adam, moyenne mobile du gradient au carr√©, dictionnaire python
    learning_rate -- le taux d'apprentissage, scalaire.
    beta1 -- Hyperparam√®tre de d√©croissance exponentielle pour les estimations du premier moment 
    beta2 -- Hyperparam√®tre de d√©croissance exponentielle pour les estimations du deuxi√®me moment 
    epsilon -- hyperparam√®tre emp√™chant la division par z√©ro dans les mises √† jour Adam

    Retourne :
    parameters -- dictionnaire python contenant vos param√®tres mis √† jour 
    v -- variable Adam, moyenne mobile du premier gradient, dictionnaire python
    s -- variable Adam, moyenne mobile du gradient au carr√©, dictionnaire python
    """
    
    L = len(parameters) // 2                 # nombre de couches dans les r√©seaux de neurones
    v_corrected = {}                         # Initialisation de l'estimation du premier moment, dictionnaire python
    s_corrected = {}                         # Initialisation de l'estimation du deuxi√®me moment, dictionnaire python
    
    # Effectuer la mise √† jour Adam sur tous les param√®tres
    for l in range(L):
        # Moyenne mobile des gradients.
        v["dW" + str(l+1)] = beta1 * v["dW" + str(l+1)] + (1 - beta1) * grads["dW
```

Dans ce code, nous impl√©mentons l'algorithme Adam, compos√© de deux fonctions :

1. **initialize\_adam**(parameters) : Cette fonction initialise les variables de l'optimiseur Adam `v` et `s` en tant que deux dictionnaires Python. Elle prend les param√®tres actuels en entr√©e et retourne `v` et `s`, tous deux √©tant des dictionnaires avec des cl√©s pour les gradients ("dW1", "db1", ..., "dWL", "dbL"). Les valeurs sont des tableaux numpy remplis de z√©ros et ont la m√™me forme que les gradients/param√®tres correspondants.

2. **update\_parameters\_with\_adam(parameters, grads, v, s, t, learning\_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8)** : Cette fonction met √† jour les param√®tres d'un r√©seau de neurones en utilisant la technique d'optimisation Adam. Elle prend les arguments suivants :

3. **parameters** : Un dictionnaire contenant les param√®tres actuels du r√©seau de neurones.

4. **grads** : Un dictionnaire contenant les gradients des param√®tres.

5. **v** : Un dictionnaire repr√©sentant la moyenne mobile des premiers moments de gradient.

6. **s** : Un dictionnaire repr√©sentant la moyenne mobile des deuxi√®mes moments de gradient.

7. **t** : Un scalaire repr√©sentant l'√©tape de temps actuelle (utilis√© pour la correction de biais).

8. **learning\_rate** : Le taux d'apprentissage, un scalaire.

9. **beta1** : L'hyperparam√®tre de d√©croissance exponentielle pour les estimations du premier moment.

10. **beta2** : L'hyperparam√®tre de d√©croissance exponentielle pour les estimations du deuxi√®me moment.

11. **epsilon** : Un petit nombre ajout√© pour √©viter la division par z√©ro dans les mises √† jour Adam.

√Ä l'int√©rieur de cette fonction, le code parcourt les couches du r√©seau de neurones et effectue les mises √† jour Adam pour chaque param√®tre. Cela inclut le calcul des moyennes mobiles des gradients et des gradients au carr√©, et l'utilisation de ces valeurs pour mettre √† jour les param√®tres. Il effectue √©galement une correction de biais pour ajuster les moyennes mobiles.

Enfin, le code retourne les param√®tres mis √† jour, `v` (estimations du premier moment), et `s` (estimations du deuxi√®me moment).

### AdamW

AdamW (le 'W' signifie 'Weight Decay') est un algorithme d'optimisation qui modifie la mani√®re dont le decay des poids est int√©gr√© dans l'algorithme Adam original. Ce changement apparemment mineur a des implications significatives pour le processus d'entra√Ænement, en particulier dans la mani√®re dont il g√®re la r√©gularisation pour pr√©venir le surapprentissage.

Cette √©tape a un impact crucial dans la g√©n√©ralisation des mod√®les d'apprentissage profond, pour construire des mod√®les qui se g√©n√©ralisent bien aux nouvelles donn√©es non vues.

Dans les optimiseurs traditionnels comme SGD, le decay des poids r√©gularise directement les param√®tres de poids du mod√®le. Mais dans Adam, ce processus est quelque peu confondu avec les taux d'apprentissage adaptatifs de l'optimiseur.

AdamW d√©couple le decay des poids des taux d'apprentissage, r√©tablissant l'effet de r√©gularisation directe vu dans SGD. Cela entra√Æne une r√©gularisation plus efficace et, souvent, de meilleures performances dans l'entra√Ænement des r√©seaux de neurones profonds.

Si vous souhaitez voir la repr√©sentation math√©matique r√©elle o√π je compare Adam et AdamW, vous pouvez consulter ce [**Tutoriel sur YouTube**](https://youtu.be/0HJ4iUQWHVI).

En choisissant AdamW, vous pouvez profiter des avantages des taux d'apprentissage adaptatifs tout en maintenant un m√©canisme de r√©gularisation plus robuste.

Cet algorithme d'optimisation a rapidement gagn√© en popularit√© dans la communaut√© du machine learning, en particulier parmi ceux qui travaillent sur des mod√®les √† grande √©chelle et des ensembles de donn√©es complexes o√π chaque bit d'efficacit√© d'optimisation compte.

### AdamW en Python

```css
import numpy as np

def initialize_adamw(parameters):
    """
    Initialise v, s et w en tant que trois dictionnaires python avec :
                - cl√©s : "dW1", "db1", ..., "dWL", "dbL"
                - valeurs : tableaux numpy de z√©ros de la m√™me forme que les gradients/param√®tres correspondants.
    """
    
    L = len(parameters) // 2  # nombre de couches dans le r√©seau de neurones
    v = {}
    s = {}
    w = {}
    
    for l in range(L):
        v["dW" + str(l+1)] = np.zeros_like(parameters["W" + str(l+1)])
        v["db" + str(l+1)] = np.zeros_like(parameters["b" + str(l+1)])
        s["dW" + str(l+1)] = np.zeros_like(parameters["W" + str(l+1)])
        s["db" + str(l+1)] = np.zeros_like(parameters["b" + str(l+1)])
        w["W" + str(l+1)] = np.copy(parameters["W" + str(l+1)])
    
    return v, s, w

def update_parameters_with_adamw(parameters, grads, v, s, w, t, learning_rate=0.01,
                                beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):
    """
    Met √† jour les param√®tres en utilisant AdamW (Adam avec decay des poids)
    
    Arguments :
    parameters -- dictionnaire python contenant vos param√®tres :
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- dictionnaire python contenant vos gradients pour chaque param√®tre :
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    v -- variable Adam, moyenne mobile du premier gradient, dictionnaire python
    s -- variable Adam, moyenne mobile du gradient au carr√©, dictionnaire python
    w -- param√®tres de poids pour le decay des poids, dictionnaire python
    t -- √©tape de temps actuelle (utilis√©e pour la correction de biais), scalaire
    learning_rate -- le taux d'apprentissage, scalaire
    beta1 -- hyperparam√®tre de d√©croissance exponentielle pour les estimations du premier moment 
    beta2 -- hyperparam√®tre de d√©croissance exponentielle pour les estimations du deuxi√®me moment 
    epsilon -- hyperparam√®tre emp√™chant la division par z√©ro dans les mises √† jour Adam
    weight_decay -- hyperparam√®tre de decay des poids, scalaire

    Retourne :
    parameters -- dictionnaire python contenant vos param√®tres mis √† jour 
    v -- variable Adam, moyenne mobile du premier gradient, dictionnaire python
    s -- variable Adam, moyenne mobile du gradient au carr√©, dictionnaire python
    """
    
    L = len(parameters) // 2  # nombre de couches dans le r√©seau de neurones
    v_corrected = {}          # Initialisation de l'estimation du premier moment, dictionnaire python
    s_corrected = {}          # Initialisation de l'estimation du deuxi√®me moment, dictionnaire python
    
    # Effectuer la mise √† jour AdamW sur tous les param√®tres
    for l in range(L):
        # Moyenne mobile des gradients
        v["dW" + str(l+1)] = beta1 * v["dW" + str(l+1)] + (1 - beta1) * grads["dW" + str(l+1)]
        v["db" + str(l+1)] = beta1 * v["db" + str(l+1)] + (1 - beta1) * grads["db" + str(l+1)]
        
        # Moyenne mobile des gradients au carr√©
        s["dW" + str(l+1)] = beta2 * s["dW" + str(l+1)] + (1 - beta2) * np.square(grads["dW" + str(l+1)])
        s["db" + str(l+1)] = beta2 * s["db" + str(l+1)] + (1 - beta2) * np.square(grads["db" + str(l+1)])
        
        # Correction de biais pour les moyennes mobiles
        v_corrected["dW" + str(l+1)] = v["dW" + str(l+1)] / (1 - np.power(beta1, t))
        v_corrected["db" + str(l+1)] = v["db" + str(l+1)] / (1 - np.power(beta1, t))
        s_corrected["dW" + str(l+1)] = s["dW" + str(l+1)] / (1 - np.power(beta2, t))
        s_corrected["db" + str(l+1)] = s["db" + str(l+1)] / (1 - np.power(beta2, t))
        
        # Mettre √† jour les param√®tres avec le decay des poids
        parameters["W" + str(l+1)] -= learning_rate * (v_corrected["dW" + str(l+1)] / (np.sqrt(s_corrected["dW" + str(l+1)]) + epsilon) + weight_decay * w["W" + str(l+1)])
        parameters["b" + str(l+1)] -= learning_rate * (v_corrected["db" + str(l+1)] / (np.sqrt(s_corrected["db" + str(l+1)]) + epsilon) + weight_decay * w["W" + str(l+1)])
    
    return parameters, v, s
```

Dans ce code, nous impl√©mentons l'algorithme d'optimisation AdamW, qui est une extension de l'optimiseur Adam avec une r√©gularisation de decay des poids ajout√©e. Passons en revue chaque partie du code :

* **initialize\_adamw(parameters)** : Cette fonction initialise les variables de l'optimiseur AdamW. Elle prend tous les param√®tres actuels d'un r√©seau de neurones en entr√©e et retourne trois dictionnaires : `v`, `s`, et `w`.

Voici ce que repr√©sente chacun de ces dictionnaires :

* **v** : Un dictionnaire pour la moyenne mobile des premiers moments de gradient. Il a des cl√©s comme "dW1", "db1", ..., "dWL", "dbL", et les valeurs sont initialis√©es en tant que tableaux numpy remplis de z√©ros de la m√™me forme que les gradients/param√®tres correspondants.

* **s** : Un dictionnaire pour la moyenne mobile des deuxi√®mes moments de gradient, similaire √† `v`.

* **w** : Un dictionnaire pour les param√®tres de poids utilis√©s pour le decay des poids. Il est initialis√© avec une copie des param√®tres de poids actuels.

**update\_parameters\_with\_adamw(parameters, grads, v, s, w, t, learning\_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, weight\_decay=0.01)** : Cette fonction effectue la mise √† jour AdamW comme nous l'avons vu dans les √©quations, pour mettre √† jour les param√®tres du r√©seau de neurones. Elle prend plusieurs arguments :

* **parameters** : Un dictionnaire contenant les param√®tres actuels du r√©seau de neurones.

* **grads** : Un dictionnaire contenant les gradients des param√®tres, calcul√©s pendant la r√©tropropagation.

* **v** : dictionnaire repr√©sentant la moyenne mobile des premiers moments de gradient.

* **s** : dictionnaire repr√©sentant la moyenne mobile des deuxi√®mes moments de gradient.

* **w** : dictionnaire contenant les param√®tres de poids pour le decay des poids.

* **t** : scalaire repr√©sentant l'√©tape de temps actuelle (utilis√©e pour la correction de biais).

* **learning\_rate** : Le taux d'apprentissage, un scalaire.

* **beta1** : hyperparam√®tre de d√©croissance exponentielle pour les estimations du premier moment (g√©n√©ralement proche de 1).

* **beta2** : hyperparam√®tre de d√©croissance exponentielle pour les estimations du deuxi√®me moment (g√©n√©ralement proche de 1).

* **epsilon** : petit nombre ajout√© pour √©viter la division par z√©ro dans les mises √† jour Adam.

* **weight\_decay** : hyperparam√®tre de decay des poids, qui contr√¥le la force de la r√©gularisation L2.

√Ä l'int√©rieur de la fonction, les √©tapes suivantes sont effectu√©es pour chaque param√®tre :

1. Mettre √† jour `v` et `s` en utilisant les gradients, similaire √† la mise √† jour standard d'Adam.

2. Effectuer la correction de biais sur `v` et `s` pour tenir compte du fait qu'ils sont initialis√©s avec des z√©ros et peuvent √™tre biais√©s vers z√©ro.

3. Mettre √† jour les param√®tres avec la r√©gularisation de decay des poids. Le decay des poids encourage des valeurs de param√®tres plus petites en soustrayant une fraction du poids actuel de la mise √† jour.

4. Retourner les param√®tres mis √† jour, `v`, et `s`.

## Chapitre 5 : R√©gularisation et G√©n√©ralisation

Dans ce chapitre, nous allons plonger dans certains concepts importants de l'Apprentissage Profond, comme :

* Le surapprentissage et le sous-apprentissage dans les r√©seaux de neurones

* Les techniques de r√©gularisation : Dropout, r√©gularisation L1/L2, normalisation par lots

* L'augmentation des donn√©es et son r√¥le dans l'am√©lioration de la robustesse du mod√®le

Commen√ßons.

### La Technique de R√©gularisation Dropout

Le Dropout est l'une des techniques de r√©gularisation les plus populaires utilis√©es pour pr√©venir le surapprentissage dans les r√©seaux de neurones. La mani√®re dont l'algorithme fonctionne est qu'il "supprime" al√©atoirement (c'est-√†-dire qu'il met √† z√©ro) un certain nombre de caract√©ristiques de sortie de la couche pendant l'entra√Ænement.

Pendant le processus d'entra√Ænement, apr√®s avoir calcul√© les activations, l'algorithme met al√©atoirement √† z√©ro une fraction *p* (le taux de dropout) des activations. Ces caract√©ristiques ne sont alors pas utilis√©es pendant le processus d'entra√Ænement dans cette passe. Ce taux de dropout p est un hyperparam√®tre g√©n√©ralement fix√© entre 0,2 et 0,5. **Notez que ce taux de dropout n'est utilis√© que pendant l'entra√Ænement.**

Pour chaque couche *l*, pour chaque exemple d'entra√Ænement *i*, et pour chaque neurone/unit√© *j*, le dropout peut √™tre repr√©sent√© math√©matiquement comme suit :

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-156.png align="left")

O√π :

* *rj*(*l*)ùü∞ est une variable al√©atoire qui suit une distribution de Bernoulli, o√π la probabilit√© de ne pas √™tre supprim√© (succ√®s : 1) est 1ùü∞\_p\_.

* *aj*(*l*)ùü∞ est l'activation du neurone *j* dans la couche *l*.

* *a*~*j*(*l*)ùü∞ est l'activation du neurone *j* apr√®s l'application du dropout.

Pendant la r√©tropropagation dans le processus d'entra√Ænement, les gradients ne sont transmis que par les neurones qui n'ont pas √©t√© supprim√©s (lorsqu'il y a eu un succ√®s dans l'essai de Bernoulli). N'oubliez pas que ce taux de dropout n'est utilis√© que pendant l'entra√Ænement.

**Ajustement de Test** : Pendant le processus de test, le dropout n'est pas appliqu√©. Au lieu de cela, les activations sont r√©duites d'un facteur de *p* pour tenir compte de l'effet du dropout pendant le processus d'entra√Ænement. Cela est n√©cessaire car pendant l'entra√Ænement, en moyenne, chaque unit√© n'est active qu'avec une probabilit√© de 1ùü∞\_p\_.

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-157.png align="left")

Cela garantit que la somme attendue des activations est la m√™me pendant l'entra√Ænement et le test.

Le Dropout cr√©e effectivement un r√©seau "aminci" avec une architecture diff√©rente pour chaque √©tape d'entra√Ænement. Parce que l'architecture du r√©seau est diff√©rente pour chaque √©chantillon d'entra√Ænement, car nous √©teignons al√©atoirement certains des neurones, cela peut √™tre vu comme l'entra√Ænement d'une collection de r√©seaux avec des poids partag√©s.

Pendant le test, vous obtenez l'avantage de moyenner les effets de ces diff√©rents r√©seaux, ce qui tend √† r√©duire le surapprentissage. Cela est d√ª au fait qu'il introduit un biais, mais plus important encore, il r√©duit consid√©rablement la variance lorsque le mod√®le est utilis√© pour la pr√©diction. Voici pourquoi :

#### Introduction de Biais

En supprimant diff√©rents sous-ensembles de neurones avec un taux p, le r√©seau est alors contraint d'apprendre des caract√©ristiques plus robustes qui sont utiles en conjonction avec de nombreux sous-ensembles al√©atoires diff√©rents des autres neurones.

Cet ajustement peut conduire √† un biais l√©g√®rement plus √©lev√© pendant l'entra√Ænement car le r√©seau est moins susceptible d'apprendre des motifs qui sont tr√®s sp√©cifiques aux donn√©es d'entra√Ænement (qui peuvent √™tre consid√©r√©s comme du bruit).

#### Diminution de la Variance

Le Dropout r√©duit la variance en emp√™chant le r√©seau de devenir trop adapt√© aux donn√©es d'entra√Ænement. Il r√©duit le risque que le r√©seau d√©pende d'une caract√©ristique particuli√®re, garantissant ainsi que le mod√®le se g√©n√©ralise mieux aux donn√©es non vues.

Cela est courant dans les m√©thodes d'ensemble en apprentissage automatique comme le Boosting, la For√™t Al√©atoire, o√π la moyenne des pr√©dictions de diff√©rents mod√®les conduit √† une r√©duction de la variance.

### **R√©gularisation Ridge (R√©gularisation L2)**

Les r√©gularisations Lasso et Ridge sont des techniques initialement d√©velopp√©es pour les mod√®les lin√©aires, mais elles peuvent √©galement √™tre appliqu√©es √† l'apprentissage profond. Dans l'apprentissage profond, ces m√©thodes de r√©gularisation fonctionnent de mani√®re similaire en ajoutant une p√©nalit√© √† la fonction de perte, mais elles doivent √™tre adapt√©es au contexte des r√©seaux de neurones. Voici comment elles fonctionnent dans l'apprentissage profond :

La r√©gularisation Ridge, √©galement appel√©e r√©gularisation L2, ajoute une p√©nalit√© √©gale au carr√© de l'amplitude des coefficients comme indiqu√© ci-dessous. Ce L\_L2 montre le terme de p√©nalisation qui est ajout√© √† la fonction de perte du r√©seau de neurones. Pour les r√©seaux de neurones, cela signifie que la p√©nalit√© est la somme des carr√©s de tous les poids dans le r√©seau.

$$L2 = \lambda \sum w_i^2$$

o√π lambda est le param√®tre de p√©nalisation, w\_i sont les poids du r√©seau de neurones.

L'effet de cela est qu'il encourage les poids √† √™tre petits mais ne les force pas √† z√©ro.

Cela est utile pour les mod√®les d'apprentissage profond o√π nous ne voulons pas n√©cessairement effectuer une s√©lection de caract√©ristiques (r√©duire la dimension du mod√®le) mais simplement vouloir pr√©venir le surapprentissage en d√©courageant les mod√®les trop complexes qui m√©morisent les donn√©es d'entra√Ænement et ne sont pas g√©n√©ralisables.

Ce terme de r√©gularisation est contr√¥l√© par un hyperparam√®tre, souvent d√©sign√© par la lettre grecque ùúÜ, qui d√©termine la force de cette p√©nalit√©. √Ä mesure que ùúÜ augmente, la p√©nalit√© pour les poids plus grands augmente, et le mod√®le est pouss√© vers des poids plus petits.

### **R√©gularisation Lasso (R√©gularisation L1)**

Lasso signifie Least Absolute Shrinkage and Selection Operator Regularization, √©galement connue sous le nom de r√©gularisation L1 bas√©e sur la norme L1.

La r√©gularisation L1 ajoute une p√©nalit√© √©gale √† la valeur absolue de l'amplitude des coefficients (la somme de ceux-ci). La formule ci-dessous montre le terme de p√©nalisation L\_L1 ajout√© √† la fonction de perte du r√©seau de neurones. Les notions sont les m√™mes que dans le cas de la R√©gularisation Ridge. Cela se traduit pour les r√©seaux de neurones en apprentissage profond comme la somme des valeurs absolues de tous les poids.

$$L1 = \lambda \sum |w_i|$$

La r√©gularisation L1 pousse certains poids √† √™tre exactement z√©ro, r√©alisant ainsi des mod√®les clairsem√©s. Dans l'apprentissage profond, cela peut conduire √† une clairsemance au sein des poids, effectuant ainsi une forme de s√©lection de caract√©ristiques en permettant au mod√®le d'ignorer certaines entr√©es.

Similaire √† la r√©gularisation L2, la force de la r√©gularisation L1 est contr√¥l√©e par un hyperparam√®tre, qui, lorsqu'il est augment√©, peut conduire √† plus de poids √©tant mis √† z√©ro.

Les r√©gularisations L1 et L2 peuvent √™tre utilis√©es individuellement ou combin√©es dans ce qu'on appelle la r√©gularisation Elastic Net comme moyen de r√©gulariser le r√©seau.

L'utilisation de ces techniques de r√©gularisation peut am√©liorer la g√©n√©ralisation des mod√®les d'apprentissage profond. Mais il est √©galement important de consid√©rer d'autres techniques plus courantes en apprentissage profond, telles que le dropout et la normalisation par lots ‚Äì ou d'utiliser toutes ces techniques ensemble (ce qui peut parfois √™tre plus efficace pour pr√©venir le surapprentissage dans les grands r√©seaux de neurones profonds).

Si vous souhaitez en savoir plus sur la r√©gularisation L1/L2, assurez-vous de consulter cette [vid√©o](https://youtu.be/NAfOLSOsyJI) et ce [tutoriel](https://www.youtube.com/watch?v=TDwpx-9M2IE) pour voir comment ces techniques de r√©gularisation p√©nalisent les poids dans le r√©seau de neurones, qui font partie de mon cours gratuit [Pr√©paration aux Entretiens en Apprentissage Profond](https://youtu.be/Lf8XNN3-8nI).

### **Normalisation par Lots**

La Normalisation par Lots est une autre technique importante utilis√©e en Apprentissage Profond qui, bien que n'√©tant pas une m√©thode de r√©gularisation au sens traditionnel, a un effet de r√©gularisation indirect.

Cette technique normalise les entr√©es de chaque couche de mani√®re √† ce qu'elles aient une moyenne d'activation de sortie de z√©ro et un √©cart-type de un. En gros comme une Distribution Gaussienne ‚Äì ce qui est la raison pour laquelle elle est appel√©e Normalisation, car nous normalisons un lot.

![Image](https://www.freecodecamp.org/news/content/images/2024/01/Screenshot-2024-01-31-at-12.44.50-PM.png align="left")

*Normalisation par Lots : [Source de l'Image](https://images.app.goo.gl/TTG1B2reRZszz5c16)*

La figure ci-dessus visualise l'id√©e derri√®re la Normalisation par Lots, qui montre que la normalisation est effectu√©e pour chaque lot, o√π toutes les observations N sont dans 1 lot, et C repr√©sente le nombre de Canaux ou de caract√©ristiques dans vos donn√©es. En gros, cette figure montre que la normalisation par lots normalise les donn√©es par 1 caract√©ristique (√† travers un seul canal) et pour toutes les observations N dans 1 lot.

Cela est r√©alis√© en ajustant et en mettant √† l'√©chelle les activations pendant l'entra√Ænement. La normalisation par lots permet √† chaque couche d'un r√©seau d'apprendre par elle-m√™me un peu plus ind√©pendamment des autres couches. Voyons comment cela fonctionne en d√©tail.

#### √âtape 1 : Moyenne et Variance du Mini-Lot :

Calculer la moyenne des activations pour un mini-lot en utilisant l'√©quation suivante :

$$\mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i$$

Dans cette √©quation,

* \_ùúÜ\_beta\_ùü∞ est la moyenne du mini-lot

* *m* est le nombre d'exemples d'entra√Ænement dans le mini-lot, et

* \_xi\_ùü∞ est l'activation de la couche actuelle avant la normalisation par lots.

Maintenant, vous devrez calculer la variance des activations pour un mini-lot. Vous pouvez le faire en utilisant l'√©quation suivante :

$$\sigma_B^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2$$

#### √âtape 2 : Normaliser les activations du mini-lot

Ensuite, la normalisation se produit :

$$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$

Dans cette √©quation,

* *x*^\_i\_ùü∞ est l'activation normalis√©e

* *ùúπ* est une petite constante ajout√©e pour la stabilit√© num√©rique (pour √©viter la division par z√©ro).

#### √âtape 3 : Appliquer les param√®tres apprenables pour l'√©chelle et le d√©calage

$$y_i = \gamma \hat{x}_i + \beta$$

Bien que l'objectif principal de la normalisation par lots soit de stabiliser et d'acc√©l√©rer le processus d'entra√Ænement des r√©seaux de neurones profonds en r√©duisant le d√©calage covariant interne, elle a √©galement un effet de r√©gularisation qui est une mani√®re de r√©duire le surapprentissage.

En ajoutant un certain niveau de bruit aux activations (puisque la moyenne et la variance sont estim√©es √† partir des donn√©es), elle peut rendre le mod√®le moins sensible aux poids sp√©cifiques des neurones, ce qui a un effet similaire au dropout car cela peut pr√©venir le surapprentissage.

La normalisation par lots peut √™tre particuli√®rement b√©n√©fique dans les r√©seaux de neurones profonds, o√π elle peut permettre l'utilisation de taux d'apprentissage plus √©lev√©s, rendre le r√©seau moins sensible √† l'initialisation, et peut r√©duire le besoin d'autres techniques de r√©gularisation telles que le dropout.

En pratique, la normalisation par lots est appliqu√©e avant la fonction d'activation d'une couche, et elle n√©cessite le maintien d'une moyenne mobile de la moyenne et de la variance √† utiliser pendant l'inf√©rence pour le processus de normalisation.

## Chapitre 6 : Probl√®me du Gradient √âvanescent

Lorsque le gradient de la perte est propag√© en arri√®re √† travers le temps et les couches, il peut diminuer vers 0 (devenir tr√®s petit). Cela conduit √† des mises √† jour tr√®s petites des poids.

Cela rend difficile pour le r√©seau de neurones d'apprendre les d√©pendances √† long terme, ce qui peut entra√Æner aucune mise √† jour dans les couches pr√©c√©dentes du r√©seau dans les param√®tres lorsque les gradients s'√©vanouissent (deviennent tr√®s petits, proches de z√©ro).

Ainsi, lorsque les gradients s'√©vanouissent, les couches initiales du r√©seau s'entra√Ænent tr√®s lentement ou pas du tout, ce qui conduit √† des performances sous-optimales.

### **Utiliser des fonctions d'activation appropri√©es**

Une fa√ßon de r√©soudre le probl√®me du gradient √©vanescent est d'utiliser une fonction d'activation appropri√©e qui ne souffre pas de saturation.

La saturation se produit lorsque, pour la valeur d'entr√©e x qui est un grand nombre positif ou un petit nombre n√©gatif, le gradient de la fonction est proche de 0 car la valeur de la fonction est dans le voisinage des valeurs extr√™mes statiques de celle-ci. Cela ralentit la mise √† jour des param√®tres. Ce ph√©nom√®ne est appel√© le probl√®me de saturation.

Les fonctions d'activation ReLU (Unit√© Lin√©aire Rectifi√©e) et Leaky ReLU ne **saturent** pas dans la direction positive, contrairement aux fonctions Sigmo√Øde ou tanh. Cela peut aider √† att√©nuer le probl√®me du gradient √©vanescent.

Leaky ReLU peut aider davantage en permettant un petit gradient non nul lorsque l'unit√© n'est pas active. Cela est important pour les cas o√π les entr√©es n√©gatives doivent √©galement √™tre prises en compte et o√π obtenir une valeur n√©gative en sortie est acceptable.

Vous pouvez trouver plus de d√©tails sur cette fonction d'activation dans la section sur les Fonctions d'Activation du Chapitre 2.

### **Utiliser l'Initialisation de Xavier ou He**

L'initialisation soigneuse des poids est importante. Une bonne initialisation telle que l'initialisation de Xavier peut aider √† emp√™cher les gradients de devenir trop petits t√¥t dans l'entra√Ænement.

L'initialisation de Xavier, √©galement connue sous le nom d'initialisation de Glorot, est une technique d'initialisation pour les param√®tres de poids dans un r√©seau de neurones. Elle est con√ßue pour r√©soudre le probl√®me des gradients √©vanescents ou explosifs dans les r√©seaux de neurones profonds lorsque les fonctions d'activation **Sigmo√Øde** et **tanh** sont utilis√©es.

Elle porte le nom de Xavier Glorot qui a formul√© cette strat√©gie bas√©e sur la compr√©hension du flux des variances √† travers un r√©seau de neurones pour maintenir les gradients √† une √©chelle raisonnable et les emp√™cher de devenir trop petits pour s'√©vanouir ou trop grands pour exploser.

Voici l'id√©e principale derri√®re l'Initialisation de Xavier :

* Pour une couche donn√©e, les poids sont initialis√©s al√©atoirement √† partir d'une distribution avec une moyenne de 0 et une variance sp√©cifique (peut √©galement √™tre 1 comme dans la distribution gaussienne) qui d√©pend du nombre de neurones d'entr√©e et du nombre de neurones de sortie.

* L'objectif de l'Initialisation de Xavier est d'avoir la variance des sorties de chaque couche √©gale √† la variance de ses entr√©es, et les gradients d'avoir une variance √©gale avant et apr√®s avoir travers√© une couche dans la propagation arri√®re.

Si c'est une **Distribution Uniforme**, les poids sont g√©n√©ralement initialis√©s avec des valeurs tir√©es de ce calcul :

$$W \sim \text{Uniform}\left(-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}, \sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}\right)\$$

$$\text{Var}(W) = \frac{2}{\sqrt{n_{\text{in}} + n_{\text{out}}}}$$

Cette variance pour l'initialisation des poids dans le R√©seau de Neurones est g√©n√©ralement d√©finie √† cette valeur ci-dessus, d√©sign√©e par Var(W) - comme dans la Variance de la matrice de Poids W, o√π :

* \_n\_\_inùü∞ est le nombre de neurones alimentant la couche

* \_n\_\_outùü∞ est le nombre de neurones auxquels les r√©sultats sont aliment√©s (c'est-√†-dire le nombre de neurones dans la couche suivante)

Si une **Distribution Normale** est utilis√©e √† la place, alors les poids sont tir√©s de celle-ci :

$$W \sim \text{Normal}\left(0, \sqrt{\frac{2}{n_{\text{in}} + n_{\text{out}}}}\right)\$$

$$\text{Var}(W) = \frac{2}{\sqrt{n_{\text{in}} + n_{\text{out}}}}$$

### **Effectuer la Normalisation par Lots**

La normalisation par lots sur la couche d'entr√©e peut aider √† maintenir une moyenne de sortie proche de 0 et un √©cart-type proche de 1 comme la Distribution Normale Standard. Cela emp√™che les gradients de devenir trop petits.

En normalisant les activations, vous stabilisez directement le r√©seau mais contr√¥lez indirectement le changement des poids de votre r√©seau. Cela signifie que les gradients restent plus constants, et en cons√©quence indirecte, les gradients de BatchNorm ne s'√©vanouiront ni n'exploseront.

### **Ajouter des Connexions R√©siduelles (surtout aux RNN ou LSTM)**

Les connexions r√©siduelles offrent des r√©sultats d'optimisation innovants pour l'entra√Ænement des r√©seaux de neurones profonds, en particulier lorsqu'il s'agit de combattre le probl√®me des gradients √©vanescents.

Cela est particuli√®rement un probl√®me lors de la manipulation des r√©seaux de neurones r√©currents (RNN) ou des r√©seaux de m√©moire √† long et court terme (LSTM), qui sont intrins√®quement profonds en raison de leur nature s√©quentielle. L'incorporation de connexions r√©siduelles dans les RNN ou LSTM peut am√©liorer consid√©rablement leurs capacit√©s d'apprentissage.

Les RNN et LSTM sont sp√©cialis√©s pour g√©rer des s√©quences de donn√©es, ce qui les rend id√©aux pour des t√¢ches comme la mod√©lisation du langage et l'analyse des s√©ries temporelles. Mais √† mesure que la longueur de la s√©quence augmente, ces r√©seaux tendent √† souffrir du probl√®me des gradients √©vanescents.

Pour r√©soudre ce probl√®me, des connexions r√©siduelles sont souvent utilis√©es pour les RNN et LSTM. En ajoutant un raccourci qui contourne une ou plusieurs couches, une connexion r√©siduelle permet au gradient de circuler √† travers le r√©seau de mani√®re plus directe. Dans le contexte des RNN et LSTM, cela signifie connecter la sortie d'un pas de temps non seulement au pas de temps suivant, mais aussi √† un ou plusieurs pas de temps ult√©rieurs.

#### Comment Impl√©menter les Connexions R√©siduelles dans les RNN et LSTM

La strat√©gie d'impl√©mentation des connexions r√©siduelles dans les RNN et LSTM est simple. Pour chaque pas de temps, nous modifions le r√©seau de sorte que la sortie ne soit pas seulement une fonction de l'entr√©e actuelle et de l'√©tat cach√© pr√©c√©dent, mais qu'elle inclue √©galement l'entr√©e directement.

Ce processus peut √™tre d√©crit comme suit o√π nous ajoutons x √† la sortie F(x). Vous pouvez √©galement voir le chemin direct pour que le gradient circule dans le r√©seau et la d√©rivation math√©matique bas√©e sur ce processus d'ajout de l'entr√©e √† la sortie :

![Image](https://www.freecodecamp.org/news/content/images/2024/02/Screenshot-2023-12-28-at-1.49.39-PM.png align="left")

*√âcr√™tage du Gradient (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

$$y = x + F(x)\$$

$$\frac{\partial E}{\partial x} = \frac{\partial E}{\partial y} \cdot \frac{\partial y}{\partial x}\$$

$$\frac{\partial E}{\partial x} = \frac{\partial E}{\partial y} \cdot (1 + F'(x))\$$

$$\frac{\partial E}{\partial x} = \frac{\partial E}{\partial y} \cdot F'(x)$$

Ici, vous pouvez voir les math√©matiques derri√®re les Connexions R√©siduelles et comment le gradient obtient le raccourci. Lorsque nous ajoutons x √† F(x) pour obtenir y, au lieu de simplement y=F(x), vous pouvez voir que lorsque nous prenons la d√©riv√©e d'une fonction E (disons notre fonction de perte) par rapport √† x. Ensuite, nous utilisons la r√®gle de la cha√Æne des math√©matiques diff√©rentielles.

Apr√®s ces transformations, vous pouvez voir que nous finissons par avoir la somme de deux valeurs :

* Gradient de la fonction de perte par rapport √† y

* Gradient de la fonction de perte par rapport √† y multipli√© par la d√©riv√©e partielle de F(x) par rapport √† x

Ainsi, vous pouvez voir ici que dans le cas o√π la connexion r√©siduelle est faite, et nous ajoutons un x suppl√©mentaire au y simple = F(x), cela finit par ajouter un Gradient suppl√©mentaire de la fonction de perte par rapport √† y sans aucun autre facteur de multiplication √† ajouter au gradient final.

Pour une explication intuitive et d√©taill√©e, consultez cette r√©ponse de tutoriel-entretien sur les [Connexions R√©siduelles](https://www.youtube.com/watch?v=bF7dUSepiLg).

**Flux de Gradient Direct** : En fournissant un raccourci pour le flux de gradient, il est moins susceptible de s'√©vanouir lorsqu'il est propag√© en arri√®re √† travers le temps. Cela garantit que m√™me les couches les plus anciennes de la s√©quence peuvent √™tre efficacement entra√Æn√©es.

**Apprentissage des Mappages d'Identit√©** : Si la fonction optimale pour un pas de temps est de copier son entr√©e vers la sortie, le r√©seau peut apprendre plus facilement ce mappage d'identit√© avec des connexions r√©siduelles. Le r√©seau peut ainsi se concentrer sur l'affinement des √©carts par rapport √† l'identit√© plut√¥t que de l'apprendre √† partir de z√©ro.

**Facilitation des Architectures Plus Profondes** : Avec l'int√©gration de connexions r√©siduelles, il devient r√©alisable de construire des RNN ou LSTM plus profonds. Cette profondeur permet au r√©seau d'apprendre des motifs et des relations plus complexes au sein des donn√©es.

## Chapitre 7 : Combattre les Gradients Explosifs

Les gradients explosifs sont le probl√®me oppos√© des Gradients √âvanescents. Les Gradients Explosifs se produisent lors de l'entra√Ænement de mod√®les d'apprentissage profond, en particulier ceux impliquant des r√©seaux de neurones pendant la phase de r√©tropropagation, lorsque les gradients deviennent trop grands et explosent.

Mais dans les r√©seaux profonds avec de nombreuses couches, ces gradients peuvent s'accumuler et cro√Ætre de mani√®re exponentielle √† travers chaque couche. Cette augmentation exponentielle est due √† la multiplication r√©p√©titive des gradients √† travers la profondeur du r√©seau, surtout lorsque les gradients sont trop grands en magnitude.

Cela entrave le processus d'apprentissage et rend le r√©seau de neurones moins efficace pour apprendre les informations importantes dans les couches.

Examinons comment nous pouvons r√©soudre ce probl√®me.

### √âcr√™tage des Gradients

Une fa√ßon de r√©soudre le probl√®me des gradients explosifs est d'utiliser l'√âcr√™tage des Gradients. L'√©cr√™tage des gradients est une technique pratique utilis√©e pour emp√™cher les gradients d'exploser pendant l'entra√Ænement des r√©seaux de neurones.

Lorsque les gradients calcul√©s sont trop grands, l'√©cr√™tage des gradients les r√©duit √† un seuil pr√©d√©termin√©. Cela garantit des mises √† jour stables et coh√©rentes des param√®tres du mod√®le.

Ce processus implique :

* **√âtape 1 : Calculer le gradient (*g*)** : Obtenir le gradient de la fonction de perte par rapport aux param√®tres du mod√®le.

* **√âtape 2 : Mettre √† l'√©chelle le gradient** : Si la norme de ce gradient ùü∞\_g\_ùü∞ est plus grande qu'un seuil sp√©cifi√© *c*, nous r√©duisons le gradient *g* pour qu'il ait la norme *c*, en maintenant sa direction. Cela est fait en d√©finissant *g* √† *c\_ùü∞\_g*/ùü∞\_g\_ùü∞

* **√âtape 3 : Mettre √† jour les param√®tres** : Nous utilisons le gradient √©cr√™t√© pour une mise √† jour contr√¥l√©e et plus mod√©r√©e.

L'√©cr√™tage des gradients garantit que le processus d'apprentissage du mod√®le ne d√©raille pas en raison de ces grandes mises √† jour r√©sultant de gradients explosifs, qui peuvent se produire en pr√©sence de pentes raides dans le paysage de perte lorsque l'optimisation devrait se produire apr√®s la r√©tropropagation.

En maintenant ces mises √† jour des param√®tres de poids et de biais dans une taille "s√ªre", cette m√©thode aide √† naviguer dans le paysage de perte plus doucement, contribuant √† une meilleure convergence de l'entra√Ænement. Le seuil c\_c\_ est un hyperparam√®tre qui n√©cessite un r√©glage pour √©quilibrer entre une vitesse d'apprentissage ad√©quate et la stabilit√©.

## Chapitre 8 : Mod√©lisation de S√©quences avec les RNN et LSTM

Dans ce chapitre, vous apprendrez √† conna√Ætre l'un des types de mod√®les de R√©seaux de Neurones les plus populaires, les R√©seaux de Neurones R√©currents (RNN).

La mod√©lisation de s√©quences est une pierre angulaire de l'apprentissage profond pour les donn√©es s√©quentielles telles que les s√©ries temporelles, la parole et le texte. Nous examinerons les m√©canismes des RNN, leurs limites inh√©rentes et l'√©volution des architectures avanc√©es telles que la M√©moire √† Long et Court Terme (LSTM).

### **Architecture des R√©seaux de Neurones R√©currents (RNN)**

Les RNN se distinguent par leur capacit√© unique √† former un graphe dirig√© le long d'une s√©quence, leur permettant d'exhiber un **comportement dynamique temporel**. Contrairement aux r√©seaux de neurones feedforward habituels, les RNN peuvent utiliser leur √©tat interne (m√©moire) pour traiter des **s√©quences** d'entr√©es.

Au c≈ìur du RNN se trouve le concept de cellule, qui est l'unit√© r√©p√©titive formant la base de la capacit√© du RNN √† maintenir une m√©moire √† travers les s√©quences d'entr√©e (l'√©l√©ment de temps et de s√©quence). √Ä un niveau √©lev√©, un RNN peut √™tre visualis√© comme suit :

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-164.png align="left")

*Architecture RNN (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Cette visualisation facilite la compr√©hension de cette architecture plus complexe. Comme vous pouvez le voir sur l'image, la couche cach√©e utilis√©e sur une observation sp√©cifique d'un ensemble de donn√©es n'est pas seulement utilis√©e pour g√©n√©rer une sortie pour cette observation (sortie y utilisant h\_t), mais elle est √©galement utilis√©e pour entra√Æner la couche cach√©e de l'observation suivante (h\_t est utilis√© avec x\_(t+1) pour obtenir h\_(t+1)).

Contrairement aux R√©seaux de Neurones de base qui ont une seule entr√©e, plusieurs couches cach√©es ind√©pendantes, puis une seule sortie Y, les RNN ont une architecture diff√©rente.

Outre le fait d'avoir une structure diff√©rente pour l'entr√©e et la sortie (c'est-√†-dire, plusieurs entr√©es et sorties), la chose la plus importante √† remarquer ici est que dans les couches cach√©es d'un RNN, les √©tats cach√©s sont interconnect√©s.

Cette propri√©t√© "d√©pendante" d'une observation aidant √† pr√©dire l'observation suivante est la raison pour laquelle les r√©seaux de neurones r√©currents sont si pratiques lorsqu'il s'agit de probl√®mes avec des √©l√©ments de temps ou de s√©quence (comme les probl√®mes d'analyse de s√©ries temporelles ou de pr√©diction du mot suivant en NLP).

### **Pseudocode des R√©seaux de Neurones R√©currents**

Pour commencer, regardons par exemple le premier pas de temps. L'√©tat cach√© au pas de temps 1 est calcul√© comme suit :

$$h_1 = f(W_{xh} \cdot X_1 + W_{hh} \cdot h_0 + b_h)$$

o√π :

* *f* est une fonction d'activation (comme ReLU ou Tanh)

* *W\_xh* est la matrice de poids d'entr√©e √† cach√©e

* *W\_hh* est la matrice de poids de cach√©e √† cach√©e

* *h\_0* est l'√©tat cach√© initial (pr√©c√©dent)

* *b\_h* est le biais de la couche cach√©e

*W\_hh* est souvent appel√© la **matrice de poids r√©currente**. C'est la matrice qui d√©finit combien chacun de ces √©tats cach√©s pr√©c√©dents contribuera au calcul de l'√©tat cach√© pr√©sent.

Ensuite, la sortie √† ce premier pas de temps est la suivante :

$$Y_1 = W_{hy} \cdot h_1 + b_y$$

o√π :

* \_W\_hy\_ùü∞ est la matrice de poids de cach√©e √† couche de sortie

* \_b\_y\_ùü∞ est le biais pour la couche de sortie

L'algorithme RNN pour tous les pas de temps de 1 √† T peut √™tre d√©crit avec le pseudocode suivant :

```python
Algorithme 1 Pas de Temps des R√©seaux de Neurones R√©currents
1: pour chaque pas de temps t = 1 √† T faire
2:     Entr√©e : Xt
3:     Initialiser : h0 √† un vecteur de z√©ros
4:     Param√®tres :
5:         Wxh : Matrice de poids de l'entr√©e √† la couche cach√©e
6:         Whh : Matrice de poids r√©currente pour la couche cach√©e
7:         Why : Matrice de poids de la couche cach√©e √† la couche de sortie
8:         bh : Biais pour la couche cach√©e
9:         by : Biais pour la couche de sortie
10:     Fonction d'activation : f (par exemple, tanh, ReLU)
11:     Mise √† jour de l'√âtat Cach√© :
12:         ht = f(Wxh . Xt + Whh . htùü∞1 + bh)
13:     Sortie :
14:         Yt = Why . ht + by
15: fin pour
```

### **Limitations des R√©seaux de Neurones R√©currents**

Maintenant, discutons des limitations des RNN et pourquoi les LSTM sont entr√©s en jeu ainsi que plus tard les Transformers ! Voici les limitations du R√©seau de Neurones R√©current :

* Probl√®me du Gradient √âvanescent

* Probl√®me du Gradient Explosif

* Calcul S√©quentiel

* Difficult√© √† G√©rer les Longues S√©quences

* Informations Contextuelles Limit√©es

Puisque nous avons d√©j√† discut√© des probl√®mes de Gradient √âvanescent et Explosif et comment les r√©soudre, nous passerons aux limitations restantes des RNN avant de discuter des variations des RNN qui abordent ces d√©fis.

Mais d'abord, notons simplement : surtout dans le cas des RNN, le Gradient √âvanescent peut entra√Æner un apprentissage tr√®s lent des couches initiales du r√©seau, voire pas du tout, ce qui rend les RNN mal adapt√©s √† l'apprentissage des d√©pendances √† long terme au sein d'une s√©quence. Et dans le cas des Gradients Explosifs, ils peuvent entra√Æner de grandes mises √† jour des param√®tres du r√©seau et, par cons√©quent, un R√©seau de Neurones R√©current instable.

#### Limitation du Calcul S√©quentiel

La nature s√©quentielle des RNN ne permet pas de parall√©lisation pendant l'entra√Ænement car le calcul de l'√©tape suivante d√©pend de l'√©tape pr√©c√©dente. Cela peut entra√Æner des processus d'entra√Ænement beaucoup plus lents par rapport aux r√©seaux de neurones qui permettent une parall√©lisation compl√®te.

#### Limitation de la Gestion des Longues S√©quences

Les RNN peuvent avoir des difficult√©s √† traiter les longues s√©quences car les informations du d√©but de la s√©quence peuvent √™tre perdues au moment o√π elles atteignent la fin en cas de probl√®me de Gradient √âvanescent.

#### Limitation des Informations Contextuelles

C'est l'une des limitations les plus importantes des RNN qui a motiv√© l'invention des Transformers. Les RNN standard n'ont pas de m√©canisme pour se souvenir ou oublier s√©lectivement les informations, ce qui peut √™tre une limitation lors du traitement de s√©quences o√π seules certaines parties sont pertinentes pour la pr√©diction.

### **Architecture de la M√©moire √† Long et Court Terme (LSTM)**

Les r√©seaux de m√©moire √† long et court terme ou LSTM sont un type sp√©cial de RNN con√ßu pour att√©nuer la plupart des limitations des RNN traditionnels. Ils int√®grent des m√©canismes que nous appelons **Cellules** qui permettent au r√©seau de r√©guler les informations qui le traversent.

Ces portes sont :

* **Porte d'Oubli** : Porte qui d√©termine quelles informations doivent √™tre supprim√©es de l'√©tat de la cellule

* **Porte d'Entr√©e** : Porte qui met √† jour l'√©tat de la cellule avec de nouvelles informations

* **Porte de Sortie** : Porte qui d√©termine quel doit √™tre le prochain √©tat cach√©

![Image](https://www.freecodecamp.org/news/content/images/2024/01/image-43.png align="left")

*Architecture LSTM (Source de l'Image : [LunarTech.ai](https://lunartech.ai/course-overview/))*

Ce diagramme repr√©sente l'architecture d'un r√©seau LSTM (Long Short-Term Memory), visualisant le flux de donn√©es √† travers ses composants √† diff√©rents pas de temps. Plongeons plus profond√©ment dans chacun de ces √©tats et le processus qui les sous-tend :

**√âtats de Cellule** : En haut, nous avons des rectangles en jaune √©tiquet√©s C\_(*tùü∞1*), *C*(t)ùü∞\_, ùü∞ *C*(t+1)\_ùü∞. Ceux-ci repr√©sentent les √©tats de cellule du LSTM √† des pas de temps cons√©cutifs.

Ces √©tats de cellule sont un composant cl√© du LSTM car ils transportent les informations pertinentes tout au long du traitement de la s√©quence. Ils contiennent les informations sur quelles informations utiliser, quelles informations oublier et quelles informations produire.

Les fl√®ches indiquent le flux et les transformations de l'√©tat de cellule d'un pas de temps √† l'autre.

**Portes** : Au milieu, vous pouvez voir les 3 portes, des blocs color√©s repr√©sentant les portes du LSTM :

* **Porte d'Oubli (Rose)** : D√©termine quelles parties de l'√©tat de cellule *C*(tùü∞1)\_ùü∞ doivent √™tre oubli√©es et quelles parties doivent √™tre retenues, pour produire l'√©tat de cellule suivant *C*(t)\_ùü∞.

* **Porte d'Entr√©e (Verte)** : D√©cide quelles valeurs seront mises √† jour dans l'√©tat de cellule en fonction de l'entr√©e au pas de temps actuel.

* **Porte de Sortie (Violette)** : D√©termine quelle partie de l'√©tat de cellule sera utilis√©e pour g√©n√©rer l'√©tat cach√© de sortie \_h\_t\_ùü∞.

Ces 3 portes contr√¥lent le flux d'informations et leur quantit√©, avec des lignes reliant l'√©tat cach√© pr√©c√©dent *h*(tùü∞1) ùü∞\_et l'√©tat de cellule √† chacune de ces portes, illustrant comment chacune d'elles contribue √† l'√©tat actuel.

**Diff√©rence entre Cellule et Portes** : Notez que les cellules et les portes sont des concepts diff√©rents, o√π, comme vous pouvez le voir sur le diagramme, la cellule est √† un niveau sup√©rieur √† celui des portes, et pour chaque pas de temps, il y a un seul √©tat de cellule mais 3 portes. L'√©tat de la cellule utilise essentiellement 3 portes pour r√©guler le flux d'informations. C'est comme une fonction qui utilise 3 valeurs d'entr√©e pour g√©n√©rer une sortie, pour le dire simplement.

Commun √† l'architecture originale des RNN, l'√©tat cach√© √† chaque pas de temps est influenc√© par l'√©tat cach√© pr√©c√©dent et l'entr√©e actuelle, ainsi que les op√©rations internes des portes communes aux LSTM.

Ces portes au sein des LSTM lui permettent d'apprendre quelles informations conserver ou supprimer au fil du temps, ce qui rend possible la capture de d√©pendances √† long terme dans les donn√©es.

#### Comment les LSTM abordent les limitations des RNN

* **R√©solution des Gradients √âvanescents et Explosifs** : Les LSTM sont con√ßus pour avoir des changements de poids plus constants, ce qui signifie que les gradients sont plus constants. Cela leur permet d'apprendre sur de nombreux pas de temps, r√©solvant ainsi les probl√®mes de gradients √©vanescents/explosifs gr√¢ce √† leur m√©canisme de gating et en maintenant un **√©tat de cellule s√©par√©**.

* **D√©pendances √† Long Terme** : En apprenant ce qu'il faut stocker et ce qu'il faut supprimer/oublier de l'√©tat de cellule, les LSTM peuvent maintenir des d√©pendances ou des relations √† long terme dans les donn√©es. Cela les rend plus efficaces pour les t√¢ches impliquant de longues s√©quences telles que celles des mod√®les de langage.

* **M√©moire S√©lective** : Les LSTM peuvent apprendre √† ne conserver que les informations pertinentes pour faire des pr√©dictions en utilisant la porte d'oubli. Oublier les donn√©es non pertinentes, ce qui les rend meilleurs pour mod√©liser des s√©quences complexes o√π la pertinence de l'information varie avec le temps, aide √† faire exactement cela.

#### Limites des LSTM

Bien que les LSTM repr√©sentent une am√©lioration significative par rapport aux RNN originaux, ils pr√©sentent encore certains inconv√©nients majeurs, tels que d'√™tre plus intensifs en calcul en raison de leur architecture complexe. Les LSTM ont un nombre plus √©lev√© de param√®tres, ce qui peut entra√Æner des temps d'entra√Ænement plus longs et n√©cessiter plus de donn√©es pour g√©n√©raliser efficacement.

De plus, similaire aux RNN, les LSTM traitent √©galement les donn√©es de mani√®re s√©quentielle, ce qui signifie qu'ils ne peuvent pas √™tre enti√®rement **parall√©lis√©s**.

Ainsi, la parall√©lisation et le temps d'entra√Ænement plus long en raison du nombre plus √©lev√© de param√®tres restent deux inconv√©nients majeurs pour les RNN et les LSTM.

## **Chapitre 9 : Pr√©paration aux Entretiens en Apprentissage Profond**

Alors que nous atteignons le point culminant de ce manuel complet, il est temps de se concentrer sur la traduction de vos nouvelles connaissances en succ√®s dans le monde r√©el.

Que vous visiez √† entrer dans l'industrie de l'IA ou que vous ayez en vue un poste convoit√© de Chercheur en IA ou d'Ing√©nieur en IA dans les entreprises FAANG, le dernier obstacle est souvent le plus difficile mais aussi le plus gratifiant : l'entretien d'embauche.

Vous devrez conna√Ætre les d√©tails et √™tre capable de r√©pondre √† des questions pi√®ges qui vont au-del√† des informations th√©oriques de surface.

Vous devrez conna√Ætre :

* Les R√©seaux de Neurones Convolutifs (pooling, padding, noyaux)

* Les R√©seaux de Neurones R√©currents (RNN), LSTM, GRU

* La Normalisation par Lots/Couche

* Les R√©seaux Adverses G√©n√©ratifs (GAN)

* Les Auto-Encodeurs (Architectures Encodeur-D√©codeur)

* Les Auto-Encodeurs Variationnels (Divergence KL, ELBO)

* Les Embeddings

* Le M√©canisme d'Attention Multi-T√™tes

* Les Transformers

Et ce ne sont l√† que quelques sujets que vous pouvez attendre pour vos entretiens plus avanc√©s/FAANG. Consultez la liste compl√®te des 100 questions de ce programme √† l'adresse suivante : [**ici**](https://www.freecodecamp.org/news/ghost/#/editor/post/6548108b21405e03f5049361).

Comprenant l'importance de cette √©tape cruciale, je suis ravi de vous pr√©senter mon cours sp√©cialement con√ßu de Pr√©paration aux Entretiens en Apprentissage Profond, sponsoris√© par LunarTech, disponible sur LunarTech.ai et Udemy. Ce cours est m√©ticuleusement con√ßu pour vous assurer d'√™tre non seulement pr√™t pour l'entretien, mais aussi pr√™t √† exceller dans le march√© du travail en IA hautement comp√©titif.

%[https://youtu.be/78U-0bS2DJg?si=P5LoxvN-1nf3So_4] 

Voici ce que couvre le cours :

1. [Partie 1 ‚Äì Les Essentiels (Cours Gratuit de 4 Heures)](https://courses.lunartech.ai/courses/deep-learning-interview-preparation-course-100-q-a-s) : Je crois en l'autonomisation de chaque passionn√© de Data Science et d'IA. Je propose donc la premi√®re partie de mon cours d'entretien absolument gratuit. Cette section comprend le premier ensemble de 50 questions d'entretien qui couvrent l'√©tendue des fondamentaux de l'apprentissage profond.

2. [**Cours Complet**](https://courses.lunartech.ai/courses/deep-learning-interview-preparation-course-100-q-a-s) ‚Äì [**[Cours Complet de Pr√©paration aux Entretiens en Apprentissage Profond - 100 Q&R 7.5 heures \]**](https://courses.lunartech.ai/courses/deep-learning-interview-preparation-course-100-q-a-s)** : Pour ceux qui sont d√©termin√©s √† ne laisser aucune pierre non retourn√©e, notre cours complet sur LunarTech.ai est l'outil ultime de pr√©paration pour les entretiens faciles mais aussi complexes en Apprentissage Profond. S'√©tendant √† 100 questions d'entretien approfondies, ce cours complet explore les nuances de l'apprentissage profond, garantissant que vous vous d√©marquez m√™me dans les entretiens les plus exigeants, y compris ceux des entreprises comme FAANG.

C'est votre opportunit√© de d√©passer le statut de candidat ‚Äì pour devenir un leader dans le domaine de l'IA.

## √Ä Propos de l'Auteur

Je suis Tatev Aslanyan, Chercheur Senior en Machine Learning et IA. J'ai eu le privil√®ge de travailler dans le domaine de la Data Science dans de nombreux pays, dont les √âtats-Unis, le Royaume-Uni, le Canada et les Pays-Bas. Je suis Co-fondateur de [**LunarTech**](https://lunartech.ai) o√π nous rendons la Data Science et l'IA plus accessibles √† tous !

Avec un MSc et un BSc en √âconom√©trie √† mon actif, mon parcours dans le domaine du Machine Learning et de l'IA a √©t√© tout simplement incroyable. Tirant parti de mes √©tudes techniques pendant mon Bachelor & Master, ainsi que de plus de 5 ans d'exp√©rience pratique dans l'industrie de la Data Science, du Machine Learning et de l'IA.

## Connectez-vous avec Moi et LunarTech

![Screenshot-2023-10-23-at-6.59.27-PM](https://www.freecodecamp.org/news/content/images/2024/01/image-5-1.png align="left")

*Source de l'Image : \[LunarTech\](https://lunartech.ai" style="box-sizing: inherit; margin: 0px; padding: 0px; border: 0px; font-style: inherit; font-variant-caps: inherit; font-weight: inherit; font-stretch: inherit; line-height: inherit; font-family: inherit; font-size-adjust: inherit; font-kerning: inherit; font-variant-alternates: inherit; font-variant-ligatures: inherit; font-variant-numeric: inherit; font-variant-east-asian: inherit; font-variant-position: inherit; font-feature-settings: inherit; font-optical-sizing: inherit; font-variation-settings: inherit; font-size: 17.6px; vertical-align: baseline; background-color: transparent; color: var(--gray90); text-decoration: underline; cursor: pointer; word-break: break-word;)*

* Suivez-moi sur [LinkedIn](https://www.linkedin.com/in/tatev-karen-aslanyan/) et [X](https://twitter.com/tatevkaren7)

* Visitez mon [Site Web Personnel pour des Ressources Gratuites](https://tatevaslanyan.com/free-resources/)

* Abonnez-vous √† [La Newsletter sur la Data Science et l'IA](https://substack.com/@lunartech)

Si vous souhaitez commencer une carri√®re dans la Data Science ou l'IA, t√©l√©chargez mon guide gratuit [Data Science and AI Handbook](https://downloads.tatevaslanyan.com/six-figure-data-science-bootcamp--5a571) ou [Fondamentaux du Machine Learning Handbook](https://join.lunartech.ai/machine-learning-fundamentals)

Meilleurs v≈ìux dans toutes vos futures entreprises !