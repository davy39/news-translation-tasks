---
title: Tutoriel de Machine Learning ‚Äì Ing√©nierie des caract√©ristiques et s√©lection
  des caract√©ristiques pour d√©butants
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2021-04-07T01:06:07.000Z'
originalURL: https://freecodecamp.org/news/feature-engineering-and-feature-selection-for-beginners
coverImage: https://www.freecodecamp.org/news/content/images/2021/04/tools-864983_1920.jpg
tags:
- name: data analysis
  slug: data-analysis
- name: Data Science
  slug: data-science
- name: Machine Learning
  slug: machine-learning
- name: Python
  slug: python
seo_title: Tutoriel de Machine Learning ‚Äì Ing√©nierie des caract√©ristiques et s√©lection
  des caract√©ristiques pour d√©butants
seo_desc: 'By Davis David

  They say data is the new oil, but we don''t use oil directly from its source. It
  has to be processed and cleaned before we use it for different purposes.

  The same applies to data, we don''t use it directly from its source. It also has
  to...'
---

Par Davis David

On dit que les **donn√©es** sont le nouveau **p√©trole**, mais nous n'utilisons pas le p√©trole directement depuis sa source. Il doit √™tre trait√© et nettoy√© avant de pouvoir √™tre utilis√© pour diff√©rentes fins.

Il en va de m√™me pour les donn√©es, nous ne les utilisons pas directement depuis leur source. Elles doivent √©galement √™tre trait√©es.

![Image](https://www.freecodecamp.org/news/content/images/2021/04/imageonline-co-merged-image.png)
_Industrie p√©troli√®re_

Cela peut √™tre un d√©fi pour les d√©butants en Machine Learning et en Data Science car les donn√©es proviennent de diff√©rentes sources avec diff√©rents types de donn√©es. Par cons√©quent, vous ne pouvez pas appliquer la m√™me m√©thode de nettoyage et de traitement √† diff√©rents types de donn√©es. 

> "L'information peut √™tre extraite des donn√©es tout comme l'√©nergie peut √™tre extraite du p√©trole." - [Adeola Adesina](https://medium.com/@adeolaadesina)

Vous devez apprendre et appliquer des m√©thodes en fonction des donn√©es que vous avez. Ensuite, vous pouvez en tirer des informations ou les utiliser pour l'entra√Ænement dans des algorithmes de machine learning ou de deep learning.

Apr√®s avoir lu cet article, vous saurez :

* Ce qu'est l'ing√©nierie des caract√©ristiques et la s√©lection des caract√©ristiques.
* Diff√©rentes m√©thodes pour g√©rer les donn√©es manquantes dans votre ensemble de donn√©es.
* Diff√©rentes m√©thodes pour g√©rer les caract√©ristiques continues.
* Diff√©rentes m√©thodes pour g√©rer les caract√©ristiques cat√©gorielles.
* Diff√©rentes m√©thodes pour la s√©lection des caract√©ristiques.

Commen√ßons.üöÄ

# Qu'est-ce que l'ing√©nierie des caract√©ristiques ?

L'ing√©nierie des caract√©ristiques fait r√©f√©rence √† un processus de s√©lection et de **transformation** des variables/caract√©ristiques dans votre ensemble de donn√©es lors de la cr√©ation d'un **mod√®le pr√©dictif** utilisant le machine learning. 

Par cons√©quent, vous devez extraire les caract√©ristiques de l'**ensemble de donn√©es brut** que vous avez collect√© avant d'entra√Æner vos donn√©es dans des algorithmes de machine learning.   
Sinon, il sera difficile d'obtenir de bonnes informations √† partir de vos donn√©es.

> Torturez les donn√©es, et elles avoueront n'importe quoi. ‚Äî Ronald Coase

L'ing√©nierie des caract√©ristiques a deux objectifs :

* Pr√©parer l'ensemble de donn√©es d'entr√©e appropri√©, compatible avec les exigences de l'algorithme de machine learning.
* Am√©liorer les **performances** des mod√®les de machine learning.

![Image](https://www.freecodecamp.org/news/content/images/2021/04/Picture1.jpg)
_Enqu√™te CrowdFlower_

Selon une enqu√™te men√©e aupr√®s de 80 Data Scientists par CrowdFlower, les Data Scientists passent **60%** de leur temps √† nettoyer et organiser les donn√©es. C'est pourquoi avoir des comp√©tences en ing√©nierie et s√©lection des caract√©ristiques est tr√®s important.  

> "√Ä la fin de la journ√©e, certains projets de machine learning r√©ussissent, et d'autres √©chouent. Qu'est-ce qui fait la diff√©rence ? De loin, le facteur le plus important est les **caract√©ristiques** utilis√©es." ‚Äî Prof. Pedro Domingos de l'Universit√© de Washington

Vous pouvez lire son article √† partir du lien suivant : "[A Few Useful Things to Know About Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)".

Maintenant que vous savez pourquoi vous devez apprendre diff√©rentes techniques d'ing√©nierie des caract√©ristiques, commen√ßons par apprendre diff√©rentes m√©thodes pour g√©rer les donn√©es manquantes.

## Comment g√©rer les donn√©es manquantes

G√©rer les donn√©es manquantes est tr√®s important car de nombreux algorithmes de machine learning ne supportent pas les donn√©es avec des valeurs manquantes. Si vous avez des valeurs manquantes dans l'ensemble de donn√©es, cela peut causer des erreurs et de mauvaises performances avec certains algorithmes de machine learning.  
  
Voici la liste des valeurs manquantes courantes que vous pouvez trouver dans votre ensemble de donn√©es.

* N/A
* null
* Vide
* ?
* none
* vide
* -
* NaN

Apprenons diff√©rentes m√©thodes pour r√©soudre le probl√®me des donn√©es manquantes.

### Suppression de variable

La suppression de variable consiste √† supprimer des variables (colonnes) avec des valeurs manquantes au cas par cas. Cette m√©thode est judicieuse lorsqu'il y a beaucoup de valeurs manquantes dans une variable et si la variable est de relativement moindre importance.

Le seul cas o√π il peut valoir la peine de supprimer une variable est lorsque ses valeurs manquantes repr√©sentent plus de **60%** des observations.

```python
# import des packages
import numpy as np 
import pandas as pd 

# lecture de l'ensemble de donn√©es 
data = pd.read_csv('path/to/data')

# d√©finir le seuil
threshold = 0.7

# suppression des colonnes avec un taux de valeurs manquantes sup√©rieur au seuil
data = data[data.columns[data.isnull().mean() < threshold]]

```

Dans l'extrait de code ci-dessus, vous pouvez voir comment j'utilise NumPy et pandas pour charger l'ensemble de donn√©es et d√©finir un seuil √† **0,7**. Cela signifie que toute colonne ayant des valeurs manquantes repr√©sentant plus de **70%** des observations sera supprim√©e de l'ensemble de donn√©es.

Je vous recommande de d√©finir votre valeur de seuil en fonction de la taille de votre ensemble de donn√©es.

### Imputation par la moyenne ou la m√©diane

Une autre technique courante consiste √† utiliser la moyenne ou la m√©diane des observations non manquantes. Cette strat√©gie peut √™tre appliqu√©e √† une caract√©ristique ayant des donn√©es num√©riques.

```python
# remplissage des valeurs manquantes avec les m√©dianes des colonnes
data = data.fillna(data.median())
```

Dans l'exemple ci-dessus, nous utilisons la **m√©thode de la m√©diane** pour remplir les valeurs manquantes dans l'ensemble de donn√©es.

### Valeur la plus courante

Cette m√©thode consiste √† remplacer les valeurs manquantes par la **valeur la plus fr√©quente** dans une colonne/caract√©ristique. C'est une bonne option pour g√©rer les colonnes/caract√©ristiques **cat√©gorielles**.

```python
# remplissage des valeurs manquantes avec les m√©dianes des colonnes
data['column_name'].fillna(data['column_name'].value_counts().idxmax(), inplace=True)
```

Ici, nous utilisons la **m√©thode value_counts()** de pandas pour compter l'occurrence de chaque valeur unique dans la colonne, puis nous remplissons la valeur manquante avec la valeur la plus courante.

## Comment g√©rer les caract√©ristiques continues

Les caract√©ristiques continues dans l'ensemble de donn√©es ont une plage de valeurs diff√©rente. Des exemples courants de caract√©ristiques continues sont l'√¢ge, le salaire, les prix et les tailles.

Il est tr√®s important de g√©rer les caract√©ristiques continues dans votre ensemble de donn√©es avant d'entra√Æner des algorithmes de machine learning. Si vous entra√Ænez votre mod√®le avec une plage de valeurs diff√©rente, le mod√®le ne performera pas bien.

Que veux-je dire lorsque je parle d'une plage de valeurs diff√©rente ? Supposons que vous avez un ensemble de donn√©es avec deux caract√©ristiques continues, **√¢ge** et **salaire**. La plage d'√¢ge sera diff√©rente de la plage de salaire, et cela peut causer des probl√®mes.

![Image](https://www.freecodecamp.org/news/content/images/2021/04/new-op212.jpg)

Voici quelques m√©thodes courantes pour g√©rer les caract√©ristiques continues :

### Normalisation Min-Max

Pour chaque valeur dans une caract√©ristique, la normalisation Min-Max soustrait la valeur minimale dans la caract√©ristique puis divise par sa plage. La plage est la diff√©rence entre le maximum original et le minimum original.

![Image](https://www.freecodecamp.org/news/content/images/2021/04/Picture2.png)

Enfin, elle met √† l'√©chelle toutes les valeurs dans une plage fixe entre **0** et **1**.

Vous pouvez utiliser la m√©thode **[MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)** de Scikit-learn qui transforme les caract√©ristiques en mettant √† l'√©chelle chaque caract√©ristique dans une plage donn√©e :

```python
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# 4 √©chantillons/observations et 2 variables/caract√©ristiques
data = np.array([[4, 6], [11, 34], [10, 17], [1, 5]])

# cr√©er la m√©thode de mise √† l'√©chelle
scaler = MinMaxScaler(feature_range=(0,1))

# ajuster et transformer les donn√©es
scaled_data = scaler.fit_transform(data)

print(scaled_data)

# [[0.3        0.03448276]
#  [1.         1.        ] 
#  [0.9        0.4137931 ] 
#  [0.         0.        ]]
```

Comme vous pouvez le voir, nos donn√©es ont √©t√© transform√©es et la plage est entre **0** et **1**.

### Standardisation

La standardisation garantit que chaque caract√©ristique a une moyenne de **0** et un √©cart-type de **1**, ramenant toutes les caract√©ristiques √† la m√™me magnitude.

Si l'√©cart-type des caract√©ristiques est **diff√©rent**, leur plage diff√©rera √©galement.

![Image](https://www.freecodecamp.org/news/content/images/2021/04/image-24.png)
_x = observation, Œº = moyenne, œÉ = √©cart-type_

Vous pouvez utiliser la m√©thode **[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)** de Scikit-learn pour standardiser les caract√©ristiques en supprimant la moyenne et en mettant √† l'√©chelle avec un √©cart-type de **1** :

```python
from sklearn.preprocessing import StandardScaler
import numpy as np

# 4 √©chantillons/observations et 2 variables/caract√©ristiques
data = np.array([[4, 1], [11, 1], [10, 4], [1, 11]])

# cr√©er la m√©thode de mise √† l'√©chelle 
scaler = StandardScaler()

# ajuster et transformer les donn√©es
scaled_data = scaler.fit_transform(data)

print(scaled_data)

# [[-0.60192927 -0.79558708]
#  [ 1.08347268 -0.79558708] 
#  [ 0.84270097 -0.06119901] 
#  [-1.32424438  1.65237317]]
```

V√©rifions que la moyenne de chaque caract√©ristique (colonne) est **0** :

```python
print(scaled_data.mean(axis=0))
```

`[0. 0.]`

Et que l'√©cart-type de chaque caract√©ristique (colonne) est **1** :

```python
print(scaled_data.std(axis=0))
```

`[1. 1.]`

## Comment g√©rer les caract√©ristiques cat√©gorielles

Les caract√©ristiques cat√©gorielles repr√©sentent des types de donn√©es qui peuvent √™tre divis√©s en groupes. Par exemple, les genres et les niveaux d'√©ducation.

Toute valeur non num√©rique doit √™tre _convertie_ en entiers ou en flottants pour √™tre utilis√©e dans la plupart des biblioth√®ques de machine learning.

Les m√©thodes courantes pour g√©rer les caract√©ristiques cat√©gorielles sont :

### Encodage par √©tiquette

L'encodage par √©tiquette consiste simplement √† convertir chaque valeur cat√©gorielle dans une colonne en un nombre.

Il est recommand√© d'utiliser l'encodage par √©tiquette pour les convertir en variables binaires.

Dans l'exemple suivant, vous apprendrez √† utiliser **[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)** de Scikit-learn pour transformer les valeurs cat√©gorielles en binaires :

```python
# import des packages
import numpy as np 
import pandas as pd 
from sklearn.preprocessing import LabelEncoder

# initialisation des donn√©es de listes.
data = {'Genre':['homme', 'femme', 'femme', 'homme','homme'],
        'Pays':['Tanzanie','Kenya', 'Tanzanie', 'Tanzanie','Kenya']}
  
# Cr√©ation du DataFrame
data = pd.DataFrame(data)


# cr√©ation de l'objet label encoder
le = LabelEncoder()
  
data['Genre']= le.fit_transform(data['Genre'])
data['Pays']= le.fit_transform(data['Pays'])

print(data) 
```

![Image](https://www.freecodecamp.org/news/content/images/2021/04/hhhjkk-1.PNG)
_Donn√©es transform√©es_

### Encodage one-hot

De loin, la m√©thode la plus courante pour repr√©senter les variables cat√©gorielles est l'encodage one-hot, ou les m√©thodes d'encodage one-out-of-N, √©galement connues sous le nom de variables muettes.

L'id√©e derri√®re les variables muettes est de remplacer une variable cat√©gorielle par une ou plusieurs nouvelles caract√©ristiques qui peuvent avoir les valeurs 0 et 1.

![Image](https://www.freecodecamp.org/news/content/images/2021/04/image-25.png)

Dans l'exemple suivant, nous utiliserons des encodeurs de la biblioth√®que Scikit-learn. **[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)** nous aidera √† cr√©er un encodage entier des √©tiquettes √† partir de nos donn√©es et **[OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)** cr√©era un encodage one-hot des valeurs encod√©es en entiers.

```python
# import des packages 
import numpy as np 
from sklearn.preprocessing import OneHotEncoder, LabelEncoder


# d√©finition de l'exemple
data = np.array(['froid', 'froid', 'chaud', 'froid', 'chaud', 'chaud', 'chaud', 'froid', 'chaud', 'chaud'])

# encodage entier
label_encoder = LabelEncoder()

# ajuster et transformer les donn√©es
integer_encoded = label_encoder.fit_transform(data)
print(integer_encoded)

# encodage one-hot
onehot_encoder = OneHotEncoder(sparse=False)

# remodeler les donn√©es
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)

# ajuster et transformer les donn√©es
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)

print(onehot_encoded)
```

Voici la sortie de **integer_encoded** par la m√©thode **LabelEncoder** :

`[0 0 2 0 1 1 2 0 2 1]`

Et voici la sortie de **onehot_encoded** par la m√©thode **OneHotEncoder** :

```
[[1. 0. 0.] 
 [1. 0. 0.] 
 [0. 0. 1.] 
 [1. 0. 0.] 
 [0. 1. 0.] 
 [0. 1. 0.] 
 [0. 0. 1.] 
 [1. 0. 0.] 
 [0. 0. 1.] 
 [0. 1. 0.]]
```

# Qu'est-ce que la s√©lection des caract√©ristiques ?

La s√©lection des caract√©ristiques est le processus o√π vous s√©lectionnez automatiquement ou manuellement les caract√©ristiques qui contribuent le plus √† votre variable de pr√©diction ou √† votre sortie.

![Image](https://www.freecodecamp.org/news/content/images/2021/04/1_XHHToil9E5EFeEh0H0rnjA.jpeg)

Avoir des caract√©ristiques non pertinentes dans vos donn√©es peut _diminuer_ la pr√©cision des mod√®les de machine learning.

Les principales raisons d'utiliser la s√©lection des caract√©ristiques sont :

* Elle permet √† l'algorithme de machine learning de s'entra√Æner plus rapidement.
* Elle r√©duit la complexit√© d'un mod√®le et le rend plus facile √† interpr√©ter.
* Elle am√©liore la pr√©cision d'un mod√®le si le bon sous-ensemble est choisi.
* Elle r√©duit le surapprentissage.

> "J'ai pr√©par√© un mod√®le en s√©lectionnant toutes les caract√©ristiques et j'ai obtenu une pr√©cision d'environ **65%**, ce qui n'est pas tr√®s bon pour un mod√®le pr√©dictif, et apr√®s avoir fait une s√©lection de caract√©ristiques et une ing√©nierie de caract√©ristiques sans faire de changements logiques dans le code de mon mod√®le, ma pr√©cision a bondi √† **81%**, ce qui est assez impressionnant" - Par Raheel Shaikh

Les m√©thodes courantes pour la s√©lection des caract√©ristiques sont :

### S√©lection univari√©e

Les tests statistiques peuvent aider √† s√©lectionner des caract√©ristiques ind√©pendantes qui ont la relation la plus forte avec la caract√©ristique cible dans votre ensemble de donn√©es. Par exemple, le test du chi-carr√©.

La biblioth√®que Scikit-learn fournit la classe **[SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)** qui peut √™tre utilis√©e avec une suite de diff√©rents tests statistiques pour s√©lectionner un nombre sp√©cifique de caract√©ristiques.

Dans l'exemple suivant, nous utilisons la classe **SelectKBest** avec le test du chi-carr√© pour trouver la meilleure caract√©ristique pour l'ensemble de donn√©es Iris :

```python
# Charger les packages
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
  
# Charger les donn√©es iris
iris_dataset = load_iris()
  
# Cr√©er les caract√©ristiques et la cible
X = iris_dataset.data
y = iris_dataset.target
  
# Convertir en donn√©es cat√©gorielles en convertissant les donn√©es en entiers
X = X.astype(int)
  
# Deux caract√©ristiques avec les statistiques de chi-carr√© les plus √©lev√©es sont s√©lectionn√©es
chi2_features = SelectKBest(chi2, k = 2)
X_kbest_features = chi2_features.fit_transform(X, y)
  
# Caract√©ristiques r√©duites
print('Nombre de caract√©ristiques originales :', X.shape[1])
print('Nombre de caract√©ristiques r√©duites :', X_kbest_features.shape[1])
```

Nombre de caract√©ristiques originales : 4   
Nombre de caract√©ristiques r√©duites : 2

Comme vous pouvez le voir, le test du chi-carr√© nous aide √† s√©lectionner **deux caract√©ristiques ind√©pendantes importantes** parmi les 4 originales qui ont la relation la plus forte avec la caract√©ristique cible.

Vous pouvez en apprendre plus sur le test du chi-carr√© ici : "[A Gentle Introduction to the Chi-Squared Test for Machine Learning](https://machinelearningmastery.com/chi-squared-test-for-machine-learning/)".

### Importance des caract√©ristiques

L'importance des caract√©ristiques vous donne un score pour chaque caract√©ristique de vos donn√©es. Plus le score est √©lev√©, plus cette caract√©ristique est **importante** ou **pertinente** pour votre caract√©ristique cible.

L'importance des caract√©ristiques est une classe int√©gr√©e qui vient avec les classificateurs bas√©s sur les arbres tels que :

* Classificateurs de for√™t al√©atoire
* Classificateurs d'arbres extra

Dans l'exemple suivant, nous allons entra√Æner le classificateur d'arbres extra sur l'ensemble de donn√©es iris et utiliser la classe int√©gr√©e **.feature_importances_** pour calculer l'importance de chaque caract√©ristique :

```python
# Charger les biblioth√®ques
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.ensemble import ExtraTreesClassifier

# Charger les donn√©es iris
iris_dataset = load_iris()
  
# Cr√©er les caract√©ristiques et la cible
X = iris_dataset.data
y = iris_dataset.target
  
# Convertir en donn√©es cat√©gorielles en convertissant les donn√©es en entiers
X = X.astype(int)
 
 # Construire le mod√®le
extra_tree_forest = ExtraTreesClassifier(n_estimators = 5,
                                        criterion ='entropy', max_features = 2)
  
# Entra√Æner le mod√®le
extra_tree_forest.fit(X, y)
  
# Calculer l'importance de chaque caract√©ristique
feature_importance = extra_tree_forest.feature_importances_
  
# Normaliser les importances individuelles
feature_importance_normalized = np.std([tree.feature_importances_ for tree in 
                                        extra_tree_forest.estimators_],
                                        axis = 0)

# Tracer un graphique √† barres pour comparer les mod√®les
plt.bar(iris_dataset.feature_names, feature_importance_normalized)
plt.xlabel('√âtiquettes des caract√©ristiques')
plt.ylabel('Importance des caract√©ristiques')
plt.title('Comparaison des diff√©rentes importances des caract√©ristiques')
plt.show()

```

![Image](https://www.freecodecamp.org/news/content/images/2021/04/feature-important.PNG)
_Caract√©ristiques importantes_

Le graphique ci-dessus montre que les caract√©ristiques les plus importantes sont **_longueur des p√©tales (cm)_** et **_largeur des p√©tales (cm)_**, et que la caract√©ristique la moins importante est **_largeur des s√©pales (cm)_**. Cela signifie que vous pouvez utiliser les caract√©ristiques les plus importantes pour entra√Æner votre mod√®le et obtenir les meilleures performances.

### Carte thermique de la matrice de corr√©lation

La corr√©lation montre comment les caract√©ristiques sont li√©es les unes aux autres ou √† la caract√©ristique cible.

La corr√©lation peut √™tre positive (une augmentation d'une valeur de la caract√©ristique augmente la valeur de la variable cible) ou n√©gative (une augmentation d'une valeur de la caract√©ristique diminue la valeur de la variable cible).

Dans l'exemple suivant, nous utiliserons l'ensemble de donn√©es des prix des maisons de Boston de la biblioth√®que Scikit-learn et la m√©thode **[corr()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html)** de pandas pour trouver la corr√©lation par paires de toutes les caract√©ristiques dans le dataframe :

```python
# Charger les biblioth√®ques
from sklearn.datasets import load_boston
import matplotlib.pyplot as plt
import seaborn as sns


# charger les donn√©es boston
boston_dataset = load_boston()

# cr√©er un dataframe pour les donn√©es boston
boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)
  
# Convertir en donn√©es cat√©gorielles en convertissant les donn√©es en entiers
#X = X.astype(int)
 
# tracer la carte thermique pour la corr√©lation
ax = sns.heatmap(boston.corr().round(2), annot=True) 

```

![Image](https://www.freecodecamp.org/news/content/images/2021/04/1_Fbfj8xjr-PwQnfjQ4CBY_g.png)

Le coefficient de corr√©lation varie de -1 √† 1. Si la valeur est proche de 1, cela signifie qu'il y a une forte corr√©lation positive entre les deux caract√©ristiques. Lorsqu'elle est proche de -1, les caract√©ristiques ont une forte corr√©lation n√©gative.  
  
Sur la figure ci-dessus, vous pouvez voir que les caract√©ristiques **TAX** et **RAD** ont une _forte corr√©lation positive_ et que les caract√©ristiques **DIS** et **NOX** ont une _forte corr√©lation n√©gative_.

Si vous d√©couvrez qu'il y a certaines caract√©ristiques dans votre ensemble de donn√©es qui sont corr√©l√©es les unes aux autres, cela signifie qu'elles transmettent la m√™me information. Il est recommand√© de supprimer l'une d'entre elles.

Vous pouvez lire plus √† ce sujet ici : [In supervised learning, why is it bad to have correlated features?](https://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features)

## Conclusion 

Les m√©thodes que j'ai expliqu√©es dans cet article vous aideront √† pr√©parer la plupart des **ensembles de donn√©es structur√©s** que vous avez. Mais si vous travaillez sur des ensembles de donn√©es non structur√©s tels que des images, du texte et de l'audio, vous devrez apprendre diff√©rentes m√©thodes qui ne sont pas expliqu√©es dans cet article.

Les articles suivants vous aideront √† apprendre comment pr√©parer des ensembles de donn√©es d'images ou de texte pour vos projets de machine learning :

* [Best Practices for Preparing and Augmenting Image Data for CNNs-Jason Brownlee](https://machinelearningmastery.com/best-practices-for-preparing-and-augmenting-image-data-for-convolutional-neural-networks/)
* [Image Pre-processing- Prince Canuma](https://towardsdatascience.com/image-pre-processing-c1aec0be3edf)
* [NLP Text Preprocessing: A Practical Guide and Template- Jiahao Weng](https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79)
* [How to Use Texthero to Prep a Text-based Dataset for Your NLP Project-Davis David](https://www.freecodecamp.org/news/how-to-work-and-understand-text-based-dataset-with-texthero/)

**F√©licitations** üëèüëè**,** vous √™tes arriv√© √† la fin de cet article ! J'esp√®re que vous avez appris quelque chose de nouveau qui vous aidera dans votre prochain projet de machine learning ou de data science.

Si vous avez appris quelque chose de nouveau ou appr√©ci√© la lecture de cet article, veuillez le partager afin que d'autres puissent le voir. En attendant, √† la prochaine !

Vous pouvez √©galement me trouver sur Twitter [@Davis_McDavid](https://twitter.com/Davis_McDavid).

Et vous pouvez lire plus d'articles comme celui-ci [ici](https://www.freecodecamp.org/news/author/davis/).