---
title: Comment affiner EasyOCR avec un ensemble de donn√©es synth√©tique
subtitle: ''
author: Eivind Kjosbakken
co_authors: []
series: null
date: '2024-01-05T17:48:17.000Z'
originalURL: https://freecodecamp.org/news/how-to-fine-tune-easyocr-with-a-synthetic-dataset
coverImage: https://www.freecodecamp.org/news/content/images/2024/01/image-53.png
tags:
- name: Artificial Intelligence
  slug: artificial-intelligence
- name: Data Science
  slug: data-science
seo_title: Comment affiner EasyOCR avec un ensemble de donn√©es synth√©tique
seo_desc: "OCR is a valuable tool that you can use to extract text from images. But\
  \ the OCR you are using may not work as intended for your specific needs. In such\
  \ situations, fine-tuning your OCR engine is the way to go. \nIn this tutorial,\
  \ I will show you how ..."
---

L'OCR est un outil pr√©cieux que vous pouvez utiliser pour extraire du texte √† partir d'images. Mais l'OCR que vous utilisez peut ne pas fonctionner comme pr√©vu pour vos besoins sp√©cifiques. Dans de telles situations, l'ajustement fin de votre moteur OCR est la solution √† adopter.

Dans ce tutoriel, je vais vous montrer comment affiner EasyOCR, un moteur OCR gratuit et open-source que vous pouvez utiliser avec Python.

## Table des mati√®res

* [Pr√©requis](#heading-prerequisites)
* [Comment installer les packages requis](#heading-how-to-install-required-packages)
* [Comment cloner le d√©p√¥t Git](#heading-how-to-clone-the-git-repository)
* [Comment obtenir un ensemble de donn√©es](#how-get-a-dataset)
* [Comment g√©n√©rer votre ensemble de donn√©es synth√©tique](#heading-how-to-generate-your-synthetic-dataset)
* [Convertir l'ensemble de donn√©es au format lmdb](#convert-the-dataset-to-lmdb-format)
* [Comment r√©cup√©rer un mod√®le OCR pr√©-entra√Æn√©](#heading-how-to-retrieve-a-pre-trained-ocr-model)
* [Comment ex√©cuter l'ajustement fin](#heading-how-to-run-the-fine-tuning)
* [Comment ex√©cuter l'inf√©rence avec votre mod√®le affin√©](#heading-how-to-run-inference-with-your-fine-tuned-model)
* [Un test qualitatif de performance](#heading-a-qualitative-test-of-performance)
* [Test quantitatif de performance](#heading-quantitative-test-of-performance)
* [Conclusion](#heading-conclusion)

## Pr√©requis

* Connaissance de base de Python.
* Connaissance de base de l'utilisation du terminal

## Comment installer les packages requis

Tout d'abord, installons les packages `pip` requis. Je recommande de cr√©er un environnement virtuel pour cela, bien que ce ne soit pas obligatoire.

Ex√©cutez les commandes ci-dessous, une ligne √† la fois :

```bash
pip install fire
pip install lmdb
pip install opencv-python
pip install natsort
pip install nltk
```

Vous devez √©galement installer PyTorch depuis [ce site web](https://pytorch.org/get-started/locally/) (choisissez vos sp√©cifications et copiez la commande pip install. La commande ci-dessous est pour mes sp√©cifications). Vous pouvez choisir soit la version GPU soit la version CPU. La diff√©rence est que l'ex√©cution du processus d'ajustement fin sera plus lente sur le CPU.

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

## Comment cloner le d√©p√¥t Git

Vous aurez besoin d'un d√©p√¥t Git qui vous aidera √† ex√©cuter l'ajustement fin. Clonez [ce d√©p√¥t Git](https://github.com/clovaai/deep-text-recognition-benchmark) avec la commande ci-dessous :

```bash
git clone https://github.com/clovaai/deep-text-recognition-benchmark
```

Le [d√©p√¥t GitHub deep-text-recognition-benchmark](https://github.com/clovaai/deep-text-recognition-benchmark) nous fournira certains fichiers utiles pour l'ajustement fin du mod√®le EasyOCR. Notez que certaines des commandes de terminal utilis√©es dans cet article ont √©t√© prises du d√©p√¥t et ensuite adapt√©es √† mes besoins, donc le d√©p√¥t vaut la peine d'√™tre lu.

J'aimerais ajouter une note ici que [Clova AI sur Git](https://github.com/clovaai) a beaucoup de bons d√©p√¥ts qui m'ont √©t√© d'une immense aide, alors n'h√©sitez pas √† v√©rifier d'autres d√©p√¥ts int√©ressants qu'ils ont.

Un autre d√©p√¥t int√©ressant qu'ils ont est le [d√©p√¥t du mod√®le Donut](https://github.com/clovaai/donut), et j'ai √©crit un [article sur l'ajustement fin du mod√®le Donut](https://python.plainenglish.io/empower-your-donut-model-for-receipts-with-self-annotated-data-51fc882b7229) que vous devriez consulter.

## Comment obtenir un ensemble de donn√©es

Avant de pouvoir affiner votre OCR, vous aurez besoin d'un ensemble de donn√©es. Vous pouvez soit t√©l√©charger un ensemble de donn√©es, soit en cr√©er un vous-m√™me.

Puisque je veux que mon OCR soit particuli√®rement bon pour scanner les tickets de caisse de supermarch√©, je vais cr√©er un ensemble de donn√©es d'articles que vous pouvez trouver dans le supermarch√©, mais n'h√©sitez pas √† cr√©er un ensemble de donn√©es √† partir de toute donn√©e dont vous avez besoin que votre OCR soit bon. Pour cette section, j'ai utilis√© [cette page GitHub](https://github.com/JaidedAI/EasyOCR/blob/master/custom_model.md).

Si vous voulez apprendre √† g√©n√©rer votre propre ensemble de donn√©es, vous pouvez passer √† la section suivante tout de suite, mais si vous voulez une solution plus simple, vous pouvez utiliser l'une des options ci-dessous :

### Option 1 ‚Äì Utiliser mon ensemble de donn√©es factice :

Si vous voulez que cette √©tape soit aussi simple que possible (recommand√© si vous testez simplement), vous pouvez t√©l√©charger un ensemble de donn√©es factice. J'en ai cr√©√© et t√©l√©charg√© un sur [ce Google Drive](https://drive.google.com/drive/folders/1rS-WFRqN9zkD3vetwcYYFmOzg_cMv9su?usp=sharing) (t√©l√©chargez le dossier entier).

### Option 2 ‚Äì T√©l√©charger un ensemble de donn√©es

Si vous voulez un ensemble de donn√©es plus grand, vous pouvez t√©l√©charger un ensemble de donn√©es depuis [cette page Dropbox](https://www.dropbox.com/sh/i39abvnefllx2si/AAAbAYRvxzRp3cIE5HzqUw3ra?dl=0) en t√©l√©chargeant le fichier data_lmdb_release.zip (notez qu'il fait un peu plus de 18 Go).

## Comment g√©n√©rer votre ensemble de donn√©es synth√©tique

Si vous voulez une approche plus int√©ressante pour cr√©er votre propre ensemble de donn√©es, vous pouvez suivre cette section. J'en ai initialement parl√© dans [cet article Medium](https://blog.devgenius.io/generating-a-fine-tuning-dataset-for-an-ocr-engine-3509167bc8a1).

Pour cette section, vous devriez utiliser un fichier Python s√©par√©.

L'avantage d'un ensemble de donn√©es synth√©tique est que vous n'avez pas besoin d'√©tiquetage intensif en main-d'≈ìuvre, car vous cr√©ez les images en fonction des descriptions textuelles fournies. Cela signifie que vous avez √† la fois l'entr√©e du mod√®le (l'image) et l'√©tiquette (le texte des images), les deux composants n√©cessaires pour affiner un mod√®le d'IA.

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-61.png)
_Cr√©ez des images synth√©tiques comme celle-ci en suivant cette section_

### Cloner le d√©p√¥t de g√©n√©ration synth√©tique

Tout d'abord, vous devez cloner [ce d√©p√¥t de g√©n√©ration de donn√©es synth√©tiques](https://github.com/Belval/TextRecognitionDataGenerator) pour pouvoir cr√©er des donn√©es synth√©tiques. Pour le cloner, ouvrez un nouveau dossier et ex√©cutez cette commande :

```bash
git clone https://github.com/Belval/TextRecognitionDataGenerator.git
```

Ce d√©p√¥t vous permet de cr√©er des images √† partir d'une description textuelle donn√©e. Vous aurez alors l'ensemble de donn√©es dont vous avez besoin : des images et un fichier txt indiquant le texte sur les images (l'√©tiquette).

### Cr√©er un fichier pour g√©n√©rer les donn√©es synth√©tiques

Maintenant, cr√©ez un nouveau fichier appel√© `generate_synth_data.py`, et ajoutez le code ci-dessous pour importer les packages utiles :

```py
from trdg.generators import (
    GeneratorFromStrings,
)
from tqdm.auto import tqdm
import os
import pandas as pd
import numpy as np
import random
```

Pour les ex√©cuter, vous avez besoin de ces installations `pip` (ex√©cutez une ligne √† la fois dans le terminal). Notez qu'une version sp√©cifique de `Pillow` est n√©cessaire (vous obtiendrez une erreur si vous avez la derni√®re version de Pillow) :

```bash
pip install trdg
pip install pandas
pip install Pillow==9.5.0
```

Ensuite, d√©finissez quelques hyperparam√®tres (donnez-leur les valeurs que vous pr√©f√©rez) :

```py
NUM_IMAGES_TO_SAVE = 10
NUM_PRICES_TO_GENERATE = 10000
```

Maintenant, vous avez besoin d'un grand ensemble de donn√©es avec des mots que vous voulez avoir sur les images que vous cr√©ez. Puisque je veux que mon OCR soit bon pour lire les tickets de caisse de supermarch√©, j'ai utilis√© [Openfoodfacts](https://no.openfoodfacts.org/), qui est un site web contenant de nombreux articles de supermarch√©.

Pour simplifier au maximum, vous pouvez utiliser le fichier CSV sur [cette page Google Drive](https://drive.google.com/file/d/1DZhRBVGpf9smuiom3JdL0QW0HEgKIqtQ/view?usp=sharing) (t√©l√©chargez-le simplement et placez-le dans votre dossier).

Notez que vous pouvez utiliser n'importe quelle autre donn√©e au lieu d'utiliser la mienne. Si vous voulez utiliser vos propres donn√©es, tout ce dont vous avez besoin est une liste de cha√Ænes de caract√®res, que vous pouvez alimenter dans le g√©n√©rateur pour cr√©er des images.

Voici comment vous pouvez lire le fichier CSV contenant les articles de supermarch√© :

```py
# helper funcs and data to generate images
df = pd.read_csv("openfoodfacts_export_csv.csv", on_bad_lines='skip', sep='\t', low_memory=True)
df[["product_name_nb", "generic_name_nb", "brands"]]
all_words = df[["product_name_nb", "generic_name_nb", "brands"]].to_numpy().flatten()
```

Ici, je charge mes propres donn√©es, mais le code sera diff√©rent si vous utilisez vos propres donn√©es.

Voici comment vous pouvez filtrer les donn√©es :

```py
# ignore np nan 
num_before = len(all_words)
all_words = [x for x in all_words if str(x) != 'nan']
after_nan_filter = len(all_words)
print("removed: ", num_before - after_nan_filter, "words because of nan values")
all_words = list(set(all_words))
print("Removed", len(all_words), "duplicates")
print("Current number of words: ", len(all_words))
```

Notez que j'imprime toujours le nombre de mots supprim√©s lors du processus de filtrage. C'est une bonne pratique, car cela vous permet d'avoir une meilleure vue d'ensemble de la taille et de la qualit√© de votre ensemble de donn√©es.

Je veux √©galement avoir un prix sur les images, donc je g√©n√®re al√©atoirement quelques prix avec le code ci-dessous :

```py
#randomly generate 2 digits between 0-99
number_strings = []
for i in range(len(all_words)*9//10): #90 percent of all words
 digits = np.random.randint(1, 100, 4)
 before_comma = f"{str(digits[0])}" #before comma is just given as 1 digit if 0-9
 after_comma = f"{str(digits[1])}" if len(str(digits[1])) == 2 else f"0{str(digits[1])}"
 number_string = f"{before_comma},{after_comma}"
 number_strings.append(number_string)

#then create 10 percent of the words with price between 100-999
for i in range(len(all_words)*1//10): #90 percent of all words
 before_comma = np.random.randint(100, 999, 1)
 after_comma = np.random.randint(1, 99, 1)
 after_comma = f"{str(after_comma[0])}" if len(str(after_comma[0])) == 2 else f"0{str(after_comma[0])}"
 number_string = f"{str(before_comma[0])},{str(after_comma)}"
 number_strings.append(number_string)
```

Le code ci-dessous combine al√©atoirement les articles de supermarch√© avec les prix :

```py
#now given word list and number list, get all combinations
all_combinations = []
for word in tqdm(all_words):
 for number in random.sample(number_strings, 20): #only need 20 prices per product for example
  for num_tabs in [1]:
   combined_string = word + "    "*num_tabs + number
   all_combinations.append(combined_string)
```

Utilisez le d√©p√¥t que vous avez clon√© pr√©c√©demment pour cr√©er les images √† partir de la liste de cha√Ænes que nous avons cr√©√©e :

```py
#generate the images
generator = GeneratorFromStrings(
    random.sample(all_combinations, 10000),

    # uncomment the lines below for some image augmentation options
    # blur=6,
    # random_blur=True,
    # random_skew=True,
    # skewing_angle=20,
    # background_type=1,
    # text_color="red",
)
```

Il existe de nombreuses options pour g√©n√©rer les donn√©es, que vous pouvez lire plus en d√©tail [ici](https://github.com/Belval/TextRecognitionDataGenerator). Certains exemples sont : changer l'arri√®re-plan, ajouter du flou et ajouter du skewing. Vous pouvez essayer cela en d√©commentant certaines des lignes dans l'extrait de code ci-dessus.

Ensuite, enregistrez les images du g√©n√©rateur dans un format sp√©cifique :

```py
# save images from generator
# if output folder doesnt exist, create it
if not os.path.exists('output'):
    os.makedirs('output')
#if labels.txt doesnt exist, create it
if not os.path.exists('output/labels.txt'):
    f = open("output/labels.txt", "w")
    f.close()

#open txt file
current_index = len(os.listdir('output')) - 1 #all images minus the labels file
f = open("output/labels.txt", "a")

for counter, (img, lbl) in tqdm(enumerate(generator), total = NUM_IMAGES_TO_SAVE):
    if (counter >= NUM_IMAGES_TO_SAVE):
        break
    # img.show()
    #save pillow image
    img.save(f'output/image{current_index}.png')
    f.write(f'image{current_index}.png {lbl}\n')
    current_index += 1
    # Do something with the pillow images here.
f.close()
```

### G√©n√©rer les donn√©es synth√©tiques

Vous pouvez ex√©cuter le fichier `generate_synth_data.py` que vous avez cr√©√© avec cette commande dans le terminal :

```bash
python generate_synth_data.py
```

Vous devriez voir une image similaire √† celle ci-dessous (vous pouvez avoir un texte diff√©rent, dans votre dossier de sortie) :

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-62.png)
_Cette image a √©t√© g√©n√©r√©e synth√©tiquement_

Vos images seront organis√©es dans l'ordre de l'image ci-dessous, o√π les fichiers `.png` sont vos images, et le fichier `labels.txt` contient le texte de chaque image. Cela vous permet d'utiliser l'ensemble de donn√©es pour l'ajustement fin.

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-63.png)
_La structure du dossier de sortie apr√®s l'ex√©cution du code ci-dessus._

F√©licitations, vous pouvez maintenant cr√©er votre propre ensemble de donn√©es synth√©tique. Puisque vous avez maintenant √† la fois une image et le texte de cette image dans un fichier `labels.txt`, vous pouvez utiliser cela pour affiner un moteur OCR, dont je parlerai plus en d√©tail ci-dessous.

## Comment convertir l'ensemble de donn√©es au format LMDB

LMDB signifie [Lightning Memory-Mapped Database Manager](http://www.lmdb.tech/doc/) et est essentiellement un encodage que vous pouvez utiliser pour votre ensemble de donn√©es afin d'entra√Æner des mod√®les d'IA.

Vous pouvez en lire plus sur la [documentation LMDB](https://lmdb.readthedocs.io/en/release/). Apr√®s avoir cr√©√© votre ensemble de donn√©es, vous devriez avoir un dossier avec vos images, et les √©tiquettes pour toutes les images (le texte dans les images) dans un fichier `labels.txt`.

Votre dossier devrait ressembler √† l'image ci-dessous, et devrait √™tre √† l'int√©rieur du dossier **deep-text-recognition** :

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-54.png)
_Comment le dossier de votre ensemble de donn√©es devrait ressembler avant la conversion au format LMDB_

**NOTE** : Assurez-vous d'avoir au moins 10 images dans votre dossier. Vous pourriez obtenir une erreur lors de l'ex√©cution du script d'entra√Ænement plus tard dans le tutoriel si vous avez moins d'images.

Vous devez apporter quelques modifications au fichier `create_lmdb_dataset.py` dans le dossier **deep-text-recognition-benchmark** :

D√©finissez la variable `map_size` √† une valeur plus basse ‚Äî j'obtenais une erreur de m√©moire disque avec la valeur pr√©c√©dente. J'ai d√©fini la nouvelle valeur pour `map_size` √† 1073741824, comme on peut le voir ci-dessous :

```py
# OLD LINE
# ...
env = lmdb.open(outputPath, map_size=1099511627776)
# ...

# NEW LINE 
# ...
env = lmdb.open(outputPath, map_size=1073741824) 
# ...
```

J'ai √©galement obtenu une erreur avec l'encodage utf, donc j'ai supprim√© l'encodage utf-8 lors de l'ouverture du `gtFile`. La nouvelle ligne ressemble alors √† ceci :

```py
# OLD LINE
# ...
with open(gtFile, 'r', encoding='utf-8') as data:
# ...

# NEW LINE
# ...
with open(gtFile, 'r') as data:
# ...
```

Enfin, j'ai chang√© la fa√ßon dont `imagePath` √©tait lu :

```py
# OLD LINE
# ...
imagePath, label = datalist[i].strip('\n').split('\t')
# ...

# NEW LINES
# ...
imagePath, label = datalist[i].strip('\n').split('.png')
imagePath += '.png'
# ...
```

Le fichier `create_lmdb_dataset.py` devrait ressembler √† ceci (code du [d√©p√¥t Git](https://github.com/clovaai/deep-text-recognition-benchmark), avec les changements ci-dessus appliqu√©s) :

```py
import fire
import os
import lmdb
import cv2

import numpy as np


def checkImageIsValid(imageBin):
    if imageBin is None:
        return False
    imageBuf = np.frombuffer(imageBin, dtype=np.uint8)
    img = cv2.imdecode(imageBuf, cv2.IMREAD_GRAYSCALE)
    imgH, imgW = img.shape[0], img.shape[1]
    if imgH * imgW == 0:
        return False
    return True


def writeCache(env, cache):
    with env.begin(write=True) as txn:
        for k, v in cache.items():
            txn.put(k, v)


def createDataset(inputPath, gtFile, outputPath, checkValid=True):
    """
    Create LMDB dataset for training and evaluation.
    ARGS:
        inputPath  : input folder path where starts imagePath
        outputPath : LMDB output path
        gtFile     : list of image path and label
        checkValid : if true, check the validity of every image
    """
    os.makedirs(outputPath, exist_ok=True)
    env = lmdb.open(outputPath, map_size=1073741824) #TODO Changed map size
    cache = {}
    cnt = 1

    with open(gtFile, 'r') as data: #TODO removed utf-8 encoding here since I have norwegian letters
        datalist = data.readlines()

    nSamples = len(datalist)
    print(nSamples)
    for i in range(nSamples):
        #TODO changed the way imagePath is found as well to match my usecase
        imagePath, label = datalist[i].strip('\n').split('.png')
        imagePath += '.png'

        # imagePath, label = datalist[i].strip('\n').split('\t')
        imagePath = os.path.join(inputPath, imagePath)

        # # only use alphanumeric data
        # if re.search('[^a-zA-Z0-9]', label):
        #     continue

        if not os.path.exists(imagePath):
            print('%s does not exist' % imagePath)
            continue
        with open(imagePath, 'rb') as f:
            imageBin = f.read()
        if checkValid:
            try:
                if not checkImageIsValid(imageBin):
                    print('%s is not a valid image' % imagePath)
                    continue
            except:
                print('error occured', i)
                with open(outputPath + '/error_image_log.txt', 'a') as log:
                    log.write('%s-th image data occured error\n' % str(i))
                continue

        imageKey = 'image-%09d'.encode() % cnt
        labelKey = 'label-%09d'.encode() % cnt
        cache[imageKey] = imageBin
        cache[labelKey] = label.encode()

        if cnt % 1000 == 0:
            writeCache(env, cache)
            cache = {}
            print('Written %d / %d' % (cnt, nSamples))
        cnt += 1
    nSamples = cnt-1
    cache['num-samples'.encode()] = str(nSamples).encode()
    writeCache(env, cache)
    print('Created dataset with %d samples' % nSamples)


if __name__ == '__main__':
    fire.Fire(createDataset)
```

Ensuite, d√©placez le dossier vers le dossier **deep-text-recognition-benchmark** (le d√©p√¥t Git que vous avez clon√©). Ensuite, ex√©cutez la commande suivante dans le terminal :

```bash
python .\create_lmdb_dataset.py <data folder name> <path to labels.txt in data folder> <output folder for your lmdb dataset>
```

O√π :

* `<data folder name>` est le nom de votre dossier avec les images et `labels.txt` (`output` dans mon cas)
* `<path to labels.txt>` est le `<data folder name>` + le `labels.txt` (donc `.\output\labels.txt` dans mon cas)
* `<output folder for your lmdb dataset>` est le nom d'un dossier qui sera cr√©√© pour votre ensemble de donn√©es converti au format LMDB (je l'ai appel√© `.\lmbd_output`)

Pour moi, c'√©tait la commande (assurez-vous d'ex√©cuter cette commande √† l'int√©rieur du dossier **deep-text-recognition-benchmark**) :

```bash
python .\create_lmdb_dataset.py .\output .\output\labels.txt .\lmbd_output
```

Maintenant, vous devriez avoir un nouveau dossier, comme dans l'image ci-dessous, dans votre dossier **deep-text-recognition-benchmark**.



![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-55.png)
_Comment le dossier de vos donn√©es converties en lmdb devrait ressembler_

**NOTE** : L'ex√©cution de la commande sur un dossier existant ne remplace pas le dossier existant. Assurez-vous de supprimer un dossier ou de donner un nouveau nom √† **lmdb_output** (c'est quelque chose avec lequel j'ai eu du mal pendant un moment, donc esp√©rons que cela vous aidera √† √©viter cette erreur).

## Comment r√©cup√©rer un mod√®le OCR pr√©-entra√Æn√©

Ensuite, vous avez besoin d'un mod√®le OCR pr√©-entra√Æn√© que vous pouvez affiner avec votre ensemble de donn√©es. Pour cela, vous pouvez aller sur [ce site Dropbox](https://drive.google.com/drive/folders/15WPsuPJDCzhp2SvYZLRj8mAlT3zmoAMW) et t√©l√©charger le mod√®le `TPS-ResNet-BiLSTM-Attn.pth`.

Placez le mod√®le dans votre dossier **deep-text-recognition-benchmark** (je sais que cela semble un peu douteux, mais c'est la partie des instructions dans le d√©p√¥t deep-text-recognition-benchmark. Le Dropbox n'est pas √† moi, et je le lie ici parce qu'il est li√© dans le d√©p√¥t Git _text-recognition-benchmark_)

## Comment ex√©cuter l'ajustement fin

Si vous ex√©cutez sur CPU (ceci peut √™tre ignor√© si vous utilisez GPU), vous obtiendrez probablement une erreur qui dit : "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False".

Cela peut √™tre corrig√© en modifiant les lignes 85 et 87 dans le fichier `train.py` :

```py
# OLD LINES
# ...
if opt.FT:
    model.load_state_dict(torch.load(opt.saved_model), strict=False)
else:
    model.load_state_dict(torch.load(opt.saved_model))
# ...


# NEW LINES (change to this if you are using CPU)
#
if opt.FT:
    model.load_state_dict(torch.load(opt.saved_model,map_location='cpu'), strict=False)
else:
    model.load_state_dict(torch.load(opt.saved_model,map_location='cpu'))
# ...
```

Enfin, vous pouvez alors ex√©cuter l'ajustement fin. Pour cela, vous pouvez utiliser la commande ci-dessous dans le terminal :

```bash
python train.py --train_data lmdb_output --valid_data lmdb_output --select_data "/" --batch_ratio 1.0 --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --batch_size 2 --data_filtering_off --workers 0 --batch_max_length 80 --num_iter 10 --valInterval 5 --saved_model TPS-ResNet-BiLSTM-Attn.pth
```

Quelques notes sur la commande :

* `data_filtering_off` est d√©fini sur `True` (vous devez simplement utiliser le drapeau, pas lui donner une variable). Je n'ai pas utilis√© `data_filtering` car je n'aurais aucun √©chantillon √† entra√Æner si le filtrage √©tait activ√©.
* Les workers ont √©t√© d√©finis sur 0 pour √©viter les erreurs. Je pense que cela a quelque chose √† voir avec les param√®tres multi-GPU, et cela est √©galement mentionn√© dans le fichier `train.py` dans le dossier **deep-text-recognition-benchmark**.
* `batch_max_length` est la longueur maximale de tout texte dans l'ensemble de donn√©es d'entra√Ænement. Si vous utilisez un ensemble de donn√©es diff√©rent, n'h√©sitez pas √† changer cette variable. Assurez-vous que cette variable est aussi grande que la cha√Æne la plus longue que vous utilisez dans votre ensemble de donn√©es, ou vous obtiendrez une erreur.
* Pour ce tutoriel, j'utilise `train_data` et `valid_data` pour faire r√©f√©rence au m√™me dossier. En pratique, je cr√©erais un dossier avec un ensemble de donn√©es d'entra√Ænement, et un pour un ensemble de donn√©es de validation et je ferais r√©f√©rence √† ceux-ci √† la place.
* J'ai d√©fini `num_iter` √† 10 pour que vous puissiez vous assurer que cela fonctionne. Naturellement, cette variable doit √™tre d√©finie beaucoup plus haut lors de l'ex√©cution de l'ajustement fin r√©el d'un mod√®le.
* `saved_model` est un param√®tre facultatif. Si vous ne le d√©finissez pas, vous entra√Ænez un mod√®le √† partir de z√©ro. Vous ne voulez probablement pas cela (car cela n√©cessitera beaucoup d'entra√Ænement), donc d√©finissez le drapeau `saved_model` sur le mod√®le existant que vous avez [t√©l√©charg√© depuis Dropbox](https://drive.google.com/drive/folders/15WPsuPJDCzhp2SvYZLRj8mAlT3zmoAMW).

## Comment ex√©cuter l'inf√©rence avec votre mod√®le affin√©

Apr√®s avoir affin√© votre mod√®le, vous voudrez ex√©cuter l'inf√©rence avec celui-ci. Pour cela, vous pouvez utiliser la commande ci-dessous :

```bash
python demo.py --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --image_folder <path to images to test on> --saved_model <path to model to use>
```

O√π :

* `<path to images to test on>` est un dossier contenant des images PNG sur lesquelles vous voulez tester. Pour moi, c'√©tait **output**
* `<path to model to use>` est le chemin vers le mod√®le enregistr√© de votre ajustement fin. Pour moi, c'√©tait **.\saved_models\TPS-ResNet-BiLSTM-Attn-Seed1111\best_accuracy.pth** (l'ajustement fin enregistre le mod√®le affin√© dans un dossier `saved_models`)

Voici la commande que j'ai utilis√©e :

```bash
python demo.py --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --image_folder output --saved_model .\saved_models\TPS-ResNet-BiLSTM-Attn-Seed1111\best_accuracy.pth
```

La commande produit simplement la pr√©diction du mod√®le et le score de confiance pour chaque image dans le dossier `<path to images to test on>`, afin que vous puissiez v√©rifier les performances du mod√®le en regardant les images vous-m√™me pour voir si le mod√®le a fait la bonne pr√©diction. Il s'agit d'un test qualitatif des performances du mod√®le.

## Un test qualitatif de performance

Pour voir si l'ajustement fin a fonctionn√©, je vais effectuer un test qualitatif des performances en testant le mod√®le original par rapport √† mon mod√®le affin√© sur 10 mots et chiffres sp√©cifiques.

Les mots que j'ai test√©s sont montr√©s ci-dessous (fusionn√©s verticalement en une seule image). J'ai d√ª rendre cela un peu difficile pour le mod√®le en ajoutant des textes inclin√©s et flous.

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-56.png)
_Images auto-cr√©√©es fusionn√©es avec [https://products.aspose.app/pdf/merger/png-to-png](https://products.aspose.app/pdf/merger/png-to-png" rel="noopener ugc nofollow). Les mots de haut en bas sont : vanskeligheter, uvanligheter, skrekkeksempel, rosenborg_

Consid√©rant que je veux que mon OCR lise les tickets de caisse norv√©giens, j'ai ajout√© quelques mots norv√©giens (les mots sont tir√©s de [http://openfoodfacts.com/](http://openfoodfacts.com/), vous pouvez en lire plus √† ce sujet dans [cet article](https://medium.com/dev-genius/generating-a-fine-tuning-dataset-for-an-ocr-engine-3509167bc8a1)).

Esp√©rons que mon mod√®le affin√© devrait mieux performer sur ces mots, car le mod√®le OCR original n'est pas habitu√© √† voir des mots norv√©giens. Mon mod√®le affin√© a √©t√© entra√Æn√© sur certains mots norv√©giens.

Les textes dans chaque image sont :

* image0 -> vanskeligheter
* image1 -> uvanligheter
* image2 -> skrekkeksempel
* image3 -> rosenborg

R√©sultats pour le mod√®le original (non affin√©) :

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-57.png)
_R√©sultats pour le mod√®le original (non affin√©) sur un test qualitatif. Vous pouvez voir que le mod√®le a du mal_

R√©sultats pour le mod√®le affin√© :

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-58.png)
_R√©sultats pour le mod√®le affin√©. Vous pouvez voir que le mod√®le atteint une pr√©cision parfaite gr√¢ce √† l'ajustement fin._

Comme vous pouvez le voir, l'ajustement fin a fonctionn√©, et le mod√®le affin√© atteint des r√©sultats parfaits dans cet exemple qualitatif.

Pour interpr√©ter vos r√©sultats qualitativement, vous devriez prendre un √©chantillon de documents repr√©sentatifs de l'ensemble de donn√©es complet et comparer manuellement la sortie de l'OCR et la v√©rit√© terrain. Cela vous donnera une id√©e de la performance du mod√®le, car vous pouvez voir √† quelle fr√©quence il commet des erreurs.

Vous devez noter que vous ne pouvez souvent pas vous attendre √† des r√©sultats parfaits de la part du moteur OCR affin√©, et vous pouvez donc utiliser l'analyse qualitative pour d√©terminer les erreurs sp√©cifiques que le mod√®le commet.

Cela pourrait, par exemple, √™tre le mod√®le ayant des difficult√©s √† reconna√Ætre certains caract√®res. Si c'est le cas, vous pouvez entra√Æner le mod√®le sur plus d'exemples de ces caract√®res pour augmenter davantage les performances de votre mod√®le.

## Test quantitatif de performance

Si vous voulez un test plus quantitatif, vous pouvez soit regarder les r√©sultats de validation qui apparaissent pendant l'ajustement fin, soit utiliser la commande ci-dessous :

```bash
python test.py --eval_data <path to test data set in lmdb format> --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --saved_model <path to model to test> --batch_max_length 70 --workers 0 --batch_size 2 --data_filtering_off
```

O√π :

* `<path to test data set in lmdb format>` est le chemin vers le dossier contenant les donn√©es de test au format LMDB. Pour moi, c'√©tait : `lmdb_norwegian_data_test`
* `<path to model to test>` est le chemin vers le mod√®le dont vous voulez tester les performances. Pour moi, c'√©tait : `saved_models/TPS-ResNet-BiLSTM-Attn-Seed1111/best_accuracy.pth`.

La commande que j'ai utilis√©e √©tait donc :

```bash
python test.py --eval_data lmdb_norwegian_data_test --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --saved_model saved_models/TPS-ResNet-BiLSTM-Attn-Seed1111/best_accuracy.pth --batch_max_length 70 --workers 0 --batch_size 2 --data_filtering_off
```

Cela produira une pr√©cision en pourcentage, donc un nombre entre 0 et 100, qui est la pr√©cision que le mod√®le OCR atteint sur votre ensemble de donn√©es de test.

Dans mon exp√©rience, le mod√®le que vous avez t√©l√©charg√© depuis Dropbox a besoin d'un peu d'entra√Ænement. Au d√©but, le mod√®le fera des pr√©dictions inexactes, mais si vous le laissez s'entra√Æner pendant 30 minutes environ, vous devriez commencer √† voir quelques am√©liorations.

J'ai ensuite ex√©cut√© le `test.py` sur les 4 images que j'ai montr√©es ci-dessus et obtenu les r√©sultats dans les images ci-dessous : avec l'ancien mod√®le (non affin√©) en haut et le nouveau mod√®le affin√© en bas.

R√©sultats de l'ancien mod√®le :

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-59.png)
_R√©sultat pour l'ancien mod√®le, qui atteint une pr√©cision de 50%._

R√©sultats du mod√®le affin√© :

![Image](https://www.freecodecamp.org/news/content/images/2023/12/image-60.png)
_R√©sultat pour le nouveau mod√®le affin√© qui atteint une pr√©cision de 100%, ce qui indique que l'ajustement fin a fonctionn√©_

Vous pouvez voir que le nouveau mod√®le affin√© performe mieux avec une pr√©cision de 100 pour cent.

## Conclusion

F√©licitations, vous pouvez maintenant affiner votre mod√®le OCR. Pour avoir un impact significatif sur un mod√®le plus grand et le g√©n√©raliser, vous devez probablement cr√©er un ensemble de donn√©es plus grand. Vous pouvez en apprendre davantage √† ce sujet dans [ce tutoriel](https://medium.com/dev-genius/generating-a-fine-tuning-dataset-for-an-ocr-engine-3509167bc8a1), puis laisser le mod√®le s'entra√Æner pendant un certain temps.

En fin de compte, le mod√®le OCR fonctionnera, esp√©rons-le, mieux pour votre cas d'utilisation sp√©cifique.

Ce tutoriel a √©t√© initialement √©crit partie par partie sur mon Medium, vous pouvez consulter chaque partie ici :

* [G√©n√©rer un ensemble de donn√©es synth√©tique pour l'ajustement fin d'un moteur OCR](https://blog.devgenius.io/generating-a-fine-tuning-dataset-for-an-ocr-engine-3509167bc8a1)
* [Comment affiner EasyOCR pour obtenir de meilleures performances OCR](https://pub.towardsai.net/how-to-fine-tune-easyocr-to-achieve-better-ocr-performance-1540f5076428)

Si vous √™tes int√©ress√© et souhaitez en savoir plus sur des sujets similaires, vous pouvez me trouver sur :

* [[1;32m‚úì[0m Medium](https://medium.com/@oieivind)
* [[1;32m‚úì[0m](https://twitter.com/Ravenspike21) [Twitter](https://twitter.com/Ravenspike21)
* [1;32m‚úì[0m[LinkedIn](https://www.linkedin.com/in/eivind-kjosbakken/)

Image de couverture : Utilisez l'OCR pour lire des documents. Image r√©alis√©e avec DALL-E. OpenAI. (2023). ChatGPT (Grand mod√®le de langage) [https://chat.openai.com](https://chat.openai.com/).