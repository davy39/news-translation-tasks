---
title: Comment construire un traducteur multimodal Makaton-Anglais pour une √©ducation
  accessible
subtitle: ''
author: OMOTAYO OMOYEMI
co_authors: []
series: null
date: '2025-09-18T01:20:45.526Z'
originalURL: https://freecodecamp.org/news/build-a-multimodal-translator-for-accessible-education
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1758158024064/bf3d7dac-0231-450a-9b40-6abf43085e49.png
tags:
- name: Accessibility
  slug: accessibility
- name: Python
  slug: python
- name: AI
  slug: ai
seo_title: Comment construire un traducteur multimodal Makaton-Anglais pour une √©ducation
  accessible
seo_desc: 'Discover how AI is transforming accessibility in education, from Makaton
  gesture translation to adaptive speech assistants like AURA. Explore multimodal
  AI '
---

Un √©l√®ve de troisi√®me entre en classe plein d'id√©es, mais au moment de contribuer, les outils qui l'entourent ne l'√©coutent pas. Sa parole est difficile √† reconna√Ætre pour les syst√®mes vocaux standards, taper au clavier lui semble lent et √©puisant, et la le√ßon avance sans que sa voix ne soit entendue. Le d√©fi n'est pas un manque de capacit√©, mais un manque d'acc√®s.

√Ä travers le monde, des millions d'apprenants font face √† des barri√®res de communication. Certains vivent avec une apraxie de la parole ou une dysarthrie, d'autres avec une mobilit√© r√©duite, des diff√©rences auditives ou des besoins neurodivers. Lorsque parler, √©crire ou pointer devient peu fiable ou fatiguant, la participation se limite, le feedback est perdu et la confiance s'√©rode lentement. Ce n'est pas une exception rare, mais une r√©alit√© quotidienne dans les salles de classe.

Ces barri√®res se manifestent de mani√®res tr√®s concr√®tes. Les √©l√®ves sont ignor√©s ou mal compris lorsqu'ils ne peuvent pas r√©pondre rapidement. Leur capacit√© est sous-√©valu√©e car leurs moyens d'expression sont restreints. Les enseignants luttent pour maintenir le rythme des cours tout en proc√©dant √† des am√©nagements individuels. Les pairs interagissent moins souvent, r√©duisant les opportunit√©s d'appartenance sociale.

Les technologies d'assistance ont aid√© au fil des ans, avec des outils comme la synth√®se vocale, les tableaux de symboles et les entr√©es gestuelles simples. Pourtant, la plupart de ces outils sont con√ßus pour un mode d'interaction unique. Ils supposent que l'apprenant va soit parler, soit taper, soit toucher. La communication r√©elle, cependant, est fluide. Les apprenants combinent naturellement gestes, parole partielle, symboles et contexte pour partager un sens, surtout lorsque la fatigue, l'anxi√©t√© ou les d√©fis moteurs entrent en jeu.

C'est l√† que l'IA moderne change la donne. Nous commen√ßons √† d√©passer les outils √† solution unique pour passer √† des syst√®mes multimodaux capables de comprendre la parole, m√™me lorsqu'elle est d√©sordonn√©e, d'interpr√©ter les gestes et les symboles visuels, de combiner les signaux pour d√©duire l'intention et de s'adapter en temps r√©el √† mesure que les capacit√©s de l'apprenant se d√©veloppent ou changent.

L'IA remod√®le l'accessibilit√© dans l'√©ducation en passant d'outils isol√©s √† des syst√®mes multimodaux et adaptatifs. Ces syst√®mes combinent le geste, la parole et un feedback intelligent pour rejoindre les apprenants l√† o√π ils en sont, tout en soutenant leur croissance au fil du temps.

Dans cet article, nous explorerons √† quoi ressemble ce changement dans la pratique, comment il peut d√©bloquer la participation, et comment le feedback adaptatif personnalise le soutien. Nous construirons √©galement une d√©mo multimodale pratique qui transforme ces id√©es en un outil pr√™t pour la classe.

## Pr√©requis

* **Un syst√®me d'exploitation :** Windows, macOS ou Linux
    
* **Python install√© (3.9 ou plus r√©cent)** ‚Äì Avec `pip` pour l'installation des packages.
    
* **√âditeur :** Visual Studio Code ou n'importe quel environnement de d√©veloppement int√©gr√© (IDE)
    
* **Bases :** √ätre √† l'aise avec l'ex√©cution de commandes dans un terminal
    
* **Mat√©riel optionnel :** Microphone (entr√©e vocale), Webcam (onglet image unique), haut-parleurs (lecture TTS)
    
* **Internet :** Requis pour SpeechRecognition par d√©faut (Google Web Speech API) et gTTS
    
* **Aucun jeu de donn√©es/mod√®le requis :** Un classificateur de gestes "stub" (bouchon) est fourni pour que la d√©mo s'ex√©cute de bout en bout
    

## Table des mati√®res

* [Pr√©requis](#heading-pre-requis)
    
* [Ce que nous avons accompli jusqu'√† pr√©sent](#heading-ce-que-nous-avons-accompli-jusqua-present)
    
* [√âtude de cas 1 : Traduire le Makaton en Anglais](#heading-etude-de-cas-1-traduire-le-makaton-en-anglais)
    
* [√âtude de cas 2 : Prototype AURA (Assistant vocal adaptatif)](#heading-etude-de-cas-2-prototype-aura-assistant-vocal-adaptatif)
    
* [Vue d'ensemble : Outils d'accessibilit√© multimodaux](#heading-vue-densemble-outils-daccessibilite-multimodaux)
    
* [Comment construire un traducteur multimodal Makaton-Anglais (Geste + Parole)](#heading-comment-construire-un-traducteur-multimodal-makaton-anglais-geste-parole)
    
* [Aper√ßu du projet](#heading-apercu-du-projet)
    
* [D√©fis et consid√©rations √©thiques](#heading-defis-et-considerations-ethiques)
    
* [Vers quoi nous nous dirigeons ensuite](#heading-vers-quoi-nous-nous-dirigeons-ensuite)
    
* [Conclusion : Construire un avenir inclusif avec l'IA](#heading-conclusion-construire-un-avenir-inclusif-avec-lia)
    

## Ce que nous avons accompli jusqu'√† pr√©sent

Ces derni√®res ann√©es ont montr√© comment l'IA peut rendre les classes plus inclusives lorsque nous nous concentrons sur l'accessibilit√©. Les d√©veloppeurs, les √©ducateurs et les chercheurs exp√©rimentent d√©j√† des outils qui comblent les lacunes de communication.

Dans [mon premier tutoriel freeCodeCamp](https://www.freecodecamp.org/news/create-a-real-time-gesture-to-text-translator/), j'ai construit un traducteur geste-vers-texte en utilisant MediaPipe. Ce projet a d√©montr√© comment la vision par ordinateur peut suivre les mouvements de la main et les convertir en texte en temps r√©el. Pour les apprenants qui comptent sur les gestes, ce type de syst√®me peut constituer une passerelle vers la participation.

Voici un exemple simplifi√© de la fa√ßon dont MediaPipe d√©tecte les points de rep√®re (landmarks) de la main :

```python
import mediapipe as mp
import cv2

# Initialiser MediaPipe Hands
mp_hands = mp.solutions.hands
hands = mp_hands.Hands()

# Commencer la capture vid√©o depuis la webcam
cap = cv2.VideoCapture(0)

# Capturer une image de la vid√©o
ret, frame = cap.read()

# Traiter l'image pour d√©tecter les points de rep√®re de la main
results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

# Afficher les points de rep√®re d√©tect√©s
print("Hand landmarks:", results.multi_hand_landmarks)
```

Ce petit bout de code montre comment MediaPipe traite une image vid√©o et extrait les points de rep√®re de la main. √Ä partir de l√†, vous pouvez classifier les gestes et les mapper √† du texte.

üëâ Vous pouvez explorer le projet complet sur [GitHub](https://github.com/tayo4christ/Gesture_Article) ou lire le tutoriel complet sur [freeCodeCamp](https://www.freecodecamp.org/news/create-a-real-time-gesture-to-text-translator/).

Dans un autre [article de freeCodeCamp](https://www.freecodecamp.org/news/build-ai-accessibility-tools-with-python/), j'ai d√©montr√© comment construire des outils d'accessibilit√© IA avec Python, tels que la reconnaissance vocale et la synth√®se vocale. Ces projets ont fourni aux lecteurs une base pour construire leurs propres outils inclusifs, et vous pouvez trouver le code source complet dans le [repository](https://github.com/tayo4christ/inclusive-ai-toolkit).

Au-del√† de ces projets individuels, le domaine plus large a √©galement r√©alis√© des progr√®s significatifs. Les avanc√©es dans la reconnaissance de la langue des signes ont am√©lior√© la pr√©cision de la capture des formes de mains et des mouvements complexes. Les syst√®mes de synth√®se vocale sont devenus plus naturels et adaptatifs, donnant aux utilisateurs des voix plus proches de la parole humaine. Les applications d'accessibilit√© mobiles et de bureau ont apport√© ces capacit√©s dans les salles de classe quotidiennes.

Ces accomplissements sont encourageants, mais ils restent limit√©s. La plupart des outils d'aujourd'hui sont encore con√ßus pour un mode unique de communication. Un syst√®me peut fonctionner pour les gestes, ou pour la parole, ou pour le texte, mais pas tous ensemble.

L'√©tape suivante est claire : nous avons besoin d'outils d'IA multimodaux et adaptatifs capables de m√©langer gestes, parole et feedback dans des syst√®mes unifi√©s. C'est l√† que se trouvent les opportunit√©s les plus passionnantes en mati√®re d'accessibilit√©, et c'est ce vers quoi nous allons nous tourner maintenant.

![Syst√®mes isol√©s vs multimodaux](https://github.com/tayo4christ/ai-accessibility-articles-assets/blob/main/single-vs-multimodal.png?raw=true align="left")

*Figure 1 : Comparaison des syst√®mes √† modalit√© unique isol√©s avec les syst√®mes d'IA multimodaux unifi√©s.*

## √âtude de cas 1 : Traduire le Makaton en Anglais

L'un de mes premiers projets dans ce domaine portait sur la traduction du Makaton vers l'Anglais.

Le Makaton est un programme linguistique qui utilise des signes et des symboles pour aider les personnes ayant des difficult√©s de parole et de langage. Il est largement utilis√© dans les classes o√π les apprenants ne s'appuient pas enti√®rement sur la parole. Le d√©fi est que, pendant qu'un apprenant communique en Makaton, ses enseignants et ses pairs travaillent souvent en anglais, ce qui cr√©e un foss√© de communication.

### Le Workflow de l'IA

Le syst√®me suivait un pipeline clair :

*Entr√©e Cam√©ra ‚Üí D√©tection de Points de Rep√®re de Main ‚Üí Classification de Geste ‚Üí Sortie de Traduction Anglaise*

![Workflow Makaton](https://github.com/tayo4christ/ai-accessibility-articles-assets/blob/main/makaton-workflow.png?raw=true align="left")

*Figure 2 : Workflow IA pour la traduction des gestes Makaton en anglais.*

* **Entr√©e Cam√©ra** : capture le signe Makaton de l'apprenant.
    
* **D√©tection de Points de Rep√®re de Main** : une biblioth√®que de vision telle que MediaPipe ou OpenCV identifie la position des doigts et des mains.
    
* **Classification de Geste** : un mod√®le de Machine Learning entra√Æn√© classifie quel signe Makaton a √©t√© fait.
    
* **Sortie de Traduction Anglaise** : le syst√®me mappe ce geste √† son mot ou sa phrase en anglais et l'affiche.
    

### Exemple en Python

Voici une version simplifi√©e de ce √† quoi ce workflow pourrait ressembler en code :

```python
# √âtape 1 : Capturer l'entr√©e
frame = camera.read()

# √âtape 2 : D√©tecter les points de rep√®re de la main
landmarks = mediapipe.process(frame)

# √âtape 3 : Classifier le geste
gesture = gesture_model.predict(landmarks)

# √âtape 4 : Traduire en anglais
translation_map = {
    "hello_sign": "Hello",
    "thank_you_sign": "Thank you"
}
text = translation_map.get(gesture, "Unknown sign")

print("Makaton sign:", gesture, " -> English:", text)
```

C'est un exemple simplifi√©, mais il montre l'id√©e centrale : mapper les gestes au sens, puis faire le pont entre ce sens et l'anglais.

### Pourquoi c'est important

Imaginez un √©l√®ve signant *merci* en Makaton et le syst√®me affichant instantan√©ment les mots √† l'√©cran. Les enseignants peuvent v√©rifier la compr√©hension, les pairs peuvent r√©pondre naturellement, et la contribution de l'apprenant devient visible pour tous.

Le point cl√© √† retenir est que l'IA peut faire le lien entre les langues bas√©es sur des symboles et des gestes et la communication parl√©e et √©crite conventionnelle. Au lieu de forcer les apprenants √† s'adapter √† des syst√®mes rigides, nous pouvons concevoir des syst√®mes qui s'adaptent √† la fa√ßon dont ils communiquent d√©j√†.

## √âtude de cas 2 : Prototype AURA (Assistant vocal adaptatif)

Un autre projet sur lequel j'ai travaill√© s'appelle [**AURA**](https://aura-apraxia-aac-a8qejouwasaqequrhetbfw.streamlit.app/), l'*Apraxia of Speech Adaptive Understanding and Relearning Assistant*. L'id√©e √©tait de concevoir un syst√®me qui non seulement reconna√Æt la parole, mais soutient √©galement les apprenants souffrant de troubles de la parole en d√©tectant les erreurs, en adaptant le feedback et en proposant des alternatives multimodales.

### Le D√©fi

La plupart des syst√®mes de reconnaissance vocale commerciaux √©chouent lorsque la parole d'une personne ne suit pas les mod√®les typiques. C'est particuli√®rement vrai pour les personnes souffrant d'apraxie de la parole, o√π les difficult√©s de planification motrice rendent la prononciation incoh√©rente. Le r√©sultat est une reconnaissance erron√©e fr√©quente, de la frustration et l'exclusion des outils qui reposent sur l'entr√©e vocale.

### Le Workflow de l'IA

Le prototype AURA utilisait une architecture en couches :

*Entr√©e vocale ‚Üí Wav2Vec2 (affin√© pour la parole d√©sordonn√©e) ‚Üí D√©tection d'erreurs CNN + BiLSTM ‚Üí Feedback par Apprentissage par Renforcement ‚Üí Sortie multimodale (Parole + Geste)*

![Workflow AURA](https://github.com/tayo4christ/ai-accessibility-articles-assets/blob/main/aura-workflow.png?raw=true align="left")

*Figure 3 : Workflow du prototype AURA, combinant parole, d√©tection d'erreurs, feedback adaptatif et sorties multimodales.*

* **Reconnaissance vocale Wav2Vec2** : affin√©e (fine-tuned) sur la parole d√©sordonn√©e pour am√©liorer la pr√©cision de la transcription.
    
* **Mod√®le CNN + BiLSTM** : classifie les erreurs d'articulation ou phonologiques en temps r√©el.
    
* **Moteur d'Apprentissage par Renforcement** : adapte les boucles de r√©troaction pour que les suggestions de th√©rapie s'am√©liorent √† mesure que l'apprenant progresse.
    
* **Entr√©e multimodale Geste-vers-Parole** : lorsque la parole est trop difficile, les gestes MediaPipe peuvent √™tre utilis√©s pour d√©clencher des sorties vocales.
    
* **Interface Streamlit** : int√®gre le tout dans une application accessible unique pour les tests.
    

Voici une vue simplifi√©e de la structure d'un module de d√©tection d'erreurs :

```python
# Exemple : Classification d'erreurs utilisant CNN + BiLSTM
import torch
import torch.nn as nn

# D√©finir le mod√®le ErrorClassifier
class ErrorClassifier(nn.Module):
    def __init__(self):
        super(ErrorClassifier, self).__init__()
        self.cnn = nn.Conv1d(in_channels=40, out_channels=64, kernel_size=3)
        self.lstm = nn.LSTM(64, 128, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(256, 3)  # Classes de sortie : ex. correct, substitution, omission

    def forward(self, x):
        x = self.cnn(x)
        x, _ = self.lstm(x)
        return self.fc(x[:, -1, :])

# Instancier le mod√®le
model = ErrorClassifier()
```

Cet extrait montre le c≈ìur du pipeline de d√©tection d'erreurs : la combinaison de couches CNN pour l'extraction de caract√©ristiques avec des BiLSTM pour la mod√©lisation de s√©quences. Le mod√®le peut signaler les erreurs d'articulation, qui guident ensuite la boucle de r√©troaction.

### Pourquoi c'est important

Avec AURA, l'objectif n'√©tait pas seulement de reconna√Ætre ce que quelqu'un disait, mais de l'aider √† communiquer plus efficacement. Le prototype s'adaptait en temps r√©el en offrant un feedback correctif, en sugg√©rant des gestes ou en changeant de mode lorsque la parole devenait difficile.

L'id√©e √† retenir est que l'IA peut √©voluer d'un simple outil de reconnaissance passif vers un partenaire actif dans l'apprentissage et la communication.

## Vue d'ensemble : Outils d'accessibilit√© multimodaux

Les deux projets que nous avons explor√©s, la traduction du Makaton vers l'anglais et la construction du prototype AURA, mettent en lumi√®re une transformation beaucoup plus vaste en cours. La technologie de l'accessibilit√© s'√©loigne des applications isol√©es √† but unique pour s'orienter vers des plateformes multimodales qui regroupent la parole, les gestes, le texte et l'IA adaptative en un seul syst√®me fluide.

### Pourquoi ce changement est important

Les avantages de ce changement sont profonds :

* **Une plus grande inclusivit√© dans les classes** : les apprenants qui s'appuient sur diff√©rents modes de communication peuvent participer sur un pied d'√©galit√©.
    
* **Soutien en temps r√©el** : les syst√®mes qui d√©tectent les erreurs ou s'adaptent aux gestes donnent aux apprenants un feedback imm√©diat plut√¥t que des corrections diff√©r√©es.
    
* **Moins de frustration** : les options multimodales signifient que si un canal √©choue (par exemple, la parole), d'autres comme le geste ou le texte peuvent prendre le relais en douceur.
    
* **Confiance et ind√©pendance** : les apprenants s'expriment plus pleinement, sans d√©pendre lourdement du personnel de soutien ou des interpr√®tes.
    

### Au-del√† de la salle de classe

L'impact de l'accessibilit√© multimodale s'√©tend √† de nombreux secteurs :

* Dans la **sant√©**, les patients ayant des difficult√©s de communication peuvent utiliser des assistants IA multimodaux pour exprimer clairement leurs besoins, r√©duisant ainsi les erreurs de diagnostic et le stress.
    
* Sur le **lieu de travail**, les employ√©s souffrant de troubles de la parole ou moteurs peuvent collaborer efficacement gr√¢ce √† des outils d'IA adaptatifs.
    
* Dans les **param√®tres communautaires**, les individus peuvent participer plus librement aux conversations, aux services et aux plateformes num√©riques, renfor√ßant ainsi l'inclusion sociale.
    

### Visualiser le changement

![Applications multimodales](https://github.com/tayo4christ/ai-accessibility-articles-assets/blob/main/multimodal-applications.png?raw=true align="left")

## Comment construire un traducteur multimodal Makaton-Anglais (Geste + Parole)

Cette d√©mo combine les deux cas d'utilisation : un outil de classe Makaton vers Anglais et le chemin de parole assist√©e de type AURA. Il donne la priorit√© au geste lorsqu'un signe est d√©tect√©, se rabat sur la parole s'il ne l'est pas, et produit une sortie anglaise unifi√©e (avec synth√®se vocale optionnelle). Nous nous concentrerons sur la couche de traduction, la fusion multimodale et une interface UI Streamlit simple.

### Structure du projet

```python
makaton_multimodal_demo/
‚îú‚îÄ‚îÄ .streamlit/
‚îÇ   ‚îî‚îÄ‚îÄ config.toml 
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îî‚îÄ‚îÄ README.txt 
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_fuse.py 
‚îî‚îÄ‚îÄ streamlit_app.py 
```

La structure fournie ci-dessus pr√©sente l'organisation d'un r√©pertoire de projet pour une d√©mo de traducteur Makaton vers Anglais multimodal utilisant Streamlit. Voici une br√®ve explication de chaque composant :

* `makaton_multimodal_demo/` : C'est le r√©pertoire racine du projet.
    
* `.streamlit/` : Ce r√©pertoire contient les fichiers de configuration pour Streamlit, qui est un Framework utilis√© pour construire des applications web en Python. Le fichier `config.toml` est optionnel et peut √™tre utilis√© pour personnaliser les param√®tres de l'application Streamlit.
    
* `assets/` : Ce r√©pertoire est destin√© √† stocker les mod√®les ou d'autres fichiers n√©cessaires au projet. Le `README.txt` sert d'espace r√©serv√© pour indiquer o√π ces fichiers doivent √™tre plac√©s.
    
* `tests/` : Ce r√©pertoire est destin√© aux scripts de test. Le fichier `test_`[`fuse.py`](http://fuse.py) contient probablement des tests pour la fonction de fusion, qui fait partie du processus de traduction multimodale.
    
* `streamlit_`[`app.py`](http://app.py) : C'est le fichier principal de l'application o√π l'application Streamlit est impl√©ment√©e. Il contient le code qui ex√©cute l'application, g√©rant l'interface utilisateur et la logique de traduction des gestes Makaton et de la parole en anglais.
    

### Installation et ex√©cution

```bash
# (optionnel) cr√©er et activer un virtualenv
python -m venv .venv

# Windows
.\.venv\Scripts\activate

# macOS/Linux
source .venv/bin/activate
```

L'extrait de code ci-dessus fournit les instructions pour cr√©er et activer un environnement virtuel Python.

1. `python -m venv .venv` : Cette commande cr√©e un nouvel environnement virtuel dans un r√©pertoire nomm√© `.venv`.
    
2. `.\.venv\Scripts\activate` (Windows) : Cette commande active l'environnement virtuel sur Windows.
    
3. `source .venv/bin/activate` (macOS/Linux) : Cette commande active l'environnement virtuel sur macOS ou Linux.
    

### Installer les d√©pendances

```python
pip install streamlit opencv-python mediapipe SpeechRecognition gTTS pydub numpy
```

La commande ci-dessus est utilis√©e pour installer plusieurs packages Python √† la fois :

* **streamlit** : Un Framework pour construire des applications web interactives en Python.
    
* **opencv-python** : Fournit OpenCV, une biblioth√®que pour les t√¢ches de vision par ordinateur.
    
* **mediapipe** : Une biblioth√®que d√©velopp√©e par Google pour les solutions de Machine Learning pour les m√©dias en direct, incluant la d√©tection des mains et des visages.
    
* **SpeechRecognition** : Une biblioth√®que pour effectuer de la reconnaissance vocale.
    
* **gTTS** : Google Text-to-Speech, pour interfacer avec l'API de synth√®se vocale de Google Translate.
    
* **pydub** : Une biblioth√®que pour le traitement audio.
    
* **numpy** : Un package fondamental pour le calcul scientifique en Python.
    

### Cr√©er `streamlit_app.py`

```python
# streamlit_app.py
from io import BytesIO
from typing import Optional
import streamlit as st

# D√©pendances optionnelles
try:
    import cv2
    import mediapipe as mp
    MP_OK = True
except Exception:
    MP_OK = False

try:
    import speech_recognition as sr
    SR_OK = True
except Exception:
    SR_OK = False

try:
    from gtts import gTTS
    GTTS_OK = True
except Exception:
    GTTS_OK = False

# --- 1) Dictionnaire Makaton minimal (√† √©tendre au besoin)
MAKATON_DICT = {
    "hello_sign": "Bonjour",
    "thank_you_sign": "Merci",
    "help_sign": "Aide",
    "toilet_sign": "Toilettes",
    "stop_sign": "Arr√™ter",
}

# --- 2) Classificateur de gestes (bouchon pour la d√©mo)
def classify_gesture(landmarks) -> Optional[str]:
    """
    Retourne un label canonique comme 'hello_sign' ou None si inconnu.
    Remplacez ce bouchon par votre mod√®le entra√Æn√© + seuil de confiance.
    """
    return "hello_sign" if landmarks else None

# --- 3) Reconnaissance vocale (chemin de secours)
def transcribe_speech(seconds: int = 3) -> Optional[str]:
    if not SR_OK:
        return None
    r = sr.Recognizer()
    try:
        with sr.Microphone() as source:
            st.info("√âcoute en cours...")
            audio = r.listen(source, phrase_time_limit=seconds)
        return r.recognize_google(audio, language="fr-FR") # Ajust√© pour le fran√ßais
    except Exception as e:
        st.warning(f"Erreur de reconnaissance vocale : {e}")
        return None

# --- 4) Logique de fusion (geste d'abord, parole en secours)
def fuse(gesture_label: Optional[str], speech_text: Optional[str]) -> str:
    if gesture_label and gesture_label in MAKATON_DICT:
        return MAKATON_DICT[gesture_label]
    if speech_text:
        return speech_text
    return "Aucune entr√©e d√©tect√©e"

# --- 5) Optionnel : extraire les points de rep√®re de la main via MediaPipe
def extract_hand_landmarks_from_image(image_bytes: bytes):
    if not MP_OK:
        return None
    try:
        import numpy as np
        np_arr = np.frombuffer(image_bytes, dtype=np.uint8)
        img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
        if img is None:
            return None

        mp_hands = mp.solutions.hands
        with mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5) as hands:
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            result = hands.process(img_rgb)

        if not result.multi_hand_landmarks:
            return None

        hand_landmarks = result.multi_hand_landmarks[0]
        return [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]
    except Exception:
        return None

# --- 6) Interface UI Streamlit
st.set_page_config(page_title="Makaton ‚Üí Fran√ßais (D√©mo Multimodale)")
st.title("Makaton ‚Üí Fran√ßais (D√©mo Multimodale)")
st.caption("Combine un traducteur Makaton de classe avec un chemin vocal assist√© (style AURA).")

with st.expander("Ce que cette d√©mo montre"):
    st.write(
        "- **Couche de traduction :** petit dictionnaire Makaton extensible.\n"
        "- **Fusion multimodale :** geste prioritaire, parole en secours.\n"
        "- **UI :** une seule page, sortie claire, synth√®se vocale optionnelle."
    )

tabs = st.tabs(["Signe Simul√©", "Webcam Image Unique (Optionnel)", "√Ä propos"])

# Onglet 1 : Simul√© (aucun mod√®le CV requis)
with tabs[0]:
    st.subheader("Geste Simul√© + Parole")
    col1, col2 = st.columns(2)

    with col1:
        simulate = st.selectbox(
            "Choisissez un signe",
            ["", "hello_sign", "thank_you_sign", "help_sign", "toilet_sign", "stop_sign"],
            index=0
        )
        gesture_label = simulate or None

    with col2:
        speech_text = st.session_state.get("speech_text")
        st.write("Parole actuelle :", speech_text or "Aucune")
        if st.button("Transcrire 3s"):
            if SR_OK:
                speech_text = transcribe_speech(3)
                st.session_state["speech_text"] = speech_text
            else:
                st.warning("SpeechRecognition non install√©.")

    output = fuse(gesture_label, st.session_state.get("speech_text"))
    st.markdown(f"### R√©sultat : **{output}**")

    if output and output != "Aucune entr√©e d√©tect√©e":
        if st.button("Lire le r√©sultat"):
            if GTTS_OK:
                mp3 = BytesIO()
                try:
                    gTTS(output, lang="fr").write_to_fp(mp3)
                    st.audio(mp3.getvalue(), format="audio/mp3")
                except Exception as e:
                    st.warning(f"√âchec TTS : {e}")
            else:
                st.warning("gTTS non install√©.")

# Onglet 2 : Capture webcam image unique
with tabs[1]:
    st.subheader("D√©tection de main image unique (Webcam)")
    if not MP_OK:
        st.warning("Installez MediaPipe + OpenCV pour activer cet onglet.")
    else:
        img = st.camera_input("Capturer une image")
        captured_label = None
        if img is not None:
            landmarks = extract_hand_landmarks_from_image(img.getvalue())
            if landmarks:
                captured_label = classify_gesture(landmarks)
                st.success("Main d√©tect√©e.")
            else:
                st.info("Aucun point de rep√®re trouv√©. Essayez un meilleur √©clairage.")

        if st.button("Transcrire 3s (onglet webcam)"):
            st.session_state["speech_text2"] = transcribe_speech(3) if SR_OK else None

        speech_text2 = st.session_state.get("speech_text2")
        st.write("Parole actuelle :", speech_text2 or "Aucune")

        output2 = fuse(captured_label, speech_text2)
        st.markdown(f"### R√©sultat : **{output2}**")

        if output2 and output2 != "Aucune entr√©e d√©tect√©e":
            if st.button("Lire le r√©sultat (onglet webcam)"):
                if GTTS_OK:
                    mp3 = BytesIO()
                    try:
                        gTTS(output2, lang="fr").write_to_fp(mp3)
                        st.audio(mp3.getvalue(), format="audio/mp3")
                    except Exception as e:
                        st.warning(f"√âchec TTS : {e}")
                else:
                    st.warning("gTTS non install√©.")
```

Ce code cr√©e une application Streamlit qui combine la reconnaissance de gestes et la reconnaissance vocale pour traduire les signes Makaton. Voici comment cela fonctionne :

1. **D√©pendances et Configuration** : Le code tente d'importer les d√©pendances optionnelles pour la d√©tection de gestes, la reconnaissance vocale et la synth√®se vocale.
    
2. **Dictionnaire Makaton** : Un dictionnaire minimal qui mappe les signes Makaton √† des mots. 
    
3. **Classificateur de Gestes** : Une fonction de substitution (`classify_gesture`) est utilis√©e pour la d√©mo.
    
4. **Reconnaissance Vocale** : La fonction `transcribe_speech` utilise la biblioth√®que SpeechRecognition comme solution de secours.
    
5. **Logique de Fusion** : La fonction `fuse` donne la priorit√© au geste sur la parole.
    
6. **Extraction de Points de Rep√®re** : Utilise MediaPipe pour extraire les coordonn√©es de la main d'une image.
    
7. **Interface UI Streamlit** : Propose des onglets pour simuler les signes ou utiliser une webcam.

### Ex√©cution

```bash
streamlit run .\streamlit_app.py
```

La commande ci-dessus lance l'application Streamlit, ouvrant le script dans un navigateur web pour interagir avec l'interface.

![Interface de l'application](https://github.com/tayo4christ/ai-accessibility-articles-assets/blob/8117234b9dc032aa0f4ff32abad92e7ad3344b81/ui-home-simulated-tab.jpg?raw=1 align="left")

*Figure ‚Äî Interface de l'application : l'onglet Signe Simul√© avant toute entr√©e.*

![Signe simul√© hello_sign](https://github.com/tayo4christ/ai-accessibility-articles-assets/blob/8117234b9dc032aa0f4ff32abad92e7ad3344b81/ui-simulated-hello-output.jpg?raw=1 align="left")

*Figure ‚Äî La s√©lection de* `hello_sign` *produit "R√©sultat : Bonjour".*

## Aper√ßu du projet

Vous avez d√©velopp√© un traducteur multimodal qui int√®gre √† la fois la reconnaissance de gestes (signes Makaton) et la reconnaissance vocale pour produire une sortie unifi√©e. Le syst√®me est con√ßu pour donner la priorit√© √† l'entr√©e gestuelle, en utilisant la parole comme solution de secours.

**Interface Utilisateur**

L'application est construite avec Streamlit, comprenant deux onglets principaux :

* **Onglet Signe Simul√©** : Permet aux utilisateurs de simuler des gestes sans n√©cessiter de capacit√©s de vision par ordinateur (CV).
    
* **Onglet Webcam Image Unique** : Utilise optionnellement une webcam pour traiter une image pour la d√©tection de gestes.
    

**Int√©gration des cas d'utilisation**

* **Traduction Makaton-Fran√ßais** : Dans un cadre scolaire, les signes Makaton d√©tect√©s sont traduits en phrases courtes, facilitant la communication.
    
* **Chemin assist√© de style AURA** : Si aucun geste n'est d√©tect√©, le syst√®me s'appuie sur l'entr√©e vocale pour g√©n√©rer une sortie.
    

**Limites de conception**

* Le classificateur de gestes est actuellement un bouchon (placeholder) et devrait √™tre remplac√© par un mod√®le entra√Æn√© avec un seuil de confiance.
    
* Le dictionnaire Makaton est minimal et peut √™tre √©tendu avec plus de phrases et de mod√®les.
    
* Le composant de reconnaissance vocale utilise un outil de base. Pour plus de robustesse, envisagez des mod√®les avanc√©s comme Wav2Vec2.
    

**Extensions sugg√©r√©es**

* Impl√©menter un seuil de confiance pour afficher les deux entr√©es (geste et parole) en cas d'incertitude.
    
* √âtendre le dictionnaire pour supporter des mod√®les √† emplacements (ex: "Je veux [objet]").
    
* Introduire un interrupteur pour choisir la priorit√© entre parole et geste.
    
* Activer la journalisation des sorties pour les enseignants avec export en CSV.
    

**Conseils de d√©pannage**

* En cas d'erreurs de microphone, v√©rifiez l'installation de `pyaudio`.
    
* Si la webcam n'est pas d√©tect√©e, v√©rifiez les permissions du navigateur.
    

Lien vers le code complet : [Multimodal_Makaton](https://github.com/tayo4christ/makaton-multimodal-demo/tree/main/makaton_multimodal_demo)

## D√©fis et consid√©rations √©thiques

Bien que la promesse des outils d'accessibilit√© multimodaux soit passionnante, les construire de mani√®re responsable n√©cessite de confronter plusieurs d√©fis techniques et √©thiques.

### Raret√© des donn√©es

L'entra√Ænement des syst√®mes d'IA n√©cessite de grands jeux de donn√©es diversifi√©s. Mais pour la parole d√©sordonn√©e ou les syst√®mes comme le Makaton, les donn√©es sont limit√©es. Sans assez d'exemples, les mod√®les risquent d'√™tre impr√©cis ou biais√©s. La collecte de donn√©es doit se faire de mani√®re √©thique, avec consentement.

### √âquit√© et Inclusion

Les syst√®mes d'IA fonctionnent souvent mieux pour certains groupes que pour d'autres. Un mod√®le entra√Æn√© sur des locuteurs fluides peut √©chouer pour ceux ayant des accents forts ou des difficult√©s de parole. L'√©quit√© signifie concevoir des mod√®les qui fonctionnent pour toutes les capacit√©s, accents et cultures.

### Confidentialit√© et S√©curit√©

Les donn√©es vocales et vid√©o sont sensibles, surtout dans les √©coles. La protection de ces donn√©es est une exigence. Les syst√®mes doivent anonymiser ou crypter les enregistrements et les stocker de mani√®re s√©curis√©e.

### Accessibilit√© des outils eux-m√™mes

Ironiquement, de nombreux "outils d'accessibilit√©" restent inaccessibles car trop chers ou complexes. Pour que l'IA r√©duise r√©ellement les barri√®res, les solutions doivent √™tre abordables, l√©g√®res et faciles √† installer dans de vraies classes.

## Vers quoi nous nous dirigeons ensuite

L'avenir des outils d'accessibilit√© IA est sp√©culatif mais prometteur. Ce que nous avons maintenant sont des prototypes ; ce qui nous attend sont des outils qui pourraient remodeler la soci√©t√©.

### Traduction Makaton Multilingue

Une direction prometteuse est la capacit√© de traduire le Makaton vers plusieurs langues. Un apprenant au Royaume-Uni pourrait signer en Makaton et voir sa contribution appara√Ætre en fran√ßais, en espagnol ou en yoruba, ouvrant les classes internationales.

### Tuteurs IA avec adaptation dynamique

Imaginez un assistant de classe capable de changer de mode en temps r√©el : si l'apprenant fatigue des gestes, l'IA propose des options bas√©es sur des symboles, s'adaptant continuellement aux forces de chaque √©l√®ve.

### Dispositifs multimodaux portables (Wearables)

Des lunettes pourraient capturer les gestes et superposer du texte, tandis que des oreillettes pourraient traduire une parole d√©sordonn√©e en un audio clair pour les pairs. L'accessibilit√© deviendrait portable et omnipr√©sente.

### Un impact plus large

Ces innovations s'alignent sur les Objectifs de D√©veloppement Durable (ODD) des Nations Unies, notamment l'**√âducation de Qualit√© (Objectif 4)** et la **R√©duction des In√©galit√©s (Objectif 10)**.

## Conclusion : Construire un avenir inclusif avec l'IA

Les outils d'accessibilit√© IA ne sont plus de simples options. Ils deviennent des moteurs essentiels de l'inclusion dans l'√©ducation, la sant√© et le travail.

Le passage des syst√®mes de reconnaissance de gestes pr√©coces aux prototypes multimodaux comme la traduction Makaton et AURA montre ce qui est possible lorsque la technologie est con√ßue autour de l'humain. Ces innovations brisent les barri√®res de communication et ouvrent de nouvelles opportunit√©s pour les apprenants souvent laiss√©s en marge.

Mais cet avenir n'est pas automatique. Il d√©pend de nos choix en tant que d√©veloppeurs, √©ducateurs et d√©cideurs. La vision est claire : un monde o√π chaque apprenant, quelle que soit sa capacit√©, peut s'exprimer pleinement et participer avec confiance.

**L'avenir de l'√©ducation est inclusif et, avec un design r√©fl√©chi, l'IA peut nous aider √† y parvenir.**