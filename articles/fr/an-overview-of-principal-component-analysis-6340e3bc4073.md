---
title: Aper√ßu de l'Analyse en Composantes Principales
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2019-05-01T16:12:23.000Z'
originalURL: https://freecodecamp.org/news/an-overview-of-principal-component-analysis-6340e3bc4073
coverImage: https://cdn-media-1.freecodecamp.org/images/1*ldDA-9rCN_gG3qaMzIA6fA.png
tags:
- name: algorithms
  slug: algorithms
- name: Data Science
  slug: data-science
- name: Machine Learning
  slug: machine-learning
- name: statistics
  slug: statistics
- name: 'tech '
  slug: tech
seo_title: Aper√ßu de l'Analyse en Composantes Principales
seo_desc: 'By Moshe Binieli

  This article will explain you what Principal Component Analysis (PCA) is, why we
  need it and how we use it. I will try to make it as simple as possible while avoiding
  hard examples or words which can cause a headache.

  A moment of hon...'
---

Par Moshe Binieli

Cet article vous expliquera ce qu'est l'Analyse en Composantes Principales (ACP), pourquoi nous en avons besoin et comment nous l'utilisons. Je vais essayer de le rendre aussi simple que possible tout en √©vitant les exemples ou les mots difficiles qui pourraient causer un mal de t√™te.

Un moment d'honn√™tet√© : pour comprendre pleinement cet article, une compr√©hension de base de certaines notions d'alg√®bre lin√©aire et de statistiques est essentielle. Prenez quelques minutes pour r√©viser les sujets suivants, si n√©cessaire, afin de faciliter la compr√©hension de l'ACP :

* vecteurs
* vecteurs propres
* valeurs propres
* variance
* covariance

### **Comment cet algorithme peut-il nous aider ? Quelles sont les utilisations de cet algorithme ?**

* Identifie les directions les plus pertinentes de la variance dans les donn√©es.
* Aide √† capturer les caract√©ristiques les plus "importantes".
* Plus facile √† effectuer des calculs sur le jeu de donn√©es apr√®s la r√©duction de dimension puisque nous avons moins de donn√©es √† traiter.
* Visualisation des donn√©es.

### Explication verbale courte.

Supposons que nous avons 10 variables dans notre jeu de donn√©es et supposons que 3 variables capturent 90 % du jeu de donn√©es, et 7 variables capturent 10 % du jeu de donn√©es.

Supposons que nous voulons visualiser 10 variables. Bien s√ªr, nous ne pouvons pas faire cela, nous pouvons visualiser seulement un maximum de 3 variables (peut-√™tre qu'√† l'avenir nous pourrons en faire plus).

Nous avons donc un probl√®me : nous ne savons pas quelles variables capturent la plus grande variabilit√© dans nos donn√©es. Pour r√©soudre ce myst√®re, nous appliquerons l'algorithme ACP. Le r√©sultat nous dira quelles sont ces variables. Cela semble cool, n'est-ce pas ? üòâ

### **Quelles sont les √©tapes pour faire fonctionner l'ACP ? Comment appliquons-nous la magie ?**

1. Prenez le jeu de donn√©es sur lequel vous voulez appliquer l'algorithme.
2. Calculez la matrice de covariance.
3. Calculez les vecteurs propres et leurs valeurs propres.
4. Triez les vecteurs propres selon leurs valeurs propres par ordre d√©croissant.
5. Choisissez les premiers K vecteurs propres (o√π k est la dimension √† laquelle nous voulons aboutir).
6. Construisez un nouveau jeu de donn√©es r√©duit.

### Temps pour un exemple avec des donn√©es r√©elles.

#### 1) **Charger le jeu de donn√©es dans une matrice :**

Notre **objectif principal** est de d√©terminer combien de variables sont les plus importantes pour nous et de rester uniquement avec elles.

Pour cet exemple, nous utiliserons le programme "Spyder" pour ex√©cuter Python. Nous utiliserons √©galement un jeu de donn√©es assez cool qui est int√©gr√© dans "sklearn.datasets" et qui s'appelle "load_iris". Vous pouvez en lire plus sur ce jeu de donn√©es sur [Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set).

Tout d'abord, nous allons charger le module iris et transformer le jeu de donn√©es en une matrice. Le jeu de donn√©es contient 4 variables avec 150 exemples. Par cons√©quent, la dimensionnalit√© de notre matrice de donn√©es est : (150, 4).

```
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
```

```
irisModule = load_iris()
dataset = np.array(irisModule.data)
```

![Image](https://cdn-media-1.freecodecamp.org/images/YXHqyLU0LAHkble6hyDqeSipKK3p3QoIC8cC)
_Visualisation du jeu de donn√©es_

Il y a plus de lignes dans ce jeu de donn√©es ‚Äî comme nous l'avons dit, il y a 150 lignes, mais nous ne pouvons en voir que 17.

Le concept de l'ACP est de r√©duire la dimensionnalit√© de la matrice en trouvant les directions qui capturent la plupart de la variabilit√© dans notre matrice de donn√©es. Par cons√©quent, nous aimerions les trouver.

#### 2) **Calculer la matrice de covariance :**

Il est temps de calculer la matrice de covariance de notre jeu de donn√©es, mais que signifie cela ? Pourquoi devons-nous calculer la matrice de covariance ? √Ä quoi ressemblera-t-elle ?

La **[variance](https://en.wikipedia.org/wiki/Variance)** est l'esp√©rance de l'√©cart quadratique d'une variable al√©atoire par rapport √† sa moyenne. Informellement, **elle mesure l'√©talement d'un ensemble de nombres par rapport √† leur moyenne.** La d√©finition math√©matique est :

![Image](https://cdn-media-1.freecodecamp.org/images/K52ek3Wc1IDyzQI3rWZcpqH3XoMVpp2452jY)

La **[covariance](https://en.wikipedia.org/wiki/Covariance)** est une mesure de la variabilit√© conjointe de deux variables al√©atoires. En d'autres termes, comment deux caract√©ristiques varient l'une par rapport √† l'autre. L'utilisation de la covariance est tr√®s courante lors de la recherche de motifs dans les donn√©es. La d√©finition math√©matique est :

![Image](https://cdn-media-1.freecodecamp.org/images/VP34PkFzIJfNHFEreKfU4KXmCjUePqm6ERtz)

√Ä partir de cette d√©finition, nous pouvons conclure que la matrice de covariance sera sym√©trique. Cela est important car cela signifie que ses vecteurs propres seront r√©els et non n√©gatifs, ce qui nous facilite la t√¢che (nous vous mettons au d√©fi de pr√©tendre que travailler avec des nombres complexes est plus facile qu'avec des nombres r√©els !)

Apr√®s avoir calcul√© la matrice de covariance, elle ressemblera √† ceci :

![Image](https://cdn-media-1.freecodecamp.org/images/aTBSknE6DhtopHBN23kDC-E4e5bCWf-mVLIV)
_Visualisation de la matrice de covariance_

Comme vous pouvez le voir, la diagonale principale est √©crite comme **V** (variance) et le reste est √©crit comme **C** (covariance), pourquoi cela ?

Parce que le calcul de la covariance de la m√™me variable revient essentiellement √† calculer sa variance (si vous n'√™tes pas s√ªr pourquoi ‚Äî prenez quelques minutes pour comprendre ce que sont la variance et la covariance).

Calculons en Python la matrice de covariance du jeu de donn√©es en utilisant le code suivant :

```
covarianceMatrix = pd.DataFrame(data = np.cov(dataset, rowvar = False), columns = irisModule.feature_names, index = irisModule.feature_names)
```

![Image](https://cdn-media-1.freecodecamp.org/images/4wUPiNCggyBdEZOjoqfTDvOpIqaqdBREVOMy)
_La matrice de covariance du jeu de donn√©es_

* Nous ne nous int√©ressons pas √† la diagonale principale, car ce sont les variances de la m√™me variable. Puisque nous essayons de trouver de nouveaux motifs dans le jeu de donn√©es, **nous ignorerons la diagonale principale.**
* Puisque la matrice est sym√©trique, covariance(a,b) = covariance(b,a), **nous ne regarderons que les valeurs sup√©rieures de la matrice de covariance (au-dessus de la diagonale).**
Une chose importante √† mentionner sur la covariance : si la covariance des variables **a** et **b** est **positive**, cela signifie qu'elles **varient dans la m√™me direction.** Si la covariance de **a** et **b** est **n√©gative**, elles varient dans des **directions diff√©rentes.**

#### 3) **Calculer les valeurs propres et les vecteurs propres :**

Comme je l'ai mentionn√© au d√©but, les valeurs propres et les vecteurs propres sont les termes de base que vous devez conna√Ætre pour comprendre cette √©tape. Par cons√©quent, je ne vais pas l'expliquer, mais plut√¥t passer √† leur calcul.

Le vecteur propre associ√© √† la plus grande valeur propre indique la direction dans laquelle les donn√©es ont le plus de variance. Par cons√©quent, en utilisant les valeurs propres, nous saurons quels vecteurs propres capturent le plus de variabilit√© dans nos donn√©es.

```
eigenvalues, eigenvectors = np.linalg.eig(covarianceMatrix)
```

Voici le vecteur des valeurs propres, le premier index du vecteur des valeurs propres est associ√© au premier index de la matrice des vecteurs propres.

Les valeurs propres :

![Image](https://cdn-media-1.freecodecamp.org/images/HEXQgmvpNE7PxzuxIwjkI2bXAlBHFp2JwrMH)
_Valores propres de la matrice de covariance_

La matrice des vecteurs propres :

![Image](https://cdn-media-1.freecodecamp.org/images/7wEPUqxyvzXKBBxVtZnm6CicuKfqg6StG9x1)
_Matrice des vecteurs propres de la matrice de covariance_

#### 4) Choisir les premi√®res K valeurs propres (K composantes principales/axes) :

Les valeurs propres nous indiquent la quantit√© de variabilit√© dans la direction de son vecteur propre correspondant. Par cons√©quent, le vecteur propre avec la plus grande valeur propre est la direction avec le plus de variabilit√©. Nous appelons ce vecteur propre la premi√®re composante principale (ou axe). Selon cette logique, le vecteur propre avec la deuxi√®me plus grande valeur propre sera appel√© la deuxi√®me composante principale, et ainsi de suite.

Nous voyons les valeurs suivantes : 
[4.224, 0.242, 0.078, 0.023]

Traduisons ces valeurs en pourcentages et visualisons-les. Nous prendrons le pourcentage que chaque valeur propre couvre dans le jeu de donn√©es.

```
totalSum = sum(eigenvalues)
variablesExplained = [(i / totalSum) for i in sorted(eigenvalues, reverse = True)]
```

![Image](https://cdn-media-1.freecodecamp.org/images/jlZXLrss28kt9sfFeJ3zLJm8KKqtZUvE503Q)

Comme vous pouvez clairement le voir, la **premi√®re** valeur propre prend **92,5 %** et la **deuxi√®me** prend **5,3 %**, et **la troisi√®me et la quatri√®me ne couvrent pas beaucoup de donn√©es du jeu de donn√©es total.** Par cons√©quent, nous pouvons facilement d√©cider de rester avec seulement **2 variables**, la premi√®re et la deuxi√®me.

```
featureVector = eigenvectors[:,:2]
```

Supprimons la troisi√®me et la quatri√®me variable du jeu de donn√©es. Il est important de dire qu'√† ce stade, nous perdons certaines informations. Il est impossible de r√©duire les dimensions sans perdre certaines informations (sous l'hypoth√®se de position g√©n√©rale). L'algorithme ACP nous indique la bonne fa√ßon de r√©duire les dimensions tout en conservant la quantit√© maximale d'informations concernant nos donn√©es.

Et le jeu de donn√©es restant ressemble √† ceci :

![Image](https://cdn-media-1.freecodecamp.org/images/4Smlm09-kewqyLQP4xWDWrIfC4wVXLMjY548)
_Vecteurs propres restants apr√®s la suppression de deux variables_

#### 5) **Construire le nouveau jeu de donn√©es r√©duit :**

Nous voulons construire un nouveau jeu de donn√©es r√©duit √† partir des K composantes principales choisies.

Nous prendrons les K composantes principales choisies (k=2 ici) qui nous donnent une matrice de taille (4, 2), et nous prendrons le jeu de donn√©es original qui est une matrice de taille (150, 4).

![Image](https://cdn-media-1.freecodecamp.org/images/oa-Zyx7jj4ZFVqnOit5TCQge5HtOzRCKEjFq)
_Les matrices avec lesquelles nous devons travailler_

Nous effectuerons une multiplication de matrices de la mani√®re suivante :

* La premi√®re matrice que nous prenons est la matrice qui contient les K composantes principales que nous avons choisies et nous transposons cette matrice.
* La deuxi√®me matrice que nous prenons est la matrice originale et nous la transposons.
* √Ä ce stade, nous effectuons une multiplication de matrices entre ces deux matrices.
* Apr√®s avoir effectu√© la multiplication de matrices, nous transposons la matrice r√©sultante.

![Image](https://cdn-media-1.freecodecamp.org/images/KhmpIrq7pfs23am9uSfFkLhY-tD16Fc2PswV)
_Multiplication de matrices_

```
featureVectorTranspose = np.transpose(featureVector)
datasetTranspose = np.transpose(dataset)
newDatasetTranspose = np.matmul(featureVectorTranspose, datasetTranspose)
newDataset = np.transpose(newDatasetTranspose)
```

Apr√®s avoir effectu√© la multiplication des matrices et transpos√© la matrice r√©sultante, voici les valeurs que nous obtenons pour les nouvelles donn√©es qui contiennent uniquement les K composantes principales que nous avons choisies.

![Image](https://cdn-media-1.freecodecamp.org/images/q3R-P6DpdinYG3m6nOFBGLuP49H-ZeCtbfbT)

### Conclusion

Comme (nous l'esp√©rons) vous pouvez maintenant le voir, l'ACP n'est pas si difficile. Nous avons r√©ussi √† r√©duire les dimensions du jeu de donn√©es assez facilement en utilisant Python.

Dans notre jeu de donn√©es, nous n'avons pas caus√© d'impact s√©rieux car nous n'avons supprim√© que 2 variables sur 4. Mais supposons que nous avons 200 variables dans notre jeu de donn√©es, et que nous avons r√©duit de 200 variables √† 3 variables ‚Äî cela devient d√©j√† plus significatif.

Esp√©rons que vous avez appris quelque chose de nouveau aujourd'hui. N'h√©sitez pas √† contacter [Chen Shani](https://www.linkedin.com/in/chen-shani-638816184/) ou [Moshe Binieli](https://www.linkedin.com/in/moshe-binieli-22b11a137/) sur LinkedIn pour toute question.