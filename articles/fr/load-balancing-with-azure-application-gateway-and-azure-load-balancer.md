---
title: Ã‰quilibrage de charge avec Azure Application Gateway et Azure Load Balancer
  â€“ Quand utiliser chacun
subtitle: ''
author: Prince Onukwili
co_authors: []
series: null
date: '2025-05-14T19:37:46.320Z'
originalURL: https://freecodecamp.org/news/load-balancing-with-azure-application-gateway-and-azure-load-balancer
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1747235455030/cb82bfb4-8d7b-47e5-ab31-126906f60b40.png
tags:
- name: Load Balancing
  slug: load-balancing
- name: Azure
  slug: azure
- name: Azure Application Gateway
  slug: azure-application-gateway
- name: virtual machine
  slug: virtual-machine
- name: Cloud
  slug: cloud
- name: '#virtual machine scale set'
  slug: virtual-machine-scale-set
- name: Load Balancer
  slug: load-balancer
seo_title: Ã‰quilibrage de charge avec Azure Application Gateway et Azure Load Balancer
  â€“ Quand utiliser chacun
seo_desc: 'Youâ€™ve probably heard someone mention load balancing when talking about
  cloud apps. Maybe even names like Azure Load Balancer, Azure Application Gateway,
  or something about Virtual Machines and Scale Sets. ğŸ˜µâ€ğŸ’«

  It all sounds important...but also a l...'
---

Vous avez probablement entendu quelqu'un mentionner l'Ã©quilibrage de charge en parlant des applications cloud. Peut-Ãªtre mÃªme des noms comme Azure Load Balancer, Azure Application Gateway, ou quelque chose sur les machines virtuelles et les groupes identiques. ğŸ˜µâ€ğŸ’«

Tout cela semble important... mais aussi un peu confus. Comme, pourquoi y a-t-il tant de piÃ¨ces mobiles ? Et que font-elles rÃ©ellement ?

Dans ce guide, nous allons tout dÃ©composer â€“ Ã©tape par Ã©tape â€“ en utilisant des exemples rÃ©els et un langage simple.

Vous apprendrez :

* Ce que sont les Ã©quilibreurs de charge (et pourquoi les applications en ont mÃªme besoin)

* Comment les applications Ã©taient dÃ©ployÃ©es avant l'existence des Ã©quilibreurs de charge (indice : tout vivait sur un seul serveur solitaire)

* Comment fonctionnent les machines virtuelles Azure â€“ et comment elles vous permettent de monter en puissance vos applications

* Ce que sont les groupes identiques de machines virtuelles, et comment ils aident Ã  gÃ©rer les pics de trafic soudains

* Les diffÃ©rences entre Azure Load Balancer et Azure Application Gateway, et quand utiliser chacun

Ã€ la fin, vous ne comprendrez pas seulement ce que font ces outils â€“ vous saurez *quand* et *pourquoi* les utiliser dans des scÃ©narios rÃ©els.

Que vous soyez un dÃ©butant curieux, un constructeur pratique, ou quelqu'un qui essaie simplement de comprendre l'Ã©cosystÃ¨me d'Azure, ce guide est pour vous.

PrÃªt Ã  dÃ©mÃªler les spaghettis du cloud ? C'est parti ! ğŸğŸš€

## ğŸ“œ Table des matiÃ¨res

1. [ğŸ§‘â€ğŸ’» Qu'est-ce que les Ã©quilibreurs de charge ?](#heading-quest-ce-que-les-equilibreurs-de-charge)

2. [ğŸ¢ Comment les applications Ã©taient dÃ©ployÃ©es avant les Ã©quilibreurs de charge](#heading-comment-les-applications-etaient-deployees-avant-les-equilibreurs-de-charge)

3. [âš™ï¸ Machines virtuelles Azure (VM) â€“ Les blocs de construction](#heading-machines-virtuelles-azure-vm-les-blocs-de-construction)

4. [ğŸ“ˆ Le besoin de mise Ã  l'Ã©chelle â€“ Vertical vs Horizontal](#heading-le-besoin-de-mise-a-lechelle-vertical-vs-horizontal)

5. [ğŸ”„ Groupes identiques de machines virtuelles Azure (VMSS) â€“ Mise Ã  l'Ã©chelle simplifiÃ©e](#heading-groupes-identiques-de-machines-virtuelles-azure-vmss-mise-a-lechelle-simplifiee)

6. [ğŸ“§ Azure Load Balancer â€“ RÃ©partition du trafic](#heading-azure-load-balancer-repartition-du-trafic)

7. [ğŸ´ Azure Application Gateway â€“ Routage intelligent pour les applications modernes](#heading-azure-application-gateway-routage-intelligent-pour-les-applications-modernes)

8. [ğŸ” Azure Load Balancer vs Azure Application Gateway](#heading-azure-load-balancer-vs-azure-application-gateway)

9. [ğŸ§‘â€ğŸ³](#heading-cas-dusage-quand-utiliser-quoi) [Cas d'usage : Quand utiliser chacun](#heading-cas-dusage-quand-utiliser-chacun)

10. [âœ… Conclusion](#heading-conclusion)

11. [Ã‰tudier plus loin ğŸ“œ](#heading-etudier-plus-loin)

12. [Ã€ propos de l'auteur ğŸ‘¨â€ğŸ’»](#heading-a-propos-de-lauteur)

## ğŸ§‘â€ğŸ’» Qu'est-ce que les Ã©quilibreurs de charge ?

Imaginez que vous gÃ©rez un petit restaurant avec un seul chef en cuisine. Tout se passe bien lorsque vous avez quelques clients â€“ chaque commande est prÃ©parÃ©e l'une aprÃ¨s l'autre, et tout le monde repart satisfait.

Mais que se passe-t-il lorsque 50 personnes entrent toutes en mÃªme temps ?

ğŸ Un chef ne peut pas gÃ©rer autant de commandes en mÃªme temps.  
â³ Les gens commencent Ã  attendre plus longtemps.  
ğŸ˜­ Certains clients partent.  
ğŸ’¥ Le chef est submergÃ© â€“ et finit par s'Ã©puiser.

C'est ce qui peut arriver Ã  un serveur (l'ordinateur qui exÃ©cute votre application) lorsque trop d'utilisateurs essaient d'y accÃ©der en mÃªme temps.

### Alors, que fait un Ã©quilibreur de charge ?

Un **Ã©quilibreur de charge** est comme un gestionnaire de restaurant intelligent. Mais au lieu de commandes de nourriture, il gÃ¨re les requÃªtes des utilisateurs â€“ les choses que les gens font lorsqu'ils ouvrent votre application, cliquent sur des boutons, ou chargent des donnÃ©es.

Disons que vous avez maintenant trois chefs (serveurs) au lieu d'un. Le travail de l'Ã©quilibreur de charge est de :

* ğŸ‘€ Surveiller les commandes entrantes (requÃªtes des utilisateurs)

* ğŸ§‘â€ğŸ³ DÃ©cider quel chef (serveur) est disponible ou le moins occupÃ©

* ğŸ Envoyer cette requÃªte au bon

* ğŸ”„ RÃ©pÃ©ter cela encore et encore, en s'assurant que tout reste rapide et fluide

Donc en termes simples, un Ã©quilibreur de charge prend tout le trafic entrant vers votre application et le distribue sur plusieurs serveurs afin qu'aucun serveur ne soit surchargÃ© â€“ cool, non ? ğŸ˜Š

### Pourquoi les Ã©quilibreurs de charge ont-ils Ã©tÃ© introduits ?

Ã€ l'Ã©poque, de nombreuses applications Ã©taient hÃ©bergÃ©es sur une seule machine â€“ appelÃ©e DÃ©ploiement sur un seul serveur.

C'Ã©tait bien lorsque vous aviez un petit nombre d'utilisateurs. Mais une fois que les choses ont commencÃ© Ã  croÃ®tre â€“ plus d'utilisateurs, plus d'actions, plus de donnÃ©es â€“ les serveurs uniques sont devenus un goulot d'Ã©tranglement :

* Ils ne pouvaient gÃ©rer qu'un nombre limitÃ© de requÃªtes.

* S'ils tombaient en panne, toute votre application cessait de fonctionner.

* La mise Ã  l'Ã©chelle (ajout de plus de puissance) Ã©tait coÃ»teuse et manuelle.

ğŸ’¡ Entrent en scÃ¨ne les **Ã©quilibreurs de charge** â€“ conÃ§us pour rÃ©soudre ce problÃ¨me en rendant possible :

* RÃ©partir le trafic sur plusieurs serveurs (afin qu'aucun serveur ne s'Ã©crase sous la pression),

* Remplacer ou redÃ©marrer des serveurs sans temps d'arrÃªt,

* Ajouter ou supprimer des serveurs selon les besoins, en fonction de la frÃ©quentation de votre application (c'est ce qu'on appelle la **mise Ã  l'Ã©chelle**).

### Un scÃ©nario simple de cas d'usage

Disons que vous construisez une boutique en ligne â€“ votre propre mini Amazon. Au dÃ©but, vous hÃ©bergez votre application sur une seule machine virtuelle Azure. Tout va bien. Mais un jour, vous lancez une Ã©norme promo et soudain... des milliers de personnes affluent pour naviguer, faire des achats et passer Ã  la caisse.

Votre seule machine virtuelle commence Ã  ralentir.

Les commandes Ã©chouent. Les gens se plaignent. Votre application de rÃªve ? Elle s'Ã©crase rapidement. ğŸ’¥

Alors, que faites-vous ?

Vous lancez deux autres machines virtuelles pour aider â€“ mais maintenant vous avez un autre problÃ¨me : *Comment diviser le trafic entre les trois ?*

C'est lÃ  que l'Ã©quilibreur de charge intervient. Il :

* Regarde chaque requÃªte utilisateur entrante

* DÃ©termine quelle machine virtuelle est disponible et la moins occupÃ©e

* Envoie la requÃªte lÃ 

* Continue Ã  faire tourner les requÃªtes en temps rÃ©el

Et le rÃ©sultat ?  
âœ… Aucune machine virtuelle n'est submergÃ©e  
âœ… Votre application reste rapide et rÃ©active  
âœ… Les utilisateurs sont heureux (et achÃ¨tent Ã  nouveau !)

![Illustration d'Ã©quilibreur de charge](https://cdn.hashnode.com/res/hashnode/image/upload/v1746980088916/41be330b-8d5b-4709-b07d-3f1a19d641e7.png align="center")

## ğŸ¢ Comment les applications Ã©taient dÃ©ployÃ©es avant les Ã©quilibreurs de charge

Avant les outils cloud comme les Ã©quilibreurs de charge, la maniÃ¨re typique de faire fonctionner une application Ã©tait assez simple : vous dÃ©ployiez toute l'application sur un seul serveur, comme gÃ©rer une petite entreprise Ã  partir d'une seule petite boutique.

### D'abord : Qu'est-ce qu'un serveur ?

Pensez Ã  un serveur comme un ordinateur spÃ©cial qui est toujours connectÃ© Ã  Internet. Son travail est de "servir" votre application aux gens lorsqu'ils visitent votre site web, ouvrent votre application ou utilisent votre service.

Dans les plateformes cloud comme Azure, nous appelons gÃ©nÃ©ralement ceux-ci des machines virtuelles (VM) â€“ essentiellement, des serveurs alimentÃ©s par logiciel que vous pouvez lancer avec quelques clics.

### Monolithes vs Microservices

Maintenant, les applications viennent sous diffÃ©rentes "formes". Les deux plus courantes sont :

* **Monolithes** : Tout est regroupÃ© dans une seule grande application. Tout le code â€“ de la connexion de l'utilisateur au panier d'achat jusqu'Ã  la caisse â€“ vit dans une seule unitÃ©.

* **Microservices** : L'application est divisÃ©e en petites applications indÃ©pendantes (services). Chaque service fait un travail â€“ comme la connexion, les paiements, les commandes â€“ et fonctionne sÃ©parÃ©ment.

#### Comment ces applications Ã©taient-elles dÃ©ployÃ©es ?

Qu'il s'agisse d'un monolithe ou d'un ensemble de microservices, ils Ã©taient gÃ©nÃ©ralement dÃ©ployÃ©s sur un seul serveur (VM).

Pour les monolithes, vous exÃ©cutiez simplement toute l'application directement sur le serveur. Pour les microservices : vous exÃ©cutiez chaque service dans un espace sÃ©parÃ© sur ce mÃªme serveur, en utilisant des **conteneurs**.

#### Attendez â€” Qu'est-ce qu'un conteneur ?

Un conteneur est comme un mini-ordinateur *Ã  l'intÃ©rieur* d'un ordinateur. Il contient tout ce dont une application a besoin pour fonctionner â€“ code, outils, paramÃ¨tres â€“ et il garde chaque application isolÃ©e des autres.

Pourquoi utiliser des conteneurs ?

* Vous pouvez exÃ©cuter plusieurs services sur le mÃªme serveur sans que leur logiciel sous-jacent (logiciel nÃ©cessaire pour que chaque application fonctionne) n'interfÃ¨re les uns avec les autres.

* C'est plus rapide et plus efficace que d'installer tout directement sur le serveur.

* Ils rendent le dÃ©placement des applications entre les environnements (par exemple, test â†’ production) super fluide (plus de "Mais, Ã§a marche sur ma machineâ€¦").

Des outils populaires comme Docker rendent le travail avec les conteneurs facile.

#### Relier le tout ensemble : Domaines, sous-domaines et proxys inverses

Lorsque votre application vit sur un serveur, vous voulez que les gens puissent y accÃ©der. C'est lÃ  que les **noms de domaine** entrent en jeu.

* Votre serveur a une adresse IP publique â€“ un ensemble de chiffres comme `102.80.1.23`, qui lui donne un identifiant unique sur l'internet public

* Mais au lieu de demander aux utilisateurs de taper des chiffres, vous liez cette IP Ã  un nom de domaine, comme `monapplicationcool.com`

Si votre application a des microservices, vous pouvez mÃªme attribuer des **sous-domaines** comme :

* `api.monapplicationcool.com` pour le backend

* `tableaubord.monapplicationcool.com` pour l'interface utilisateur

* `paiements.monapplicationcool.com` pour les paiements

Pour gÃ©rer tout cela, vous utiliseriez un **proxy inverse** (comme Nginx ou Apache). Il Ã©coute sur le domaine principal et les sous-domaines, et redirige le trafic vers la bonne application ou service.

Exemple :

* Quelqu'un visite `tableaubord.monapplicationcool.com`

* Le proxy inverse vÃ©rifie le domaine et redirige la requÃªte vers le bon conteneur exÃ©cutant le service de tableau de bord

Et pour aider Ã  toute cette configuration â€“ du dÃ©ploiement des conteneurs Ã  la configuration des proxys inverses â€“ il existe des outils conviviaux pour les dÃ©veloppeurs comme [Coolify](https://coolify.io). Coolify est une plateforme open-source qui facilite grandement le travail des dÃ©veloppeurs et des Ã©quipes DevOps pour :

* DÃ©ployer des applications dans des conteneurs

* Configurer des domaines et sous-domaines

* Configurer des proxys inverses â€“ le tout Ã  partir d'un tableau de bord propre, sans commandes terminales complexes nÃ©cessaires

![Exemple de tableau de bord Coolify](https://cdn.hashnode.com/res/hashnode/image/upload/v1746979943646/a6525a09-f44a-4e00-a945-7bded3483b0d.jpeg align="center")

Tout cela Ã©tait configurÃ© sur UN SEUL SERVEUR/VM. Mais voici le problÃ¨me : lorsque ce serveur Ã©tait surchargÃ© ou tombait en panneâ€¦ğŸ’¥ tout s'arrÃªtait.

C'est pourquoi nous avions besoin d'une meilleure faÃ§on. Et c'est lÃ  que la **mise Ã  l'Ã©chelle** et l'**Ã©quilibrage de charge** sont entrÃ©s en jeu â€“ pour garder les applications en marche sans problÃ¨me, quel que soit le trafic.

## âš™ï¸ Machines virtuelles Azure (VM) â€“ Les blocs de construction

![Illustration de machine virtuelle](https://cdn.hashnode.com/res/hashnode/image/upload/v1746980948928/eb6a7fb2-7432-42ed-8cbd-bff6c8250d4e.jpeg align="center")

En ce qui concerne l'exÃ©cution d'applications dans le cloud, les **machines virtuelles (VM)** sont les blocs de construction de base â€“ un peu comme louer un appartement dans un gratte-ciel numÃ©rique gÃ©ant.

Vous n'avez pas besoin d'acheter tout le bÃ¢timent (c'est-Ã -dire des serveurs physiques), vous louez simplement l'espace dont vous avez besoin, quand vous en avez besoin.

### Qu'est-ce qu'une machine virtuelle exactement ?

Une machine virtuelle est un ordinateur basÃ© sur un logiciel qui s'exÃ©cute Ã  l'intÃ©rieur d'un ordinateur rÃ©el et physique (un serveur) â€“ hÃ©bergÃ© dans un centre de donnÃ©es, comme ceux gÃ©rÃ©s par Microsoft Azure.

Elle ressemble et se comporte comme un ordinateur normal :

* Elle a un systÃ¨me d'exploitation (Windows, Linux)

* Vous pouvez installer des applications

* Elle a de la mÃ©moire (RAM), du stockage (disques) et un CPU

Mais le meilleur ? Vous n'avez pas Ã  vous soucier du matÃ©riel. Azure s'en occupe en coulisses â€“ tout ce que vous avez Ã  faire, c'est dire :

> Â« HÃ© Azure, donne-moi une VM Linux avec 4 Go de RAM et 2 CPU. Â»

Et boom ğŸ’¥ â€“ elle se lance en quelques minutes.

### Pourquoi utiliser une VM ?

Disons que vous avez construit une application web â€“ c'est juste un simple blog. Vous voulez la dÃ©ployer et la rendre accessible au monde.

Voici ce que vous pouvez faire avec une VM :

* La configurer avec votre OS prÃ©fÃ©rÃ© (par exemple, Ubuntu)

* Installer des serveurs web comme Nginx ou Apache

* DÃ©ployer votre application

* L'associer Ã  votre nom de domaine

* Laisser le monde visiter votre blog sur [`monbloggenial.com`](http://monbloggenial.com)

C'est votre propre environnement personnel â€“ pas de partage, contrÃ´le total.

## ğŸ“ˆ Le besoin de mise Ã  l'Ã©chelle â€“ Vertical vs Horizontal

Imaginez que votre application grandit. Au dÃ©but, ce ne sont que quelques utilisateurs. Puis quelques centaines. Puis des milliers se connectent, passent des commandes, discutent, tÃ©lÃ©chargent des photos â€“ tout en mÃªme temps ğŸ˜®

Soudain, votre serveur (VM) est sous pression. C'est comme essayer de verser une inondation Ã  travers une paille.

### Alors, que faites-vous lorsqu'un seul serveur ne suffit pas ?

C'est lÃ  que la mise Ã  l'Ã©chelle intervient â€“ l'art de mettre Ã  niveau l'infrastructure de votre application pour suivre le trafic.

Il existe deux faÃ§ons principales de mettre Ã  l'Ã©chelle :

#### ğŸ§‘â€ğŸ³ Option 1 : Mise Ã  l'Ã©chelle verticale (alias Scaling Up)

Vous prenez votre VM existante et lui donnez plus de puissance :

* Ajoutez plus de CPU ğŸ§‘â€ğŸ³

* Augmentez la RAM ğŸ§‘â€ğŸ³

* Ajoutez des disques plus rapides âš¡

Pensez-y comme passer d'une voiture normale Ã  une voiture de sport. C'est le mÃªme vÃ©hicule, juste plus rapide et plus puissant.

**Avantages :**

* Simple Ã  faire

* Aucun changement majeur dans la configuration de votre application

**InconvÃ©nients :**

* Il y a une limite Ã  la quantitÃ© de mise Ã  niveau possible

* Toujours un point de dÃ©faillance unique : si la VM tombe en panne, tout s'arrÃªte ğŸ˜¬

#### ğŸ§‘â€ğŸ³ Option 2 : Mise Ã  l'Ã©chelle horizontale (alias Scaling Out)

Au lieu de booster un seul serveur, vous ajoutez plus de serveurs â€“ plusieurs VM exÃ©cutant des copies de votre application.

Maintenant :

* Les utilisateurs peuvent Ãªtre distribuÃ©s sur toutes ces VM

* Si l'une tombe en panne, les autres continuent de servir le trafic

* Vous pouvez *dynamiquement* ajouter ou supprimer des VM en fonction du trafic

C'est comme ouvrir plus de caisses dans un supermarchÃ© bondÃ© ğŸ›’

**Avantages :**

* La charge est uniformÃ©ment distribuÃ©e. Par exemple, si un serveur gÃ©rait auparavant 100 % du trafic, l'ajout de deux serveurs supplÃ©mentaires entraÃ®nerait une rÃ©partition du trafic d'environ 33 % Ã  34 % pour chaque serveur.

* AmÃ©liore Ã  la fois les performances et la fiabilitÃ©

* Vous pouvez mettre Ã  l'Ã©chelle en fonction de la demande en temps rÃ©el, c'est-Ã -dire l'afflux de trafic

**InconvÃ©nients :**

* NÃ©cessite quelque chose pour rÃ©partir le trafic entre les VM â€“ Ã‰quilibreurs de charge

* Plus coÃ»teux. Vous finissez par payer le montant initial pour 1 VM (par exemple 30 $) pour le nombre de VM que vous fournissez â€“ si vous fournissez 3 VM Ã  30 $ chacune, vous finissez par payer 90 $ Ã  la fin du mois

### Exemple rapide dans la vie rÃ©elle

Disons que vous avez lancÃ© un site de commerce Ã©lectronique pour des baskets ğŸ‘Ÿ Le trafic explose pendant une grande vente ? Votre mise Ã  l'Ã©chelle verticale (VM plus grande) pourrait s'Ã©touffer.

Mais avec la mise Ã  l'Ã©chelle horizontale :

* Vous lancez 5 VM dans diffÃ©rentes rÃ©gions

* Le trafic est partagÃ© entre elles

* Si une VM ralentit, les autres gÃ¨rent la charge

#### Alors, rappelez-vous ğŸ‘‹

| Type de mise Ã  l'Ã©chelle | Description | Avantages | InconvÃ©nients |
| --- | --- | --- | --- |
| ğŸ§‘â€ğŸ³ Mise Ã  l'Ã©chelle verticale | Rendre 1 VM plus puissante (ajout de plus de puissance CPU, SSD, RAM, bande passante, etc.) | Configuration facile, moins de changements | Limites matÃ©rielles, 1 point de dÃ©faillance - Si ce 1 serveur/VM tombe en panne, votre application aussi :( |
| ğŸ§‘â€ğŸ³ Mise Ã  l'Ã©chelle horizontale | Ajouter plus de VM pour gÃ©rer le trafic | Flexible, fiable | NÃ©cessite une logique de distribution de trafic (Ã‰quilibreur de charge). GÃ©nÃ©ralement plus coÃ»teux (le prix de 1 VM multipliÃ© par le nombre de VM) |

## ğŸ”„ Groupes identiques de machines virtuelles Azure (VMSS) â€“ Mise Ã  l'Ã©chelle simplifiÃ©e

D'accord â€“ nous avons donc parlÃ© de la **mise Ã  l'Ã©chelle horizontale** : ajouter plusieurs VM pour gÃ©rer le trafic croissant. Cela semble gÃ©nial, non ?

Mais voici le problÃ¨me : crÃ©er et configurer manuellement 5, 10 ou 100 VM... chaque fois que votre application est occupÃ©e ? Oui, ce n'est pas amusant ğŸ˜³

### EntrÃ©e : Groupes identiques de machines virtuelles (VMSS)

VMSS est la maniÃ¨re d'Azure d'automatiser la mise Ã  l'Ã©chelle horizontale. Au lieu de crÃ©er chaque VM une par une, vous dÃ©finissez un modÃ¨le, et Azure s'occupe du reste :

* Combien de VM dÃ©marrer

* Comment les configurer (OS, applications, paramÃ¨tres) âš™ï¸

* Quand ajouter ou supprimer des VM en fonction du trafic ğŸ“ˆğŸ“‰

### Une analogie simple ğŸ§ƒ

Pensez Ã  VMSS comme Ã  un distributeur de jus lors d'une fÃªte :

* Au dÃ©but, il verse dans 2 verres (VM)

* Si 10 invitÃ©s arrivent ? Il commence Ã  remplir 5 verres

* La fÃªte ralentit ? Retour Ã  2 verres

Vous n'avez jamais Ã  remplir manuellement â€“ le distributeur s'ajuste tout seul. ğŸ‰

### Comment cela fonctionne (sans le jargon ğŸ˜Œ)

1. **Vous dÃ©finissez les rÃ¨gles :** Â« Si l'utilisation du CPU dÃ©passe 70 %, ajoutez 2 VM supplÃ©mentaires. Â»

2. **Azure surveille le trafic et ajuste le nombre de VM** automatiquement.

3. **Toutes les VM sont identiques** â€“ comme des clones, toutes exÃ©cutant la mÃªme configuration d'application.

4. **Il fonctionne avec Azure Load Balancer** pour rÃ©partir le trafic sur toutes ces VM en douceur.

### Exemple concret : Application de livraison de nourriture ğŸ”ğŸ“±

Vous avez construit une application oÃ¹ les utilisateurs commandent de la nourriture. Pendant le dÃ©jeuner et le dÃ®ner, le trafic explose.

ğŸ’¡ Avec VMSS :

* Vous commencez avec 3 VM le matin

* Ã€ 12h, Azure voit une utilisation Ã©levÃ©e du CPU, donc il lance 5 VM supplÃ©mentaires

* Ã€ 15h, le trafic baisse, donc Azure supprime les VM supplÃ©mentaires

Vous ne payez que pour ce que vous utilisez. Et les utilisateurs ont une expÃ©rience fluide â€“ pas de retards, pas de plantages ğŸ‘

![Illustration de mise Ã  l'Ã©chelle automatique](https://cdn.hashnode.com/res/hashnode/image/upload/v1746982520998/7fe3c997-fc8f-418a-861b-e999905ca43c.png align="center")

## ğŸ“§ Azure Load Balancer â€“ RÃ©partition du trafic

Ã€ ce stade, vous savez que votre application peut vivre sur plusieurs machines virtuelles (VM), et que vous pouvez les mettre Ã  l'Ã©chelle facilement en utilisant des groupes identiques de machines virtuelles (VMSS).

Mais voici la grande question : lorsque les utilisateurs commencent Ã  accÃ©der Ã  votre application â€“ des centaines, voire des milliers Ã  la fois â€“ comment vous assurez-vous que tout ce trafic est rÃ©parti Ã©quitablement et efficacement entre ces VM ?

Vous ne voulez pas qu'une VM soit submergÃ©e tandis que d'autres se la coulent douce. Vous avez besoin d'un intermÃ©diaire â€“ quelque chose d'assez intelligent pour Ã©quilibrer la charge.

C'est lÃ  qu'intervient **Azure Load Balancer**. C'est la maniÃ¨re d'Azure de dire : Â« Ne vous inquiÃ©tez pas, je m'en occupe Â» lorsque le trafic commence Ã  affluer.

### ğŸŸï¸ Alors, qu'est-ce qu'Azure Load Balancer ?

Azure Load Balancer est un **directeur de trafic**. Il prend le trafic entrant depuis Internet (ou mÃªme des sources internes au sein de votre rÃ©seau) et le rÃ©partit intelligemment sur plusieurs machines backend â€“ gÃ©nÃ©ralement des VM.

C'est comme avoir une rÃ©ceptionniste bien formÃ©e qui achemine chaque client vers le prochain agent disponible, afin que personne n'attende trop longtemps et que personne ne soit submergÃ© ğŸ˜ƒ.

Et le meilleur ? Tout ce processus se dÃ©roule en arriÃ¨re-plan â€“ rapide, silencieux et transparent. Les utilisateurs visitant votre application ne savent pas qu'un gestionnaire de trafic travaille en coulisses. Ils voient simplement une expÃ©rience rapide et rÃ©active.

### ğŸŒ L'IP Frontend â€“ La face publique de votre application

Chaque Azure Load Balancer est liÃ© Ã  une **IP Frontend**, qui est essentiellement l'adresse IP publique de votre application â€“ celle Ã  laquelle les utilisateurs se connectent lorsqu'ils ouvrent `www.votreapp.com`.

Cette IP agit comme le point d'entrÃ©e. Tout le trafic utilisateur passe d'abord par elle. Mais le Load Balancer n'exÃ©cute pas rÃ©ellement votre application. Au lieu de cela, il accepte le trafic et le transfÃ¨re Ã  l'une des VM du pool backend (nous y viendrons bientÃ´t).

Vous pouvez configurer cette IP Frontend pour qu'elle soit soit publique (accessible via Internet) soit privÃ©e (utilisÃ©e pour le trafic interne au sein de votre rÃ©seau cloud â€“ par exemple, entre microservices ou outils internes).

![Illustration de l'adresse IP Frontend](https://cdn.hashnode.com/res/hashnode/image/upload/v1747055268951/5afbb738-d00d-4f49-9709-2fa1fe7cffdd.png align="center")

### ğŸ–¥ï¸â€ğŸ’» Backend Pool â€“ OÃ¹ la magie opÃ¨re

DerriÃ¨re chaque Azure Load Balancer se trouve un **backend pool** â€“ un groupe de VM (ou d'instances de VM Scale Set) oÃ¹ votre application rÃ©elle est en cours d'exÃ©cution. Ce sont les vrais travailleurs, faisant tout le travail lourd.

Lorsque le trafic atteint l'IP Frontend, le Load Balancer prend cette requÃªte et la transfÃ¨re Ã  l'une des VM du backend pool.

Mais il ne choisit pas simplement au hasard. Il vÃ©rifie d'abord quelques choses â€“ comme si la VM est saine, si elle est dÃ©jÃ  occupÃ©e, et quelles rÃ¨gles vous avez dÃ©finies.

Chaque VM du pool exÃ©cute gÃ©nÃ©ralement la mÃªme application ou service. Cela signifie que n'importe laquelle d'entre elles peut gÃ©rer n'importe quelle requÃªte entrante, ce qui est ce qui rend l'Ã©quilibrage de charge possible en premier lieu.

![Illustration du backend pool](https://cdn.hashnode.com/res/hashnode/image/upload/v1747055337014/e831056d-7c0c-49d9-b05a-6d3dbe3edc76.png align="center")

### ğŸ§‘â€âš•ï¸ Health Probes â€“ Garder un Å“il sur les VM

Maintenant, comment le Load Balancer sait-il quelle VM est saine ou non ? C'est lÃ  que les **health probes** entrent en jeu. Pensez Ã  eux comme des vÃ©rifications rÃ©guliÃ¨res.

Vous configurez le Load Balancer pour qu'il "ping" pÃ©riodiquement chaque VM â€“ peut-Ãªtre en atteignant une URL spÃ©cifique (comme `/health`) ou un certain port (comme 80 pour HTTP). Si une VM ne rÃ©pond pas correctement, Azure la marque comme non saine et la retire temporairement de la rotation.

Cela garantit que les utilisateurs ne sont jamais dirigÃ©s vers une instance cassÃ©e ou non rÃ©active de votre application. Et une fois que la VM redevient saine, elle est automatiquement rÃ©ajoutÃ©e au pool.

### âš™ï¸ Load Balancing Rules â€“ Qui obtient quoi ?

Ensuite, nous avons les **Load Balancing Rules**. Ce sont les instructions qui disent Ã  Azure Load Balancer exactement comment se comporter.

Vous pouvez dÃ©finir des rÃ¨gles comme :

* Â« TransfÃ©rer tout le trafic HTTP (port 80) vers les VM du backend pool sur le port 80 Â»

* Â« TransfÃ©rer le trafic HTTPS (port 443) vers les VM sur le port 443 Â»

* Â« Ne router le trafic que vers les VM saines Â»

Ces rÃ¨gles rendent Azure Load Balancer hautement personnalisable. Vous dÃ©cidez comment le trafic circule, quels protocoles prendre en charge, et comment gÃ©rer les ports backend. C'est comme personnaliser les rÃ¨gles d'une course de relais â€“ qui obtient le tÃ©moin et quand.

### ğŸ‘Ÿ Exemple concret : Rush de vente de baskets

Imaginez que vous gÃ©rez une boutique de baskets en ligne sur `www.sneakerblast.com`. Vous lancez une vente flash, et des milliers d'utilisateurs se ruent sur votre site web en mÃªme temps.

GrÃ¢ce Ã  votre Azure Load Balancer, voici ce qui se passe :

1. Tous ces utilisateurs arrivent sur votre IP Frontend, la face publique de votre site.

2. Le Load Balancer accepte le trafic et vÃ©rifie les health probes de toutes les VM dans le backend pool.

3. En fonction de ses rÃ¨gles, il transfÃ¨re chaque utilisateur vers une VM saine et disponible.

4. Une VM peut servir un utilisateur Ã  Lagos, une autre Ã  Nairobi, une autre Ã  Accra â€“ tout cela de maniÃ¨re transparente.

Si une VM tombe en panne ou ralentit ? Le Load Balancer le dÃ©tecte instantanÃ©ment et arrÃªte de router le trafic vers elle jusqu'Ã  ce qu'elle soit de nouveau en ligne.

C'est une gestion fluide du trafic sans aucun effort manuel.

## ğŸ´ Azure Application Gateway â€“ Routage intelligent pour les applications modernes

Jusqu'Ã  prÃ©sent, nous avons vu comment Azure Load Balancer vous aide Ã  rÃ©partir le trafic sur plusieurs VM exÃ©cutant un seul service â€“ comme une application monolithique ou un frontend web.

Disons que vous avez une application web dÃ©ployÃ©e sur une VM. Elle Ã©coute sur le port 80, et vous l'avez mise Ã  l'Ã©chelle en 3 instances. L'Azure Load Balancer prend les requÃªtes d'Internet et les rÃ©partit sur les 3 instances du mÃªme service. Facile, non ?

Vous pouvez mÃªme lier l'adresse IP publique du Load Balancer Ã  votre domaine â€“ comme `mondomaine.com` â€“ afin que les utilisateurs puissent visiter votre site normalement.

### ğŸ§‘â€ğŸ³ Mais que se passe-t-il si vous avez *plusieurs* services ?

Maintenant, imaginez que vous Ãªtes passÃ© au-delÃ  d'une seule application. Vous construisez quelque chose de plus moderne, comme un ensemble de microservices.

Vous avez maintenant :

* Un service de paiement Ã©coutant sur le port 5000

* Un service d'authentification sur le port 6000

* Un service d'achat sur le port 7000

Tous dÃ©ployÃ©s sur les mÃªmes VM (ou Virtual Machine Scale Set), juste sur diffÃ©rents ports.

Voici le problÃ¨me : un Azure Load Balancer est conÃ§u pour router le trafic vers *un* backend pool â€“ essentiellement un service â€“ sur un port. Si vous le liez Ã  `mondomaine.com`, il ne peut envoyer le trafic que vers l'un de vos microservices. ğŸ˜¬

Alorsâ€¦ que faites-vous ?

Vous pourriez penser : Â« Laissez-moi simplement crÃ©er un Load Balancer sÃ©parÃ© pour chaque service ! Â» ğŸ¤”

Mais cela signifie :

* Vous devrez payer pour plusieurs load balancers

* Vous finirez par gÃ©rer 3-5 adresses IP publiques

* Vous devrez peut-Ãªtre mÃªme acheter plusieurs domaines comme `monpaiement.com`, `monauth.com`, etc. pour router correctement les utilisateurs

AÃ¯e. C'est impratique, dÃ©sordonnÃ©, *et* coÃ»teux ğŸ˜–ğŸ’°

### ğŸ‰ EntrÃ©e : Azure Application Gateway

**Azure Application Gateway** rÃ©sout ce problÃ¨me magnifiquement. Il est conÃ§u pour router le trafic intelligemment â€“ non pas vers un seul service, mais vers plusieurs services en utilisant une seule passerelle.

Voici comment cela fonctionne :

1. Vous crÃ©ez une seule IP frontend publique (comme `52.160.100.5`)

2. Vous liez cette adresse IP Ã  votre domaine principal, par exemple `mondomaine.com`

3. Ensuite, vous dÃ©finissez plusieurs backend pools â€“ un pour chaque service :

   * Service de paiement (port 5000)

   * Service d'auth (port 6000)

   * Service d'achat (port 7000)

4. Ensuite, vous configurez des rÃ¨gles de routage qui dÃ©cident comment transfÃ©rer chaque requÃªte.

### âœ¨ Deux faÃ§ons de router avec Application Gateway

Vous pouvez configurer un **routage intelligent** basÃ© sur :

* **Chemins d'URL** :

   * `mondomaine.com/paiement` â†’ Service de paiement

   * `mondomaine.com/auth` â†’ Service d'auth

* **Sous-domaines** (en-tÃªtes d'hÃ´te) :

   * `paiement.mondomaine.com` â†’ Service de paiement

   * `auth.mondomaine.com` â†’ Service d'auth

De cette faÃ§on, tous vos services partagent une seule IP publique et un seul domaine â€“ super propre, super efficace ğŸ‘

### ğŸ§‘â€ğŸ³ ScÃ©nario rÃ©el (DÃ©cortiquons-le)

Disons que vous construisez une plateforme de startup qui a trois microservices clÃ©s :

* **Service de paiement** qui gÃ¨re les transactions

* **Service d'authentification** qui gÃ¨re la connexion et l'identitÃ© de l'utilisateur

* **Service d'achat** qui gÃ¨re la commande de produits

Chaque service est conteneurisÃ© et dÃ©ployÃ© sur la mÃªme VM (ou sur plusieurs VM Ã  l'aide d'un groupe identique de machines virtuelles). Mais â€“ et c'est la clÃ© â€“ ils Ã©coutent tous sur des **ports diffÃ©rents** Ã  l'intÃ©rieur des VM :

* Paiement â†’ port 3000

* Auth â†’ port 6000

* Achat â†’ port 7000

Maintenant, sans une solution de routage intelligente, vous seriez coincÃ© en essayant d'exposer seulement l'un de ces services en utilisant un Azure Load Balancer standard. Mais vous avez besoin que les trois soient accessibles depuis Internet â€“ et vous ne voulez pas payer pour ou gÃ©rer 3 Load Balancers diffÃ©rents ğŸ˜…

Alors, que faites-vous ?

### ğŸ§‘â€ğŸ³ Utilisation d'Azure Application Gateway pour router le trafic intelligemment

Voici comment vous pouvez rÃ©soudre ce problÃ¨me en utilisant **un** Application Gateway :

1. DÃ©ployez vos microservices Ã  l'intÃ©rieur de chaque VM :

   * Chaque service s'exÃ©cute sur un port spÃ©cifique

   * Toutes les VM de votre groupe identique sont identiques (elles contiennent les trois services)

2. CrÃ©ez des pools backend dans Application Gateway :

   * Un pool backend pour le service de paiement (pointant vers le port 3000 sur toutes les VM)

   * Un pour le service d'auth (port 6000)

   * Un autre pour le service d'achat (port 7000)

3. CrÃ©ez des rÃ¨gles de routage :

   * Option A (Routage basÃ© sur le chemin) :

      * Les requÃªtes vers `mondomaine.com/paiement` â†’ vont au pool backend de paiement

      * Les requÃªtes vers `mondomaine.com/auth` â†’ vont au pool backend d'auth

      * Les requÃªtes vers `mondomaine.com/achat` â†’ vont au pool backend d'achat

   * Option B (Routage basÃ© sur le sous-domaine) :

      * `paiement.mondomaine.com` â†’ service de paiement

      * `auth.mondomaine.com` â†’ service d'auth

      * `achat.mondomaine.com` â†’ service d'achat

Vous dites simplement Ã  l'Application Gateway : Â« HÃ©, si une requÃªte arrive pour cette URL ou ce sous-domaine, envoie-la Ã  ce port sur ces VM. Â» Et il fait exactement cela â€“ de maniÃ¨re cohÃ©rente et intelligente ğŸ”

### ğŸ“§ Alors, que se passe-t-il rÃ©ellement ?

Imaginez qu'un utilisateur visite `mondomaine.com/auth`. Voici ce qui se passe en coulisses :

1. Le DNS traduit `mondomaine.com` en l'IP publique de votre Application Gateway

2. La passerelle reÃ§oit la requÃªte

3. Elle vÃ©rifie vos rÃ¨gles de routage

4. Elle voit que `/auth` doit aller au pool backend pour le port 6000

5. Elle transfÃ¨re la requÃªte Ã  l'une des VM exÃ©cutant le service d'auth

6. La rÃ©ponse revient Ã  l'utilisateur â€“ rapide et transparente âœ¨

Cela se produit en millisecondes, pour chaque requÃªte. Et parce que l'Application Gateway est conscient de plusieurs ports et services, il peut gÃ©rer la logique de routage qu'un Load Balancer ordinaire ne peut tout simplement pas faire.

![Illustration de l'Application Gateway](https://cdn.hashnode.com/res/hashnode/image/upload/v1747056436345/7ea97231-d2ee-4f63-aff1-50595e7c06e0.png align="center")

## ğŸ” Azure Load Balancer vs Azure Application Gateway

Ã€ ce stade, vous avez vu comment les deux outils aident Ã  router le trafic dans Azure â€“ mais ils rÃ©solvent des problÃ¨mes diffÃ©rents.

DÃ©cortiquons comment ils se comparent, et quand vous devriez utiliser l'un plutÃ´t que l'autre ğŸ‘‹

### ğŸ¢ 1. **Logique de routage**

**Azure Load Balancer**  
Il distribue simplement le trafic entrant de maniÃ¨re Ã©gale sur un pool de VM. Il ne se soucie pas *de ce qu'est* la requÃªte â€“ il Ã©quilibre simplement la charge.  
  
Imaginez un livreur qui ne pose pas de questions â€“ il dÃ©pose simplement chaque colis Ã  la prochaine maison disponible.  
  
C'est ce que fait Azure Load Balancer : il envoie le trafic Ã  l'un de vos serveurs sans regarder Ã  l'intÃ©rieur de la requÃªte.

**Azure Application Gateway**  
C'est le plus intelligent. Il regarde *ce qu'il y a Ã  l'intÃ©rieur* de chaque requÃªte (comme le chemin de l'URL ou le domaine) et prend des dÃ©cisions intelligentes.

Comme un livreur plus intelligent qui regarde l'adresse et dÃ©cide oÃ¹ aller : "Oh ! Celui-ci est pour le bureau des paiements, pas pour le bureau principal."  
  
C'est ce que fait Application Gateway : il lit la requÃªte (comme l'URL ou le nom de domaine) et l'envoie au bon endroit selon les rÃ¨gles de routage.

### ğŸŒ 2. **Protocoles gÃ©rÃ©s**

**Load Balancer**  
Fonctionne au niveau de la couche transport (couche 4 dans le modÃ¨le OSI). Il gÃ¨re le trafic TCP/UDP â€“ le trafic rÃ©seau brut, comme HTTP, le streaming vidÃ©o, les jeux, etc.

**Application Gateway**  
Fonctionne au niveau de la couche application (couche 7). Il gÃ¨re uniquement le trafic web â€“ comme les sites web et les applications (HTTP/HTTPS) â€“ et il peut effectivement lire ce qui est demandÃ©, comme :

* "Allez Ã  /login"

* "Allez Ã  [payment.mydomain.com](http://payment.mydomain.com)".

TL;DR : Load Balancer pousse simplement des paquets. App Gateway *lit* rÃ©ellement vos requÃªtes web.

### ğŸ”§ 3. **ScÃ©narios de cas d'usage**

| Situation | Meilleur choix |
| --- | --- |
| Vous avez une grande application et vous voulez simplement rÃ©partir les utilisateurs sur les serveurs | âœ… Load Balancer |
| Vous avez plusieurs services (comme login, paiement, etc.) et vous devez envoyer les utilisateurs au bon | âœ… Application Gateway |
| Vous voulez utiliser des sous-domaines (comme [login.mysite.com](http://login.mysite.com)) | âœ… Application Gateway |
| Vous voulez sÃ©curiser votre site web avec HTTPS et un pare-feu d'application web (WAF) | âœ… Application Gateway |
| Vous voulez la configuration la plus simple et le coÃ»t le plus bas | âœ… Load Balancer |

### ğŸ”’ 4. **Terminaison SSL et fonctionnalitÃ©s de sÃ©curitÃ©**

**Load Balancer** ne gÃ¨re pas les trucs de sÃ©curitÃ©. Vous devrez sÃ©curiser chaque serveur vous-mÃªme (par exemple, configurer HTTPS sur chacun).

**Application Gateway** peut tout sÃ©curiser en un seul endroit â€“ vous tÃ©lÃ©chargez votre certificat SSL une fois et il gÃ¨re HTTPS pour tous les services.

Il peut Ã©galement vous protÃ©ger des pirates et du mauvais trafic avec quelque chose appelÃ© **WAF (Web Application Firewall)**, qui protÃ¨ge votre application contre les menaces comme l'injection SQL, XSS, etc. (vous devez configurer cela manuellement).

### ğŸ’° 5. **Tarification et complexitÃ©**

**Load Balancer** est moins cher et plus facile Ã  configurer. IdÃ©al lorsque vous n'avez pas besoin de fonctionnalitÃ©s sophistiquÃ©es.

**Application Gateway** coÃ»te plus cher, mais vous donne plus de contrÃ´le et moins de maux de tÃªte lorsque vous travaillez avec des applications complexes et des microservices.

Essayer d'utiliser Load Balancer pour plusieurs services ? Vous devrez crÃ©er un Load Balancer par service, ce qui devient coÃ»teux et impratique.

### ğŸ§‘â€ğŸ³ Tableau rÃ©capitulatif

| FonctionnalitÃ© | Load Balancer | Application Gateway |
| --- | --- | --- |
| Peut-il comprendre la requÃªte ? | âŒ Non | âœ… Oui |
| Peut-il router en fonction de l'URL ou du sous-domaine ? | âŒ Non | âœ… Oui |
| Peut-il gÃ©rer le trafic HTTPS sÃ©curisÃ© ? | âŒ Non | âœ… Oui |
| Est-il bon pour les applications simples ? | âœ… Oui | âœ… Oui |
| Est-il bon pour les applications complexes avec de nombreux services ? | âŒ Non | âœ… Oui |
| CoÃ»t | ğŸ’² Plus bas | ğŸ’° Plus Ã©levÃ© |

## ğŸ§‘â€ğŸ³ Cas d'usage : Quand utiliser chacun

Il n'y a pas de solution universelle lorsqu'il s'agit d'hÃ©berger des applications dans le cloud. La bonne configuration dÃ©pend de ce que vous construisez, de la quantitÃ© de trafic que vous attendez et de la complexitÃ© de votre application.

Passons en revue 4 scÃ©narios de cas d'usage diffÃ©rents, en commenÃ§ant par la configuration la plus basique jusqu'Ã  une architecture entiÃ¨rement auto-scalÃ©e et routÃ©e intelligemment.

### 1ï¸âƒ£ **Instance unique de VM â€“ Pour les petits projets ou outils internes**

**Utilisez ceci lorsque :**  
Vous commencez tout juste. Vous avez construit une petite application â€“ peut-Ãªtre un portfolio, un blog ou un projet secondaire â€“ et vous voulez la rendre accessible, OU Vous Ãªtes une startup qui vient de lancer.

**Comment cela fonctionne :**  
Vous lancez une seule VM Azure, installez votre application dessus et ouvrez le port sur lequel elle Ã©coute (par exemple, le port 80 pour un serveur web). Vous pouvez ensuite attacher une IP publique Ã  la VM et la lier Ã  un domaine personnalisÃ© comme `monapplicationgeniale.com`.

**Exemples concrets :**

* Un dÃ©veloppeur hÃ©bergeant un site web de portfolio ou un blog

* Une startup testant un nouveau produit avec seulement quelques utilisateurs

* Un outil interne d'entreprise pour une petite Ã©quipe

**Avantages :**

* Configuration super simple

* CoÃ»t faible

* ContrÃ´le total de votre environnement

**InconvÃ©nients :**

* Si la VM tombe en panne, votre application tombe en panne

* Pas de mise Ã  l'Ã©chelle automatique â€“ les performances peuvent baisser avec les pics de trafic (la seule faÃ§on de s'adapter Ã  l'augmentation de l'utilisation du CPU/mÃ©moire due Ã  l'afflux de trafic est via la mise Ã  l'Ã©chelle verticale manuelle de la VM)

* Vous maintenez et surveillez tout manuellement

### 2ï¸âƒ£ **Mise Ã  l'Ã©chelle horizontale manuelle â€“ Pour les applications avec un trafic moyen et prÃ©visible**

**Utilisez ceci lorsque :**  
Votre application grandit â€“ peut-Ãªtre avez-vous quelques milliers d'utilisateurs maintenant, et les performances comptent. Vous voulez plus d'un serveur pour que votre application ne plante pas pendant les heures de pointe.

**Comment cela fonctionne :**  
Vous crÃ©ez manuellement 2 ou 3 VM Azure avec la mÃªme configuration d'application. Vous ajoutez ensuite un Load Balancer devant pour rÃ©partir le trafic Ã©quitablement entre elles.

**Exemples concrets :**

* Une entreprise avec un portail client

* Un site web scolaire qui gÃ¨re des connexions rÃ©guliÃ¨res, la diffusion de vidÃ©os de cours, etc. pendant les heures de classe

* Une application qui reÃ§oit du trafic principalement pendant la journÃ©e (charge prÃ©visible)

**Avantages :**

* Meilleure performance et disponibilitÃ©

* La charge est partagÃ©e entre plusieurs VM

* Vous pouvez mettre Ã  l'Ã©chelle manuellement si nÃ©cessaire

**InconvÃ©nients :**

* Vous devez ajouter ou supprimer manuellement des VM â€“ ce qui demande des efforts

* Vous devez toujours surveiller les performances manuellement

* Pas d'automatisation ou d'auto-rÃ©paration intÃ©grÃ©e

### 3ï¸âƒ£ **Auto-scaling avec VM Scale Sets + Azure Load Balancer â€“ Pour les applications avec un trafic irrÃ©gulier ou imprÃ©visible**

**Utilisez ceci lorsque :**  
Vous construisez quelque chose de plus sÃ©rieux â€“ le trafic arrive par vagues (par exemple, une application de rÃ©servation de coachs/fitness), et vous ne voulez pas passer votre temps Ã  mettre Ã  l'Ã©chelle des VM toute la journÃ©e. Vous voulez qu'Azure mette automatiquement Ã  l'Ã©chelle votre infrastructure pour vous.

**Comment cela fonctionne :**  
Vous configurez un groupe identique de machines virtuelles (VMSS) qui peut automatiquement crÃ©er plus de VM lorsque c'est nÃ©cessaire (comme pendant un trafic Ã©levÃ©), et les supprimer lorsque les choses se calment ğŸ’°. Un Load Balancer distribue le trafic sur toutes ces VM.

**Exemples concrets :**

* Une plateforme de mÃ©dias oÃ¹ les gens tÃ©lÃ©chargent des vidÃ©os ou des photos

* Un site de shopping qui connaÃ®t des pics pendant les promotions, par exemple les Black Fridays

* Une plateforme de rÃ©servation avec un trafic de pointe en soirÃ©e/week-end

**Avantages :**

* Mise Ã  l'Ã©chelle automatique â€“ Ã©conomise du temps et de l'argent

* Haute disponibilitÃ© : les VM peuvent Ãªtre remplacÃ©es si l'une tombe en panne

* Facile Ã  dÃ©velopper Ã  mesure que votre base d'utilisateurs grandit

**InconvÃ©nients :**

* Fonctionne mieux si votre application est monolithique (un seul grand service)

* Pas de support pour le routage du trafic vers des services spÃ©cifiques â€“ rÃ©partit simplement le trafic sur les VM

* Le Load Balancer ne peut pas regarder les chemins d'URL ou les sous-domaines

### 4ï¸âƒ£ **VM Scale Set + Azure Application Gateway â€“ Pour les microservices ou les applications web complexes**

**Utilisez ceci lorsque :**  
Vous avez une application moderne multi-services â€“ peut-Ãªtre construite avec des microservices. Chaque service (comme les paiements, l'authentification, la recherche, etc.) vit sur un port diffÃ©rent ou mÃªme dans un conteneur.

Vous voulez router le trafic intelligemment â€“ comme `/login` va au service d'auth, `/pay` aux paiements, et `/search` au service de recherche â€“ tout cela sur le mÃªme domaine.

**Comment cela fonctionne :**  
Vous utilisez toujours un groupe identique de machines virtuelles pour l'auto-scaling, mais au lieu d'un Load Balancer basique, vous ajoutez une Application Gateway. Elle peut inspecter chaque requÃªte et l'envoyer au bon service en fonction de choses comme :

* Chemin d'URL (par exemple, `/payments`, `/orders`)

* Sous-domaine (par exemple, `payments.mydomain.com`, `auth.mydomain.com`)

**Exemples concrets :**

* Un produit SaaS complet avec plusieurs services

* Un site de commerce Ã©lectronique avec caisse, compte, commandes et tableaux de bord d'administration

* Une entreprise migrant d'un monolithe vers une configuration de microservices

**Avantages :**

* Routage intelligent basÃ© sur le chemin ou le sous-domaine

* Tout fonctionne sous une seule IP publique et un seul domaine

* Gestion sÃ©curisÃ©e HTTPS + pare-feu d'application web (WAF) optionnel

* Auto-scaling et haute disponibilitÃ©

**InconvÃ©nients :**

* Configuration plus complexe

* CoÃ»t lÃ©gÃ¨rement plus Ã©levÃ© en raison de l'Application Gateway

* NÃ©cessite une planification autour des numÃ©ros de port et des pools backend

### ğŸ§‘â€ğŸ³ Tableau rÃ©capitulatif rapide

| Configuration | Meilleur pour | Mise Ã  l'Ã©chelle | Logique de routage | CoÃ»t | FacilitÃ© |
| --- | --- | --- | --- | --- | --- |
| â˜€ï¸ VM unique | Petits sites, applications personnelles | âŒ (Manuelle) | âŒ Une seule application | ğŸ’² (Le plus bas) | â­â­â­â­ |
| ğŸ§‘â€ğŸ³ Mise Ã  l'Ã©chelle horizontale manuelle + Load Balancer | Applications de taille moyenne, trafic prÃ©visible | âœ… (Manuelle) | âŒ Une seule application | ğŸ’²ğŸ’²ğŸ’² (en raison de plusieurs VM fonctionnant simultanÃ©ment sans rÃ©duction â€“ mÃªme sans trafic) | â­â­ (en raison de la mise Ã  l'Ã©chelle manuelle) |
| ğŸ”„ VMSS + Load Balancer | Applications occupÃ©es, trafic irrÃ©gulier | âœ… (Auto) | âŒ Une seule application | ğŸ’²ğŸ’² | â­â­â­ |
| ğŸ´ VMSS + App Gateway | Microservices, applications modernes | âœ… (Auto) | âœ… Routage intelligent (impliquant plusieurs microservices) | ğŸ’²ğŸ’²ğŸ’²ğŸ’²(Le plus Ã©levÃ©) | â­â­ |

## âœ… Conclusion

Ã€ ce stade, vous Ãªtes passÃ© de simplement entendre les mots Â« Ã©quilibreur de charge Â» ou Â« groupe identique Â» Ã  comprendre exactement comment ils fonctionnent, quand les utiliser et quels problÃ¨mes ils rÃ©solvent. Que vous lanciez une petite application ou que vous mettiez Ã  l'Ã©chelle un service Ã  fort trafic, Azure vous offre des outils flexibles et puissants pour grandir en toute confiance.

Nous avons commencÃ© par le tout dÃ©but â€“ une seule machine virtuelle. C'est simple et idÃ©al pour les petites applications, mais cela devient rapidement un goulot d'Ã©tranglement Ã  mesure que le trafic augmente.

C'est lÃ  que la mise Ã  l'Ã©chelle intervient. Nous avons explorÃ© :

* ğŸ§‘â€ğŸ³ **La mise Ã  l'Ã©chelle verticale** â€“ Mettre Ã  niveau la mÃªme VM (solution rapide, mais limitÃ©e)

* ğŸ§‘â€ğŸ³ **La mise Ã  l'Ã©chelle horizontale** â€“ Ajouter plus de VM pour mieux gÃ©rer le trafic

Ensuite, nous avons introduit les groupes identiques de machines virtuelles Azure (VMSS) â€“ qui donnent vie Ã  la mise Ã  l'Ã©chelle automatique. Plus d'intervention manuelle â€“ Azure peut mettre Ã  l'Ã©chelle vos serveurs en fonction de la demande.

Mais lÃ  oÃ¹ les choses deviennent vraiment intelligentes, c'est avec les Ã©quilibreurs de charge :

* ğŸ“§ **Azure Load Balancer** aide Ã  rÃ©partir le trafic sur vos VM â€“ idÃ©al pour les applications Ã  service unique

* ğŸ´ **Azure Application Gateway** va plus loin en routant les requÃªtes en fonction des chemins d'URL ou des sous-domaines â€“ parfait pour les applications multi-services ou microservices

### ğŸŒ± TL;DR â€“ Que devriez-vous utiliser ?

* **VM unique** : Pour les projets secondaires, les portfolios ou les outils internes

* **Mise Ã  l'Ã©chelle manuelle + Load Balancer** : Pour les applications de taille moyenne avec une charge prÃ©visible

* **VMSS + Load Balancer** : Pour les applications monolithiques avec des besoins de mise Ã  l'Ã©chelle automatique

* **VMSS + Application Gateway** : Inclut Ã©galement la mise Ã  l'Ã©chelle automatique mais pour les microservices ou les besoins de routage intelligent

### ğŸ’¡ RÃ©flexions finales

Les applications cloud grandissent â€“ vite. Et avec la croissance vient la complexitÃ©. Mais avec la bonne configuration Azure, vous pouvez rester une longueur d'avance sur votre trafic, servir les utilisateurs mieux, et garder les coÃ»ts sous contrÃ´le.

Rappelez-vous : vous n'avez pas besoin de commencer grand. Commencez petit, comprenez les schÃ©mas de trafic de votre application, et mettez Ã  l'Ã©chelle seulement lorsque vous en avez besoin. Des outils comme Azure VM Scale Sets, Load Balancer, et Application Gateway vous donnent le contrÃ´le et la puissance de construire des applications modernes et Ã©volutives sans sur-ingÃ©nierie.

Merci d'avoir suivi ce guide approfondi. J'espÃ¨re que cela a rendu les choses plus claires, plus simples, et peut-Ãªtre mÃªme un peu amusantes ğŸ˜Š

## **Ã‰tudier plus loin ğŸ“œ**

Si vous souhaitez en savoir plus sur les machines virtuelles Azure, les groupes identiques, le Load Balancer et l'Application Gateway, vous pouvez consulter les cours ci-dessous :

* [PrÃ©paration Ã  l'examen Microsoft Azure Fundamentals AZ-900](https://www.coursera.org/specializations/microsoft-azure-fundamentals-az900-exam-prep) â€“ Microsoft, Coursera

* [Tutoriel Azure Virtual Machine | CrÃ©ation d'une machine virtuelle dans Azure | Formation Azure | Simplilearn](https://youtu.be/QOv_-xBXkpo?si=kSijmQdev5cQbRKl) â€“ YouTube

* [Groupes identiques de machines virtuelles](https://youtu.be/wN4lRWHUHA0?si=kWBGXhXZTnVgzuEj) â€“ YouTube

* [Azure Load Balancer | Tutoriel Azure Load Balancer | Tout sur le Load Balancer | Edureka](https://youtu.be/VqBGjddK5VY?si=diLGQfuW5i0lxbse) â€“ YouTube

* [PlongÃ©e approfondie dans Azure Application Gateway | ExpliquÃ© Ã©tape par Ã©tape](https://youtu.be/V9EP4jAg4QM?si=t7EqQjw1eNHqOtjK) â€“ YouTube

## **Ã€ propos de l'auteur ğŸ‘¨â€ğŸ’»**

Salut, je suis Prince ! Je suis un ingÃ©nieur DevOps et architecte cloud passionnÃ© par la construction, le dÃ©ploiement et la gestion d'applications Ã©volutives et le partage de connaissances avec la communautÃ© technologique.

Si vous avez aimÃ© cet article, vous pouvez en apprendre plus sur moi en explorant plus de mes blogs et projets sur mon [profil LinkedIn](https://www.linkedin.com/in/prince-onukwili-a82143233/). Vous pouvez trouver mes [articles LinkedIn ici](https://www.linkedin.com/in/prince-onukwili-a82143233/details/publications/). Vous pouvez Ã©galement [visiter mon site web](https://prince-onuk.vercel.app/achievements#articles) pour lire plus de mes articles. Restons en contact et grandissons ensemble ! ğŸ˜Š