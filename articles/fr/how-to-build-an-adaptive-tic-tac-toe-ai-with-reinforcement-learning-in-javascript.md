---
title: Comment cr√©er une IA adaptative de Morpion avec l'apprentissage par renforcement
  en JavaScript
subtitle: ''
author: Mayur Vekariya
co_authors: []
series: null
date: '2025-10-07T20:49:27.122Z'
originalURL: https://freecodecamp.org/news/how-to-build-an-adaptive-tic-tac-toe-ai-with-reinforcement-learning-in-javascript
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1759870150966/f65a07a6-123b-45e2-a3f2-bc099638825a.png
tags:
- name: JavaScript
  slug: javascript
- name: AI
  slug: ai
- name: Reinforcement Learning
  slug: reinforcement-learning
- name: Artificial Intelligence
  slug: artificial-intelligence
seo_title: Comment cr√©er une IA adaptative de Morpion avec l'apprentissage par renforcement
  en JavaScript
seo_desc: Reinforcement learning (RL) is one of the most powerful paradigms in artificial
  intelligence. Unlike supervised learning where you train models on labeled datasets,
  RL agents learn through direct interaction with their environment, receiving rewards
  ...
---

L'apprentissage par renforcement (Reinforcement Learning - RL) est l'un des paradigmes les plus puissants de l'intelligence artificielle. Contrairement √† l'apprentissage supervis√© o√π vous entra√Ænez des mod√®les sur des ensembles de donn√©es √©tiquet√©s, les agents RL apprennent par interaction directe avec leur environnement, recevant des r√©compenses ou des p√©nalit√©s pour leurs actions.

Dans ce tutoriel, vous allez construire une IA de Morpion (Tic-Tac-Toe) qui apprend des strat√©gies optimales gr√¢ce au Q-learning, un algorithme fondamental du RL. Vous impl√©menterez des niveaux de difficult√© adaptatifs, visualiserez le processus d'apprentissage en temps r√©el et explorerez des techniques d'optimisation avanc√©es.

√Ä la fin de ce tutoriel, vous disposerez d'une application web pr√™te pour la production qui d√©montre des concepts pratiques de RL ‚Äì le tout s'ex√©cutant directement dans le navigateur avec du JavaScript pur (vanilla).

## Ce que vous allez apprendre

Dans ce tutoriel, vous apprendrez :

* Les concepts de base de l'apprentissage par renforcement, notamment le Q-learning, l'exploration vs exploitation et le fa√ßonnement des r√©compenses (reward shaping).
    
* Comment impl√©menter un algorithme de Q-learning complet avec gestion d'√©tat.
    
* Des techniques avanc√©es comme la d√©croissance d'epsilon (epsilon decay) et le rejeu d'exp√©rience (experience replay).
    
* Comment construire un jeu interactif avec l'API Canvas HTML5 et des contr√¥les r√©actifs.
    
* L'optimisation des performances pour la prise de d√©cision de l'IA en temps r√©el.
    
* Des techniques de visualisation pour comprendre le processus d'apprentissage de l'IA.
    

## Pr√©requis

Pour tirer le meilleur parti de ce tutoriel, vous devriez avoir :

* Une solide compr√©hension de JavaScript (syntaxe ES6+, classes, m√©thodes de tableau).
    
* Une familiarit√© avec l'API Canvas HTML5 pour le rendu graphique.
    
* Des connaissances de base en algorithmes et structures de donn√©es.
    
* Une compr√©hension du JavaScript asynchrone (Promises, async/await).
    

Vous n'avez besoin d'aucune exp√©rience pr√©alable en machine learning, car j'expliquerai tous les concepts de RL √† partir de z√©ro.

## Table des mati√®res

* [Pourquoi utiliser l'apprentissage par renforcement pour l'IA de jeu ?](#heading-pourquoi-utiliser-l-apprentissage-par-renforcement-pour-l-ia-de-jeu)
    
* [Comprendre le Q-Learning : Les bases](#heading-comprendre-le-q-learning-les-bases)
    
* [Aper√ßu de l'architecture du projet](#heading-apercu-de-l-architecture-du-projet)
    
* [Comment construire l'interface HTML avec Tailwind CSS](#heading-comment-construire-l-interface-html-avec-tailwind-css)
    
* [Comment impl√©menter l'algorithme de Q-Learning](#heading-comment-implementer-l-algorithme-de-q-learning)
    
* [Comprendre les fonctionnalit√©s am√©lior√©es](#heading-comprendre-les-fonctionnalites-ameliorees)
    
* [Comment tester votre impl√©mentation](#heading-comment-tester-votre-implementation)
    
* [Optimisations avanc√©es et extensions](#heading-optimisations-avancees-et-extensions)
    
* [Pi√®ges courants et solutions](#heading-pieges-courants-et-solutions)
    
* [Comment √©tendre ceci √† d'autres jeux](#heading-comment-etendre-ceci-a-d-autres-jeux)
    
* [Conclusion](#heading-conclusion)
    

## Pourquoi utiliser l'apprentissage par renforcement pour l'IA de jeu ?

Les jeux constituent un environnement id√©al pour l'apprentissage du RL car ils poss√®dent :

1. **Des repr√©sentations d'√©tat claires** ‚Äì Le plateau de jeu √† n'importe quel moment.
    
2. **Des espaces d'action discrets** ‚Äì Un ensemble fini de coups valides.
    
3. **Un retour imm√©diat** ‚Äì R√©sultats de victoire, d√©faite ou match nul.
    
4. **Des r√®gles d√©terministes** ‚Äì Comportement coh√©rent d'une partie √† l'autre.
    

L'IA de jeu traditionnelle utilise des techniques comme le minimax avec √©lagage alpha-b√™ta. Bien qu'efficaces, ces approches n√©cessitent de programmer explicitement les strat√©gies de jeu. Les agents RL, en revanche, d√©couvrent les strat√©gies optimales par l'exp√©rience ‚Äì tout comme les humains apprennent par la pratique.

Le Morpion est un excellent point de d√©part car :

* L'espace d'√©tat est g√©rable (5 478 positions uniques).
    
* Les parties sont courtes, permettant une it√©ration rapide.
    
* Un jeu parfait est r√©alisable, offrant une m√©trique de succ√®s claire.
    
* Les concepts sont transposables √† des jeux plus complexes.
    

## Comprendre le Q-Learning : Les bases

Le [Q-learning](https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/) est un algorithme de RL bas√© sur la valeur et sans mod√®le (model-free). Voici ce que cela signifie :

* **Sans mod√®le** signifie que l'agent n'a pas besoin de comprendre les r√®gles du jeu. Il apprend purement par l'exp√©rience.
    
* **Bas√© sur la valeur** signifie que l'agent apprend la ¬´ valeur ¬ª de chaque action dans chaque √©tat, puis choisit l'action ayant la valeur la plus √©lev√©e.
    

### Composants cl√©s

Il y a quelques composants cl√©s que vous devrez comprendre avant de construire ce jeu.

Tout d'abord, nous avons l'**√©tat (s)**, qui est ici la configuration actuelle du plateau de jeu. Nous le repr√©sentons par une cha√Æne de 9 caract√®res (par exemple, `"XO-X-----"` o√π `-` repr√©sente les cellules vides).

Ensuite, nous avons l'**action (a)**, qui est un coup que l'IA peut jouer. Nous le repr√©sentons par un index de 0 √† 8 correspondant aux positions sur le plateau.

Puis il y a la **r√©compense (r)**, le retour num√©rique de l'environnement :

* `+1` pour une victoire
    
* `-1` pour une d√©faite
    
* `0` pour les matchs nuls ou les parties en cours
    

Nous avons √©galement la **Table Q (Q-Table)**, une table de correspondance stockant Q(s,a) ‚Äì la r√©compense cumulative attendue pour l'action `a` dans l'√©tat `s`.

Et enfin, il y a la **politique (policy)**, la strat√©gie de choix des actions. Nous utilisons une politique epsilon-greedy qui √©quilibre l'exploration et l'exploitation.

### La r√®gle de mise √† jour du Q-Learning

Le c≈ìur du Q-learning est cette formule de mise √† jour :

```bash
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
```

O√π :

* `Œ±` (alpha) = Taux d'apprentissage (0 √† 1) ‚Äì de combien mettre √† jour la valeur Q.
    
* `Œ≥` (gamma) = Facteur de remise (0 √† 1) ‚Äì quelle valeur accorder aux r√©compenses futures.
    
* `s'` = √âtat suivant apr√®s avoir effectu√© l'action `a`.
    
* `max Q(s',a')` = Valeur Q la plus √©lev√©e disponible dans l'√©tat suivant.
    

Cette formule impl√©mente l'**apprentissage par diff√©rence temporelle (temporal difference learning)**. Cela signifie qu'elle met √† jour notre estimation de Q(s,a) en fonction de la diff√©rence entre notre estimation actuelle et une meilleure estimation utilisant la r√©compense r√©elle re√ßue plus la meilleure r√©compense future possible.

### Comment fonctionne l'Exploration vs Exploitation

Un d√©fi critique dans l'apprentissage par renforcement est le compromis ¬´ exploration vs exploitation ¬ª. Pour comprendre pourquoi c'est difficile, imaginez que vous choisissiez un endroit pour d√Æner.

* **Exploitation :** Vous pourriez aller √† votre restaurant pr√©f√©r√©. Vous savez que la nourriture est bonne et vous √™tes presque assur√© d'un repas satisfaisant. C'est un choix s√ªr et fiable qui maximise votre r√©compense imm√©diate bas√©e sur l'exp√©rience pass√©e.
    
* **Exploration :** Vous pourriez essayer un nouveau restaurant inconnu. Cela pourrait √™tre un d√©sastre, ou vous pourriez d√©couvrir un nouveau favori encore meilleur que l'ancien. C'est un choix risqu√© qui n'offre aucune garantie imm√©diate, mais c'est le seul moyen de recueillir de nouvelles informations et de trouver potentiellement une meilleure strat√©gie √† long terme.
    

Le m√™me dilemme s'applique √† notre IA. Si elle n'exploite que ses connaissances actuelles, elle pourrait rester bloqu√©e sur une strat√©gie m√©diocre, sans jamais d√©couvrir les coups brillants qui m√®nent √† une victoire garantie. Si elle ne fait qu'explorer en effectuant des mouvements al√©atoires, elle n'apprendra jamais √† utiliser les bonnes strat√©gies qu'elle trouve et jouera mal.

La cl√© est d'√©quilibrer les deux : explorer suffisamment pour trouver des strat√©gies optimales, mais exploiter ces connaissances pour gagner des parties.

Pour atteindre cet √©quilibre, nous utilisons une **strat√©gie epsilon-greedy (œµ)**. C'est un moyen simple mais puissant de g√©rer ce compromis :

1. Nous choisissons une petite valeur pour epsilon (œµ), par exemple 0,1 (ce qui repr√©sente une probabilit√© de 10 %).
    
2. Avant que l'IA ne joue, elle g√©n√®re un nombre al√©atoire entre 0 et 1.
    
3. **Si le nombre al√©atoire est inf√©rieur √† œµ (la chance de 10 %) :** L'IA ignore sa strat√©gie et choisit un coup al√©atoire parmi ceux disponibles. C'est l'**exploration**.
    
4. **Si le nombre al√©atoire est sup√©rieur ou √©gal √† œµ (la chance de 90 %) :** L'IA choisit le meilleur coup connu dans sa table Q. C'est l'**exploitation**.
    

Cela garantit que l'IA joue principalement pour gagner, tout en consacrant une petite fraction de ses coups √† essayer de nouvelles choses. Nous impl√©menterons √©galement la **d√©croissance d'epsilon (epsilon decay)** ‚Äì en commen√ßant par une valeur œµ plus √©lev√©e pour encourager l'exploration lorsque l'IA est inexp√©riment√©e, et en la diminuant progressivement √† mesure que l'IA apprend et devient plus confiante dans sa strat√©gie.

## Aper√ßu de l'architecture du projet

Avant de commencer √† coder, voici la structure de l'application que vous allez construire :

```bash
tic-tac-toe-ai/
‚îú‚îÄ‚îÄ index.html          # Interface du jeu avec Tailwind CSS
‚îî‚îÄ‚îÄ game.js            # Logique compl√®te du jeu et de l'IA
```

Vous organiserez votre code en deux classes principales dans game.js :

1. **QLearning** : Impl√©mente l'algorithme de Q-learning.
    
2. **TicTacToe** : G√®re l'√©tat du jeu et le rendu.
    

## Comment construire l'interface HTML avec Tailwind CSS

Cr√©ez un fichier `index.html` avec le CDN Tailwind CSS :

```xml
<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>IA de Morpion avec Q-Learning</title>
  <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gradient-to-br from-purple-600 to-purple-900 min-h-screen flex items-center justify-center p-4">
  
  <div class="bg-white rounded-3xl shadow-2xl p-8 max-w-5xl w-full">
    <!-- Header -->
    <div class="text-center mb-8">
      <h1 class="text-4xl font-bold text-gray-800 mb-2">üéÆ IA de Morpion</h1>
      <p class="text-gray-600 text-lg">Regardez l'IA apprendre via l'apprentissage par renforcement</p>
    </div>

    <!-- Training Indicator -->
    <div id="trainingIndicator" class="hidden bg-yellow-100 border-l-4 border-yellow-500 text-yellow-700 p-4 mb-6 rounded">
      <p class="font-semibold">ü§ñ L'IA s'entra√Æne... <span id="trainingProgress"></span></p>
    </div>

    <!-- Main Game Area -->
    <div class="grid md:grid-cols-2 gap-8">
      
      <!-- Canvas Section -->
      <div class="flex flex-col items-center">
        <canvas id="gameCanvas" width="400" height="400" 
                class="border-4 border-purple-500 rounded-xl shadow-lg cursor-pointer hover:scale-[1.02] transition-transform">
        </canvas>
        <div id="gameStatus" class="mt-4 text-xl font-bold text-gray-700 min-h-[30px]">
          √Ä vous de jouer ! (X)
        </div>
      </div>

      <!-- Controls Section -->
      <div class="space-y-6">
        
        <!-- Game Controls -->
        <div class="bg-gray-50 rounded-xl p-6">
          <h3 class="text-xl font-bold text-gray-800 mb-4">Contr√¥les du jeu</h3>
          <div class="space-y-3">
            <button onclick="game.reset()" 
                    class="w-full bg-purple-600 hover:bg-purple-700 text-white font-semibold py-3 px-6 rounded-lg transition-all hover:-translate-y-0.5 shadow-md hover:shadow-lg">
              Nouvelle partie
            </button>
            <button onclick="game.startTraining()" 
                    class="w-full bg-green-600 hover:bg-green-700 text-white font-semibold py-3 px-6 rounded-lg transition-all hover:-translate-y-0.5 shadow-md hover:shadow-lg">
              Entra√Æner l'IA (1000 parties)
            </button>
            <button onclick="game.resetAI()" 
                    class="w-full bg-red-600 hover:bg-red-700 text-white font-semibold py-3 px-6 rounded-lg transition-all hover:-translate-y-0.5 shadow-md hover:shadow-lg">
              R√©initialiser la m√©moire de l'IA
            </button>
          </div>
        </div>

        <!-- Difficulty Selector -->
        <div class="bg-gray-50 rounded-xl p-6">
          <h3 class="text-xl font-bold text-gray-800 mb-4">Niveau de difficult√©</h3>
          <div class="grid grid-cols-3 gap-2">
            <button onclick="game.setDifficulty('beginner')" id="diffBeginner"
                    class="py-2 px-4 rounded-lg font-semibold text-sm transition-all bg-green-100 text-green-700 hover:bg-green-200">
              üå± D√©butant
            </button>
            <button onclick="game.setDifficulty('intermediate')" id="diffIntermediate"
                    class="py-2 px-4 rounded-lg font-semibold text-sm transition-all bg-white text-gray-700 hover:bg-gray-100 border-2 border-purple-500">
              üéØ Moyen
            </button>
            <button onclick="game.setDifficulty('expert')" id="diffExpert"
                    class="py-2 px-4 rounded-lg font-semibold text-sm transition-all bg-white text-gray-700 hover:bg-gray-100">
              üî• Expert
            </button>
          </div>
        </div>

        <!-- AI Parameters -->
        <div class="bg-gray-50 rounded-xl p-6">
          <h3 class="text-xl font-bold text-gray-800 mb-4">Param√®tres de l'IA</h3>
          
          <div class="space-y-4">
            <!-- Learning Rate -->
            <div>
              <div class="flex justify-between items-center mb-2">
                <label class="text-sm font-medium text-gray-700 flex items-center gap-1">
                  Taux d'apprentissage (Œ±)
                  <span class="group relative">
                    <span class="cursor-help text-purple-500">‚ìò</span>
                    <span class="invisible group-hover:visible absolute left-0 top-6 w-64 bg-gray-900 text-white text-xs rounded-lg p-3 z-10 shadow-xl">
                      Contr√¥le la rapidit√© avec laquelle l'IA met √† jour ses connaissances. Valeurs √©lev√©es = apprentissage plus rapide mais moins de stabilit√©. Recommand√© : 0.1-0.3
                    </span>
                  </span>
                </label>
                <span id="learningRateValue" class="text-sm font-bold text-purple-600">0.1</span>
              </div>
              <input type="range" id="learningRate" min="0.01" max="0.5" step="0.01" value="0.1"
                     class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer">
            </div>

            <!-- Discount Factor -->
            <div>
              <div class="flex justify-between items-center mb-2">
                <label class="text-sm font-medium text-gray-700 flex items-center gap-1">
                  Facteur de remise (Œ≥)
                  <span class="group relative">
                    <span class="cursor-help text-purple-500">‚ìò</span>
                    <span class="invisible group-hover:visible absolute left-0 top-6 w-64 bg-gray-900 text-white text-xs rounded-lg p-3 z-10 shadow-xl">
                      D√©termine l'importance que l'IA accorde aux r√©compenses futures par rapport aux r√©compenses imm√©diates. √âlev√© = vision √† plus long terme. Recommand√© : 0.85-0.95
                    </span>
                  </span>
                </label>
                <span id="discountFactorValue" class="text-sm font-bold text-purple-600">0.9</span>
              </div>
              <input type="range" id="discountFactor" min="0.5" max="0.99" step="0.01" value="0.9"
                     class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer">
            </div>

            <!-- Exploration Rate -->
            <div>
              <div class="flex justify-between items-center mb-2">
                <label class="text-sm font-medium text-gray-700 flex items-center gap-1">
                  Taux d'exploration (Œµ)
                  <span class="group relative">
                    <span class="cursor-help text-purple-500">‚ìò</span>
                    <span class="invisible group-hover:visible absolute left-0 top-6 w-64 bg-gray-900 text-white text-xs rounded-lg p-3 z-10 shadow-xl">
                      Probabilit√© que l'IA tente des coups al√©atoires plut√¥t que d'utiliser sa strat√©gie apprise. √âlev√© = plus d'exp√©rimentation. R√©glez sur 0.01 pour un jeu optimal apr√®s l'entra√Ænement.
                    </span>
                  </span>
                </label>
                <span id="explorationRateValue" class="text-sm font-bold text-purple-600">0.1</span>
              </div>
              <input type="range" id="explorationRate" min="0" max="0.5" step="0.01" value="0.1"
                     class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer">
            </div>
          </div>
        </div>

        <!-- Statistics -->
        <div class="bg-gray-50 rounded-xl p-6">
          <h3 class="text-xl font-bold text-gray-800 mb-4">Statistiques</h3>
          <div class="grid grid-cols-3 gap-3">
            <div class="bg-white rounded-lg p-3 text-center shadow-sm">
              <div class="text-xs text-gray-600 mb-1">Parties</div>
              <div id="gamesPlayed" class="text-2xl font-bold text-gray-800">0</div>
            </div>
            <div class="bg-white rounded-lg p-3 text-center shadow-sm">
              <div class="text-xs text-gray-600 mb-1">IA Gagne</div>
              <div id="aiWins" class="text-2xl font-bold text-green-600">0</div>
            </div>
            <div class="bg-white rounded-lg p-3 text-center shadow-sm">
              <div class="text-xs text-gray-600 mb-1">Vous Gagnez</div>
              <div id="playerWins" class="text-2xl font-bold text-red-600">0</div>
            </div>
            <div class="bg-white rounded-lg p-3 text-center shadow-sm">
              <div class="text-xs text-gray-600 mb-1">Nuls</div>
              <div id="draws" class="text-2xl font-bold text-gray-600">0</div>
            </div>
            <div class="bg-white rounded-lg p-3 text-center shadow-sm">
              <div class="text-xs text-gray-600 mb-1">√âtats appris</div>
              <div id="statesLearned" class="text-2xl font-bold text-purple-600">0</div>
            </div>
            <div class="bg-white rounded-lg p-3 text-center shadow-sm">
              <div class="text-xs text-gray-600 mb-1">Taux de vict.</div>
              <div id="winRate" class="text-2xl font-bold text-blue-600">0%</div>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>

  <script src="game.js"></script>
</body>
</html>
```

Cette structure HTML cr√©e une interface moderne et r√©active en utilisant les classes utilitaires de Tailwind CSS. La mise en page utilise une grille √† deux colonnes sur les √©crans moyens et plus grands, avec le canvas du jeu √† gauche et tous les contr√¥les √† droite. L'indicateur d'entra√Ænement est initialement masqu√© et n'appara√Æt que pendant les sessions d'entra√Ænement de l'IA.

Tous les √©l√©ments interactifs (boutons, curseurs) utilisent des gestionnaires `onclick` et des √©v√©nements `oninput` pour communiquer avec la logique du jeu en JavaScript. Le syst√®me d'infobulles utilise les √©tats de survol de groupe CSS pour afficher du texte explicatif lorsque les utilisateurs survolent les ic√¥nes d'information, les aidant √† comprendre chaque param√®tre sans encombrer l'interface.

Parlons un peu plus en d√©tail de certaines parties cl√©s du code :

* **Section d'en-t√™te** : Affiche le titre et le sous-titre du jeu pour pr√©senter l'application aux utilisateurs.
    
* **Indicateur d'entra√Ænement** : Une banni√®re jaune qui n'appara√Æt que pendant les sessions d'entra√Ænement de l'IA, affichant les mises √† jour de progression toutes les 50 parties. Cela fournit un retour visuel pour que les utilisateurs sachent que l'entra√Ænement est en cours.
    
* **Section Canvas** : Contient l'√©l√©ment Canvas HTML5 o√π le plateau de jeu est dessin√©. Le canvas fait 400x400 pixels et est stylis√© avec des classes Tailwind pour les bordures et les effets de survol. En dessous se trouve un message d'√©tat qui se met √† jour en fonction de l'√©tat du jeu.
    
* **Contr√¥les du jeu** : Trois boutons principaux qui permettent aux utilisateurs de commencer une nouvelle partie, d'entra√Æner l'IA via 1000 parties en auto-jeu, ou de r√©initialiser compl√®tement la m√©moire de l'IA (effacement de la table Q).
    
* **S√©lecteur de difficult√©** : Trois boutons pour choisir la difficult√© de l'IA. Le mode D√©butant fait jouer l'IA de mani√®re al√©atoire 70 % du temps, le mode Moyen utilise le Q-learning, et le mode Expert impl√©mente un jeu minimax parfait.
    
* **Param√®tres de l'IA** : Trois curseurs de plage avec des infobulles qui permettent aux utilisateurs d'ajuster les hyperparam√®tres de base de l'apprentissage par renforcement en temps r√©el.
    
* **Panneau de statistiques** : Une grille de six cartes affichant des m√©triques en temps r√©el, notamment les parties jou√©es, les victoires/d√©faites/nuls, les √©tats appris et le pourcentage de victoires de l'IA.

## Comment impl√©menter l'algorithme de Q-Learning

Maintenant, donnons vie √† la th√©orie. Cr√©ez un fichier `game.js`. Nous allons construire ce fichier √©tape par √©tape, mais si vous √™tes bloqu√© √† un moment donn√© ou si vous voulez voir le code complet pour r√©f√©rence, vous pouvez trouver la version finale [sur **GitHub** ici](https://github.com/mayur9210/tic-tac-toe-ai/blob/main/game.js).

Notre code sera structur√© en deux classes principales : `QLearning`, qui g√©rera le ¬´ cerveau ¬ª de l'IA et la logique d'apprentissage, et `TicTacToe`, qui g√©rera l'√©tat du jeu, le rendu et l'interaction avec l'utilisateur.

### La classe `QLearning` : Le cerveau de l'IA

Cette classe contiendra toute la logique de l'[agent d'apprentissage par renforcement](https://github.com/mayur9210/tic-tac-toe-ai/blob/main/game.js). Construisons-la morceau par morceau.

#### 1. Constructeur et gestion de la Table Q

Tout d'abord, configurons le `constructor` et une m√©thode pour acc√©der √† notre table Q. La table Q sera un `Map` JavaScript, ce qui est tr√®s efficace pour stocker et r√©cup√©rer des paires cl√©-valeur o√π la cl√© (l'√©tat du plateau) est une cha√Æne de caract√®res.

```javascript
// Dans game.js

// Agent Q-Learning avec support localStorage
class QLearning {
  constructor(lr = 0.1, gamma = 0.9, epsilon = 0.1) {
    this.q = new Map(); // Stocke les valeurs Q : { √©tat => [q_action_0, q_action_1, ...] }
    this.lr = lr; // Taux d'apprentissage (Œ±)
    this.gamma = gamma; // Facteur de remise (Œ≥)
    this.epsilon = epsilon; // Taux d'exploration (Œµ)
    this.difficulty = 'intermediate';
  }

  getQ(state) {
    if (!this.q.has(state)) {
      this.q.set(state, Array(9).fill(0));
    }
    return this.q.get(state);
  }
```

* Le `constructor` initialise nos trois hyperparam√®tres cl√©s (Œ±, Œ≥, Œµ) et la table Q elle-m√™me.
    
* `getQ(state)` est une fonction utilitaire cruciale. Elle r√©cup√®re en toute s√©curit√© le tableau des valeurs Q pour un √©tat de plateau donn√©. Si l'IA n'a jamais vu cet √©tat auparavant, elle cr√©e une nouvelle entr√©e dans la map avec un tableau de neuf z√©ros, repr√©sentant une valeur Q initiale de 0 pour chaque coup possible.
    

#### 2. Choisir une action (La strat√©gie Epsilon-Greedy)

Ensuite, nous allons impl√©menter la m√©thode `getAction`. C'est ici que l'IA d√©cide quel coup jouer, en int√©grant nos niveaux de difficult√© et la strat√©gie epsilon-greedy.

```javascript
  getAction(state, available) {
    // Comportement bas√© sur la difficult√©
    if (this.difficulty === 'beginner') {
      // 70% de coups al√©atoires pour le d√©butant
      if (Math.random() < 0.7) {
        return available[~~(Math.random() * available.length)];
      }
    } else if (this.difficulty === 'expert') {
      // Utilise minimax pour un jeu parfait
      return this.getMinimaxAction(state, available);
    }

    // Interm√©diaire : epsilon-greedy
    if (Math.random() < this.epsilon) {
      return available[~~(Math.random() * available.length)];
    }
    const q = this.getQ(state);
    return available.reduce((best, a) => q[a] > q[best] ? a : best, available[0]);
  }
```

* La logique v√©rifie d'abord la difficult√©. 'Beginner' est principalement al√©atoire, tandis qu' 'Expert' s'en remet √† un algorithme de jeu parfait s√©par√©.
    
* Pour le niveau 'Intermediate', elle impl√©mente la logique epsilon-greedy. Avec une probabilit√© Œµ, elle explore (choisit un coup al√©atoire). Sinon, elle exploite (choisit le meilleur coup connu dans la table Q).
    

#### 3. La r√®gle d'apprentissage

La m√©thode `update` est le c≈ìur de l'algorithme. C'est l'impl√©mentation directe de la formule de Q-learning dont nous avons discut√© plus t√¥t.

*Q(s, a) ‚Üê Q(s, a) + Œ± [r + Œ≥ max(a') Q(s', a') ‚àí Q(s, a)]*

```javascript
  update(s, a, r, s2, available2) {
    const q = this.getQ(s);
    const maxQ2 = available2.length ? Math.max(...available2.map(a_prime => this.getQ(s2)[a_prime])) : 0;
    q[a] += this.lr * (r + this.gamma * maxQ2 - q[a]);
  }
```

* `maxQ2` calcule la partie `max Q(s',a')` de la formule ‚Äì la meilleure valeur Q possible que l'IA peut obtenir lors de son prochain coup.
    
* La derni√®re ligne est une traduction directe de la formule, mettant √† jour la valeur de l'action qui vient d'√™tre effectu√©e en fonction de la r√©compense et du potentiel futur.
    

#### 4. Minimax pour le mode Expert

Pour notre niveau 'Expert', nous allons impl√©menter l'algorithme minimax, un algorithme r√©cursif classique de la th√©orie des jeux qui garantit un jeu parfait.

```javascript
  getMinimaxAction(state, available) {
    let bestScore = -Infinity;
    let bestMove = available[0];

    for (const move of available) {
      const newState = state.substring(0, move) + 'O' + state.substring(move + 1);
      const score = this.minimax(newState, 0, false);
      if (score > bestScore) {
        bestScore = score;
        bestMove = move;
      }
    }
    return bestMove;
  }

  minimax(state, depth, isMaximizing) {
    const winner = this.checkWinnerStatic(state);
    if (winner === 'O') return 10 - depth;
    if (winner === 'X') return depth - 10;
    if (winner === 'draw') return 0;

    const available = [...state].map((c, i) => c === '-' ? i : null).filter(x => x !== null);
    
    if (isMaximizing) {
      let best = -Infinity;
      for (const move of available) {
        const newState = state.substring(0, move) + 'O' + state.substring(move + 1);
        best = Math.max(best, this.minimax(newState, depth + 1, false));
      }
      return best;
    } else {
      let best = Infinity;
      for (const move of available) {
        const newState = state.substring(0, move) + 'X' + state.substring(move + 1);
        best = Math.min(best, this.minimax(newState, depth + 1, true));
      }
      return best;
    }
  }

  checkWinnerStatic(state) {
    const patterns = [[0,1,2],[3,4,5],[6,7,8],[0,3,6],[1,4,7],[2,5,8],[0,4,8],[2,4,6]];
    for (const p of patterns) {
      if (state[p[0]] !== '-' && state[p[0]] === state[p[1]] && state[p[1]] === state[p[2]]) {
        return state[p[0]];
      }
    }
    return state.includes('-') ? null : 'draw';
  }
```

#### 5. M√©thodes utilitaires et de persistance

Enfin, ajoutons des m√©thodes pour la d√©croissance d'epsilon, la r√©initialisation de la m√©moire de l'IA et la sauvegarde/chargement de la table Q dans le `localStorage`.

```javascript
  decay() {
    this.epsilon = Math.max(0.01, this.epsilon * 0.995);
  }

  reset() {
    this.q.clear();
    this.epsilon = 0.1;
  }

  save() {
    const data = {
      q: Array.from(this.q.entries()),
      lr: this.lr,
      gamma: this.gamma,
      epsilon: this.epsilon,
      difficulty: this.difficulty
    };
    localStorage.setItem('tictactoe_ai', JSON.stringify(data));
  }

  load() {
    const saved = localStorage.getItem('tictactoe_ai');
    if (!saved) return false;
    
    try {
      const data = JSON.parse(saved);
      this.q = new Map(data.q);
      this.lr = data.lr;
      this.gamma = data.gamma;
      this.epsilon = data.epsilon;
      this.difficulty = data.difficulty || 'intermediate';
      return true;
    } catch (e) {
      console.error('√âchec du chargement de l\'√©tat de l\'IA :', e);
      return false;
    }
  }

  clearStorage() {
    localStorage.removeItem('tictactoe_ai');
  }
}
```

### La classe `TicTacToe` : G√©rer le jeu

Maintenant que nous avons notre ¬´ cerveau ¬ª d'IA, nous devons construire le jeu autour. Cette classe g√©rera le rendu du plateau, le traitement des clics de l'utilisateur, le flux du jeu et l'appel de l'IA quand c'est son tour.

#### 1. Constructeur et initialisation des contr√¥les

Le constructeur configure l'√©tat initial du jeu, obtient une r√©f√©rence au canvas HTML et connecte les √©couteurs d'√©v√©nements pour les entr√©es utilisateur.

```javascript
class TicTacToe {
  constructor() {
    this.board = '---------';
    this.ai = new QLearning();
    this.stats = { played: 0, aiWins: 0, playerWins: 0, draws: 0 };
    this.training = false;
    this.gameOver = false;

    this.canvas = document.getElementById('gameCanvas');
    this.ctx = this.canvas.getContext('2d');
    this.cellSize = 133.33;

    this.canvas.onclick = e => this.handleClick(e);
    this.initControls();
    this.loadState();
    this.draw();
  }

  initControls() {
    ['learningRate', 'discountFactor', 'explorationRate'].forEach(id => {
      const el = document.getElementById(id);
      el.oninput = e => {
        const val = parseFloat(e.target.value);
        document.getElementById(id + 'Value').textContent = val.toFixed(2);
        if (id === 'learningRate') this.ai.lr = val;
        if (id === 'discountFactor') this.ai.gamma = val;
        if (id === 'explorationRate') this.ai.epsilon = val;
        this.saveState();
      };
    });
  }
```

`initControls` connecte nos curseurs HTML aux param√®tres de l'IA, permettant des ajustements en temps r√©el.

#### 2. M√©thodes de difficult√© et d'interface utilisateur

Ces m√©thodes g√®rent le r√©glage de la difficult√© et mettent √† jour l'interface utilisateur en cons√©quence.

```javascript
  setDifficulty(level) {
    this.ai.difficulty = level;
    
    // Mise √† jour des styles de boutons
    ['beginner', 'intermediate', 'expert'].forEach(diff => {
      const btn = document.getElementById(`diff${diff.charAt(0).toUpperCase() + diff.slice(1)}`);
      if (diff === level) {
        btn.className = 'py-2 px-4 rounded-lg font-semibold text-sm transition-all bg-purple-600 text-white border-2 border-purple-600';
      } else {
        btn.className = 'py-2 px-4 rounded-lg font-semibold text-sm transition-all bg-white text-gray-700 hover:bg-gray-100';
      }
    });

    if (level === 'beginner') this.setStatus('üå± Mode D√©butant : l\'IA fait plus d\'erreurs');
    else if (level === 'intermediate') this.setStatus('üéØ Mode Moyen : IA √©quilibr√©e utilisant le Q-learning');
    else this.setStatus('üî• Mode Expert : IA parfaite utilisant l\'algorithme minimax');

    this.saveState();
  }
```

#### 3. Dessin et rendu

Ces m√©thodes utilisent l'API Canvas HTML5 pour repr√©senter visuellement l'√©tat du jeu.

```javascript
  draw() {
    const { ctx, canvas, cellSize } = this;
    ctx.fillStyle = '#fff';
    ctx.fillRect(0, 0, canvas.width, canvas.height);

    ctx.strokeStyle = '#8b5cf6';
    ctx.lineWidth = 4;
    for (let i = 1; i < 3; i++) {
      ctx.beginPath();
      ctx.moveTo(i * cellSize, 0);
      ctx.lineTo(i * cellSize, canvas.height);
      ctx.stroke();
      ctx.beginPath();
      ctx.moveTo(0, i * cellSize);
      ctx.lineTo(canvas.width, i * cellSize);
      ctx.stroke();
    }

    for (let i = 0; i < 9; i++) {
      const symbol = this.board[i];
      if (symbol === '-') continue;
      
      const x = (i % 3) * cellSize + cellSize / 2;
      const y = ~~(i / 3) * cellSize + cellSize / 2;
      
      ctx.strokeStyle = symbol === 'X' ? '#ef4444' : '#10b981';
      ctx.lineWidth = 8;
      ctx.lineCap = 'round';

      if (symbol === 'X') {
        const s = cellSize * 0.3;
        ctx.beginPath();
        ctx.moveTo(x - s, y - s);
        ctx.lineTo(x + s, y + s);
        ctx.stroke();
        ctx.beginPath();
        ctx.moveTo(x + s, y - s);
        ctx.lineTo(x - s, y + s);
        ctx.stroke();
      } else {
        ctx.beginPath();
        ctx.arc(x, y, cellSize * 0.3, 0, Math.PI * 2);
        ctx.stroke();
      }
    }

    const winner = this.checkWinner();
    if (winner?.line) this.drawWinLine(winner.line);
  }

  drawWinLine(line) {
    const [a, , c] = line;
    const startX = (a % 3) * this.cellSize + this.cellSize / 2;
    const startY = ~~(a / 3) * this.cellSize + this.cellSize / 2;
    const endX = (c % 3) * this.cellSize + this.cellSize / 2;
    const endY = ~~(c / 3) * this.cellSize + this.cellSize / 2;

    this.ctx.strokeStyle = '#fbbf24';
    this.ctx.lineWidth = 6;
    this.ctx.beginPath();
    this.ctx.moveTo(startX, startY);
    this.ctx.lineTo(endX, endY);
    this.ctx.stroke();
  }
```

#### 4. Interaction du joueur et boucle de jeu

C'est la logique interactive centrale. `handleClick` traduit un clic en une position sur le plateau, `move` met √† jour l'√©tat, et `aiMove` obtient une action de la classe `QLearning` et l'ex√©cute.

```javascript
  handleClick(e) {
    if (this.gameOver || this.training) return;
    
    const rect = this.canvas.getBoundingClientRect();
    const col = ~~((e.clientX - rect.left) / this.cellSize);
    const row = ~~((e.clientY - rect.top) / this.cellSize);
    const idx = row * 3 + col;

    if (this.board[idx] === '-') {
      this.move(idx, 'X');
      if (!this.gameOver) setTimeout(() => this.aiMove(), 300);
    }
  }

  move(idx, player) {
    if (this.board[idx] !== '-' || this.gameOver) return false;
    this.board = this.board.substring(0, idx) + player + this.board.substring(idx + 1);
    this.draw();
    this.checkGameOver();
    return true;
  }

  aiMove() {
    if (this.gameOver) return;
    
    const state = this.board;
    const available = this.getAvailable();
    const action = this.ai.getAction(state, available);
    
    this.move(action, 'O');
    
    const winner = this.checkWinner();
    const reward = winner?.winner === 'O' ? 1 : winner?.winner === 'X' ? -1 : 0;
    this.ai.update(state, action, reward, this.board, this.getAvailable());
  }
```

Apr√®s le mouvement de l'IA, elle appelle imm√©diatement `this.ai.update()` pour apprendre du r√©sultat de son action.

#### 5. Le moteur de r√®gles

Ces utilitaires d√©terminent l'√©tat du jeu : coups disponibles, gagnant et conditions de fin de partie.

```javascript
  getAvailable() {
    return [...this.board].map((c, i) => c === '-' ? i : null).filter(x => x !== null);
  }

  checkWinner() {
    const patterns = [[0,1,2],[3,4,5],[6,7,8],[0,3,6],[1,4,7],[2,5,8],[0,4,8],[2,4,6]];
    for (const p of patterns) {
      if (this.board[p[0]] !== '-' && 
          this.board[p[0]] === this.board[p[1]] && 
          this.board[p[1]] === this.board[p[2]]) {
        return { winner: this.board[p[0]], line: p };
      }
    }
    return this.board.includes('-') ? null : { winner: 'draw', line: null };
  }

  checkGameOver() {
    const result = this.checkWinner();
    if (!result) return;

    this.gameOver = true;
    this.stats.played++;

    if (result.winner === 'X') {
      this.stats.playerWins++;
      if (!this.training) this.setStatus('üéâ Vous avez gagn√© !');
    } else if (result.winner === 'O') {
      this.stats.aiWins++;
      if (!this.training) this.setStatus('ü§ñ L\'IA a gagn√© !');
    } else {
      this.stats.draws++;
      if (!this.training) this.setStatus('ü§ù Match nul !');
    }

    if (!this.training) {
      this.updateStats();
      this.saveState();
    }
  }
```

#### 6. Mises √† jour de l'interface utilisateur et des statistiques

Ces m√©thodes connectent l'√©tat interne du jeu aux √©l√©ments HTML, affichant les messages d'√©tat et les statistiques.

```javascript
  setStatus(msg) {
    document.getElementById('gameStatus').textContent = msg;
  }

  updateStats() {
    document.getElementById('gamesPlayed').textContent = this.stats.played;
    document.getElementById('aiWins').textContent = this.stats.aiWins;
    document.getElementById('playerWins').textContent = this.stats.playerWins;
    document.getElementById('draws').textContent = this.stats.draws;
    document.getElementById('statesLearned').textContent = this.ai.q.size;
    
    const winRate = this.stats.played ? (this.stats.aiWins / this.stats.played * 100).toFixed(1) : 0;
    document.getElementById('winRate').textContent = `${winRate}%`;
  }
```

#### 7. Gestion du jeu et de l'IA

Ces m√©thodes sont reli√©es aux boutons de contr√¥le pour r√©initialiser le jeu ou la m√©moire de l'IA.

```javascript
  reset() {
    this.board = '---------';
    this.gameOver = false;
    this.draw();
    this.setStatus('√Ä vous de jouer ! (X)');
  }

  resetAI() {
    if (confirm('R√©initialiser la m√©moire de l\'IA ? Tous les progr√®s seront perdus.')) {
      this.ai.reset();
      this.ai.clearStorage();
      this.stats = { played: 0, aiWins: 0, playerWins: 0, draws: 0 };
      this.updateStats();
      this.reset();
      this.setStatus('M√©moire de l\'IA r√©initialis√©e !');
      localStorage.removeItem('tictactoe_stats');
    }
  }
```

#### 8. La boucle d'entra√Ænement en auto-jeu

C'est la logique du bouton ¬´ Entra√Æner l'IA ¬ª, permettant √† l'IA d'apprendre rapidement en jouant contre elle-m√™me.

```javascript
  async startTraining() {
    this.training = true;
    document.getElementById('trainingIndicator').classList.remove('hidden');
    
    const originalEpsilon = this.ai.epsilon;
    this.ai.epsilon = 0.3; // Plus d'exploration pendant l'entra√Ænement

    for (let i = 0; i < 1000; i++) {
      await this.trainGame();
      this.ai.decay();
      if (i % 50 === 0) {
        document.getElementById('trainingProgress').textContent = `${i + 1}/1000`;
        await new Promise(r => setTimeout(r, 0)); // Permet √† l'UI de se mettre √† jour
      }
    }

    this.ai.epsilon = originalEpsilon;
    this.training = false;
    document.getElementById('trainingIndicator').classList.add('hidden');
    this.updateStats();
    this.reset();
    this.setStatus('Entra√Ænement termin√© !');
    this.saveState();
  }

  async trainGame() {
    this.board = '---------';
    this.gameOver = false;
    const moves = [];

    while (!this.gameOver && this.getAvailable().length > 0) {
      const state = this.board;
      const available = this.getAvailable();
      // Les joueurs altern√©s (X et O) sont tous deux l'IA
      const player = moves.length % 2 === 0 ? 'X' : 'O'; 
      const action = this.ai.getAction(state, available);
      
      moves.push({ state, action, player });
      this.move(action, player);
    }

    const winner = this.checkWinner();
    // Assigner les r√©compenses apr√®s la fin de la partie
    moves.forEach(m => {
      const reward = winner?.winner === m.player ? 1 : (winner?.winner && winner.winner !== m.player) ? -1 : 0;
      this.ai.update(m.state, m.action, reward, this.board, []);
    });
  }
```

#### 9. Persistance de l'√©tat

Ces m√©thodes orchestrent la sauvegarde et le chargement de l'√©tat du jeu et de la m√©moire de l'IA dans le `localStorage`.

```javascript
  saveState() {
    this.ai.save();
    localStorage.setItem('tictactoe_stats', JSON.stringify(this.stats));
  }

  loadState() {
    if (this.ai.load()) {
      const savedStats = localStorage.getItem('tictactoe_stats');
      if (savedStats) {
        this.stats = JSON.parse(savedStats);
      }
      this.updateStats();
      this.setDifficulty(this.ai.difficulty);
      
      // Mettre √† jour les curseurs pour refl√©ter l'√©tat charg√© de l'IA
      document.getElementById('learningRate').value = this.ai.lr;
      document.getElementById('learningRateValue').textContent = this.ai.lr.toFixed(2);
      document.getElementById('discountFactor').value = this.ai.gamma;
      document.getElementById('discountFactorValue').textContent = this.ai.gamma.toFixed(2);
      document.getElementById('explorationRate').value = this.ai.epsilon;
      document.getElementById('explorationRateValue').textContent = this.ai.epsilon.toFixed(2);
      
      console.log('‚úì √âtat de l\'IA charg√© depuis le localStorage');
    }
  }
}
```

#### 10. Initialisation du jeu

Enfin, ajoutez cet extrait √† la fin de `game.js` pour cr√©er une instance du jeu une fois le document HTML charg√©.

```javascript
let game;
window.addEventListener('DOMContentLoaded', () => {
  game = new TicTacToe();
});
```

Ceci compl√®te notre impl√©mentation ! Vous avez maintenant un fichier `game.js` enti√®rement fonctionnel. Si vous avez rencontr√© des probl√®mes ou si vous voulez v√©rifier votre travail, vous pouvez comparer votre code avec le fichier source complet disponible sur GitHub : [https://github.com/mayur9210/tic-tac-toe-ai/blob/main/game.js](https://github.com/mayur9210/tic-tac-toe-ai/blob/main/game.js).

## Comprendre les fonctionnalit√©s am√©lior√©es

Au-del√† de la logique de base du Q-learning, cette impl√©mentation inclut plusieurs fonctionnalit√©s am√©lior√©es pour cr√©er une application compl√®te, conviviale et √©ducative. Explorons ce qu'elles sont et comment elles fonctionnent.

### 1. Niveaux de difficult√© adaptatifs

Le jeu prend en charge trois modes de difficult√© distincts pour s'adapter aux diff√©rents joueurs :

* **D√©butant (üå±) :** Ce mode est con√ßu pour les nouveaux joueurs. L'IA effectue des coups al√©atoires 70 % du temps, offrant une grande chance au joueur de gagner et d'apprendre les r√®gles du jeu.
    
* **Moyen (üéØ) :** C'est le mode standard o√π l'IA utilise l'algorithme de Q-learning avec une strat√©gie epsilon-greedy. Elle pr√©sente un adversaire stimulant mais √©quitable qui s'am√©liore avec le temps.
    
* **Expert (üî•) :** Ce mode passe de l'apprentissage par renforcement √† l'algorithme classique **minimax**. Cet algorithme joue une partie parfaite, ce qui signifie qu'il est impossible √† battre (le mieux qu'un joueur puisse obtenir est un match nul). Cela sert de r√©f√©rence pour un jeu optimal.
    

### 2. Autres fonctionnalit√©s am√©lior√©es

En plus des niveaux de difficult√©, l'application comprend :

* **R√©glage des param√®tres de l'IA en temps r√©el :** Les curseurs de l'interface utilisateur vous permettent d'ajuster le taux d'apprentissage (Œ±), le facteur de remise (Œ≥) et le taux d'exploration (Œµ) √† la vol√©e. Cela vous permet d'observer directement comment diff√©rents hyperparam√®tres affectent la vitesse d'apprentissage et les performances de l'IA.
    
* **Persistance avec localStorage :** L'IA sauvegarde automatiquement sa table Q et vos statistiques de jeu dans le stockage local du navigateur. Lorsque vous fermez l'onglet et revenez plus tard, l'IA se souviendra de tout ce qu'elle a appris.
    
* **Mode d'entra√Ænement d√©di√© en auto-jeu :** Le bouton ¬´ Entra√Æner l'IA ¬ª permet √† l'IA de jouer 1 000 parties contre elle-m√™me en quelques secondes. Cela remplit rapidement la table Q et est bien plus efficace que d'apprendre uniquement √† partir de parties jou√©es par des humains.
    

## Mise en pratique : Un test guid√©

Une fois que vous avez les fichiers HTML (`index.html`) et JavaScript (`game.js`) dans le m√™me r√©pertoire, ouvrez le fichier HTML dans un navigateur web pour tester toutes les fonctionnalit√©s. Lorsque vous ouvrez le fichier HTML, il devrait ressembler √† l'image ci-dessous.

J'ai √©galement [h√©berg√© ce fichier sur GitHub Pages](https://mayur9210.github.io/tic-tac-toe-ai/) si vous voulez voir comment il fonctionne.

Maintenant que l'application est lanc√©e, voyons comment tester les fonctionnalit√©s et observer de visu le processus d'apprentissage de l'IA. Ce test interactif est la partie la plus gratifiante, car vous verrez les concepts abstraits prendre vie.

### √âtape 1 : D√©fier l'IA non entra√Æn√©e

Lorsque vous chargez le jeu pour la premi√®re fois, l'IA est une page blanche. Sa table Q est vide. Assurez-vous que la difficult√© est r√©gl√©e sur **üå± D√©butant** et jouez une partie contre elle. Vous la trouverez probablement tr√®s facile √† battre. Elle fait des coups al√©atoires et insens√©s car elle n'a aucune exp√©rience. Notez que le nombre d'¬´ √âtats appris ¬ª dans le panneau de statistiques est tr√®s bas.

### √âtape 2 : Entra√Æner l'IA

Passons maintenant √† la magie. Cliquez sur le bouton **¬´ Entra√Æner l'IA (1000 parties) ¬ª**. Vous verrez l'indicateur d'entra√Ænement jaune appara√Ætre avec un compteur de progression. En ces quelques secondes, l'IA joue 1 000 parties contre elle-m√™me, apprenant rapidement de ses victoires, d√©faites et nuls. Pour chaque coup de chaque partie, elle met √† jour sa table Q, renfor√ßant les bonnes strat√©gies et p√©nalisant les mauvaises.

### √âtape 3 : D√©fier l'IA entra√Æn√©e

Une fois l'entra√Ænement termin√©, jouez une autre partie en difficult√© **üéØ Moyen**. La diff√©rence devrait √™tre spectaculaire. L'IA jouera d√©sormais de mani√®re strat√©gique, bloquant vos victoires et pr√©parant les siennes. Ce n'est plus une proie facile. V√©rifiez √† nouveau le panneau de statistiques : vous verrez que le nombre d'¬´ √âtats appris ¬ª a consid√©rablement augment√©, repr√©sentant toutes les nouvelles positions de plateau qu'elle comprend d√©sormais.

### √âtape 4 : Exp√©rimenter avec les contr√¥les

Maintenant que vous avez une IA entra√Æn√©e, exp√©rimentez les autres fonctionnalit√©s :

* **Passer en üî• Expert :** Jouez contre l'algorithme minimax. Remarquez que vous ne pouvez pas gagner. Cela d√©montre la puissance d'un algorithme de jeu parfait.
    
* **Ajuster les param√®tres :** R√©glez le curseur du taux d'exploration (Œµ) sur 0. L'IA deviendra compl√®tement d√©terministe, choisissant toujours le coup ayant la valeur Q la plus √©lev√©e. R√©glez-le sur 0,5 et regardez-la redevenir plus erratique et exp√©rimentale.
    
* **R√©initialiser l'IA :** Cliquez sur le bouton ¬´ R√©initialiser la m√©moire de l'IA ¬ª. Cela effacera sa table Q. Si vous jouez contre elle maintenant, vous constaterez qu'elle est revenue √† son √©tat initial non entra√Æn√©. Cela confirme que son ¬´ intelligence ¬ª √©tait stock√©e dans la table Q que vous venez d'effacer.
    

### V√©rification de l'impl√©mentation avec des tests automatis√©s

Bien que jouer au jeu vous donne une bonne id√©e du comportement de l'IA, les tests automatis√©s sont cruciaux pour confirmer par programmation que le code sous-jacent est correct. C'est diff√©rent du test manuel que vous venez d'effectuer. Ici, nous √©crivons du code pour v√©rifier notre code.

La suite de tests suivante valide les trois fonctionnalit√©s les plus critiques : le changement de difficult√©, la persistance des donn√©es avec `localStorage` et l'infaillibilit√© de l'IA experte minimax. Vous pouvez ex√©cuter ces tests en copiant et collant le code dans la console de d√©veloppement de votre navigateur pendant que le jeu est ouvert.

```javascript
function runTests() {
  console.log('üß™ Ex√©cution des tests am√©lior√©s...');
  
  // Test 1 : Changement de difficult√©
  const g1 = new TicTacToe();
  g1.setDifficulty('beginner');
  console.assert(g1.ai.difficulty === 'beginner', '‚úì Le changement de difficult√© fonctionne');
  
  // Test 2 : Persistance localStorage
  const g2 = new TicTacToe();
  g2.ai.q.set('test-state', [1, 2, 3, 4, 5, 6, 7, 8, 9]);
  g2.saveState();
  const g3 = new TicTacToe();
  console.assert(g3.ai.q.has('test-state'), '‚úì La persistance localStorage fonctionne');
  
  // Test 3 : Minimax ne perd jamais
  const g4 = new TicTacToe();
  g4.setDifficulty('expert');
  let expertLosses = 0;
  for (let i = 0; i < 100; i++) {
    g4.reset();
    while (!g4.gameOver) {
      const available = g4.getAvailable();
      const move = available[~~(Math.random() * available.length)];
      g4.move(move, 'X');
      if (!g4.gameOver) g4.aiMove();
    }
    const winner = g4.checkWinner();
    if (winner?.winner === 'X') expertLosses++;
  }
  console.assert(expertLosses === 0, '‚úì L\'IA experte ne perd jamais');
  
  console.log('‚úÖ Tous les tests ont r√©ussi !');
}
```

Comment fonctionnent ces tests :

1. **Changement de difficult√© :** Le premier test cr√©e une instance de jeu, d√©finit la difficult√© et v√©rifie que la propri√©t√© interne de l'IA a √©t√© mise √† jour correctement.
    
2. **Persistance :** Le deuxi√®me test simule la sauvegarde de l'√©tat de l'IA. Il ajoute une entr√©e factice √† la table Q, la sauvegarde, cr√©e une *nouvelle* instance de jeu (simulant un rechargement de page) et v√©rifie que la nouvelle instance a charg√© avec succ√®s les donn√©es sauvegard√©es.
    
3. **Correction du mode expert :** Le troisi√®me test, le plus rigoureux, joue 100 parties contre l'IA experte en utilisant des coups al√©atoires pour le joueur. Il v√©rifie ensuite que l'IA experte n'a pas perdu une seule partie, prouvant que l'impl√©mentation du minimax est correcte.
    

Vous pouvez ex√©cuter ces tests dans la console de votre navigateur apr√®s avoir charg√© le jeu, comme le montre la capture d'√©cran ci-dessous.

![Ex√©cution des tests](https://cdn.hashnode.com/res/hashnode/image/upload/v1759790825366/aedc84b7-5399-4067-bf2c-b0b488192c62.png align="center")

## Optimisations avanc√©es et extensions

Maintenant que vous avez l'impl√©mentation compl√®te, voici des moyens de l'√©tendre davantage :

### Comment impl√©menter la r√©duction de sym√©trie

Vous pouvez r√©duire l'espace d'√©tat en reconnaissant les positions de plateau √©quivalentes :

```javascript
getCanonicalState(s) {
  const transforms = [
    s, this.rot90(s), this.rot180(s), this.rot270(s),
    this.flip(s), this.flip(this.rot90(s)), 
    this.flip(this.rot180(s)), this.flip(this.rot270(s))
  ];
  return transforms.sort()[0];
}

rot90(s) {
  const b = s.split('');
  return [b[6],b[3],b[0],b[7],b[4],b[1],b[8],b[5],b[2]].join('');
}

rot180(s) {
  return s.split('').reverse().join('');
}

rot270(s) {
  const b = s.split('');
  return [b[2],b[5],b[8],b[1],b[4],b[7],b[0],b[3],b[6]].join('');
}

flip(s) {
  const b = s.split('');
  return [b[2],b[1],b[0],b[5],b[4],b[3],b[8],b[7],b[6]].join('');
}
```

Cette technique de r√©duction de sym√©trie acc√©l√®re l'apprentissage de l'IA en reconnaissant les positions de plateau √©quivalentes.

**Comment √ßa marche :**

* **getCanonicalState()** : G√©n√®re les 8 versions sym√©triques d'un √©tat de plateau (4 rotations + 4 versions retourn√©es) et renvoie la premi√®re par ordre alphab√©tique comme repr√©sentation standard.
    
* **rot90()** : Fait pivoter le plateau de 90¬∞ dans le sens des aiguilles d'une montre en remappant les indices de position.
    
* **rot180()** : Rotation de 180¬∞ en inversant le tableau du plateau.
    
* **rot270()** : Rotation de 270¬∞ dans le sens des aiguilles d'une montre (ou 90¬∞ dans le sens inverse).
    
* **flip()** : Miroir horizontal du plateau.
    

**Pourquoi c'est important :** En ne stockant que les √©tats canoniques dans la table Q, l'IA r√©duit les positions uniques d'environ 5 500 √† environ 700, ce qui rend l'apprentissage **8 fois plus rapide**.

**Exemple :** Ces plateaux sont consid√©r√©s comme identiques :

```bash
X-- --- --X
--- = --- = ---
--- --- ---
(original) (rotation 180¬∞) (miroir horizontal)
```

Tous trois correspondent au m√™me √©tat canonique, l'IA n'a donc besoin d'en apprendre qu'un seul au lieu de trois.

Modifiez `getQ()` pour utiliser les √©tats canoniques. Cela r√©duit le temps d'apprentissage par 8 puisque l'IA reconna√Æt les positions pivot√©es et retourn√©es comme √©quivalentes.

### Comment ajouter une fonctionnalit√© d'exportation et d'importation

Vous pouvez √©galement permettre aux utilisateurs de partager des mod√®les d'IA entra√Æn√©s :

```javascript
exportAI() {
  const data = {
    q: Array.from(this.ai.q.entries()),
    stats: this.stats,
    difficulty: this.ai.difficulty,
    timestamp: Date.now()
  };
  
  const blob = new Blob([JSON.stringify(data)], { type: 'application/json' });
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a');
  a.href = url;
  a.download = `tictactoe-ai-${Date.now()}.json`;
  a.click();
  URL.revokeObjectURL(url);
}

importAI(file) {
  const reader = new FileReader();
  reader.onload = (e) => {
    try {
      const data = JSON.parse(e.target.result);
      this.ai.q = new Map(data.q);
      this.stats = data.stats;
      this.ai.difficulty = data.difficulty;
      this.updateStats();
      this.setStatus('‚úì IA import√©e avec succ√®s !');
    } catch (err) {
      this.setStatus('‚úó √âchec de l\'importation : fichier invalide');
    }
  };
  reader.readAsText(file);
}
```

Ces m√©thodes permettent de partager des mod√®les d'IA entra√Æn√©s entre utilisateurs. La m√©thode `exportAI()` regroupe l'√©tat complet de l'IA (table Q, statistiques, difficult√© et horodatage) dans un objet JSON, cr√©e un Blob √† partir de la cha√Æne JSON, g√©n√®re une URL de t√©l√©chargement temporaire, cr√©e et clique par programmation sur un lien de t√©l√©chargement, puis nettoie l'URL. Le nom du fichier inclut un horodatage pour le suivi des versions.

La m√©thode `importAI()` utilise FileReader pour lire de mani√®re asynchrone un fichier JSON t√©l√©charg√©, l'analyse, reconstruit la Map √† partir du tableau d'entr√©es, restaure tout l'√©tat du jeu et met √† jour l'affichage. La gestion des erreurs capture les fichiers JSON invalides ou corrompus.

### Comment ajouter une visualisation par carte de chaleur (Heatmap) des valeurs Q

Voici comment vous pouvez visualiser la prise de d√©cision de l'IA :

```javascript
drawQValueHeatmap() {
  const state = this.board;
  const qValues = this.ai.getQ(state);
  const available = this.getAvailable();
  
  if (available.length === 0) return;
  
  const maxQ = Math.max(...available.map(i => qValues[i]));
  const minQ = Math.min(...available.map(i => qValues[i]));
  const range = maxQ - minQ || 1;

  this.ctx.globalAlpha = 0.3;
  for (const i of available) {
    const normalized = (qValues[i] - minQ) / range;
    const row = ~~(i / 3);
    const col = i % 3;
    
    // Vert pour les valeurs Q √©lev√©es, rouge pour les basses
    const hue = normalized * 120;
    this.ctx.fillStyle = `hsl(${hue}, 70%, 50%)`;
    this.ctx.fillRect(
      col * this.cellSize + 5,
      row * this.cellSize + 5,
      this.cellSize - 10,
      this.cellSize - 10
    );
    
    // Dessiner la valeur Q
    this.ctx.globalAlpha = 1;
    this.ctx.fillStyle = '#000';
    this.ctx.font = '14px monospace';
    this.ctx.fillText(
      qValues[i].toFixed(2),
      col * this.cellSize + 10,
      row * this.cellSize + 25
    );
  }
  this.ctx.globalAlpha = 1;
}
```

Cette m√©thode de visualisation cr√©e une carte de chaleur cod√©e par couleur montrant la confiance de l'IA dans chaque coup disponible.

Elle r√©cup√®re d'abord les valeurs Q pour l'√©tat actuel et trouve les valeurs min/max parmi les positions disponibles pour normaliser les donn√©es. Pour chaque cellule vide, elle calcule un score normalis√© (0 √† 1), le convertit en une valeur de teinte (0¬∞ rouge pour les valeurs basses, 120¬∞ vert pour les valeurs hautes) en utilisant l'espace colorim√©trique HSL, et remplit la cellule avec un rectangle color√© semi-transparent. Elle superpose ensuite la valeur Q r√©elle sous forme de texte pour une inspection pr√©cise.

Cela vous donne un retour visuel instantan√© sur les coups que l'IA consid√®re comme les plus prometteurs. Les cellules vertes sont de bons coups, les cellules rouges sont de mauvais coups.

## Pi√®ges courants et solutions

### Probl√®me 1 : L'IA ne s'am√©liore pas

* **Cause** : Le taux d'apprentissage est trop bas ou il n'y a pas eu assez d'entra√Ænement.
    
* **Solution** : Augmentez le taux d'apprentissage entre 0,2 et 0,3, et entra√Ænez sur plus de 2 000 parties.
    

### Probl√®me 2 : L'IA fait des coups al√©atoires

* **Cause** : Le taux d'exploration est trop √©lev√© apr√®s l'entra√Ænement.
    
* **Solution** : R√©duisez le taux d'exploration √† 0,01 une fois l'entra√Ænement termin√©.
    

### Probl√®me 3 : Performances lentes

* **Cause** : La repr√©sentation de l'√©tat ou la recherche dans la table Q est inefficace.
    
* **Solution** : Utilisez un Map au lieu d'objets et impl√©mentez la mise en cache d'√©tat.
    

### Probl√®me 4 : L'IA sur-apprend (overfitting) une seule strat√©gie

* **Cause** : Il n'y a pas assez d'exploration pendant l'entra√Ænement.
    
* **Solution** : Commencez par un taux d'exploration √©lev√© (Œµ=0,5) et diminuez-le progressivement.
    

## Comment √©tendre ceci √† d'autres jeux

Ce framework s'adapte √† d'autres jeux :

* **Puissance 4** : √âtat de 42 caract√®res, 7 actions (colonnes).
    
* **Blackjack** : L'√©tat inclut les valeurs des mains et la carte du croupier.
    
* **Snake** : Les √©tats continus n√©cessitent une approximation de fonction.
    

## Conclusion

Vous avez construit un syst√®me complet d'apprentissage par renforcement en JavaScript. Ce projet d√©montre :

* Les concepts de base du RL avec une impl√©mentation pratique.
    
* Une architecture de code propre et maintenable.
    
* L'entra√Ænement et la visualisation en temps r√©el.
    
* Des techniques avanc√©es comme la d√©croissance d'epsilon et l'auto-jeu.
    
* Trois niveaux de difficult√©, de d√©butant √† expert.
    
* La persistance des donn√©es avec localStorage.
    
* Des infobulles interactives pour l'apprentissage.
    

La base de Q-learning que vous avez impl√©ment√©e alimente des techniques plus avanc√©es comme les r√©seaux de neurones Q profonds (Deep Q-Networks - DQN) utilis√©s dans l'IA de jeu moderne.

## Prochaines √©tapes

Voici quelques pistes pour continuer √† apprendre :

1. Ajoutez plus de niveaux de difficult√© avec des param√®tres personnalis√©s.
    
2. Impl√©mentez la persistance d'√©tat avec IndexedDB pour des tables Q plus grandes.
    
3. Cr√©ez un mode multijoueur avec observation par l'IA.
    
4. Construisez une version avec r√©seau de neurones en utilisant TensorFlow.js.
    
5. √âtendez le projet au Puissance 4 ou aux fins de parties d'√©checs.
    

### Ressources pour aller plus loin

* [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html) par Sutton et Barto (manuel en ligne gratuit).
    
* [OpenAI Spinning Up](https://spinningup.openai.com/) ‚Äì ressource compl√®te sur le RL.
    
* [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/) ‚Äì conf√©rences vid√©o de Berkeley.
    
* [Documentation Stable-Baselines3](https://stable-baselines3.readthedocs.io/) ‚Äì impl√©mentations de RL pour la production.