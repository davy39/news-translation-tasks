---
title: Comment construire de meilleurs mod√®les de Machine Learning
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2021-04-23T16:22:43.000Z'
originalURL: https://freecodecamp.org/news/how-to-build-better-machine-learning-models
coverImage: https://www.freecodecamp.org/news/content/images/2021/04/pexels-pixabay-373543.jpg
tags:
- name: Data Science
  slug: data-science
- name: Machine Learning
  slug: machine-learning
- name: neural networks
  slug: neural-networks
seo_title: Comment construire de meilleurs mod√®les de Machine Learning
seo_desc: "By Rishit Dagli\nHello developers \U0001F44B. If you have built Deep Neural\
  \ Networks before, you might know that it can involve a lot of experimentation.\
  \ \nIn this article, I will share with you some useful tips and guidelines that\
  \ you can use to better build ..."
---

Par Rishit Dagli

Bonjour les d√©veloppeurs üëã. Si vous avez d√©j√† construit des r√©seaux de neurones profonds (Deep Neural Networks), vous savez peut-√™tre que cela peut impliquer beaucoup d'exp√©rimentation.

Dans cet article, je vais partager avec vous quelques conseils et directives utiles que vous pouvez utiliser pour mieux construire de meilleurs mod√®les de deep learning. Ces astuces devraient vous faciliter grandement le d√©veloppement d'un bon r√©seau.

Vous pouvez choisir les conseils que vous utilisez, car certains seront plus utiles pour les projets sur lesquels vous travaillez. Tout ce qui est mentionn√© dans cet article n'am√©liorera pas forc√©ment directement les performances de vos mod√®les.

## Une approche de haut niveau pour le r√©glage des hyperparam√®tresüïπÔ∏è

L'un des aspects les plus p√©nibles de l'entra√Ænement des r√©seaux de neurones profonds est le grand nombre d'hyperparam√®tres auxquels vous devez faire face.

Il peut s'agir de votre taux d'apprentissage (learning rate) **Œ±**, du facteur d'actualisation **œÅ** et d'epsilon **Œµ** si vous utilisez l'optimiseur RMSprop ([Hinton et al.](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)) ou des taux de d√©croissance exponentielle **Œ≤‚ÇÅ** et **Œ≤‚ÇÇ** si vous utilisez l'optimiseur Adam ([Kingma et al.](https://arxiv.org/abs/1412.6980)).

Vous devez √©galement choisir le nombre de couches dans le r√©seau ou le nombre d'unit√©s cach√©es pour les couches. Vous utilisez peut-√™tre des planificateurs de taux d'apprentissage (learning rate schedulers) et souhaiteriez configurer ces fonctionnalit√©s et bien d'autres encore üò´ ! Nous avons d√©finitivement besoin de moyens pour mieux organiser notre processus de r√©glage des hyperparam√®tres.

Un algorithme courant que j'ai tendance √† utiliser pour organiser ma recherche d'hyperparam√®tres est la Recherche Al√©atoire (Random Search). Bien qu'il existe d'autres algorithmes qui pourraient √™tre meilleurs, je finis g√©n√©ralement par l'utiliser quand m√™me.

Disons, pour les besoins de cet exemple, que vous voulez r√©gler deux hyperparam√®tres et que vous soup√ßonnez que les valeurs optimales pour les deux se situent quelque part entre un et cinq.

L'id√©e ici est qu'au lieu de choisir syst√©matiquement vingt-cinq valeurs √† essayer comme (1, 1), (1, 2) et ainsi de suite, il serait plus efficace de s√©lectionner vingt-cinq points au hasard.

![Image](https://lh3.googleusercontent.com/MLzfMgeWASsgXEsq2XUGxo8QFl99R-4TA_--azr_k7F9rkEhh31esm47zemiPDTIPrjNWQjmlpEXtstqgcopQnEgF0R2CsNDPTuwaPq-_54IgaGp0Dkjd7TCMe3oWe-gjiVnrc2Y)
_Bas√© sur les notes de cours d'[Andrew Ng](https://www.andrewng.org/)<span class="-mobiledoc-kit__atom">‚Äå‚Äå</span>_

Voici un exemple simple avec TensorFlow o√π j'essaie d'utiliser la Recherche Al√©atoire sur le jeu de donn√©es Fashion MNIST pour le taux d'apprentissage et le nombre d'unit√©s dans la premi√®re couche Dense :

```py
import kerastuner as kt
import tensorflow as tf

def model_builder(hp):
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
  
  # R√©gler le nombre d'unit√©s dans la premi√®re couche Dense
  # Choisir une valeur optimale entre 32 et 512
  hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)
  model.add(tf.keras.layers.Dense(units = hp_units, activation = 'relu'))
  model.add(tf.keras.layers.Dense(10))

  # R√©gler le taux d'apprentissage pour l'optimiseur 
  # Choisir une valeur optimale parmi 0.01, 0.001 ou 0.0001
  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) 
  
  model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = hp_learning_rate),
                loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), 
                metrics = ['accuracy'])
  
  return model
  
tuner = kt.RandomSearch(model_builder,
                        objective = 'val_accuracy', 
                        max_trials = 10,
                        directory = 'random_search_starter',
                        project_name = 'intro_to_kt') 
                     
tuner.search(img_train, label_train, epochs = 10, validation_data = (img_test, label_test))

# Quel √©tait le meilleur mod√®le ?
best_model = tuner.get_best_models(1)[0]

# Quels √©taient les meilleurs hyperparam√®tres ?
best_hyperparameters = tuner.get_best_hyperparameters(1)[0] 
```

Ici, je soup√ßonne qu'un nombre optimal d'unit√©s dans la premi√®re couche Dense se situerait quelque part entre 32 et 512, et que mon taux d'apprentissage serait l'un des suivants : 1e-2, 1e-3 ou 1e-4.

Par cons√©quent, comme le montre cet exemple, je fixe ma valeur minimale pour le nombre d'unit√©s √† 32 et la valeur maximale √† 512 avec un pas de 32. Ensuite, au lieu de coder en dur une valeur pour le nombre d'unit√©s, je sp√©cifie une plage √† essayer.

```py
hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)
model.add(tf.keras.layers.Dense(units = hp_units, activation = 'relu'))
```

Nous faisons de m√™me pour notre taux d'apprentissage, mais notre taux d'apprentissage est simplement l'un des choix 1e-2, 1e-3 ou 1e-4 plut√¥t qu'une plage.

```py
hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])
optimizer = tf.keras.optimizers.Adam(learning_rate = hp_learning_rate)
```

Enfin, nous effectuons la Recherche Al√©atoire et sp√©cifions que parmi tous les mod√®les que nous construisons, le mod√®le ayant la pr√©cision de validation la plus √©lev√©e sera appel√© le meilleur mod√®le. Ou simplement que l'objectif est d'obtenir une bonne pr√©cision de validation.

```py
tuner = kt.RandomSearch(model_builder,
                        objective = 'val_accuracy', 
                        max_trials = 10,
                        directory = 'random_search_starter',
                        project_name = 'intro_to_kt') 
                     
tuner.search(img_train, label_train, epochs = 10, validation_data = (img_test, label_test))
```

Apr√®s cela, je veux √©galement r√©cup√©rer le meilleur mod√®le et le meilleur choix d'hyperparam√®tres. J'aimerais toutefois souligner que l'utilisation de `get_best_models` est g√©n√©ralement consid√©r√©e comme un raccourci.

Pour obtenir les meilleures performances, vous devriez r√©entra√Æner votre mod√®le avec les meilleurs hyperparam√®tres obtenus sur l'ensemble du jeu de donn√©es.

```py
# Quel √©tait le meilleur mod√®le ?
best_model = tuner.get_best_models(1)[0]

# Quels √©taient les meilleurs hyperparam√®tres ?
best_hyperparameters = tuner.get_best_hyperparameters(1)[0] 
```

Je ne parlerai pas de ce code en d√©tail dans cet article, mais vous pouvez lire [cet article](https://towardsdatascience.com/the-art-of-hyperparameter-tuning-in-deep-neural-nets-by-example-685cb5429a38) que j'ai √©crit il y a quelque temps si vous le souhaitez.

## Utiliser l'entra√Ænement en pr√©cision mixte pour les grands r√©seauxüé®

Plus votre r√©seau de neurones est grand, plus vos r√©sultats sont pr√©cis (en g√©n√©ral). √Ä mesure que la taille des mod√®les augmente, les besoins en m√©moire et en calcul pour l'entra√Ænement de ces mod√®les augmentent √©galement.

L'id√©e de l'utilisation de l'entra√Ænement en pr√©cision mixte (Mixed Precision Training) (NVIDIA, [Micikevicius et al.](https://arxiv.org/abs/1710.03740)) est d'entra√Æner des r√©seaux de neurones profonds en utilisant des nombres √† virgule flottante en demi-pr√©cision, ce qui vous permet d'entra√Æner de grands r√©seaux de neurones beaucoup plus rapidement avec une diminution nulle ou n√©gligeable des performances des r√©seaux.

Mais, je tiens √† souligner que cette technique ne devrait √™tre utilis√©e que pour les grands mod√®les avec plus de 100 millions de param√®tres environ.

Bien que la pr√©cision mixte fonctionne sur la plupart des mat√©riels, elle n'acc√©l√©rera les mod√®les que sur les GPU NVIDIA r√©cents (par exemple Tesla V100 et Tesla T4) et les Cloud TPU.

Je veux vous donner une id√©e des gains de performance lors de l'utilisation de la pr√©cision mixte. Lorsque j'ai entra√Æn√© un mod√®le ResNet sur mon instance GCP Notebook (compos√©e d'un Tesla V100), le temps d'entra√Ænement √©tait presque trois fois meilleur et presque 1,5 fois sur une instance Cloud TPU avec presque aucune diff√©rence de pr√©cision. Le code pour mesurer les acc√©l√©rations ci-dessus a √©t√© tir√© de [cet exemple](https://www.tensorflow.org/guide/mixed_precision).

Pour augmenter encore votre d√©bit d'entra√Ænement, vous pourriez √©galement envisager d'utiliser une taille de lot (batch size) plus grande ‚Äì et puisque nous utilisons des tenseurs float16, vous ne devriez pas manquer de m√©moire.

Il est √©galement assez facile d'impl√©menter la pr√©cision mixte avec TensorFlow. Avec TensorFlow, vous pouvez facilement utiliser le module [tf.keras.mixed_precision](https://www.freecodecamp.org/news/p/d63b23cb-c1f8-4997-87c1-6c5c44ea9e14/tf.keras.mixed_precision) qui vous permet de mettre en place une politique de type de donn√©es (pour utiliser float16) et d'appliquer √©galement une mise √† l'√©chelle de la perte (loss scaling) pour √©viter le sous-d√©passement (underflow).

Voici un exemple minimaliste d'utilisation de l'entra√Ænement en pr√©cision mixte sur un r√©seau :

```py
import tensorflow as tf

policy = tf.keras.mixed_precision.Policy('mixed_float16')
tf.keras.mixed_precision.set_global_policy(policy)

inputs = keras.Input(shape=(784,))
x = tf.keras.layers.Dense(4096, activation='relu')(inputs)
x = tf.keras.layers.Dense(4096, activation='relu')(x)
x = layers.Dense(10)(x)
outputs = layers.Activation('softmax', dtype='float32')(x)
model = keras.Model(inputs=inputs, outputs=outputs)

model.compile(...)
model.fit(...)
```

Dans cet exemple, nous d√©finissons d'abord la politique dtype sur float16, ce qui implique que toutes les couches de notre mod√®le utiliseront automatiquement float16.

Apr√®s cela, nous construisons un mod√®le, mais nous surchargeons le type de donn√©es pour la derni√®re couche ou la couche de sortie en float32 pour √©viter tout probl√®me num√©rique. Id√©alement, vos couches de sortie devraient √™tre en float32.

Note : J'ai construit un mod√®le avec autant d'unit√©s pour que nous puissions voir une diff√©rence dans le temps d'entra√Ænement avec l'entra√Ænement en pr√©cision mixte, car cela fonctionne bien pour les grands mod√®les.

Si vous cherchez plus d'inspiration pour utiliser l'entra√Ænement en pr√©cision mixte, voici une image d√©montrant l'acc√©l√©ration pour plusieurs mod√®les par Google Cloud sur un TPU :

![Image](https://lh6.googleusercontent.com/jDx-lq4Ll6Ihre2G5_JIYRDr1ogkMUCHiNcQ8g_WXz3cpGeylmICsQtQkV5JE9wcwZswzImi57AfNzWPEqBuLWfabl405AbH4HsZH6eOKs8kEF_zjZRKkQ6qQjLGk-JSca3rCGU7)
_Acc√©l√©rations sur un Cloud TPU_

## Utiliser le Grad Check pour la r√©tropropagation ‚úîÔ∏è

Dans plusieurs sc√©narios, j'ai d√ª impl√©menter moi-m√™me un r√©seau de neurones. Et l'impl√©mentation de la r√©tropropagation (backpropagation) est typiquement l'aspect le plus sujet aux erreurs et le plus difficile √† d√©boguer.

Avec une r√©tropropagation incorrecte, votre mod√®le pourrait apprendre quelque chose qui semble raisonnable, ce qui le rend encore plus difficile √† d√©boguer. Alors, ne serait-ce pas g√©nial si nous pouvions impl√©menter quelque chose qui nous permettrait de d√©boguer nos r√©seaux de neurones facilement ?

J'utilise souvent le Gradient Check (v√©rification du gradient) lors de l'impl√©mentation de la r√©tropropagation pour m'aider √† la d√©boguer. L'id√©e ici est d'approcher les gradients en utilisant une approche num√©rique. Si elle est proche des gradients calcul√©s par l'algorithme de r√©tropropagation, vous pouvez alors √™tre plus confiant dans le fait que la r√©tropropagation a √©t√© impl√©ment√©e correctement.

√Ä l'heure actuelle, vous pouvez utiliser cette expression en termes standards pour obtenir un vecteur que nous appellerons `dŒ∏[approx]` :

![Image](https://lh5.googleusercontent.com/BMyeu-1N1INBGjyDzdc_MNpRVToTt6lmidWN5CYualOQ67wvF_rki1axuSeCGkWNxr4dHnp1kA0zP6E3HmUw3SeofkUHhwsElB0kEvtst2220ycNfQCZGoumHnNQzWb8r_mST8Ep)
_Calculer les gradients approximatifs<span class="-mobiledoc-kit__atom">‚Äå‚Äå</span>_

Si vous cherchez le raisonnement derri√®re cela, vous pouvez en savoir plus dans [cet article](https://towardsdatascience.com/debugging-your-neural-nets-and-checking-your-gradients-f4d7f55da167) que j'ai √©crit.

Nous avons donc maintenant deux vecteurs `dŒ∏[approx]` et `dŒ∏` (calcul√© par r√©tropropagation). Et ceux-ci devraient √™tre presque √©gaux l'un √† l'autre. Vous pourriez simplement calculer la distance euclidienne entre ces deux vecteurs et utiliser ce tableau de r√©f√©rence pour vous aider √† d√©boguer vos r√©seaux :

![Image](https://lh5.googleusercontent.com/R-vrp1hq3psZmldrPYkupqofV7KOSWi0URLihhHAN5etHlR8U2kHdGE1XEAu-A9E_4w2Q8OmLXBZoYyyxJzIYwxG50dDPUSGL2gYw8U_lKCQtHXauUIUMa62H0mYp4eUO1LiJNnP)
_Tableau de r√©f√©rence_

## Mettez vos jeux de donn√©es en cache üíæ

Mettre en cache les jeux de donn√©es est une id√©e simple, mais je ne l'ai pas vue beaucoup utilis√©e. L'id√©e ici est de parcourir l'int√©gralit√© du jeu de donn√©es et de le mettre en cache soit dans un fichier, soit en m√©moire (s'il s'agit d'un petit jeu de donn√©es).

Cela devrait vous √©viter d'effectuer certaines op√©rations CPU co√ªteuses comme l'ouverture de fichiers et la lecture de donn√©es √† chaque √©poque.

Cela signifie √©galement que votre premi√®re √©poque prendrait comparativement plus de temps üìâ puisque vous effectueriez id√©alement toutes les op√©rations comme l'ouverture des fichiers et la lecture des donn√©es lors de la premi√®re √©poque, puis leur mise en cache. Mais les √©poques suivantes devraient √™tre beaucoup plus rapides puisque vous utiliseriez les donn√©es mises en cache.

Cela semble vraiment √™tre une id√©e tr√®s simple √† mettre en ≈ìuvre, n'est-ce pas ? Voici un exemple avec TensorFlow montrant comment vous pouvez tr√®s facilement mettre en cache des jeux de donn√©es. Il montre √©galement l'acc√©l√©ration üöÄ r√©sultant de la mise en ≈ìuvre de cette id√©e. Retrouvez le code complet de l'exemple ci-dessous dans [ce gist](https://gist.github.com/Rishit-dagli/5d06c69c69e990f9e15249e15002bb07) de ma part.

![Image](https://lh5.googleusercontent.com/uMIS-r7tn2VD85nNQ1mNTyqaDwcTUeyV2mY47q1UkJvEEGoemFcuYPVgcyVDyG3E2a0iz9rrdimRGG9m9mOOEVZai_UiS1IRmiuvWYwOrmxHNuh711H0UVYum3o4u-8sWqcHrmvt)
_Un exemple simple de mise en cache des jeux de donn√©es et l'acc√©l√©ration obtenue_

## Comment s'attaquer au surapprentissage ‚≠ê

Lorsque vous travaillez avec des r√©seaux de neurones, le surapprentissage (overfitting) et le sous-apprentissage (underfitting) peuvent √™tre deux des probl√®mes les plus courants auxquels vous faites face. Cette section traite de certaines approches courantes que j'utilise pour r√©soudre ces probl√®mes.

Vous le savez peut-√™tre, mais un biais √©lev√© (high bias) vous fera manquer une relation entre les caract√©ristiques et les √©tiquettes (sous-apprentissage) et une variance √©lev√©e (high variance) am√®nera le mod√®le √† capturer le bruit et √† surapprendre les donn√©es d'entra√Ænement.

Je pense que le moyen le plus efficace de r√©soudre le surapprentissage est d'obtenir plus de donn√©es ‚Äì bien que vous puissiez √©galement augmenter vos donn√©es (data augmentation). Un avantage des r√©seaux de neurones profonds est que leurs performances s'am√©liorent √† mesure qu'ils re√ßoivent de plus en plus de donn√©es.

Mais dans beaucoup de situations, il peut √™tre trop co√ªteux d'obtenir plus de donn√©es ou tout simplement impossible de le faire. Dans ce cas, parlons de quelques autres m√©thodes que vous pourriez utiliser pour lutter contre le surapprentissage.

En plus d'obtenir plus de donn√©es ou d'augmenter vos donn√©es, vous pourriez √©galement lutter contre le surapprentissage soit en changeant l'architecture du r√©seau, soit en appliquant certaines modifications aux poids du r√©seau. Voyons ces deux m√©thodes.

### Changer l'architecture du mod√®le

Un moyen simple de changer l'architecture pour qu'elle ne surapprenne pas serait d'utiliser la Recherche Al√©atoire pour tomber sur une bonne architecture. Ou vous pourriez essayer l'√©lagage (pruning) de n≈ìuds de votre mod√®le, abaissant ainsi la capacit√© de votre mod√®le.

Nous avons d√©j√† parl√© de la Recherche Al√©atoire, mais au cas o√π vous voudriez voir un exemple d'√©lagage, vous pourriez jeter un ≈ìil au [Guide d'√©lagage pour l'optimisation des mod√®les TensorFlow](https://www.tensorflow.org/model_optimization/guide/pruning).

### Modifier les poids du r√©seau

Dans cette section, nous verrons quelques m√©thodes que j'utilise couramment pour pr√©venir le surapprentissage en modifiant les poids d'un r√©seau.

#### R√©gularisation des poids

Pour revenir sur ce que nous avons discut√©, "les mod√®les plus simples sont moins susceptibles de surapprendre que les mod√®les complexes". Nous essayons de limiter la complexit√© du r√©seau en for√ßant ses poids √† ne prendre que de petites valeurs.

Pour ce faire, nous ajouterons √† notre fonction de perte un terme qui peut p√©naliser notre mod√®le s'il a des poids √©lev√©s. Souvent, les r√©gularisations L‚ÇÅ et L‚ÇÇ sont utilis√©es, la diff√©rence √©tant :

* L1 - La p√©nalit√© ajout√©e est ‚àù √† |coefficients de poids|
* L2 - La p√©nalit√© ajout√©e est ‚àù √† |coefficients de poids|**¬≤**

o√π |x| repr√©sente les valeurs absolues.

Remarquez-vous la diff√©rence entre L1 et L2, le terme au carr√© ? √Ä cause de cela, L1 pourrait pousser les poids √† √™tre √©gaux √† z√©ro alors que L2 aurait des poids tendant vers z√©ro mais pas nuls.

Si vous √™tes curieux d'explorer cela davantage, [cet article](https://towardsdatascience.com/solving-overfitting-in-neural-nets-with-regularization-301c31a7735f) approfondit les r√©gularisations et pourrait vous aider.

C'est aussi la raison exacte pour laquelle j'ai tendance √† utiliser la r√©gularisation L2 plus que la L1. Voyons un exemple de cela avec TensorFlow.

Voici un code pour cr√©er une couche Dense simple avec 3 unit√©s et la r√©gularisation L2 :

```py
import tensorflow as tf
tf.keras.layers.Dense(3, kernel_regularizer = tf.keras.regularizers.L2(0.1))
```

Pour apporter plus de clart√© sur ce que cela fait, comme nous l'avons discut√© plus haut, cela ajouterait un terme (0,1 √ó valeur_coefficient_poids¬≤) √† la fonction de perte qui fonctionne comme une p√©nalit√© pour les poids tr√®s importants. De plus, il est aussi facile de remplacer L2 par L1 dans le code ci-dessus pour impl√©menter L1 pour votre couche.

#### Dropouts

La premi√®re chose que je fais quand je construis un mod√®le et que je fais face au surapprentissage est d'essayer d'utiliser des dropouts ([Srivastava et al.](https://jmlr.org/papers/v15/srivastava14a.html)). L'id√©e ici est d'abandonner al√©atoirement ou de mettre √† z√©ro (ignorer) x % des caract√©ristiques de sortie de la couche pendant l'entra√Ænement.

Nous faisons cela pour emp√™cher les n≈ìuds individuels de d√©pendre de la sortie d'autres n≈ìuds et pour les emp√™cher de trop co-s'adapter √† partir d'autres n≈ìuds.

Les dropouts sont assez faciles √† impl√©menter avec TensorFlow puisqu'ils sont disponibles sous forme de couches. Voici un exemple o√π j'essaie de construire un mod√®le pour diff√©rencier des images de chiens et de chats avec Dropout pour r√©duire le surapprentissage :

```py
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu',input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

Comme vous pouvez le voir dans le code ci-dessus, vous pouvez directement utiliser `tf.keras.layers.dropout` pour impl√©menter le dropout, en lui passant la fraction de caract√©ristiques de sortie √† ignorer (ici 20 % des caract√©ristiques de sortie).

#### Arr√™t pr√©coce (Early stopping)

L'arr√™t pr√©coce est une autre m√©thode de r√©gularisation que j'utilise souvent. L'id√©e ici est de surveiller les performances du mod√®le √† chaque √©poque sur un ensemble de validation et d'interrompre l'entra√Ænement lorsque vous remplissez une condition sp√©cifi√©e pour les performances de validation (comme arr√™ter l'entra√Ænement quand la perte < 0,5).

Il s'av√®re que la condition de base dont nous avons parl√© plus haut fonctionne √† merveille si votre erreur d'entra√Ænement et votre erreur de validation ressemblent √† ce qu'il y a dans cette image. Dans ce cas, l'arr√™t pr√©coce arr√™terait simplement l'entra√Ænement lorsqu'il atteint la bo√Æte rouge (pour la d√©monstration) et emp√™cherait purement et simplement le surapprentissage.

> C'est (l'arr√™t pr√©coce) une technique de r√©gularisation si simple et efficace que Geoffrey Hinton l'a appel√©e un "beautiful free lunch". ‚Äì Hands-On Machine Learning with Scikit-Learn and TensorFlow par Aurelien Geron

![Image](https://www.freecodecamp.org/news/content/images/2021/04/image-99.png)
_Adapt√© de [Lutz Prechelt](https://link.springer.com/chapter/10.1007/978-3-642-35289-8_5)_

Cependant, dans certains cas, vous ne vous retrouveriez pas avec des choix aussi simples pour identifier le crit√®re ou savoir quand l'arr√™t pr√©coce devrait arr√™ter l'entra√Ænement du mod√®le.

Pour la port√©e de cet article, nous ne parlerons pas d'autres crit√®res ici, mais je vous recommande de consulter "[Early Stopping ‚Äî But When, Lutz Prechelt](https://link.springer.com/chapter/10.1007/978-3-642-35289-8_5)" que j'utilise beaucoup pour aider √† d√©cider des crit√®res.

Voyons un exemple d'arr√™t pr√©coce en action avec TensorFlow :

```py
import tensorflow as tf

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
model = tf.keras.models.Sequential([...])
model.compile(...)
model.fit(..., callbacks = [callback])
```

Dans l'exemple ci-dessus, nous cr√©ons un callback EarlyStopping et sp√©cifions que nous voulons surveiller nos valeurs de perte. Nous sp√©cifions √©galement qu'il doit arr√™ter l'entra√Ænement s'il ne voit pas d'am√©liorations notables des valeurs de perte pendant 3 √©poques. Enfin, lors de l'entra√Ænement du mod√®le, nous sp√©cifions qu'il doit utiliser ce callback.

De plus, pour les besoins de cet exemple, je montre un mod√®le Sequential ‚Äì mais cela pourrait fonctionner exactement de la m√™me mani√®re avec un mod√®le cr√©√© avec l'API fonctionnelle ou des mod√®les sous-class√©s √©galement.

## Merci de m'avoir lu !

Merci d'√™tre rest√© avec moi jusqu'√† la fin. J'esp√®re que vous b√©n√©ficierez de cet article et que vous int√©grerez ces conseils dans vos propres exp√©riences.

Je suis impatient de voir s'ils vous aident √©galement √† am√©liorer les performances de vos r√©seaux de neurones. Si vous avez des commentaires ou des suggestions pour moi, n'h√©sitez pas √† me [contacter sur Twitter](https://twitter.com/rishit_dagli).