---
title: Comment ex√©cuter un LLM localement pour interagir avec vos documents
subtitle: ''
author: Zoe Isabel Sen√≥n
date: '2026-01-10T00:38:09.929Z'
originalURL: https://acceleratingutopia.com/how-to-local-llm-private-docs/
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1767976983680/2e3671cd-4280-4a32-9508-47fe9c06ab22.png
tags:
- name: 'LLM''s '
  slug: llms
- name: ollama
  slug: ollama
- name: AI
  slug: ai
seo_title: Comment ex√©cuter un LLM localement pour interagir avec vos documents
seo_desc: Learn how to run a LLM like ChatGPT locally to interact with your personal
  or business documents privately.
---

La plupart des outils d'IA vous obligent √† envoyer vos prompts et vos fichiers √† des serveurs tiers. C'est inacceptable si vos donn√©es incluent des journaux intimes, des notes de recherche ou des documents professionnels sensibles (contrats, pr√©sentations de conseil d'administration, fichiers RH, donn√©es financi√®res). La bonne nouvelle : vous pouvez ex√©cuter des LLM performants localement (sur un ordinateur portable ou votre propre serveur) et interroger vos documents sans envoyer un seul octet dans le cloud.

Dans ce tutoriel, vous apprendrez √† ex√©cuter un LLM de mani√®re locale et priv√©e, afin de pouvoir rechercher et discuter avec des journaux et des documents professionnels sensibles sur votre propre machine. Nous installerons **Ollama** et **OpenWebUI**, choisirons un mod√®le adapt√© √† votre mat√©riel, activerons la recherche de documents priv√©s avec **nomic-embed-text**, et cr√©erons une base de connaissances locale pour que tout reste sur le disque.

## Table des mati√®res

* [Pr√©requis](#heading-prerequis)
    
* [Installation](#heading-installation)
    
* [Param√®tres pour les documents](#heading-parametres-pour-les-documents)
    
* [Comment t√©l√©charger vos documents](#heading-comment-telecharger-vos-documents)
    
    * [(Optionnel) Ajouter une instruction syst√®me](#heading-optionnel-ajouter-une-instruction-systeme)
        
* [Comment ex√©cuter votre LLM localement](#heading-comment-executer-votre-llm-localement)
    
* [Conclusion](#heading-conclusion)
    

## Pr√©requis

Vous aurez besoin d'un terminal (tous les syst√®mes ‚Äî Windows, Mac, Linux ‚Äî en incluent un, et vous pouvez trouver le v√¥tre avec une recherche rapide), et soit de Python et pip, soit de Docker, selon votre m√©thode d'installation pr√©f√©r√©e pour OpenWebUI.

## Installation

Vous aurez besoin d' [**Ollama**](https://ollama.com/download) et d' [**OpenWebUI**](https://docs.openwebui.com/getting-started/quick-start/). Ollama ex√©cute les mod√®les, tandis qu'OpenWebUI vous offre une interface de navigation pour interagir avec votre LLM local, comme vous le feriez avec ChatGPT.

### √âtape 1 : Installer Ollama

T√©l√©chargez et installez Ollama depuis son [site officiel](https://ollama.com/download). Des installateurs sont disponibles pour **macOS**, **Linux** et **Windows**. Une fois install√©, v√©rifiez qu'il fonctionne en ouvrant un terminal et en ex√©cutant :

```bash
ollama list
```

Si Ollama est en cours d'ex√©cution, cela renverra une liste de mod√®les actifs (ou une liste vide).

### √âtape 2 : Installer OpenWebUI

Vous pouvez installer OpenWebUI soit avec Python (pip), soit avec Docker. Ici, nous allons montrer comment le faire avec pip, mais vous pouvez trouver les instructions pour Docker sur la [documentation officielle d'openwebui](https://docs.openwebui.com/getting-started/quick-start/).

Installez OpenWebUI avec la commande suivante :

```bash
pip install open-webui
```

Cela fonctionne sur **macOS, Linux et Windows**, tant que vous avez Python ‚â• 3.9 install√©.

Ensuite, d√©marrez le serveur :

```bash
open-webui serve
```

Puis ouvrez votre navigateur et allez sur :

```bash
http://localhost:8080
```

### √âtape 3 : Installer un mod√®le

Choisissez un mod√®le dans la [liste des mod√®les Ollama](https://ollama.com/library) et t√©l√©chargez-le localement en copiant la commande fournie.

![Capture d'√©cran de la page de t√©l√©chargement du mod√®le avec une fl√®che pointant vers la case en haut √† droite qui inclut la commande d'installation avec un raccourci pour copier-coller](https://cdn.hashnode.com/res/hashnode/image/upload/v1758302463715/fbbaabf7-6612-460c-8e09-1c5143eacc1a.png align="center")

Par exemple :

```bash
ollama pull gemma3:4b
```

Si vous n'√™tes pas s√ªr du mod√®le que votre machine peut supporter, demandez √† une IA d'en recommander un en fonction de votre mat√©riel. Les mod√®les plus petits (1B‚Äì4B) sont plus s√ªrs sur les ordinateurs portables.

Je recommanderais Gemma3 comme point de d√©part (vous pouvez t√©l√©charger plusieurs mod√®les et basculer facilement de l'un √† l'autre). Choisissez le **nombre de param√®tres** √† la fin (":4b", ":1b", etc.) en vous basant sur ce guide :

* Niveau 1 (petits ordinateurs portables ou ordinateurs peu puissants) : RAM ‚â§ 8 Go ou pas de GPU ‚Üí 1B‚Äì2B.
    
* Niveau 2 : RAM 16 Go, GPU faible ‚Üí 2B‚Äì4B.
    
* Niveau 3 : RAM ‚â• 16 Go, 6‚Äì8 Go de VRAM ‚Üí 4B‚Äì9B.
    
* Niveau 4 : RAM ‚â• 32 Go, 12 Go+ de VRAM ‚Üí 12B+.
    

Une fois que vous avez install√© Ollama et le mod√®le souhait√©, confirmez qu'ils sont actifs en ex√©cutant `ollama list` dans le terminal :

![Image montrant le r√©sultat de l'ex√©cution de la commande "ollama list" (montre la liste des mod√®les t√©l√©charg√©s, dans ce cas "gemma3:1b")](https://cdn.hashnode.com/res/hashnode/image/upload/v1767465401368/d1b8abc0-7aaa-4c2f-ad4c-30ae908f9e8b.png align="left")

Lancez WebOpenUI pour d√©marrer l'interface du navigateur avec :

```bash
open-webui serve
```

Puis rendez-vous sur [http://localhost:8080/](http://localhost:8080/). Vous √™tes maintenant pr√™t √† commencer √† utiliser votre LLM localement !

**Note** : il vous demandera des identifiants de connexion, mais ceux-ci n'ont pas vraiment d'importance si vous avez l'intention de l'utiliser uniquement localement.

![Capture d'√©cran de l'interface d'une instance en cours d'ex√©cution d'OpenWebUI, montrant la page d'accueil, qui comprend une zone de saisie de texte au centre avec l'espace r√©serv√© "comment puis-je vous aider aujourd'hui ?", et un panneau lat√©ral avec la liste des discussions pr√©c√©dentes, et des liens vers "recherche", "notes", "espace de travail" et "nouvelle discussion", ainsi qu'un bouton de r√©glage. En haut, il y a un s√©lecteur de mod√®le qui a actuellement "gemma3:1b" s√©lectionn√© comme mod√®le √† utiliser.](https://cdn.hashnode.com/res/hashnode/image/upload/v1758302486263/14d93c7e-415c-463f-82da-fc515f28663a.png align="center")

## Param√®tres pour les documents

Nous allons maintenant configurer tout ce dont nous avons besoin pour interagir avec nos documents locaux. Tout d'abord, nous devons installer le mod√®le ¬´ [**nomic-embed-text**](https://ollama.com/library/nomic-embed-text) ¬ª pour traiter nos documents. Installez-le avec :

```bash
ollama pull nomic-embed-text
```

**Note** : Si vous vous demandez pourquoi nous avons besoin d'un autre mod√®le (nomic-embed-text) en plus de notre mod√®le principal :

* Le mod√®le d'incorporation (`nomic-embed-text`) transforme chaque segment de texte de vos documents en un vecteur num√©rique afin qu'OpenWebUI puisse trouver rapidement des segments s√©mantiquement similaires lorsque vous posez une question.
    
* Le mod√®le de chat (par exemple `gemma3:1b`) re√ßoit votre question ainsi que ces segments r√©cup√©r√©s comme contexte et g√©n√®re la r√©ponse en langage naturel.
    

Ensuite, vous devriez activer la fonction ¬´ **m√©moire** ¬ª si vous voulez que le LLM se souvienne du contexte de vos conversations pass√©es dans les futures.

T√©l√©chargez la fonction de m√©moire adaptative [**ici**](https://openwebui.com/f/alexgrama7/adaptive_memory_v2). Les fonctions sont comme des plug-ins.

![Capture d'√©cran montrant la page (site web) pour la fonction "adaptive memory v3". Elle montre un gros bouton "get", qui, lorsqu'on clique dessus, ouvre une vue contextuelle nomm√©e "Open WebUI URL" avec l'espace r√©serv√© actuel √©tant "http:localhost:8080" (le port WebUI par d√©faut) et un bouton pour "importer vers WebUI" et un autre en dessous pour "T√©l√©charger en tant qu'exportation JSON" au cas o√π le premier ne fonctionnerait pas)](https://cdn.hashnode.com/res/hashnode/image/upload/v1758302505221/b247316c-0863-410a-84c9-abc084a6631f.png align="center")

Nous allons maintenant mettre √† jour nos param√®tres pour activer ces fonctionnalit√©s. Cliquez sur votre nom dans le coin inf√©rieur gauche, puis sur ¬´ Param√®tres ¬ª.

![Capture d'√©cran montrant le panneau de menu qui appara√Æt en cliquant sur l'ic√¥ne ronde en bas √† gauche avec l'initiale et le nom de l'utilisateur, montrant une liste d'options, commen√ßant par "Param√®tres" et suivie de "Discussions archiv√©es", "Playground", "Panneau d'administration" et "Se d√©connecter"](https://cdn.hashnode.com/res/hashnode/image/upload/v1758302517617/e73983f3-0e36-4c0a-a61c-96a0a42f1fab.png align="left")

Cliquez sur le premier, puis allez dans ¬´ Personnalisation ¬ª et activez ¬´ M√©moire ¬ª.

![Capture d'√©cran du panneau des param√®tres d'OpenWebUI avec l'onglet Personnalisation ouvert et le bouton M√©moire activ√© pour enregistrer le contexte des conversations pass√©es.](https://cdn.hashnode.com/res/hashnode/image/upload/v1752935284007/aa42c76b-f38c-4485-b442-8844c6c3a544.png align="center")

Nous allons maintenant acc√©der √† l'autre panneau de param√®tres (¬´ Panneau d'administration ¬ª). Cliquez √† nouveau sur votre nom dans le coin inf√©rieur gauche et allez dans **Panneau d'administration ‚Üí Param√®tres ‚Üí Documents**.

![Capture d'√©cran de la page Administration d'OpenWebUI ‚Üí Param√®tres ‚Üí Documents, montrant un champ de saisie de texte appel√© "Taille du segment" actuellement r√©gl√© sur 512](https://cdn.hashnode.com/res/hashnode/image/upload/v1758302570583/96784c55-484b-4c66-bdc4-ce23a7e901a1.png align="center")

Dans cette section (Panneau d'administration ‚Üí Param√®tres ‚Üí Documents), trouvez la section ¬´ **Embedding** ¬ª, allez dans ¬´ **Embedding Model Engine** ¬ª et choisissez Ollama (trouvez le menu d√©roulant √† droite). Laissez la cl√© API vide.

Maintenant, sous ¬´ **Embedding Model** ¬ª, √©crivez `nomic-embed-text`. Allez ensuite dans ¬´ Retrieval ¬ª ‚Üí activez ¬´ Full Context Mode ¬ª.

### Param√®tres de d√©coupage (chunking)

Vous devez √©galement d√©finir la **taille des segments** (chunk size) et le **chevauchement** (overlap). OpenWebUI divise les documents en segments plus petits avant de les indexer, car les mod√®les ne peuvent pas incorporer ou r√©cup√©rer des textes tr√®s longs d'un seul tenant.

Une bonne valeur par d√©faut est de **128 √† 512 tokens par segment**, avec un **chevauchement de 10 √† 20 %**. Les segments plus grands pr√©servent davantage de contexte mais sont plus lents et plus gourmands en m√©moire, tandis que les segments plus petits sont plus rapides mais peuvent perdre le sens global. Le chevauchement aide √† √©viter que le contexte important ne soit coup√© lors de la division du texte.

Voici un tableau indicatif, mais je recommande d'obtenir les valeurs recommand√©es pour votre cas d'utilisation et votre configuration sp√©cifiques en les partageant (y compris le mod√®le de GPU ou d'ordinateur portable, le stockage, la RAM, etc.) avec un LLM comme ChatGPT ou Claude, **car changer les valeurs de d√©coupage/chevauchement plus tard n√©cessite de ret√©l√©charger les documents.**

### Segments/chevauchement sugg√©r√©s par niveau

| **Niveau / sc√©nario** | **Mat√©riel typique** | **Taille du segment (tokens)** | **Chevauchement (%)** | **Notes** |
| --- | --- | --- | --- | --- |
| Niveau 1 ‚Äì contraint | ‚â§8 Go RAM, pas de GPU ou GPU faible | 128‚Äì256 | 10‚Äì15 | Priorise la vitesse et la faible utilisation de la m√©moire. |
| Niveau 2 ‚Äì moyen | 16 Go RAM, GPU modeste ou CPU puissant | 256‚Äì384 | 15‚Äì20 | √âquilibre entre contexte et performance. |
| Niveau 3 ‚Äì confortable | ‚â•16 Go RAM, 6‚Äì8 Go VRAM | 384‚Äì512 | 15‚Äì20 | Plus de s√©mantique par segment, reste pratique. |
| PDF techniques denses / docs juridiques | Tous, mais surtout Niveaux 2‚Äì3 | 384‚Äì512 | 15‚Äì20 | Garde les paragraphes et les arguments intacts. |
| Notes courtes, tickets, e-mails | Tous | 128‚Äì256 | 10‚Äì15 | Les √©l√©ments sont petits, les grands segments ne sont pas n√©cessaires. |
| Requ√™tes tr√®s longues, besoin de nombreux segments r√©cup√©r√©s | Tous avec une fen√™tre de contexte plus large | 256‚Äì384 | 10‚Äì15 | Les segments plus petits permettent d'ins√©rer plus de morceaux dans le contexte. |

## Comment t√©l√©charger vos documents

Maintenant, l'√©tape finale : le t√©l√©chargement de vos documents ! Allez dans ¬´ Espace de travail ¬ª dans le panneau lat√©ral, puis ¬´ Connaissance ¬ª, et cr√©ez une nouvelle collection (base de donn√©es). Vous pouvez commencer √† t√©l√©charger des fichiers ici.

![Capture d'√©cran de la page "Espace de travail" (apr√®s avoir cliqu√© sur "espace de travail" dans le panneau lat√©ral) mettant en √©vidence le bouton "Espace de travail" sur le c√¥t√© gauche, l'onglet "Connaissance" √©tant s√©lectionn√© parmi les options en haut de cette page d'Espace de travail, puis "T√©l√©charger des fichiers" qui est la premi√®re option affich√©e sur la liste apr√®s avoir cliqu√© sur le bouton "+" (plus) √† droite de la saisie de texte avec l'espace r√©serv√© qui dit "Rechercher dans la collection".](https://cdn.hashnode.com/res/hashnode/image/upload/v1758302584485/63c04901-f5d3-4ac7-bab5-b23362fb83cb.png align="center")

<div data-node-type="callout">
<div data-node-type="callout-emoji">‚ö†Ô∏è</div>
<div data-node-type="callout-text">Assurez-vous de v√©rifier s'il y a des erreurs pendant le t√©l√©chargement. Malheureusement, elles ne s'affichent que sous forme de fen√™tres contextuelles temporaires. Certaines erreurs peuvent √™tre dues au format de vos fichiers, alors assurez-vous de v√©rifier la console pour d'autres journaux d'erreurs.</div>
</div>

Ensuite, dans ¬´ Espace de travail ¬ª, passez √† l'onglet ¬´ Mod√®les ¬ª et cr√©ez un nouveau mod√®le personnalis√©. Cr√©er un mod√®le personnalis√© et y attacher votre base de connaissances indique √† OpenWebUI de rechercher automatiquement dans votre collection de documents et d'inclure les segments les plus pertinents comme contexte chaque fois que vous posez une question.

![Capture d'√©cran de la page "Espace de travail" (apr√®s avoir cliqu√© sur "espace de travail" dans le panneau lat√©ral), mettant en √©vidence le premier onglet/option dans le menu sup√©rieur nomm√© "Mod√®les", qui, lorsqu'on clique dessus, affiche la liste des mod√®les personnalis√©s et une option pour en cr√©er de nouveaux (dans ce cas, l'utilisateur en a cr√©√© un appel√© "Gemma-custom-knowledge")](https://cdn.hashnode.com/res/hashnode/image/upload/v1758302593445/b5316a4a-8c8a-4348-a31e-1c10fe0e1abb.png align="center")

Ici, assurez-vous de s√©lectionner votre mod√®le (dans mon cas ¬´ gemma3:1b ¬ª) et d'attacher votre base de connaissances.

![Capture d'√©cran de la page de cr√©ation de mod√®le, mettant en √©vidence les options s√©lectionnables sous le champ "Mod√®le de base (de)", mettant sp√©cifiquement en √©vidence "gemma3:1b" ou le mod√®le de choix, sous l'option s√©lectionn√©e par d√©faut "s√©lectionner un mod√®le de base". Le deuxi√®me √©l√©ment mis en √©vidence en rouge est l'autre champ ci-dessous intitul√© "Connaissance", avec un bouton appel√© "S√©lectionner la connaissance". Il y a 2 autres √©l√©ments mis en √©vidence en jaune (indiquant une priorit√© plus faible) : le premier est "Param√®tres du mod√®le" qui comprend un champ de saisie "instruction syst√®me" juste en dessous, et l'autre est "Filtres" qui comprend plusieurs options s√©lectionnables en fonction des diff√©rents plugins ou "fonctions" install√©s.](https://cdn.hashnode.com/res/hashnode/image/upload/v1758302604758/df0c7948-bb9b-4615-8f09-21faaa64fdde.png align="center")

![Capture d'√©cran montrant les options disponibles apr√®s avoir cliqu√© sur "S√©lectionner la connaissance" sous "Connaissance", mettant en √©vidence l'option qui dit "COLLECTION" en vert suivie du titre "Test-knowledge-base" (exemple de titre choisi par l'auteur) et de la description ajout√©e par l'auteur ("ajout de mes documents")](https://cdn.hashnode.com/res/hashnode/image/upload/v1758302612285/8247d1c3-5f84-42de-9861-34416d0b7f10.png align="center")

### (Optionnel) Ajouter une instruction syst√®me

Lors de la cr√©ation de votre mod√®le personnalis√© dans **Espace de travail ‚Üí Mod√®les**, vous pouvez d√©finir une **instruction syst√®me** (system prompt) que le mod√®le utilisera pour le contexte tout au long de vos conversations.

Voici quelques exemples d'informations que vous pourriez vouloir ajouter :

* contexte sur vous-m√™me *("Je suis un √©tudiant de 20 ans en bio-ing√©nierie int√©ress√© par...")*
    
* votre style de communication pr√©f√©r√© *("sans fioritures", "sois direct", "sois analytique"...)*
    
* contexte sur la structure de vos donn√©es
    

**Exemple d'instruction syst√®me :**

> Tu es un assistant r√©fl√©chi et analytique qui m'aide √† explorer des sch√©mas et des id√©es dans mes journaux personnels. Sois direct, √©vite les sp√©culations et distingue clairement les faits issus des documents de l'interpr√©tation.

Cette instruction s'appliquera automatiquement √† chaque discussion utilisant ce mod√®le personnalis√©, aidant √† garder des r√©ponses coh√©rentes et align√©es avec vos objectifs.

## Comment ex√©cuter votre LLM localement

Ouvrez maintenant une nouvelle discussion et assurez-vous de s√©lectionner votre mod√®le personnalis√© :

![Capture d'√©cran montrant la page "Nouvelle discussion" apr√®s avoir cliqu√© sur le symbole/bouton "+" (plus) √† c√¥t√© du nom du mod√®le personnalis√©. Elle montre les options affich√©es lors du clic sur le champ de saisie qui indique "Rechercher un mod√®le" comme espace r√©serv√©, et l'option mise en √©vidence √† l'int√©rieur est le nom du mod√®le personnalis√© (dans ce cas, l'auteur a choisi le nom "Gemma-custom-knowledge")](https://cdn.hashnode.com/res/hashnode/image/upload/v1758302621012/241f461c-acf6-41ae-b68d-ad187790aef4.png align="center")

Vous √™tes maintenant pr√™t √† discuter avec vos propres documents dans un environnement local priv√© !

<div data-node-type="callout">
<div data-node-type="callout-emoji">‚ö†Ô∏è</div>
<div data-node-type="callout-text"><strong>Note</strong> : Par d√©faut, l'interface/navigateur arr√™tera de diffuser la r√©ponse apr√®s cinq minutes, m√™me s'il continuera √† traiter votre requ√™te en arri√®re-plan. Cela signifie que si votre requ√™te prend plus de cinq minutes √† √™tre trait√©e, elle ne sera pas affich√©e sur le navigateur. Vous pouvez recharger la page et cliquer sur ¬´ continuer la r√©ponse ¬ª pour obtenir le dernier r√©sultat.</div>
</div>

<div data-node-type="callout">
<div data-node-type="callout-emoji">üí°</div>
<div data-node-type="callout-text">Je recommande d'installer la fonction (plugin) <a target="_self" rel="noopener noreferrer nofollow" href="https://openwebui.com/f/alexgrama7/enhanced_context_tracker_v4" style="pointer-events: none">Enhanced Context Tracker</a> pour avoir plus de visibilit√© sur la progression de votre requ√™te.</div>
</div>

## Conclusion

Vous disposez maintenant d'une pile LLM priv√©e (**Ollama** pour les mod√®les, **OpenWebUI** pour l'interface utilisateur et **nomic-embed-text** pour les incorporations) reli√©e √† votre base de connaissances sur disque. Vos journaux et documents professionnels restent locaux ; rien n'est envoy√© √† des tiers. Les r√©glages principaux sont simples : choisissez un mod√®le adapt√© √† votre mat√©riel, activez la m√©moire et la r√©cup√©ration du contexte complet, utilisez un d√©coupage/chevauchement raisonnable, et v√©rifiez la console lorsque les ex√©cutions stagnent.

Si vous avez besoin de plus de puissance, d√©ployez la m√™me configuration sur votre propre serveur tout en conservant les garanties de confidentialit√©. √Ä partir de l√†, it√©rez sur le choix du mod√®le, le d√©coupage et les instructions, et ajoutez les fonctions optionnelles si vous avez besoin d'une visibilit√© plus profonde lors de travaux longs.