---
title: Comment construire des agents autonomes en utilisant le cha√Ænage de prompts
  avec des primitives d'IA (sans frameworks)
subtitle: ''
author: Maham Codes
co_authors: []
series: null
date: '2025-04-21T15:22:42.999Z'
originalURL: https://freecodecamp.org/news/build-autonomous-agents-using-prompt-chaining-with-ai-primitives
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1745248868960/12efd5ab-3d9b-4c93-979f-45bde796639b.png
tags:
- name: llm
  slug: llm
- name: ai-agent
  slug: ai-agent
- name: AI
  slug: ai
seo_title: Comment construire des agents autonomes en utilisant le cha√Ænage de prompts
  avec des primitives d'IA (sans frameworks)
seo_desc: 'Autonomous agents might sound complex, but they don‚Äôt have to be. These
  are AI systems that can make decisions and take actions on their own to achieve
  a goal ‚Äì usually by using LLMs, various tools, and memory to reason through a task.

  You can build ...'
---

Les agents autonomes peuvent sembler complexes, mais ils ne le sont pas n√©cessairement. Ce sont des syst√®mes d'IA capables de prendre des d√©cisions et d'agir de mani√®re autonome pour atteindre un objectif, g√©n√©ralement en utilisant des LLMs, divers outils et une m√©moire pour raisonner √† travers une t√¢che.

Vous pouvez construire des syst√®mes agentiques puissants sans frameworks lourds ni moteurs d'orchestration. L'une des m√©thodes les plus simples et efficaces consiste √† utiliser les architectures agentiques de Langbase (construites avec des primitives d'IA qui ne n√©cessitent pas de framework pour d√©ployer des syst√®mes agentiques d'IA scalables).

Dans cet article, nous allons explorer l'une des architectures agentiques de Langbase : le cha√Ænage de prompts. Nous verrons pourquoi c'est utile et comment l'impl√©menter en construisant un agent de cha√Ænage de prompts.

### Table des mati√®res

1. [Pr√©requis](#heading-prerequisites)

2. [Primitives d'IA (architecture agentique)](#heading-ai-primitives-agentic-architecture)

3. [Qu'est-ce que le cha√Ænage de prompts ?](#heading-quest-ce-que-le-chainage-de-prompts)

4. [Architecture de cha√Ænage de prompts](#heading-architecture-de-chainage-de-prompts)

5. [SDK Langbase](#heading-langbase-sdk)

6. [Construction d'un agent de cha√Ænage de prompts en utilisant Langbase Pipes](#heading-construction-dun-agent-de-chainage-de-prompts-en-utilisant-httpslangbasecomlangbase-pipes)

   * [√âtape 1 : Installation de votre projet](#heading-etape-1-installation-de-votre-projet)

   * [√âtape 2 : Obtenir la cl√© API Langbase](#heading-etape-2-obtenir-la-cle-api-langbase)

   * [√âtape 3 : Ajouter les cl√©s API LLM](#heading-etape-3-ajouter-les-cles-api-llm)

   * [√âtape 4 : Ajouter la logique dans le fichier prompt-chaining.ts](#heading-etape-4-ajouter-la-logique-dans-le-fichier-prompt-chainingts)

   * [√âtape 5 : Ex√©cuter le fichier](#heading-etape-5-executer-le-fichier)

7. [Le r√©sultat](#heading-le-resultat)

## Pr√©requis

Avant de commencer √† cr√©er un agent de cha√Ænage de prompts, vous devez avoir les √©l√©ments suivants pr√™ts.

Dans ce tutoriel, j'utiliserai la pile technologique suivante :

* [Langbase](http://langbase.com/) ‚Äì la plateforme pour construire et d√©ployer vos agents IA serverless.

* [SDK Langbase](https://langbase.com/docs/sdk) ‚Äì un SDK IA TypeScript, con√ßu pour fonctionner avec JavaScript, TypeScript, Node.js, Next.js, React, et autres.

* [OpenAI](https://openai.com/) ‚Äì pour obtenir la cl√© LLM pour le mod√®le pr√©f√©r√©.

Vous devrez √©galement :

* Vous inscrire sur Langbase pour obtenir acc√®s √† la cl√© API.

* Vous inscrire sur OpenAI pour g√©n√©rer la cl√© LLM pour le mod√®le que vous souhaitez utiliser (pour cette d√©monstration, j'utiliserai le mod√®le `openai:gpt-4o-mini`). Vous pouvez g√©n√©rer la cl√© [ici](https://platform.openai.com/api-keys).

## Primitives d'IA (Architecture Agentique)

Une approche au niveau des primitives d'IA signifie construire des syst√®mes d'IA en utilisant les blocs de construction les plus basiques, sans d√©pendre d'abstractions lourdes, de moteurs d'orchestration ou de frameworks complets.

Les agents Pipe et Memory de Langbase servent de blocs de construction.

Les [agents Pipe](https://langbase.com/docs/pipe) sur Langbase sont diff√©rents des autres agents. Ce sont des agents IA serverless avec des outils agentiques qui peuvent fonctionner avec n'importe quel langage ou framework. Les agents Pipe sont facilement d√©ployables, et avec une seule API, ils vous permettent de connecter 250+ LLMs √† n'importe quelle donn√©e pour construire n'importe quel workflow d'API de d√©veloppeur.

Les [agents m√©moire de Langbase](https://langbase.com/docs/memory) (solution de m√©moire √† long terme) sont con√ßus pour acqu√©rir, traiter, conserver et r√©cup√©rer des informations de mani√®re transparente. Ils attachent dynamiquement des donn√©es priv√©es √† n'importe quel LLM, permettant des r√©ponses contextuelles en temps r√©el et r√©duisant les hallucinations. La m√©moire, lorsqu'elle est connect√©e √† un agent pipe, devient un agent m√©moire.

Avec ces blocs de construction (primitives d'IA), vous pouvez construire des workflows agentiques complets. Pour cela, les architectures agentiques de Langbase servent de mod√®le pour construire, d√©ployer et mettre √† l'√©chelle des agents autonomes.

Examinons l'une des architectures agentiques : le cha√Ænage de prompts.

## Qu'est-ce que le cha√Ænage de prompts ?

Le cha√Ænage de prompts est une architecture d'agent o√π une t√¢che est d√©compos√©e en une s√©quence de prompts. Chaque √©tape transmet sa sortie √† la suivante, permettant au LLM de g√©rer des workflows plus complexes avec une plus grande pr√©cision.

Cela est particuli√®rement utile pour des t√¢ches structur√©es comme :

* La synth√®se et l'analyse de documents

* La g√©n√©ration de contenu en plusieurs √©tapes

* La transformation et le nettoyage de donn√©es

* La validation et l'affinage de contenu

Plut√¥t que de compter sur un seul prompt pour tout faire, vous divisez le travail en √©tapes cibl√©es. Cela facilite le d√©bogage, am√©liore la qualit√© de la sortie et introduit des "points de contr√¥le" naturels dans votre workflow IA.

## Architecture de cha√Ænage de prompts

Voici une architecture de r√©f√©rence expliquant le workflow :

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXchmBDXvU8DXnQu7EjqKoSUTdxQ__KsTZemZ9yaTGpeCAMUc1RX_Swby9NOtxXwONFdKGPrjFcjVZhQmQoKe1eu2nceFWGLaPA8bpu-JYB7rh4ChJmExLRRWJzjB4686HjUsP_t?key=l4b_IFG3ufUXGX7WLcs4Dknq align="left")

Ce diagramme est une r√©f√©rence visuelle pour montrer comment le cha√Ænage de prompts peut √™tre utilis√© pour construire un syst√®me agentique l√©ger en utilisant simplement des appels LLM et une logique conditionnelle, sans aucun framework lourd.

Voici une d√©composition de ce qui se passe dans le flux :

1. **Entr√©e ‚Üí Appel LLM**

   * Prend l'entr√©e initiale et ex√©cute le premier appel LLM.

   * Produit la Sortie 1.

2. **Porte**

   * √âvalue la Sortie 1 pour d√©cider de l'√©tape suivante.

   * Agit comme un point de contr√¥le conditionnel (par exemple, succ√®s/√©chec, validation d'intention, seuil de confiance).

3. **Si la Porte passe :**

   * Passe √† l'Appel LLM 2 avec la Sortie 1 comme entr√©e.

   * L'Appel LLM 2 produit la Sortie 2.

   * La Sortie 2 va dans l'Appel LLM 3, qui g√©n√®re le r√©sultat final.

   * La sortie finale s'√©coule vers la Sortie.

4. **Si la Porte √©choue :**

   * Le flux se termine pr√©matur√©ment √† la Sortie.

   * √âvite les appels LLM suppl√©mentaires, √©conomisant du calcul et √©vitant les sorties invalides.

## SDK Langbase

Le SDK Langbase facilite la construction d'agents IA puissants en utilisant TypeScript. Il vous donne tout ce dont vous avez besoin pour travailler avec n'importe quel LLM, connecter vos propres mod√®les d'embedding, g√©rer la m√©moire des documents et construire des agents IA capables de raisonner et de r√©pondre.

Le SDK est con√ßu pour fonctionner avec Node.js, Next.js, React ou toute pile JavaScript moderne. Vous pouvez l'utiliser pour t√©l√©charger des documents, cr√©er une m√©moire s√©mantique et ex√©cuter des workflows IA (appel√©s agents Pipes) avec seulement quelques lignes de code.

Langbase est une plateforme IA API-first, et son SDK TypeScript simplifie l'exp√©rience, rendant facile de commencer sans avoir √† g√©rer l'infrastructure. Il suffit d'ajouter votre cl√© API, d'√©crire votre logique, et vous √™tes pr√™t √† partir.

Maintenant que vous connaissez le SDK Langbase, commen√ßons √† construire l'agent de cha√Ænage de prompts.

## Construction d'un agent de cha√Ænage de prompts en utilisant Langbase Pipes

Parcourons un syst√®me agentique r√©el de cha√Ænage de prompts construit en utilisant les agents Pipe de Langbase (agents IA serverless avec des APIs unifi√©es pour chaque LLM). Pour cela, nous allons configurer un projet Node.js de base.

Nous allons impl√©menter un pipeline de contenu marketing de produit s√©quentiel qui transforme une description brute de produit en une copie marketing polie √† travers trois √©tapes (c'est-√†-dire la cr√©ation de trois agents Pipe) :

### Premi√®re √©tape (Agent de synth√®se) :

* Prend une description brute de produit

* La condense en deux phrases concises

* A une porte de qualit√© qui v√©rifie si le r√©sum√© est suffisamment d√©taill√© (au moins 10 mots)

### Deuxi√®me √©tape (Agent de fonctionnalit√©s) :

* Prend le r√©sum√© de l'√©tape 1

* Extrait et formate les fonctionnalit√©s cl√©s du produit sous forme de puces

### √âtape finale (Agent de copie marketing) :

* Prend les puces de l'√©tape 2

* G√©n√®re une copie marketing raffin√©e pour le produit

Toutes les √©tapes utiliseront le mod√®le OpenAI 4o-mini via le SDK Langbase. Le meilleur, c'est que vous pouvez utiliser diff√©rents mod√®les LLM pour chaque √©tape/cr√©ation d'agent Pipe √©galement.

Ce qui rend cela int√©ressant, c'est son approche par pipeline. Chaque √©tape s'appuie sur la sortie de l'√©tape pr√©c√©dente, avec une v√©rification de qualit√© apr√®s l'√©tape de synth√®se pour garantir que le pipeline maintient des normes √©lev√©es.

Commen√ßons par la cr√©ation de ce syst√®me agentique de cha√Ænage de prompts.

### √âtape 1 : Installation de votre projet

Je vais construire une application Node.js de base en TypeScript qui utilise le SDK Langbase pour cr√©er un syst√®me agentique de cha√Ænage de prompts scalable. Il fonctionnera sans aucun framework, suivant une approche au niveau des primitives d'IA.

Pour commencer, cr√©ez un nouveau r√©pertoire pour votre projet et naviguez jusqu'√† lui :

```bash
mkdir agentic-architecture && cd agentic-architecture
```

Ensuite, initialisez un projet Node.js et cr√©ez un fichier TypeScript en ex√©cutant cette commande dans votre terminal :

```bash
npm init -y && touch prompt-chaining.ts
```

Le fichier `prompt-chaining.ts` contiendra le code de toutes les cr√©ations d'agents.

Apr√®s cela, nous utiliserons le SDK Langbase pour cr√©er les agents et `dotenv` pour g√©rer les variables d'environnement. Installons donc ces d√©pendances.

```bash
npm i langbase dotenv
```

### √âtape 2 : Obtenir la cl√© API Langbase

Chaque requ√™te que vous envoyez √† Langbase n√©cessite une cl√© API. Vous pouvez g√©n√©rer des cl√©s API depuis le [Langbase studio](https://studio.langbase.com/) en suivant ces √©tapes :

1. Passez √† votre compte utilisateur ou organisation.

2. Dans la barre lat√©rale, cliquez sur le menu `Param√®tres`.

3. Dans la section des param√®tres d√©veloppeur, cliquez sur le lien `Cl√©s API Langbase`.

4. √Ä partir de l√†, vous pouvez cr√©er une nouvelle cl√© API ou g√©rer celles existantes.

Pour plus de d√©tails, vous pouvez consulter la documentation des cl√©s API Langbase.

Apr√®s avoir g√©n√©r√© la cl√© API, cr√©ez un fichier `.env` √† la racine de votre projet et ajoutez votre cl√© API Langbase :

```bash
LANGBASE_API_KEY=xxxxxxxxx
```

Remplacez xxxxxxxxx par votre cl√© API Langbase.

### √âtape 3 : Ajouter les cl√©s API LLM

Une fois que vous avez la cl√© API Langbase, vous aurez √©galement besoin de la cl√© LLM pour ex√©cuter l'agent RAG. Si vous avez configur√© des cl√©s API LLM dans votre profil, la m√©moire IA et le pipe de l'agent les utiliseront automatiquement. Sinon, naviguez vers la page des cl√©s API LLM et ajoutez des cl√©s pour diff√©rents fournisseurs comme OpenAI, Anthropic, etc.

Suivez ces √©tapes pour ajouter les cl√©s LLM :

1. Ajoutez les cl√©s API LLM dans votre compte en utilisant le studio Langbase

2. Passez √† votre compte utilisateur ou organisation.

3. Dans la barre lat√©rale, cliquez sur le menu `Param√®tres`.

4. Dans la section des param√®tres d√©veloppeur, cliquez sur le lien `Cl√©s API LLM`.

5. √Ä partir de l√†, vous pouvez ajouter des cl√©s API LLM pour diff√©rents fournisseurs comme OpenAI, TogetherAI, Anthropic, etc.

### √âtape 4 : Ajouter la logique dans le fichier `prompt-chaining.ts`

Dans le fichier `prompt-chaining.ts` que vous avez cr√©√© √† l'√©tape 1, ajoutez le code suivant :

```typescript
import dotenv from 'dotenv';
import { Langbase } from 'langbase';


dotenv.config();


const langbase = new Langbase({
   apiKey: process.env.LANGBASE_API_KEY!
});


async function main(inputText: string) {
   // √âtapes de cha√Ænage de prompts
   const steps = [
       {
           name: `summary-agent-${Date.now()}`,
           model: 'openai:gpt-4o-mini',
           description:
               'r√©sumer la description du produit en deux phrases concises',
           prompt: `Veuillez r√©sumer la description de produit suivante en deux phrases
           concises :\n`
       },
       {
           name: `features-agent-${Date.now()}`,
           model: 'openai:gpt-4o-mini',
           description: 'extraire les caract√©ristiques cl√©s du produit sous forme de puces',
           prompt: `Sur la base du r√©sum√© suivant, listez les caract√©ristiques cl√©s du produit sous
           forme de puces :\n`
       },
       {
           name: `marketing-copy-agent-${Date.now()}`,
           model: 'openai:gpt-4o-mini',
           description:
               'g√©n√©rer une copie marketing polie en utilisant les puces',
           prompt: `En utilisant les puces suivantes des caract√©ristiques du produit, g√©n√©rez une
           copie marketing convaincante et raffin√©e pour le produit, soyez pr√©cis :\n`
       }
   ];


   //  Cr√©er les agents pipe
   await Promise.all(
       steps.map(step =>
           langbase.pipes.create({
               name: step.name,
               model: step.model,
               messages: [
                   {
                       role: 'system',
                       content: `Vous √™tes un assistant utile qui peut ${step.description}.`
                   }
               ]
           })
       )
   );


   // Initialiser les donn√©es avec l'entr√©e brute.
   let data = inputText;


   try {
       // Traiter chaque √©tape du workflow de mani√®re s√©quentielle.
       for (const step of steps) {
           // Appeler le LLM pour l'√©tape actuelle.
           const response = await langbase.pipes.run({
               stream: false,
               name: step.name,
               messages: [{ role: 'user', content: `${step.prompt} ${data}` }]
           });


           data = response.completion;


           console.log(`√âtape : ${step.name} \n\n R√©ponse : ${data}`);


           // Porte sur la sortie de l'agent de synth√®se pour s'assurer qu'elle n'est pas trop br√®ve.
           // Si le r√©sum√© fait moins de 10 mots, lever une erreur pour arr√™ter le workflow.
           if (step.name === 'summary-agent' && data.split(' ').length < 10) {
               throw new Error(
                   'Porte d√©clench√©e pour l\'agent de synth√®se. Le r√©sum√© est trop bref. Sortie du workflow.'
               );
               return;
           }
       }
   } catch (error) {
       console.error('Erreur dans le workflow principal :', error);
   }


   // La copie marketing finale raffin√©e
   console.log('Copie Marketing de Produit Finale Raffin√©e :', data);
}


const inputText = `Notre nouvelle montre intelligente est un appareil polyvalent dot√© d'un √©cran haute r√©solution,
une longue dur√©e de vie de la batterie, un suivi de la condition physique et une connectivit√© smartphone. Elle est con√ßue pour
un usage quotidien et est r√©sistante √† l'eau. Avec des capteurs de pointe et un design √©l√©gant, elle est
parfaite pour les individus passionn√©s de technologie.`;


main(inputText);
```

Voici une d√©composition du code ci-dessus :

Configuration et initialisation :

* `dotenv` charge les variables `env` depuis le fichier `.env` pour un acc√®s s√©curis√© √† la cl√© API.

* Langbase est import√© depuis le SDK pour interagir avec l'API.

* Une instance cliente Langbase est cr√©√©e en utilisant votre cl√© API.

D√©finir les √©tapes IA (cha√Æne de prompts) :

* Trois agents IA (√©tapes) sont d√©finis pour un pipeline :

   1. **Agent de Synth√®se** : R√©sume l'entr√©e de la description du produit en 2 phrases.

   2. **Agent d'Extraction de Fonctionnalit√©s** : Extrait les fonctionnalit√©s cl√©s du r√©sum√© sous forme de puces.

   3. **Agent de Copie Marketing** : Transforme les puces en une copie marketing polie.

* Chaque agent utilise `openai:gpt-4o-mini` comme LLM.

Cr√©er les Pipes Langbase (agents) :

* Les pipes Langbase sont cr√©√©s pour chaque √©tape en utilisant `langbase.pipes.create(...)`.

* Chaque pipe a un nom unique (horodat√©) et un message syst√®me guidant son but.

Ex√©cuter le workflow (traitement s√©quentiel) :

* Le texte d'entr√©e circule √† travers chaque √©tape une par une :

   * La sortie d'une √©tape devient l'entr√©e pour la suivante.

   * Les pipes sont ex√©cut√©s en utilisant `langbase.pipes.run(...)`.

* Les sorties interm√©diaires sont journalis√©es apr√®s chaque √©tape.

V√©rification de validation (contr√¥le) :

* Si la sortie du r√©sum√© est trop courte (moins de 10 mots), le workflow s'arr√™te avec une erreur.

Sortie finale :

* Apr√®s toutes les √©tapes, le r√©sultat final est une copie marketing raffin√©e imprim√©e sur la console.

Pour cet article, nous utilisons une description de produit de montre intelligente de d√©monstration pour voir le r√©sultat dans le champ `inputText`.

### √âtape 5 : Ex√©cuter le fichier

Pour ex√©cuter le fichier `prompt-chaining.ts` afin de voir les r√©sultats, vous devez :

* Ajouter TypeScript comme d√©pendance

* Ajouter un script pour ex√©cuter les fichiers TypeScript

* Ajouter un fichier de configuration TypeScript

Pour cela, installons d'abord `pnpm` en ex√©cutant cette commande dans votre terminal :

```bash
pnpm install
```

Ensuite, dans votre terminal, ex√©cutez cette commande pour ajouter les d√©pendances et fichiers de configuration pertinents :

```bash
pnpm add -D typescript ts-node @types/node
```

Apr√®s cela, cr√©ez un fichier de configuration TypeScript `tsconfig.json` :

```bash
pnpm exec tsc --init
```

Et mettez √† jour le `package.json` pour ajouter le script pertinent. Voici √† quoi votre `package.json` devrait ressembler apr√®s la mise √† jour :

```json
{
 "name": "agentic-architectures",
 "version": "1.0.0",
 "main": "index.js",
 "scripts": {
   "test": "echo \"Error: no test specified\" && exit 1",
   "prompt-chaining": "ts-node prompt-chaining.ts"
 },
 "keywords": [],
 "author": "",
 "license": "ISC",
 "description": "",
 "dependencies": {
   "dotenv": "^16.5.0",
   "langbase": "^1.1.55"
 },
 "devDependencies": {
   "@types/node": "^22.14.1",
   "ts-node": "^10.9.2",
   "typescript": "^5.8.3"
 }
}
```

Maintenant, ex√©cutons le projet avec pnpm run prompt-chaining

## Le r√©sultat

Apr√®s avoir ex√©cut√© le projet, vous verrez le r√©sultat de l'exemple de description de produit de montre intelligente dans votre console comme suit :

```bash
√âtape : summarize-description
R√©ponse : Cette montre intelligente combine le suivi de la condition physique et la connectivit√© smartphone avec un √©cran haute r√©solution et une batterie longue dur√©e. Con√ßue pour un usage quotidien avec une construction √©l√©gante et r√©sistante √† l'eau, elle est id√©ale pour les passionn√©s de technologie.

√âtape : extract-features
R√©ponse : Voici les caract√©ristiques cl√©s du produit extraites du r√©sum√© :

Suivi de la condition physique
Connectivit√© smartphone
√âcran haute r√©solution
Batterie longue dur√©e
Design √©l√©gant
Construction r√©sistante √† l'eau
Con√ßue pour un usage quotidien
√âtape : refine-marketing-copy
R√©ponse : ## √âlevez votre quotidien avec une connectivit√© transparente et des performances in√©gal√©es.

D√©couvrez la fusion parfaite du style et de la fonctionnalit√© avec notre appareil r√©volutionnaire, con√ßu pour s'int√©grer de mani√®re transparente dans votre mode de vie actif. Restez motiv√© et inform√© avec le suivi complet de la condition physique, tout en restant facilement connect√© via la connectivit√© smartphone.

Plongez dans une clart√© vibrante avec l'√©cran haute r√©solution √©poustouflant, et traversez votre journ√©e sans interruption gr√¢ce √† la batterie longue dur√©e. Encadr√© dans un design √©l√©gant, cet appareil est aussi styl√© que pratique.

Con√ßu pour r√©sister aux rigueurs de la vie quotidienne, la construction r√©sistante √† l'eau garantit un port sans souci, qu'il pleuve ou qu'il fasse beau. Con√ßu pour le confort et la performance, cet appareil est con√ßu pour un usage quotidien, vous permettant de vivre votre meilleure vie, sans effort.
```

C'est ainsi que vous pouvez construire un syst√®me agentique de cha√Ænage de prompts avec des primitives d'IA (sans framework) en utilisant le SDK Langbase et les architectures agentiques de Langbase.

Merci d'avoir lu !

Connectez-vous avec moi par üëå :

* En vous abonnant √† ma cha√Æne [YouTube](https://www.youtube.com/@AIwithMahamCodes). Si vous souhaitez apprendre sur l'IA et les agents.

* En vous abonnant √† ma newsletter gratuite ["The Agentic Engineer"](https://mahamcodes.substack.com/) o√π je partage toutes les derni√®res nouvelles/tendances/emplois en IA et agents, et bien plus encore.

* Suivez-moi sur [X (Twitter)](https://x.com/MahamDev).