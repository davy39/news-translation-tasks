---
title: Comment construire des agents IA RAG avec TypeScript
subtitle: ''
author: Maham Codes
co_authors: []
series: null
date: '2025-04-16T14:45:59.795Z'
originalURL: https://freecodecamp.org/news/how-to-build-rag-ai-agents-with-typescript
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1744814746615/72626297-def9-466a-8c1a-2cdb1b411300.png
tags:
- name: 'RAG '
  slug: rag
- name: ai agents
  slug: ai-agents
- name: llm
  slug: llm
- name: TypeScript
  slug: typescript
seo_title: Comment construire des agents IA RAG avec TypeScript
seo_desc: The most powerful AI systems don‚Äôt just generate ‚Äì they also retrieve, reason,
  and respond with context. Retrieval-Augmented Generation (RAG) is how we get there.
  It combines the strengths of search and generation to build more accurate, reliable,
  an...
---

Les syst√®mes IA les plus puissants ne se contentent pas de g√©n√©rer - ils r√©cup√®rent √©galement, raisonnent et r√©pondent avec contexte. La g√©n√©ration augment√©e par r√©cup√©ration (RAG) est la m√©thode pour y parvenir. Elle combine les forces de la recherche et de la g√©n√©ration pour construire des syst√®mes IA plus pr√©cis, fiables et conscients du contexte.

Dans ce guide, vous allez construire un agent IA bas√© sur RAG en TypeScript en utilisant le SDK Langbase. Vous allez int√©grer vos propres donn√©es comme m√©moire, utiliser n'importe quel mod√®le d'embedding, r√©cup√©rer le contexte pertinent et appeler un LLM pour g√©n√©rer une r√©ponse pr√©cise.

√Ä la fin de ce tutoriel, vous aurez un syst√®me RAG fonctionnel qui :

* Stocke et r√©cup√®re des documents avec une m√©moire s√©mantique

* Utilise des embeddings personnalis√©s pour la recherche vectorielle

* G√®re les requ√™tes utilisateur avec un contexte pertinent

* G√©n√®re des r√©ponses via OpenAI, Anthropic ou n'importe quel LLM

### Table des mati√®res

* [Pr√©requis](#heading-prerequisites)

* [Qu'est-ce que le RAG agentique ?](#heading-quest-ce-que-le-rag-agentique)

* [SDK Langbase](#heading-langbase-sdk)

* [√âtape 1 : Installation de votre projet](#heading-etape-1-installation-de-votre-projet)

* [√âtape 2 : Obtenir la cl√© API Langbase](#heading-etape-2-obtenir-la-cle-api-langbase)

* [√âtape 3 : Ajouter les cl√©s API LLM](#heading-etape-3-ajouter-les-cles-api-llm)

* [√âtape 4 : Cr√©er une m√©moire IA agentique](#heading-etape-4-creer-une-memoire-ia-agentique)

* [√âtape 5 : Ajouter des documents √† la m√©moire IA](#heading-etape-5-ajouter-des-documents-a-la-memoire-ia)

* [√âtape 6 : Effectuer une r√©cup√©ration RAG](#heading-etape-6-effectuer-une-recuperation-rag)

* [√âtape 7 : Cr√©er un agent pipe de support](#heading-etape-7-creer-un-agent-pipe-de-support)

* [√âtape 8 : G√©n√©rer des r√©ponses RAG](#heading-etape-8-generer-des-reponses-rag)

* [Le r√©sultat](#heading-le-resultat)

## Pr√©requis

Avant de commencer √† cr√©er un agent IA bas√© sur RAG, vous devrez avoir certains outils pr√™ts √† l'emploi.

Dans ce tutoriel, j'utiliserai la pile technologique suivante :

* [Langbase](http://langbase.com/) - la plateforme pour construire et d√©ployer vos agents IA serverless.

* [SDK Langbase](https://langbase.com/docs/sdk) - un SDK IA TypeScript, con√ßu pour fonctionner avec JavaScript, TypeScript, Node.js, Next.js, React, et similaires.

* [OpenAI](https://openai.com/) - pour obtenir la cl√© LLM pour le mod√®le pr√©f√©r√©.

Vous devrez √©galement :

* Vous inscrire sur Langbase pour obtenir acc√®s √† la cl√© API.

* Vous inscrire sur OpenAI pour g√©n√©rer la cl√© LLM pour le mod√®le que vous souhaitez utiliser (pour cette d√©monstration, j'utiliserai le mod√®le `openai:text-embedding-3-large`). Vous pouvez g√©n√©rer la cl√© [ici](https://platform.openai.com/api-keys).

## Qu'est-ce que le RAG agentique ?

La g√©n√©ration augment√©e par r√©cup√©ration (RAG) est une architecture pour optimiser les performances d'un mod√®le d'intelligence artificielle (IA) en le connectant avec des bases de connaissances externes. Le RAG aide les grands mod√®les de langage (LLM) √† fournir des r√©ponses plus pertinentes avec une qualit√© sup√©rieure.

Lorsque nous utilisons des agents IA pour faciliter le RAG, cela devient le **RAG agentique**. Les syst√®mes RAG agentiques ajoutent des agents IA au pipeline RAG pour augmenter l'adaptabilit√© et la pr√©cision. Compar√© aux syst√®mes RAG traditionnels, le RAG agentique permet aux LLM de conduire la r√©cup√©ration d'informations √† partir de multiples sources et de g√©rer des flux de travail plus complexes.

Voici la comparaison tabulaire du RAG vs RAG agentique :

| **Fonctionnalit√©** | **RAG** | **RAG agentique** |
| --- | --- | --- |
| Complexit√© des t√¢ches | T√¢ches de requ√™te simples - pas de prise de d√©cision complexe | G√®re des t√¢ches complexes et multi-√©tapes en utilisant plusieurs outils et agents |
| Prise de d√©cision | Limit√©e - pas d'autonomie | Les agents d√©cident quoi r√©cup√©rer, comment noter, raisonner, r√©fl√©chir et g√©n√©rer |
| Raisonnement multi-√©tapes | Requ√™tes et r√©ponses en une seule √©tape uniquement | Supporte le raisonnement multi-√©tapes avec r√©cup√©ration, notation, filtrage et √©valuation |
| R√¥le cl√© | LLM + donn√©es externes pour les r√©ponses | Ajoute des agents intelligents pour la r√©cup√©ration, la g√©n√©ration, la critique et l'orchestration |
| R√©cup√©ration de donn√©es en temps r√©el | Non support√© | Con√ßu pour la r√©cup√©ration en temps r√©el et l'int√©gration dynamique |
| Int√©gration de la r√©cup√©ration | Bases de donn√©es vectorielles statiques et pr√©d√©finies | Les agents r√©cup√®rent dynamiquement √† partir de sources diverses et flexibles |
| Conscience du contexte | Contexte statique - pas d'adaptabilit√© √† l'ex√©cution | √âlev√©e - les agents s'adaptent aux requ√™tes, extraient le contexte pertinent et r√©cup√®rent des donn√©es en direct si n√©cessaire |

## SDK Langbase

Le SDK Langbase facilite la construction d'outils IA puissants en utilisant TypeScript. Il vous donne tout ce dont vous avez besoin pour travailler avec n'importe quel LLM, connecter vos propres mod√®les d'embedding, g√©rer la m√©moire des documents et construire des agents IA capables de raisonner et de r√©pondre.

Le SDK est con√ßu pour fonctionner avec Node.js, Next.js, React ou n'importe quelle pile JavaScript moderne. Vous pouvez l'utiliser pour t√©l√©charger des documents, cr√©er une m√©moire s√©mantique et ex√©cuter des flux de travail IA (appel√©s agents Pipe) avec seulement quelques lignes de code.

Langbase est une plateforme IA API-first. Son SDK TypeScript simplifie l'exp√©rience, rendant facile de commencer sans avoir √† g√©rer l'infrastructure. Il suffit d'ajouter votre cl√© API, d'√©crire votre logique, et vous √™tes pr√™t √† partir.

Maintenant que vous connaissez le SDK Langbase, commen√ßons √† construire l'agent RAG.

## √âtape 1 : Installation de votre projet

Nous allons construire une application Node.js basique en TypeScript qui utilise le SDK Langbase pour cr√©er un syst√®me RAG agentique. Pour cela, cr√©ez un nouveau r√©pertoire pour votre projet et naviguez jusqu'√† lui.

```bash
mkdir agentic-rag && cd agentic-rag
```

Ensuite, initialisez un projet Node.js et cr√©ez diff√©rents fichiers TypeScript en ex√©cutant cette commande dans votre terminal :

```bash
npm init -y && touch index.ts agents.ts create-memory.ts upload-docs.ts create-pipe.ts
```

Voici une description de ce que chaque fichier fera dans le projet :

* **index.ts** : Il s'agit g√©n√©ralement du point d'entr√©e d'un projet TypeScript. Il orchestrera la cr√©ation de l'agent, la configuration de la m√©moire et le t√©l√©chargement des documents.

* **agents.ts** : Ce fichier g√®re la cr√©ation et la configuration des agents IA.

* **create-memory.ts** : Ce fichier configure la m√©moire Langbase (RAG) pour le stockage et la r√©cup√©ration du contexte.

* **upload-docs.ts** : Ce fichier t√©l√©chargera des documents vers la m√©moire afin que les agents puissent y acc√©der et les utiliser.

* **create-pipe.ts** : Ce fichier configure un [agent Pipe Langbase](https://langbase.com/docs/pipe/quickstart) qui est un agent IA serverless avec des API unifi√©es pour chaque LLM.

Apr√®s cela, nous utiliserons le SDK Langbase pour cr√©er des agents RAG et `dotenv` pour g√©rer les variables d'environnement. Alors, installons ces d√©pendances.

```bash
npm i langbase dotenv
```

## √âtape 2 : Obtenir la cl√© API Langbase

Chaque requ√™te que vous envoyez √† Langbase n√©cessite une cl√© API. Vous pouvez g√©n√©rer des cl√©s API √† partir du [studio Langbase](https://studio.langbase.com/) en suivant ces √©tapes :

1. Passez √† votre compte utilisateur ou organisation.

2. Dans la barre lat√©rale, cliquez sur le menu `Param√®tres`.

3. Dans la section des param√®tres d√©veloppeur, cliquez sur le lien `Cl√©s API Langbase`.

4. √Ä partir de l√†, vous pouvez cr√©er une nouvelle cl√© API ou g√©rer celles existantes.

Pour plus de d√©tails, consultez la documentation des cl√©s API Langbase.

Apr√®s avoir g√©n√©r√© la cl√© API, cr√©ez un fichier `.env` √† la racine de votre projet et ajoutez votre cl√© API Langbase :

```bash
LANGBASE_API_KEY=xxxxxxxxx
```

Remplacez xxxxxxxxx par votre cl√© API Langbase.

## √âtape 3 : Ajouter les cl√©s API LLM

Une fois que vous avez la cl√© API Langbase, vous aurez √©galement besoin de la cl√© LLM pour ex√©cuter l'agent RAG. Si vous avez configur√© des cl√©s API LLM dans votre profil, la m√©moire IA et l'agent pipe les utiliseront automatiquement. Sinon, naviguez vers la page des cl√©s API LLM et ajoutez des cl√©s pour diff√©rents fournisseurs comme OpenAI, Anthropic, etc.

Suivez ces √©tapes pour ajouter les cl√©s LLM :

1. Ajoutez les cl√©s API LLM dans votre compte en utilisant le studio Langbase

2. Passez √† votre compte utilisateur ou organisation.

3. Dans la barre lat√©rale, cliquez sur le menu `Param√®tres`.

4. Dans la section des param√®tres d√©veloppeur, cliquez sur le lien `Cl√©s API LLM`.

5. √Ä partir de l√†, vous pouvez ajouter des cl√©s API LLM pour diff√©rents fournisseurs comme OpenAI, TogetherAI, Anthropic, etc.

## √âtape 4 : Cr√©er une m√©moire IA agentique

Utilisons maintenant le SDK Langbase pour cr√©er une m√©moire IA (agent de m√©moire Langbase) o√π votre agent peut stocker et r√©cup√©rer du contexte.

Les agents de m√©moire serverless Langbase (solution de m√©moire √† long terme) sont con√ßus pour acqu√©rir, traiter, retenir et r√©cup√©rer des informations de mani√®re transparente. Ils attachent dynamiquement des donn√©es priv√©es √† n'importe quel LLM, permettant des r√©ponses conscientes du contexte en temps r√©el et r√©duisant les hallucinations.

Ces agents combinent le stockage vectoriel, le RAG et l'acc√®s √† Internet pour cr√©er une API de recherche de contexte g√©r√©e puissante. Vous pouvez les utiliser pour construire des applications IA plus intelligentes et plus capables.

Dans une configuration RAG, la m√©moire - lorsqu'elle est connect√©e directement √† un agent Pipe Langbase - devient un agent de m√©moire. Ce couplage donne au LLM la capacit√© de r√©cup√©rer des donn√©es pertinentes et de fournir des r√©ponses pr√©cises et contextuellement exactes - r√©pondant aux limitations des LLM lorsqu'il s'agit de g√©rer des donn√©es priv√©es.

Pour la cr√©er, ajoutez le code suivant au fichier `create-memory.ts` que vous avez cr√©√© √† l'√©tape 1 :

```typescript
import 'dotenv/config';
import {Langbase} from 'langbase';

const langbase = new Langbase({
	apiKey: process.env.LANGBASE_API_KEY!,
});

async function main() {
	const memory = await langbase.memories.create({
		name: 'knowledge-base',
		description: 'Une m√©moire IA pour l\'atelier de m√©moire agentique',
		embedding_model: 'openai:text-embedding-3-large'
	});

	console.log('M√©moire IA:', memory);
}

main();
```

Voici ce qui se passe dans le code ci-dessus :

* Importe le package `dotenv` pour charger les variables d'environnement.

* Importe la classe `Langbase` du package langbase.

* Cr√©e une nouvelle instance de la classe Langbase avec votre cl√© API.

* Utilise la m√©thode `memories.create` pour cr√©er une nouvelle m√©moire IA.

* D√©finit le nom et la description de la m√©moire.

* Utilise le mod√®le `openai:text-embedding-3-large` pour l'embedding.

* Affiche la m√©moire cr√©√©e dans la console.

Apr√®s cela, cr√©ons la m√©moire agentique en ex√©cutant le fichier `create-memory.ts`.

```bash
npx tsx create-memory.ts
```

Cela cr√©era une m√©moire IA et affichera les d√©tails de la m√©moire dans la console.

## √âtape 5 : Ajouter des documents √† la m√©moire IA

Maintenant que vous avez cr√©√© un agent de m√©moire IA, l'√©tape suivante consiste √† ajouter des documents. Ces documents serviront de contexte que votre agent pourra r√©f√©rencer lors des interactions.

Tout d'abord, cr√©ez un r√©pertoire docs √† la racine de votre projet et ajoutez deux fichiers texte d'exemple :

* [agent-architectures.txt](https://langbase.com/docs/examples/agent-architectures)

* [langbase-faq.txt](http://langbase.com/docs)

Ensuite, ouvrez le fichier `upload-docs.ts` cr√©√© √† l'√©tape 1 et collez le code suivant :

```typescript
import 'dotenv/config';
import { Langbase } from 'langbase';
import { readFile } from 'fs/promises';
import path from 'path';

const langbase = new Langbase({
	apiKey: process.env.LANGBASE_API_KEY!,
});

async function main() {
	const cwd = process.cwd();
	const memoryName = 'knowledge-base';

	// T√©l√©charger le document d'architecture de l'agent
	const agentArchitecture = await readFile(path.join(cwd, 'docs', 'agent-architectures.txt'));
	const agentResult = await langbase.memories.documents.upload({
		memoryName,
		contentType: 'text/plain',
		documentName: 'agent-architectures.txt',
		document: agentArchitecture,
		meta: { category: 'Examples', topic: 'Agent architecture' },
	});

	console.log(agentResult.ok ? '‚úì Document de l\'agent t√©l√©charg√©' : '‚úó √âchec du t√©l√©chargement du document de l\'agent');

	// T√©l√©charger le document FAQ
	const langbaseFaq = await readFile(path.join(cwd, 'docs', 'langbase-faq.txt'));
	const faqResult = await langbase.memories.documents.upload({
		memoryName,
		contentType: 'text/plain',
		documentName: 'langbase-faq.txt',
		document: langbaseFaq,
		meta: { category: 'Support', topic: 'FAQ Langbase' },
	});

	console.log(faqResult.ok ? '‚úì Document FAQ t√©l√©charg√©' : '‚úó √âchec du t√©l√©chargement du document FAQ');
}

main();
```

D√©composons ce qui se passe dans ce code :

* `dotenv/config` est utilis√© pour charger les variables d'environnement √† partir de votre fichier .env.

* Langbase est import√© du SDK pour interagir avec l'API.

* `readFile` du module `fs/promises` lit chaque fichier de document de mani√®re asynchrone.

* `path.join()` garantit que les chemins de fichiers fonctionnent sur diff√©rents syst√®mes d'exploitation.

* Une instance cliente Langbase est cr√©√©e en utilisant votre cl√© API.

* `memories.documents.upload` est utilis√© pour t√©l√©charger chaque fichier .txt vers la m√©moire IA.

* Chaque t√©l√©chargement inclut des m√©tadonn√©es comme `category` et `topic` pour aider √† organiser le contenu.

* Le succ√®s ou l'√©chec du t√©l√©chargement est enregistr√© dans la console.

Cette √©tape garantit que votre agent IA aura un contenu r√©el √† extraire - FAQ, documents d'architecture ou tout autre contenu que vous t√©l√©chargez en m√©moire.

Ensuite, ex√©cutez le fichier `upload-docs.ts` pour t√©l√©charger les documents vers la m√©moire IA en utilisant cette commande dans votre terminal. Cela t√©l√©chargera les documents vers la m√©moire IA :

```bash
npx tsx upload-docs.ts
```

## √âtape 6 : Effectuer une r√©cup√©ration RAG

Dans cette √©tape, nous allons effectuer une r√©cup√©ration RAG contre une requ√™te en utilisant les documents que nous avons t√©l√©charg√©s vers la m√©moire IA.

Ajoutez le code suivant √† votre fichier `agents.ts` cr√©√© √† l'√©tape 1 :

```typescript
import 'dotenv/config';
import { Langbase } from 'langbase';

const langbase = new Langbase({
	apiKey: process.env.LANGBASE_API_KEY!,
});

export async function runMemoryAgent(query: string) {
	const chunks = await langbase.memories.retrieve({
		query,
		topK: 4,
		memory: [
			{
				name: 'knowledge-base',
			},
		],
	});

	return chunks;
}
```

D√©composons ce que cela fait :

* Importe la classe `Langbase` du SDK Langbase.

* Initialise le client Langbase en utilisant votre cl√© API √† partir des variables d'environnement.

* D√©finit une fonction asynchrone `runMemoryAgent` qui prend une cha√Æne de requ√™te en entr√©e.

* Utilise `memories.retrieve` pour interroger la m√©moire afin d'obtenir les chunks les plus pertinents, en r√©cup√©rant les 4 meilleurs r√©sultats (`topK: 4`) de la m√©moire nomm√©e "knowledge-base".

* Retourne les chunks de m√©moire r√©cup√©r√©s.

Maintenant, ajoutons le code suivant au fichier `index.ts` cr√©√© √† l'√©tape 1 pour ex√©cuter l'agent de m√©moire :

```typescript
import { runMemoryAgent } from './agents';

async function main() {
	const chunks = await runMemoryAgent('Qu\'est-ce que la parall√©lisation des agents ?');
	console.log('Chunk de m√©moire:', chunks);
}

main();
```

Ce code ex√©cute une requ√™te de m√©moire Langbase pour ¬´ Qu'est-ce que la parall√©lisation des agents ? ¬ª. Il utilise `runMemoryAgent` pour r√©cup√©rer les chunks correspondants de votre m√©moire IA et enregistre les r√©sultats. C'est ainsi que vous r√©cup√©rez des connaissances pertinentes avec RAG.

Apr√®s cela, ex√©cutez le fichier `index.ts` pour effectuer une r√©cup√©ration RAG contre la requ√™te en utilisant cette commande dans le terminal :

```bash
npx tsx index.ts
```

Vous verrez la sortie de l'agent de m√©moire sous forme de chunks de m√©moire r√©cup√©r√©s dans la console comme suit :

```bash
[
  {
    text: '---\n' +
      '\n' +
      '## Parall√©lisation des agents\n' +
      '\n' +
      'La parall√©lisation ex√©cute plusieurs t√¢ches LLM en m√™me temps pour am√©liorer la vitesse ou la pr√©cision. Elle fonctionne en divisant une t√¢che en parties ind√©pendantes (sectionnement) ou en g√©n√©rant plusieurs r√©ponses pour comparaison (vote).\n' +
      '\n' +
      'Le vote est une m√©thode de parall√©lisation o√π plusieurs appels LLM g√©n√®rent diff√©rentes r√©ponses pour la m√™me t√¢che. Le meilleur r√©sultat est s√©lectionn√© en fonction de l\'accord, de r√®gles pr√©d√©finies ou de l\'√©valuation de la qualit√©, am√©liorant la pr√©cision et la fiabilit√©.\n' +
      '\n' +
      "`Ce code impl√©mente un syst√®me d\'analyse d\'emails qui traite les emails entrants via plusieurs agents IA parall√®les pour d√©terminer si et comment ils doivent √™tre trait√©s. Voici la d√©composition :",
    similarity: 0.7146744132041931,
    meta: {
      docName: 'agent-architectures.txt',
      documentName: 'agent-architectures.txt',
      category: 'Examples',
      topic: 'Agent architecture'
    }
  },
  {
    text: 'async function main(inputText: string) {\n' +
      '\ttry {\n' +
      '\t\t// Cr√©er les pipes d\'abord\n' +
      '\t\tawait createPipes();\n' +
      '\n' +
      '\t\t// √âtape A : D√©terminer vers quel agent router\n' +
      '\t\tconst route = await routerAgent(inputText);\n' +
      "\t\tconsole.log('D√©cision du routeur:', route);\n" +
      '\n' +
      '\t\t// √âtape B : Appeler l\'agent appropri√©\n' +
      '\t\tconst agent = agentConfigs[route.agent];\n' +
      '\n' +
      '\t\tconst response = await langbase.pipes.run({\n' +
      '\t\t\tstream: false,\n' +
      '\t\t\tname: agent.name,\n' +
      '\t\t\tmessages: [\n' +
      "\t\t\t\t{ role: 'user', content: `${agent.prompt} ${inputText}` }\n" +
      '\t\t\t]\n' +
      '\t\t});\n' +
      '\n' +
      '\t\t// Sortie finale\n' +
      '\t\tconsole.log(\n' +
      '\t\t\t`Agent: ${agent.name} \\n\\n R√©ponse: ${response.completion}`\n' +
      '\t\t);\n' +
      '\t} catch (error) {\n' +
      "\t\tconsole.error('Erreur dans le flux de travail principal:', error);\n" +
      '\t}\n' +
      '}\n' +
      '\n' +
      '// Exemple d\'utilisation:\n' +
      "const inputText = 'Pourquoi les jours sont-ils plus courts en hiver ?';\n" +
      '\n' +
      'main(inputText);\n' +
      '```\n' +
      '\n' +
      '\n' +
      '---\n' +
      '\n' +
      '## Parall√©lisation des agents\n' +
      '\n' +
      'La parall√©lisation ex√©cute plusieurs t√¢ches LLM en m√™me temps pour am√©liorer la vitesse ou la pr√©cision. Elle fonctionne en divisant une t√¢che en parties ind√©pendantes (sectionnement) ou en g√©n√©rant plusieurs r√©ponses pour comparaison (vote).',
    similarity: 0.5911030173301697,
    meta: {
      docName: 'agent-architectures.txt',
      documentName: 'agent-architectures.txt',
      category: 'Examples',
      topic: 'Agent architecture'
    }
  },
  {
    text: "`Ce code impl√©mente un syst√®me d\'orchestration de t√¢ches sophistiqu√© avec g√©n√©ration dynamique de sous-t√¢ches et traitement parall√®le. Voici comment cela fonctionne :\n" +
      '\n' +
      '1. Agent Orchestrateur (Phase de planification) :\n' +
      '   - Prend une t√¢che complexe en entr√©e\n' +
      '   - Analyse la t√¢che et la d√©compose en sous-t√¢ches plus petites et g√©rables\n' +
      '   - Retourne √† la fois une analyse et une liste de sous-t√¢ches au format JSON\n' +
      '\n' +
      '2. Agents Travailleurs (Phase d\'ex√©cution) :\n' +
      '   - Plusieurs travailleurs s\'ex√©cutent en parall√®le en utilisant Promise.all()\n' +
      '   - Chaque travailleur re√ßoit :\n' +
      '     - La t√¢che originale pour le contexte\n' +
      '     - Leur sous-t√¢che sp√©cifique √† accomplir\n' +
      '   - Tous les travailleurs utilisent le mod√®le Gemini 2.0 Flash\n' +
      '\n' +
      '3. Agent Synth√©tiseur (Phase d\'int√©gration) :\n' +
      '   - Prend toutes les sorties des travailleurs\n' +
      '   - Les combine en un r√©sultat final coh√©sif\n' +
      '   - Assure que les morceaux s\'encha√Ænent naturellement',
    similarity: 0.5393730401992798,
    meta: {
      docName: 'agent-architectures.txt',
      documentName: 'agent-architectures.txt',
      category: 'Examples',
      topic: 'Agent architecture'
    }
  },
  {
    text: "`Ce code impl√©mente un syst√®me d\'analyse d\'emails qui traite les emails entrants via plusieurs agents IA parall√®les pour d√©terminer si et comment ils doivent √™tre trait√©s. Voici la d√©composition :\n" +
      '\n' +
      '1. Trois agents sp√©cialis√©s s\'ex√©cutant en parall√®le :\n' +
      '   - Agent d\'analyse de sentiment : D√©termine si le ton de l\'email est positif, n√©gatif ou neutre\n' +
      '   - Agent de r√©sum√© : Cr√©e un r√©sum√© concis du contenu de l\'email\n' +
      '   - Agent d√©cideur : Prend les sorties des autres agents et d√©cide :\n' +
      '     - Si l\'email n√©cessite une r√©ponse\n' +
      "     - S\'il s\'agit de spam\n" +
      '     - Niveau de priorit√© (faible, moyen, √©lev√©, urgent)\n' +
      '\n' +
      '2. Le flux de travail :\n' +
      '   - Prend un email en entr√©e\n' +
      '   - Ex√©cute l\'analyse de sentiment et la g√©n√©ration de r√©sum√© en parall√®le en utilisant Promise.all()\n' +
      '   - Alimente ces r√©sultats √† l\'agent d√©cideur\n' +
      '   - Produit un objet de d√©cision final avec les exigences de r√©ponse\n' +
      '\n' +
      '3. Tous les agents utilisent le mod√®le Gemini 2.0 Flash et sont structur√©s pour retourner des r√©ponses JSON analys√©es',
    similarity: 0.49115753173828125,
    meta: {
      docName: 'agent-architectures.txt',
      documentName: 'agent-architectures.txt',
      category: 'Examples',
      topic: 'Agent architecture'
    }
  }
]
```

## √âtape 7 : Cr√©er un agent Pipe de support

Dans cette √©tape, nous allons cr√©er un agent de support en utilisant le SDK Langbase. Ajoutez le code suivant au fichier `create-pipe.ts` cr√©√© √† l'√©tape 1 :

```typescript
import 'dotenv/config';
import { Langbase } from 'langbase';

const langbase = new Langbase({
	apiKey: process.env.LANGBASE_API_KEY!,
});

async function main() {
	const supportAgent = await langbase.pipes.create({
		name: `ai-support-agent`,
		description: `Un agent IA pour aider les utilisateurs avec leurs requ√™tes.`,
		messages: [
			{
				role: `system`,
				content: `Vous √™tes un assistant IA utile.
				Vous aiderez les utilisateurs avec leurs requ√™tes.
				Assurez-vous toujours de fournir des informations pr√©cises et pertinentes.`,
			},
		],
	});

	console.log('Agent de support:', supportAgent);
}

main();
```

Passons en revue le code ci-dessus :

* Initialise le SDK Langbase avec votre cl√© API.

* Utilise la m√©thode `pipes.create` pour cr√©er un nouvel agent pipe.

* Affiche l'agent pipe cr√©√© dans la console.

Maintenant, ex√©cutez le fichier `create-pipe.ts` pour cr√©er l'agent pipe en utilisant cette commande dans votre terminal :

```bash
npx tsx create-pipe.ts
```

Cela cr√©era un agent de support et affichera les d√©tails de l'agent dans la console.

## √âtape 8 : G√©n√©rer des r√©ponses RAG

Jusqu'√† pr√©sent, nous avons cr√©√© un agent de m√©moire Langbase, ajout√© des documents, effectu√© une r√©cup√©ration RAG contre une requ√™te et cr√©√© un agent de support en utilisant l'agent Pipe Langbase. La seule chose restante dans la cr√©ation de cet agent RAG complet est la g√©n√©ration de r√©ponses compl√®tes en utilisant les LLM.

Pour ce faire, ajoutez le code suivant au fichier `agents.ts` cr√©√© √† l'√©tape 1 :

```typescript
import 'dotenv/config';
import { Langbase, MemoryRetrieveResponse } from 'langbase';

const langbase = new Langbase({
	apiKey: process.env.LANGBASE_API_KEY!,
});

export async function runAiSupportAgent({
	chunks,
	query,
}: {
	chunks: MemoryRetrieveResponse[];
	query: string;
}) {
	const systemPrompt = await getSystemPrompt(chunks);

	const { completion } = await langbase.pipes.run({
		stream: false,
		name: 'ai-support-agent',
		messages: [
			{
				role: 'system',
				content: systemPrompt,
			},
			{
				role: 'user',
				content: query,
			},
		],
	});

	return completion;
}

async function getSystemPrompt(chunks: MemoryRetrieveResponse[]) {
	let chunksText = '';
	for (const chunk of chunks) {
		chunksText += chunk.text + '\n';
	}

	const systemPrompt = `
	Vous √™tes un assistant IA utile.
	Vous aiderez les utilisateurs avec leurs requ√™tes.

	Assurez-vous toujours de fournir des informations pr√©cises et pertinentes.
	Ci-dessous se trouve un CONTEXTE pour vous aider √† r√©pondre aux questions. R√©pondez UNIQUEMENT √† partir du CONTEXTE. Le CONTEXTE se compose de plusieurs chunks d'informations. Chaque chunk a une source mentionn√©e √† la fin.

Pour chaque partie de r√©ponse que vous fournissez, citez la source entre crochets comme ceci : [1].

√Ä la fin de la r√©ponse, listez toujours chaque source avec son num√©ro correspondant et fournissez le nom du document. Comme ceci [1] NomDuFichier.doc. S'il y a une URL, faites-en un hyperlien sur le nom.

Si vous ne connaissez pas la r√©ponse, dites-le. Demandez plus de contexte si n√©cessaire.
	${chunksText}`;

	return systemPrompt;
}

export async function runMemoryAgent(query: string) {
	const chunks = await langbase.memories.retrieve({
		query,
		topK: 4,
		memory: [
			{
				name: 'knowledge-base',
			},
		],
	});

	return chunks;
}
```

Le code ci-dessus :

* Cr√©e une fonction `runAiSupportAgent` qui prend des chunks et une requ√™te en entr√©e.

* Utilise la m√©thode `pipes.run` pour g√©n√©rer des r√©ponses en utilisant le LLM.

* Cr√©e une fonction `getSystemPrompt` pour g√©n√©rer une invite syst√®me pour le LLM.

* Combine les chunks r√©cup√©r√©s pour cr√©er une invite syst√®me.

* Retourne la compl√©tion g√©n√©r√©e.

Maintenant, ex√©cutons l'agent de support avec les chunks de m√©moire IA. Ajoutez le code suivant au fichier `index.ts` :

```typescript
import { runMemoryAgent, runAiSupportAgent } from './agents';

async function main() {
	const query = 'Qu\'est-ce que la parall√©lisation des agents ?';
	const chunks = await runMemoryAgent(query);

	const completion = await runAiSupportAgent({
		chunks,
		query,
	});

	console.log('Compl√©tion:', completion);
}

main();
```

Ce code ex√©cute deux agents : l'un pour r√©cup√©rer les chunks de m√©moire pertinents pour une requ√™te (`runMemoryAgent`), et un autre (`runAiSupportAgent`) pour g√©n√©rer une r√©ponse finale en utilisant ces chunks.

Ensuite, ex√©cutez le fichier `index.ts` pour g√©n√©rer des r√©ponses en utilisant le LLM.

```bash
npx tsx index.ts
```

### Le r√©sultat

Apr√®s avoir ex√©cut√© l'agent de support, vous verrez la sortie suivante g√©n√©r√©e dans votre console :

```bash
Compl√©tion : La parall√©lisation des agents est un processus qui ex√©cute plusieurs t√¢ches LLM (mod√®le de langage) simultan√©ment pour am√©liorer la vitesse ou la pr√©cision. Cette technique peut √™tre mise en ≈ìuvre de deux mani√®res principales :

1. **Sectionnement** : Une t√¢che est divis√©e en parties ind√©pendantes qui peuvent √™tre trait√©es simultan√©ment.
2. **Vote** : Plusieurs appels LLM g√©n√®rent diff√©rentes r√©ponses pour la m√™me t√¢che, et le meilleur r√©sultat est s√©lectionn√© en fonction de l'accord, de r√®gles pr√©d√©finies ou de l'√©valuation de la qualit√©. Cette approche am√©liore la pr√©cision et la fiabilit√© en comparant diverses sorties.

En pratique, la parall√©lisation des agents implique l'orchestration de plusieurs agents sp√©cialis√©s pour g√©rer diff√©rents aspects d'une t√¢che, permettant un traitement efficace et des r√©sultats am√©lior√©s.

Si vous avez besoin d'exemples plus d√©taill√©s ou d'√©claircissements suppl√©mentaires, n'h√©sitez pas √† demander !
```

C'est ainsi que vous pouvez construire un syst√®me RAG agentique avec TypeScript en utilisant le SDK Langbase.

Merci d'avoir lu !

Connectez-vous avec moi par üëã :

* En vous abonnant √† ma cha√Æne [YouTube](https://www.youtube.com/@AIwithMahamCodes) si vous voulez apprendre sur l'IA et les agents.

* En vous abonnant √† ma newsletter gratuite [The Agentic Engineer](https://mahamcodes.substack.com/) o√π je partage toutes les derni√®res nouvelles/tendances/emplois sur l'IA et les agents et bien plus encore.

* Suivez-moi sur [X (Twitter)](https://x.com/MahamDev).