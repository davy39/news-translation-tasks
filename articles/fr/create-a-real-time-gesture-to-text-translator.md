---
title: Comment cr√©er un traducteur de gestes en texte en temps r√©el avec Python et
  Mediapipe
subtitle: ''
author: OMOTAYO OMOYEMI
co_authors: []
series: null
date: '2025-08-18T14:00:13.783Z'
originalURL: https://freecodecamp.org/news/create-a-real-time-gesture-to-text-translator
coverImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1755525484024/9f4c42e0-dbfd-4f04-9223-0a2169abd1fb.png
tags:
- name: Computer Vision
  slug: computer-vision
- name: Accessibility
  slug: accessibility
- name: Machine Learning
  slug: machine-learning
seo_title: Comment cr√©er un traducteur de gestes en texte en temps r√©el avec Python
  et Mediapipe
seo_desc: 'Sign and symbol languages, like Makaton and American Sign Language (ASL),
  are powerful communication tools. However, they can create challenges when communicating
  with people who don''t understand them.

  As a researcher working on AI for accessibility,...'
---

Les langages de signes et de symboles, comme le Makaton et l'American Sign Language (ASL), sont de puissants outils de communication. Cependant, ils peuvent poser des d√©fis lors de la communication avec des personnes qui ne les comprennent pas.

En tant que chercheur travaillant sur l'IA pour l'accessibilit√©, j'ai voulu explorer comment l'apprentissage automatique (machine learning) et la vision par ordinateur pourraient combler ce foss√©. Le r√©sultat est un traducteur de gestes en texte en temps r√©el construit avec Python et Mediapipe, capable de d√©tecter les gestes de la main et de les convertir instantan√©ment en texte.

Dans ce tutoriel, vous apprendrez √† construire votre propre version √† partir de z√©ro, m√™me si vous n'avez jamais utilis√© Mediapipe auparavant.

√Ä la fin, vous saurez comment :

* D√©tecter et suivre les mouvements de la main en temps r√©el.
    
* Classifier les gestes √† l'aide d'un mod√®le simple d'apprentissage automatique.
    
* Convertir les gestes reconnus en texte.
    
* √âtendre le syst√®me pour des applications ax√©es sur l'accessibilit√©.
    

## Pr√©requis

Avant de suivre ce tutoriel, vous devriez avoir :

* **Des connaissances de base en Python** ‚Äì Vous devez √™tre √† l'aise pour √©crire et ex√©cuter des scripts Python.
    
* **Une familiarit√© avec la ligne de commande** ‚Äì Vous l'utiliserez pour ex√©cuter des scripts et installer des d√©pendances.
    
* **Une webcam fonctionnelle** ‚Äì N√©cessaire pour capturer et reconna√Ætre les gestes en temps r√©el.
    
* **Python install√© (3.8 ou plus r√©cent)** ‚Äì Avec `pip` pour l'installation des paquets.
    
* **Une certaine compr√©hension des bases du machine learning** ‚Äì Savoir ce que sont les donn√©es d'entra√Ænement et les mod√®les vous aidera, mais j'expliquerai les parties cl√©s en cours de route.
    
* **Une connexion internet** ‚Äì Pour installer les biblioth√®ques telles que Mediapipe et OpenCV.
    

Si vous √™tes compl√®tement nouveau sur Mediapipe ou OpenCV, ne vous inqui√©tez pas, je passerai en revue les parties essentielles que vous devez conna√Ætre pour faire fonctionner ce projet.

## Table des mati√®res

* [Pr√©requis](#heading-pr√©requis)
    
* [Pourquoi cela est important](#heading-pourquoi-cela-est-important)
    
* [Outils et technologies](#heading-outils-et-technologies)
    
* [√âtape 1 : Comment installer les biblioth√®ques requises](#heading-√©tape-1-comment-installer-les-biblioth√®ques-requises)
    
* [√âtape 2 : Comment Mediapipe suit les mains](#heading-√©tape-2-comment-mediapipe-suit-les-mains)
    
* [√âtape 3 : Pipeline du projet](#heading-√©tape-3-pipeline-du-projet)
    
* [√âtape 4 : Comment collecter les donn√©es de gestes](#heading-√©tape-4-comment-collecter-les-donn√©es-de-gestes)
    
* [√âtape 5 : Comment entra√Æner un classificateur de gestes](#heading-√©tape-5-comment-entra√Æner-un-classificateur-de-gestes)
    
* [√âtape 6 : Traduction de gestes en texte en temps r√©el](#heading-√©tape-6-traduction-de-gestes-en-texte-en-temps-r√©el)
    
* [√âtape 7 : √âtendre le projet](#heading-√©tape-7-√©tendre-le-projet)
    
* [Consid√©rations √©thiques et d'accessibilit√©](#heading-consid√©rations-√©thiques-et-daccessibilit√©)
    
* [Conclusion](#heading-conclusion)
    

## Pourquoi cela est important

La communication accessible est un droit, pas un privil√®ge. Les traducteurs de gestes en texte peuvent :

* Aider les personnes n'utilisant pas la langue des signes √† communiquer avec les utilisateurs de langages de signes/symboles.
    
* Aider dans les contextes √©ducatifs pour les enfants ayant des difficult√©s de communication.
    
* Soutenir les personnes ayant des troubles de l'√©locution.
    

**Note :** Ce projet est une preuve de concept et doit √™tre test√© avec des ensembles de donn√©es diversifi√©s avant un d√©ploiement en conditions r√©elles.

## Outils et technologies

Nous utiliserons :

| Outil | Objectif |
| --- | --- |
| **Python** | Langage de programmation principal |
| **Mediapipe** | Suivi des mains et d√©tection de gestes en temps r√©el |
| **OpenCV** | Entr√©e webcam et affichage vid√©o |
| **NumPy** | Traitement des donn√©es |
| **Scikit-learn** | Classification des gestes |

## √âtape 1 : Comment installer les biblioth√®ques requises

Avant d'installer les d√©pendances, assurez-vous d'avoir install√© la version 3.8 de Python ou une version sup√©rieure (par exemple, Python 3.8, 3.9, 3.10 ou plus r√©cent). Vous pouvez v√©rifier votre version actuelle de Python en ouvrant un terminal (Invite de commandes sur Windows, ou Terminal sur macOS/Linux) et en tapant :

```bash
python --version
```

ou

```bash
python3 --version
```

Vous devez confirmer que votre version de Python est 3.8 ou sup√©rieure car Mediapipe et certaines d√©pendances n√©cessitent des fonctionnalit√©s de langage modernes et des binary wheels. Si les commandes ci-dessus affichent une version ant√©rieure √† 3.8, vous devrez installer une version plus r√©cente de Python avant de continuer.

**Windows :**

1. Appuyez sur **Touche Windows + R**
    
2. Tapez `cmd` et appuyez sur Entr√©e pour ouvrir l'invite de commandes
    
3. Tapez l'une des commandes ci-dessus et appuyez sur Entr√©e
    

**macOS/Linux :**

1. Ouvrez votre application **Terminal**
    
2. Tapez l'une des commandes ci-dessus et appuyez sur Entr√©e
    

Si votre version de Python est ant√©rieure √† 3.8, vous devrez [t√©l√©charger et installer une version plus r√©cente sur le site officiel de Python](https://www.python.org/downloads/).

Une fois que Python est pr√™t, vous pouvez installer les biblioth√®ques requises √† l'aide de `pip` :

```bash
pip install mediapipe opencv-python numpy scikit-learn pandas
```

Cette commande installe toutes les biblioth√®ques dont vous aurez besoin pour le projet :

* **Mediapipe** ‚Äì suivi des mains et d√©tection des points de rep√®re (landmarks) en temps r√©el.
    
* **OpenCV** ‚Äì lecture des images de votre webcam et dessin des superpositions.
    
* **Pandas** ‚Äì stockage des donn√©es de points de rep√®re collect√©es dans un fichier CSV pour l'entra√Ænement.
    
* **Scikit-learn** ‚Äì entra√Ænement et √©valuation du mod√®le de classification des gestes.
    

## √âtape 2 : Comment Mediapipe suit les mains

La solution Hand Tracking de Mediapipe d√©tecte 21 points de rep√®re (landmarks) cl√©s pour chaque main, y compris le bout des doigts, les articulations et le poignet, jusqu'√† **30+ FPS**, m√™me sur du mat√©riel modeste.

Voici un diagramme conceptuel des points de rep√®re :

![Diagramme montrant la num√©rotation des points de rep√®re de la main Mediapipe et les connexions entre les articulations](https://github.com/tayo4christ/Gesture_Article/blob/7598826bb530d5bd1cd40251d6f56f35653b6b51/images/landmarks_concept.png?raw=true align="left")

Et voici √† quoi ressemble le suivi en temps r√©el :

![GIF anim√© montrant le suivi de main en 3D de Mediapipe d√©tectant les articulations et les os des doigts en temps r√©el](https://github.com/tayo4christ/Gesture_Article/blob/7598826bb530d5bd1cd40251d6f56f35653b6b51/images/hand_tracking_3d_android_gpu.gif?raw=true align="left")

Chaque point de rep√®re poss√®de des coordonn√©es `(x, y, z)` relatives √† la taille de l'image, ce qui facilite la mesure des angles et des positions pour la classification des gestes.

## √âtape 3 : Pipeline du projet

Voici comment le syst√®me fonctionne, de la webcam √† la sortie texte :

![Organigramme du pipeline montrant comment l'entr√©e gestuelle circule via le suivi de la main, l'extraction de caract√©ristiques, la classification des gestes et la sortie texte finale](https://github.com/tayo4christ/Gesture_Article/blob/7598826bb530d5bd1cd40251d6f56f35653b6b51/diagrams/pipeline_flowchart.png?raw=true align="left")

* **Capture** : Les images de la webcam sont captur√©es via OpenCV.
    
* **D√©tection** : Mediapipe localise les points de rep√®re de la main.
    
* **Vectorisation** : Les points de rep√®re sont aplatis en un vecteur num√©rique.
    
* **Classification** : Un mod√®le d'apprentissage automatique pr√©dit le geste.
    
* **Sortie** : Le geste reconnu est affich√© sous forme de texte.
    

Exemple de d√©tection de main de base :

```python
import cv2
import mediapipe as mp

mp_hands = mp.solutions.hands
mp_draw = mp.solutions.drawing_utils

cap = cv2.VideoCapture(0)

with mp_hands.Hands(max_num_hands=1) as hands:
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

        cv2.imshow("Hand Tracking", frame)
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

cap.release()
cv2.destroyAllWindows()
```

Le code ci-dessus ouvre la webcam et traite chaque image avec la solution Hands de Mediapipe. L'image est ensuite convertie en RGB (comme l'attend Mediapipe), lance la d√©tection et, si une main est trouv√©e, dessine les 21 points de rep√®re et leurs connexions sur l'image. Vous pouvez appuyer sur `q` pour fermer la fen√™tre. Ce fragment de code v√©rifie votre configuration et vous permet de voir que le suivi des points de rep√®re fonctionne avant de continuer.

## √âtape 4 : Comment collecter les donn√©es de gestes

Avant de pouvoir entra√Æner notre mod√®le, nous avons besoin d'un ensemble de donn√©es de **gestes √©tiquet√©s**. Chaque geste sera stock√© dans un fichier CSV (`gesture_data.csv`) contenant les coordonn√©es 3D des points de rep√®re pour tous les points de main d√©tect√©s.

Par exemple, nous collecterons des donn√©es pour trois gestes :

* **thumbs\_up** ‚Äì la pose classique du pouce lev√©.
    
* **open\_palm** ‚Äì une main plate, doigts tendus (comme un "high five").
    
* **ok** ‚Äì le signe "OK", form√© en touchant le pouce et l'index.
    

Vous pouvez collecter des √©chantillons pour chaque geste en ex√©cutant :

```bash
python src/collect_data.py --label thumbs_up --samples 200
```

```bash
python src/collect_data.py --label open_palm --samples 200
```

```bash
python src/collect_data.py --label ok --samples 200
```

**Explication de la commande :**

* `--label` ‚Üí le nom du geste que vous enregistrez. Cette √©tiquette sera stock√©e √† c√¥t√© de chaque ligne de coordonn√©es dans le CSV.
    
* `--samples` ‚Üí le nombre d'images √† capturer pour ce geste. Plus d'√©chantillons m√®nent g√©n√©ralement √† une meilleure pr√©cision.
    

**Comment fonctionne le processus :**

1. Lorsque vous lancez une commande, votre webcam s'ouvre.
    
2. Faites le geste sp√©cifi√© devant la cam√©ra.
    
3. Le script utilisera MediaPipe Hands pour d√©tecter les 21 points de rep√®re de la main (chacun avec des coordonn√©es `x`, `y`, `z`).
    
4. Ces 63 nombres (21 √ó 3) sont stock√©s dans une ligne du fichier CSV, avec l'√©tiquette du geste.
    
5. Le compteur en haut suivra le nombre d'√©chantillons collect√©s.
    
6. Lorsque le nombre d'√©chantillons atteint votre cible (`--samples`), le script se fermera automatiquement.
    

**Exemple de ce √† quoi ressemble le CSV :**

![Exemple de gesture_data.csv](https://raw.githubusercontent.com/tayo4christ/Gesture_Article/26db13366407e5b5d230a6c7dd7923e34a9f2a19/screenshots/gesture_data.webp align="left")

Chaque ligne contient :

* **x0, y0, z0 ‚Ä¶ x20, y20, z20** ‚Üí coordonn√©es de chaque point de rep√®re de la main.
    
* **label** ‚Üí le nom du geste.
    

**Exemple de collecte de donn√©es en cours :**

![Capture d'√©cran de l'interface de collecte de donn√©es capturant les points de rep√®re d'un geste de la main via webcam](https://github.com/tayo4christ/Gesture_Article/blob/7598826bb530d5bd1cd40251d6f56f35653b6b51/screenshots/detection_example.jpg?raw=true align="left")

Dans la capture d'√©cran ci-dessus, le script capture **10 sur 10** √©chantillons `thumbs_up`.

üìå **Astuce :** Assurez-vous que votre main est bien visible et bien √©clair√©e. R√©p√©tez le processus pour tous les gestes que vous souhaitez entra√Æner.

## √âtape 5 : Comment entra√Æner un classificateur de gestes

Une fois que vous avez assez d'√©chantillons pour chaque geste, entra√Ænez un mod√®le :

```bash
python src/train_model.py --data data/gesture_data.csv --label palm_open
```

Ce script :

* Charge l'ensemble de donn√©es CSV.
    
* Divise les donn√©es en ensembles d'entra√Ænement et de test.
    
* Entra√Æne un Random Forest Classifier.
    
* Affiche la pr√©cision et un rapport de classification.
    
* Sauvegarde le mod√®le entra√Æn√©.
    

Logique d'entra√Ænement principale :

```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import pickle

# Charger le dataset
df = pd.read_csv("data/gesture_data.csv")

# S√©parer les caract√©ristiques et les √©tiquettes
X = df.drop("label", axis=1)
y = df["label"]

# Initialiser et entra√Æner le Random Forest Classifier
model = RandomForestClassifier()
model.fit(X, y)

# Sauvegarder le mod√®le entra√Æn√© dans un fichier
with open("data/gesture_model.pkl", "wb") as f:
    pickle.dump(model, f)
```

Ce bloc charge l'ensemble de donn√©es de gestes depuis `data/gesture_data.csv` et le divise en :

* `X` ‚Äì les caract√©ristiques d'entr√©e (les coordonn√©es 3D des points de rep√®re pour chaque √©chantillon de geste).
    
* `y` ‚Äì les √©tiquettes (noms des gestes comme `thumbs_up`, `open_palm`, `ok`).
    

Nous avons ensuite cr√©√© un Random Forest Classifier, qui est bien adapt√© aux donn√©es num√©riques et fonctionne de mani√®re fiable sans trop de r√©glages. Le mod√®le apprend des motifs dans les positions des points de rep√®re qui correspondent √† chaque geste.  
Enfin, nous avons sauvegard√© le mod√®le entra√Æn√© sous le nom `data/gesture_model.pkl` afin qu'il puisse √™tre charg√© plus tard pour la reconnaissance de gestes en temps r√©el sans r√©entra√Ænement.

## √âtape 6 : Traduction de gestes en texte en temps r√©el

Chargez le mod√®le et lancez le traducteur :

```bash
python src/gesture_to_text.py --model data/gesture_model.pkl
```

Cette commande ex√©cute le script de reconnaissance de gestes en temps r√©el.

* L'argument `--model` indique au script quel fichier de mod√®le entra√Æn√© charger ‚Äî dans ce cas, `gesture_model.pkl` que nous avons sauvegard√© pr√©c√©demment.
    
* Une fois lanc√©, le script ouvre votre webcam, d√©tecte les points de rep√®re de votre main et utilise le mod√®le pour pr√©dire le geste.
    
* Le nom du geste pr√©dit appara√Æt sous forme de texte sur le flux vid√©o.
    
* Appuyez sur `q` pour quitter la fen√™tre quand vous avez termin√©.
    

Logique de pr√©diction principale :

```python
with open("data/gesture_model.pkl", "rb") as f:
    model = pickle.load(f)

if results.multi_hand_landmarks:
    for hand_landmarks in results.multi_hand_landmarks:
        coords = []
        for lm in hand_landmarks.landmark:
            coords.extend([lm.x, lm.y, lm.z])
        gesture = model.predict([coords])[0]
        cv2.putText(frame, gesture, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
```

Ce code charge le mod√®le de reconnaissance de gestes entra√Æn√© depuis `gesture_model.pkl`.  
Si des mains sont d√©tect√©es (`results.multi_hand_landmarks`), il boucle √† travers chaque main d√©tect√©e et :

1. **Extrait les coordonn√©es** ‚Äì pour chacun des 21 points de rep√®re, il ajoute les valeurs `x`, `y` et `z` √† la liste `coords`.
    
2. **Effectue une pr√©diction** ‚Äì transmet `coords` √† la m√©thode `predict` du mod√®le pour obtenir l'√©tiquette de geste la plus probable.
    
3. **Affiche le r√©sultat** ‚Äì utilise `cv2.putText` pour dessiner le nom du geste pr√©dit sur le flux vid√©o.
    

C'est l'√©tape de prise de d√©cision en temps r√©el qui transforme les donn√©es brutes des points de rep√®re de Mediapipe en une √©tiquette de geste lisible.

Vous devriez voir le geste reconnu en haut du flux vid√©o :

![Capture d'√©cran du r√©sultat de la reconnaissance de gestes en temps r√©el superposant l'√©tiquette 'palm_open' sur le flux vid√©o](https://github.com/tayo4christ/Gesture_Article/blob/7598826bb530d5bd1cd40251d6f56f35653b6b51/screenshots/text_output.jpg?raw=true align="left")

## √âtape 7 : √âtendre le projet

Vous pouvez aller plus loin avec ce projet en :

* **Ajoutant la synth√®se vocale** : Utilisez `pyttsx3` pour faire prononcer les mots reconnus.
    
* **Prenant en charge plus de gestes** : √âlargissez votre ensemble de donn√©es.
    
* **D√©ployant dans le navigateur** : Utilisez TensorFlow.js pour une reconnaissance bas√©e sur le Web.
    
* **Testant avec de vrais utilisateurs** : Particuli√®rement dans des contextes d'accessibilit√©.
    

## Consid√©rations √©thiques et d'accessibilit√©

Avant le d√©ploiement :

* **Diversit√© des donn√©es** : Entra√Ænez avec des gestes provenant de diff√©rentes teintes de peau, tailles de mains et conditions d'√©clairage.
    
* **Confidentialit√©** : Ne stockez que les coordonn√©es des points de rep√®re, sauf si vous avez le consentement pour le stockage vid√©o.
    
* **Contexte culturel** : Certains gestes ont des significations diff√©rentes selon les cultures.
    

## Conclusion

Dans ce tutoriel, nous avons explor√© comment utiliser Python, Mediapipe et l'apprentissage automatique pour construire un traducteur de gestes en texte en temps r√©el. Cette technologie pr√©sente un potentiel passionnant pour l'accessibilit√© et la communication inclusive et, avec un d√©veloppement plus pouss√©, pourrait devenir un outil puissant pour briser les barri√®res linguistiques.

Vous pouvez trouver le code complet et les ressources ici :

**D√©p√¥t GitHub** ‚Äì [Gesture\_Article](https://github.com/tayo4christ/Gesture_Article)