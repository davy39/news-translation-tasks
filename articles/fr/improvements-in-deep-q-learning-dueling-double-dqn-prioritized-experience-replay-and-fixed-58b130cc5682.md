---
title: 'Am√©liorations dans l''apprentissage par renforcement profond : Dueling Double
  DQN, Prioritized Experience Replay et cibles Q fixes'
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2018-07-06T00:10:13.000Z'
originalURL: https://freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682
coverImage: https://cdn-media-1.freecodecamp.org/images/1*idlcWBCQGKJ2rMjKPwAKiQ.png
tags:
- name: Artificial Intelligence
  slug: artificial-intelligence
- name: Deep Learning
  slug: deep-learning
- name: Machine Learning
  slug: machine-learning
- name: Reinforcement Learning
  slug: reinforcement-learning
- name: 'tech '
  slug: tech
seo_title: 'Am√©liorations dans l''apprentissage par renforcement profond : Dueling
  Double DQN, Prioritized Experience Replay et cibles Q fixes'
seo_desc: 'By Thomas Simonini


  This article is part of Deep Reinforcement Learning Course with Tensorflow ?Ô∏è. Check
  the syllabus here.


  In our last article about Deep Q Learning with Tensorflow, we implemented an agent
  that learns to play a simple version of Do...'
---

Par Thomas Simonini

> Cet article fait partie du cours d'apprentissage par renforcement profond avec Tensorflow üéØ. Consultez le programme [ici](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/).

Dans notre dernier article sur [l'apprentissage par renforcement profond avec Tensorflow](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8), nous avons impl√©ment√© un agent qui apprend √† jouer √† une version simple de Doom. Dans la version vid√©o, [nous avons entra√Æn√© un agent DQN qui joue √† Space Invaders](https://www.youtube.com/watch?v=gCJyVX98KJ4).

Cependant, pendant l'entra√Ænement, nous avons vu qu'il y avait beaucoup de variabilit√©.

L'apprentissage par renforcement profond (Deep Q-Learning) a √©t√© introduit en 2014. Depuis, de nombreuses am√©liorations ont √©t√© apport√©es. Aujourd'hui, nous allons voir quatre strat√©gies qui am√©liorent ‚Äî de mani√®re dramatique ‚Äî l'entra√Ænement et les r√©sultats de nos agents DQN :

* Cibles Q fixes
* Double DQN
* Dueling DQN (aka DDQN)
* Prioritized Experience Replay (aka PER)

Nous allons impl√©menter un agent qui apprend √† jouer √† Doom Deadly Corridor. Notre IA doit naviguer vers l'objectif fondamental (le gilet), et s'assurer de survivre en m√™me temps en tuant des ennemis.

### Cibles Q fixes

#### Th√©orie

Nous avons vu dans l'article sur l'apprentissage par renforcement profond que, lorsque nous voulons calculer l'erreur TD (aka la perte), nous calculons la diff√©rence entre la cible TD (Q_target) et la valeur Q actuelle (estimation de Q).

![Image](https://cdn-media-1.freecodecamp.org/images/1*Zplt-1wTWu_7BGmZCBFjbQ.png)

Mais **nous n'avons aucune id√©e de la vraie cible TD.** Nous devons l'estimer. En utilisant l'√©quation de Bellman, nous avons vu que la cible TD est simplement la r√©compense de prendre cette action dans cet √©tat plus la valeur Q la plus √©lev√©e actualis√©e pour l'√©tat suivant.

![Image](https://cdn-media-1.freecodecamp.org/images/1*KsQ46R8zyTQlKGv91xi6ww.png)

Cependant, le probl√®me est que nous utilisons les m√™mes param√®tres (poids) pour estimer la cible **et** la valeur Q. Par cons√©quent, il y a une grande corr√©lation entre la cible TD et les param√®tres (w) que nous changeons.

Cela signifie qu'√† chaque √©tape de l'entra√Ænement, **nos valeurs Q changent mais la valeur cible change aussi.** Donc, nous nous rapprochons de notre cible mais la cible bouge aussi. C'est comme courir apr√®s une cible mouvante ! Cela conduit √† de grandes oscillations pendant l'entra√Ænement.

C'est comme si vous √©tiez un cowboy (l'estimation Q) et que vous vouliez attraper la vache (la cible Q), vous devez vous rapprocher (r√©duire l'erreur).

![Image](https://cdn-media-1.freecodecamp.org/images/1*BCsZHA3cO3zsQySkRuWPEw.png)

√Ä chaque √©tape, vous essayez de vous approcher de la vache, qui bouge aussi √† chaque √©tape (parce que vous utilisez les m√™mes param√®tres).

![Image](https://cdn-media-1.freecodecamp.org/images/1*aKuCo_MvnoCa148m3U9YXg.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*T5MwyKNbDmG9Vb_fQg1t-w.png)

Cela conduit √† un chemin de poursuite tr√®s √©trange (de grandes oscillations pendant l'entra√Ænement).

![Image](https://cdn-media-1.freecodecamp.org/images/1*Kt6H_kh_rfSu7EkN9bU0oA.png)

Au lieu de cela, nous pouvons utiliser l'id√©e des cibles Q fixes introduite par DeepMind :

* Utiliser un r√©seau s√©par√© avec des param√®tres fixes (appelons-le w-) pour estimer la cible TD.
* √Ä chaque √©tape Tau, nous copions les param√®tres de notre r√©seau DQN pour mettre √† jour le r√©seau cible.

![Image](https://cdn-media-1.freecodecamp.org/images/1*D9i0I2EO7LKL2aAb2HLfTg.png)

Gr√¢ce √† cette proc√©dure, nous aurons un apprentissage plus stable car la fonction cible reste fixe pendant un certain temps.

#### Impl√©mentation

L'impl√©mentation des cibles Q fixes est assez simple :

* D'abord, nous cr√©ons deux r√©seaux (`DQNetwork`, `TargetNetwork`)

* Ensuite, nous cr√©ons une fonction qui prendra les param√®tres de notre `DQNetwork` et les copiera dans notre `TargetNetwork`

* Enfin, pendant l'entra√Ænement, nous calculons la cible TD en utilisant notre r√©seau cible. Nous mettons √† jour le r√©seau cible avec le `DQNetwork` toutes les `tau` √©tapes (`tau` est un hyper-param√®tre que nous d√©finissons).

### Double DQN

#### Th√©orie

Les Double DQN, ou Double Learning, ont √©t√© introduits [par Hado van Hasselt](https://papers.nips.cc/paper/3964-double-q-learning). Cette m√©thode **g√®re le probl√®me de la surestimation des valeurs Q.**

Pour comprendre ce probl√®me, rappelez-vous comment nous calculons la cible TD :

![Image](https://cdn-media-1.freecodecamp.org/images/1*KsQ46R8zyTQlKGv91xi6ww.png)

En calculant la cible TD, nous sommes confront√©s √† un probl√®me simple : comment √™tre s√ªr que **la meilleure action pour l'√©tat suivant est l'action avec la valeur Q la plus √©lev√©e ?**

Nous savons que la pr√©cision des valeurs Q d√©pend des actions que nous avons essay√©es **et** des √©tats voisins que nous avons explor√©s.

Par cons√©quent, au d√©but de l'entra√Ænement, nous n'avons pas assez d'informations sur la meilleure action √† prendre. Par cons√©quent, prendre la valeur Q maximale (qui est bruyante) comme la meilleure action √† prendre peut conduire √† des faux positifs. Si des actions non optimales se voient r√©guli√®rement **attribuer une valeur Q plus √©lev√©e que l'action optimale, l'apprentissage sera compliqu√©.**

La solution est : lorsque nous calculons la cible Q, nous utilisons deux r√©seaux pour d√©coupler la s√©lection de l'action de la g√©n√©ration de la valeur Q cible. Nous :

* utilisons notre r√©seau DQN pour s√©lectionner la meilleure action √† prendre pour l'√©tat suivant (l'action avec la valeur Q la plus √©lev√©e).
* utilisons notre r√©seau cible pour calculer la valeur Q cible de prendre cette action √† l'√©tat suivant.

![Image](https://cdn-media-1.freecodecamp.org/images/1*g5l4q162gDRZAAsFWtX7Nw.png)

Par cons√©quent, le Double DQN nous aide √† r√©duire la surestimation des valeurs Q et, par cons√©quent, nous aide √† nous entra√Æner plus rapidement et √† avoir un apprentissage plus stable.

#### Impl√©mentation

![Image](https://cdn-media-1.freecodecamp.org/images/1*oyGR6gJ4WyqeKOfq0Cd8iQ.png)

### Dueling DQN (aka DDQN)

#### Th√©orie

Rappelez-vous que les valeurs Q correspondent **√† la qualit√© d'√™tre dans cet √©tat et de prendre une action dans cet √©tat Q(s,a).**

Nous pouvons donc d√©composer Q(s,a) comme la somme de :

* **V(s)** : la valeur d'√™tre dans cet √©tat
* **A(s,a)** : l'avantage de prendre cette action dans cet √©tat (√† quel point il est meilleur de prendre cette action par rapport √† toutes les autres actions possibles dans cet √©tat).

![Image](https://cdn-media-1.freecodecamp.org/images/1*yPtkPCxjXP2TbK8VlUuXtA.png)

Avec le DDQN, nous voulons s√©parer l'estimateur de ces deux √©l√©ments, en utilisant deux nouveaux flux :

* un qui estime la **valeur de l'√©tat V(s)**
* un qui estime l'**avantage pour chaque action A(s,a)**

![Image](https://cdn-media-1.freecodecamp.org/images/1*FkHqwA2eSGixdS-3dvVoMA.png)

Et ensuite nous combinons ces deux flux **√† travers une couche d'agr√©gation sp√©ciale pour obtenir une estimation de Q(s,a).**

Attendez ? **Mais pourquoi devons-nous calculer ces deux √©l√©ments s√©par√©ment si ensuite nous les combinons ?**

En d√©couplant l'estimation, intuitivement notre DDQN peut apprendre quels √©tats sont (ou ne sont pas) pr√©cieux **sans** avoir √† apprendre l'effet de chaque action √† chaque √©tat (puisqu'il calcule aussi V(s)).

Avec notre DQN normal, nous devons calculer la valeur de chaque action dans cet √©tat. **Mais quel est l'int√©r√™t si la valeur de l'√©tat est mauvaise ?** Quel est l'int√©r√™t de calculer toutes les actions dans un √©tat lorsque toutes ces actions m√®nent √† la mort ?

Par cons√©quent, en d√©couplant, nous sommes capables de calculer V(s). Cela est particuli√®rement **utile pour les √©tats o√π leurs actions n'affectent pas l'environnement de mani√®re pertinente.** Dans ce cas, il est inutile de calculer la valeur de chaque action. Par exemple, se d√©placer √† droite ou √† gauche n'a d'importance que s'il y a un risque de collision. Et, dans la plupart des √©tats, le choix de l'action n'a aucun effet sur ce qui se passe.

Cela sera plus clair si nous prenons l'exemple dans l'article [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/pdf/1511.06581.pdf).

![Image](https://cdn-media-1.freecodecamp.org/images/0*qor_kPiSwiWt8uQF)

Nous voyons que le flux du r√©seau de valeur pr√™te attention (le flou orange) √† la route, et en particulier √† l'horizon o√π les voitures apparaissent. Il pr√™te √©galement attention au score.

D'autre part, le flux d'avantage dans la premi√®re image √† droite ne pr√™te pas beaucoup d'attention √† la route, car il n'y a pas de voitures devant (donc le choix de l'action est pratiquement sans importance). Mais, dans la deuxi√®me image, il pr√™te attention, car il y a une voiture imm√©diatement devant lui, et faire un choix d'action est crucial et tr√®s pertinent.

Concernant la couche d'agr√©gation, nous voulons g√©n√©rer les valeurs Q pour chaque action dans cet √©tat. Nous pourrions √™tre tent√©s de combiner les flux comme suit :

![Image](https://cdn-media-1.freecodecamp.org/images/0*ue6KTm1dRQ0A6sM4)

Mais si nous faisons cela, nous tomberons dans le **probl√®me d'identifiabilit√©**, c'est-√†-dire ‚Äî √©tant donn√© Q(s,a), nous sommes incapables de trouver A(s,a) et V(s).

Et ne pas pouvoir trouver V(s) et A(s,a) √©tant donn√© Q(s,a) sera un probl√®me pour notre r√©tropropagation. Pour √©viter ce probl√®me, nous pouvons forcer notre estimateur de fonction d'avantage √† avoir un avantage de 0 √† l'action choisie.

Pour ce faire, nous soustrayons l'avantage moyen de toutes les actions possibles de l'√©tat.

![Image](https://cdn-media-1.freecodecamp.org/images/0*kt9_Z41qxgiI0CDl)

Par cons√©quent, cette architecture nous aide √† acc√©l√©rer l'entra√Ænement. Nous pouvons calculer la valeur d'un √©tat sans calculer Q(s,a) pour chaque action dans cet √©tat. Et cela peut nous aider √† trouver des valeurs Q beaucoup plus fiables pour chaque action en d√©couplant l'estimation entre deux flux.

#### Impl√©mentation

La seule chose √† faire est de modifier l'architecture DQN en ajoutant ces nouveaux flux :

### Prioritized Experience Replay

#### Th√©orie

Le Prioritized Experience Replay (PER) a √©t√© introduit en 2015 par [Tom Schaul](https://arxiv.org/search?searchtype=author&query=Schaul%2C+T). L'id√©e est que certaines exp√©riences peuvent √™tre plus importantes que d'autres pour notre entra√Ænement, mais peuvent se produire moins fr√©quemment.

Parce que nous √©chantillonnons le lot uniform√©ment (en s√©lectionnant les exp√©riences al√©atoirement), ces exp√©riences riches qui se produisent rarement ont pratiquement aucune chance d'√™tre s√©lectionn√©es.

C'est pourquoi, avec le PER, nous essayons de changer la distribution d'√©chantillonnage en utilisant un crit√®re pour d√©finir la priorit√© de chaque tuple d'exp√©rience.

Nous voulons prendre en priorit√© **les exp√©riences o√π il y a une grande diff√©rence entre notre pr√©diction et la cible TD, car cela signifie que nous avons beaucoup √† apprendre √† ce sujet.**

Nous utilisons la valeur absolue de l'amplitude de notre erreur TD :

![Image](https://cdn-media-1.freecodecamp.org/images/0*0qPwzal3qBIP0eFb)

Et nous **mettons cette priorit√© dans l'exp√©rience de chaque tampon de relecture.**

![Image](https://cdn-media-1.freecodecamp.org/images/0*iKTTN92E7wwnlh-E)

Mais nous ne pouvons pas simplement faire une priorisation gloutonne, car cela conduirait toujours √† entra√Æner les m√™mes exp√©riences (qui ont une grande priorit√©), et donc √† un sur-apprentissage.

Nous introduisons donc une priorisation stochastique, **qui g√©n√®re la probabilit√© d'√™tre choisi pour une relecture.**

![Image](https://cdn-media-1.freecodecamp.org/images/0*iCkLY7L3R3mWEh_O)

Par cons√©quent, √† chaque √©tape, nous obtiendrons un lot d'√©chantillons avec cette distribution de probabilit√© et nous entra√Ænerons notre r√©seau sur celui-ci.

Mais, nous avons toujours un probl√®me ici. Rappelez-vous qu'avec le Experience Replay normal, nous utilisons une r√®gle de mise √† jour stochastique. Par cons√©quent, **la mani√®re dont nous √©chantillonnons les exp√©riences doit correspondre √† la distribution sous-jacente dont elles proviennent.**

Lorsque nous avons une exp√©rience normale, nous s√©lectionnons nos exp√©riences dans une distribution normale ‚Äî simplement, nous s√©lectionnons nos exp√©riences al√©atoirement. Il n'y a pas de biais, car chaque exp√©rience a la m√™me chance d'√™tre prise, donc nous pouvons mettre √† jour nos poids normalement.

**Mais**, parce que nous utilisons un √©chantillonnage par priorit√©, l'√©chantillonnage purement al√©atoire est abandonn√©. Par cons√©quent, nous introduisons un biais envers les √©chantillons √† haute priorit√© (plus de chances d'√™tre s√©lectionn√©s).

Et, si nous mettons √† jour nos poids normalement, nous prenons un risque de sur-apprentissage. Les √©chantillons qui ont une haute priorit√© sont susceptibles d'√™tre utilis√©s pour l'entra√Ænement de nombreuses fois par rapport aux exp√©riences √† faible priorit√© (= biais). Par cons√©quent, nous mettrons √† jour nos poids avec seulement une petite portion d'exp√©riences que nous consid√©rons comme vraiment int√©ressantes.

Pour corriger ce biais, nous utilisons des poids d'√©chantillonnage d'importance (IS) qui ajusteront la mise √† jour en r√©duisant les poids des √©chantillons souvent vus.

![Image](https://cdn-media-1.freecodecamp.org/images/0*Lf3KBrOdyBYcOVqB)

Les poids correspondant aux √©chantillons √† haute priorit√© ont tr√®s peu d'ajustement (parce que le r√©seau verra ces exp√©riences de nombreuses fois), tandis que ceux correspondant aux √©chantillons √† faible priorit√© auront une mise √† jour compl√®te.

Le r√¥le de **b** est de contr√¥ler combien ces poids d'√©chantillonnage d'importance affectent l'apprentissage. En pratique, le param√®tre b est recuit jusqu'√† 1 sur la dur√©e de l'entra√Ænement, car ces poids sont plus importants **√† la fin de l'apprentissage lorsque nos valeurs Q commencent √† converger.** La nature non biais√©e des mises √† jour est la plus importante pr√®s de la convergence, comme expliqu√© dans cet [article](http://pemami4911.github.io/paper-summaries/deep-rl/2016/01/26/prioritizing-experience-replay.html).

#### Impl√©mentation

Cette fois, l'impl√©mentation sera un peu plus √©labor√©e.

Tout d'abord, nous ne pouvons pas simplement impl√©menter le PER en triant tous les tampons de relecture d'exp√©rience selon leurs priorit√©s. Cela ne sera pas efficace du tout en raison de **O(nlogn) pour l'insertion et O(n) pour l'√©chantillonnage.**

Comme expliqu√© dans [cet article tr√®s bon](https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/), nous devons utiliser une autre structure de donn√©es au lieu de trier un tableau ‚Äî un sumtree **non tri√©.**

Un sumtree est un arbre binaire, c'est-√†-dire un arbre avec un maximum de deux enfants pour chaque n≈ìud. Les feuilles (n≈ìuds les plus profonds) contiennent les valeurs de priorit√©, et un tableau de donn√©es qui pointe vers les feuilles contient les exp√©riences.

La mise √† jour de l'arbre et l'√©chantillonnage seront tr√®s efficaces (O(log n)).

![Image](https://cdn-media-1.freecodecamp.org/images/1*Go9DNr7YY-wMGdIQ7HQduQ.png)

Ensuite, nous cr√©ons un objet m√©moire qui contiendra notre sumtree et nos donn√©es.

Ensuite, pour √©chantillonner un minibatch de taille k, la plage [0, priorit√©_totale] sera divis√©e en k plages. Une valeur est √©chantillonn√©e uniform√©ment √† partir de chaque plage.

Enfin, les transitions (exp√©riences) qui correspondent √† chacune de ces valeurs √©chantillonn√©es sont r√©cup√©r√©es √† partir du sumtree.

Cela sera beaucoup plus clair lorsque nous plongerons dans les d√©tails complets dans le notebook.

### Agent Deathmatch de Doom

Cet agent est un Dueling Double Deep Q Learning avec PER et cibles Q fixes.

> Nous avons fait un tutoriel vid√©o de l'impl√©mentation :

> Le notebook est [ici](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/Dueling%20Deep%20Q%20Learning%20with%20Doom%20(%2B%20double%20DQNs%20and%20Prioritized%20Experience%20Replay).ipynb)

C'est tout ! Vous venez de cr√©er un agent plus intelligent qui apprend √† jouer √† Doom. G√©nial ! Rappelez-vous que si vous voulez avoir un agent avec de tr√®s bonnes performances, **vous avez besoin de beaucoup plus d'heures de GPU (environ deux jours d'entra√Ænement) !**

![Image](https://cdn-media-1.freecodecamp.org/images/1*pN5raRODUzEQOLw0egyXYg.gif)

**Cependant, avec seulement 2-3 heures d'entra√Ænement sur CPU** (oui CPU), notre agent a compris qu'il devait tuer les ennemis avant de pouvoir avancer. S'ils avancent sans tuer les ennemis, ils seront tu√©s avant d'obtenir le gilet.

N'oubliez pas d'impl√©menter chaque partie du code par vous-m√™me. Il est vraiment important d'essayer de modifier le code que je vous ai donn√©. Essayez d'ajouter des √©poques, changez l'architecture, ajoutez des valeurs Q fixes, changez le taux d'apprentissage, utilisez un environnement plus difficile... et ainsi de suite. Exp√©rimentez, amusez-vous !

Rappelez-vous que c'√©tait un gros article, alors assurez-vous de vraiment comprendre pourquoi nous utilisons ces nouvelles strat√©gies, comment elles fonctionnent, et les avantages de les utiliser.

Dans le prochain article, nous apprendrons une m√©thode hybride g√©niale entre les algorithmes d'apprentissage par renforcement bas√©s sur la valeur et ceux bas√©s sur les politiques. **C'est une base pour les algorithmes de pointe** : Advantage Actor Critic (A2C). Vous impl√©menterez un agent qui apprend √† jouer √† Outrun !

![Image](https://cdn-media-1.freecodecamp.org/images/1*0M5OiOwKemAwkObBy1K6VQ.gif)

Si vous avez aim√© mon article, **cliquez sur le ? ci-dessous autant de fois que vous avez aim√© l'article** pour que d'autres personnes puissent le voir ici sur Medium. Et n'oubliez pas de me suivre !

Si vous avez des pens√©es, des commentaires, des questions, n'h√©sitez pas √† commenter ci-dessous ou √† m'envoyer un email : hello@simoninithomas.com, ou tweetez-moi [@ThomasSimonini](https://twitter.com/ThomasSimonini).

![Image](https://cdn-media-1.freecodecamp.org/images/1*_yN1FzvEFDmlObiYsstIzg.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*mD-f5VN1SWYvhrZAbvSu_w.png)

![Image](https://cdn-media-1.freecodecamp.org/images/1*PqiptT-Cdi8uwosxuFn2DQ.png)

Continuez √† apprendre, restez g√©nial !

#### Cours d'apprentissage par renforcement profond avec Tensorflow üéØ

üìã [Programme](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)

üìπ [Version vid√©o](https://www.youtube.com/channel/UC8XuSf1eD9AF8x8J19ha5og?view_as=subscriber)

Partie 1 : [Une introduction √† l'apprentissage par renforcement](https://medium.com/p/4339519de419/edit)

Partie 2 : [Plonger plus profond√©ment dans l'apprentissage par renforcement avec Q-Learning](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)

Partie 3 : [Une introduction √† l'apprentissage par renforcement profond : jouons √† Doom](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)

Partie 3+ : [Am√©liorations dans l'apprentissage par renforcement profond : Dueling Double DQN, Prioritized Experience Replay et cibles Q fixes](https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682)

Partie 4 : [Une introduction aux gradients de politique avec Doom et Cartpole](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)

Partie 5 : [Une introduction aux m√©thodes Advantage Actor Critic : jouons √† Sonic the Hedgehog !](https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d)

Partie 6 : [Optimisation des politiques proximales (PPO) avec Sonic the Hedgehog 2 et 3](https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e)

Partie 7 : [L'apprentissage par curiosit√© rendu facile Partie I](https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359)