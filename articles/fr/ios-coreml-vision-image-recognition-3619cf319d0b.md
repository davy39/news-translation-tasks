---
title: Comment cr√©er une application iOS de reconnaissance d'images avec CoreML et
  les API Vision d'Apple
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2017-09-01T18:54:19.000Z'
originalURL: https://freecodecamp.org/news/ios-coreml-vision-image-recognition-3619cf319d0b
coverImage: https://cdn-media-1.freecodecamp.org/images/1*vm51CWzLgOE2mTHwWdQENw.png
tags:
- name: image recognition
  slug: image-recognition
- name: iOS
  slug: ios
- name: Machine Learning
  slug: machine-learning
- name: mobile app development
  slug: mobile-app-development
- name: 'tech '
  slug: tech
seo_title: Comment cr√©er une application iOS de reconnaissance d'images avec CoreML
  et les API Vision d'Apple
seo_desc: 'By Mark Mansur

  With the release of CoreML and new Vision APIs at this year‚Äôs Apple World Wide Developers
  Conference, machine learning has never been easier to get into. Today I‚Äôm going
  to show you how to build a simple image recognition app.

  We will ...'
---

Par Mark Mansur

Avec la sortie de [CoreML](https://developer.apple.com/documentation/coreml) et des nouvelles API Vision lors de la conf√©rence mondiale des d√©veloppeurs Apple de cette ann√©e, le machine learning n'a jamais √©t√© aussi facile √† prendre en main. Aujourd'hui, je vais vous montrer comment cr√©er une application simple de reconnaissance d'images.

Nous allons apprendre comment acc√©der √† la cam√©ra de l'iPhone et comment transmettre ce que la cam√©ra voit √† un mod√®le de machine learning pour analyse. Nous allons tout faire de mani√®re programmatique, sans utiliser de storyboards ! Fou, je sais.

Voici un aper√ßu de ce que nous allons accomplir aujourd'hui :

```swift
//
//  ViewController.swift
//  cameraTest
//
//  Cr√©√© par Mark Mansur le 2017-08-01.
//  Copyright ¬© 2017 Mark Mansur. Tous droits r√©serv√©s.
//
import UIKit
import AVFoundation
import Vision

class ViewController: UIViewController, AVCaptureVideoDataOutputSampleBufferDelegate {
    let label: UILabel = {
        let label = UILabel()
        label.textColor = .white
        label.translatesAutoresizingMaskIntoConstraints = false
        label.text = "Label"
        label.font = label.font.withSize(30)
        return label
    }()
    
    override func viewDidLoad() {
        super.viewDidLoad()
        
        setupCaptureSession()
        
        view.addSubview(label)
        setupLabel()
    }
    
    func setupCaptureSession() {
        let captureSession = AVCaptureSession()
        
        // recherche des dispositifs de capture disponibles
        let availableDevices = AVCaptureDevice.DiscoverySession(deviceTypes: [.builtInWideAngleCamera], mediaType: AVMediaType.video, position: .back).devices
        
        // configuration du dispositif de capture, ajout de l'entr√©e √† notre session de capture
        do {
            if let captureDevice = availableDevices.first {
                let captureDeviceInput = try AVCaptureDeviceInput(device: captureDevice)
                captureSession.addInput(captureDeviceInput)
            }
        } catch {
            print(error.localizedDescription)
        }
        
        // configuration de la sortie, ajout de la sortie √† notre session de capture
        let captureOutput = AVCaptureVideoDataOutput()
        captureOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "videoQueue"))
        captureSession.addOutput(captureOutput)
        
        let previewLayer = AVCaptureVideoPreviewLayer(session: captureSession)
        previewLayer.frame = view.frame
        view.layer.addSublayer(previewLayer)
        
        captureSession.startRunning()
    }
    
    // appel√© chaque fois qu'une image est captur√©e
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let model = try? VNCoreMLModel(for: Resnet50().model) else {return}
        
        let request = VNCoreMLRequest(model: model) { (finishedRequest, error) in
            
            guard let results = finishedRequest.results as? [VNClassificationObservation] else { return }
            guard let Observation = results.first else { return }
            
            DispatchQueue.main.async(execute: {
                self.label.text = "\(Observation.identifier)"
            })
        }
        guard let pixelBuffer: CVPixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        
        // ex√©cute la requ√™te
        try? VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:]).perform([request])
    }
    
    func setupLabel() {
        label.centerXAnchor.constraint(equalTo: view.centerXAnchor).isActive = true
        label.bottomAnchor.constraint(equalTo: view.bottomAnchor, constant: -50).isActive = true
    }
}
```

### üëãüèª √âtape 1 : Cr√©er un nouveau projet.

Lancez Xcode et cr√©ez une nouvelle application √† vue unique. Donnez-lui un nom, peut-√™tre ¬´ ImageRecognition ¬ª. Choisissez Swift comme langage principal et enregistrez votre nouveau projet.

### üëã √âtape 2 : Dire adieu au storyboard.

Pour ce tutoriel, nous allons tout faire de mani√®re programmatique, sans avoir besoin du storyboard. Peut-√™tre que j'expliquerai pourquoi dans un autre article.

Supprimez `main.storyboard`.

Acc√©dez √† `info.plist` et faites d√©filer jusqu'√† Deployment Info. Nous devons dire √† Xcode que nous n'utilisons plus le storyboard.

Supprimez l'interface principale.

![Image](https://cdn-media-1.freecodecamp.org/images/1*W-p1_py_aMgNrnBh4ljJOg.png)

Sans le storyboard, nous devons cr√©er manuellement la fen√™tre de l'application et le contr√¥leur de vue racine.

Ajoutez ce qui suit √† la fonction `application()` dans `AppDelegate.swift` :

```swift

func application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplicationLaunchOptionsKey: Any]?) -> Bool {
        // Point de substitution pour la personnalisation apr√®s le lancement de l'application.
        
        window = UIWindow()
        window?.makeKeyAndVisible()
        let vc = ViewController()
        
        window?.rootViewController = vc
        return true
    }
```

Nous cr√©ons manuellement la fen√™tre de l'application avec `UIWindow()`, cr√©ons notre contr√¥leur de vue et disons √† la fen√™tre de l'utiliser comme contr√¥leur de vue racine.

L'application devrait maintenant se construire et s'ex√©cuter sans le storyboard üòä

### ‚öôÔ∏è √âtape 3 : Configurer AVCaptureSession.

Avant de commencer, importez UIKit, AVFoundation et Vision. L'objet AVCaptureSession g√®re l'activit√© de capture et manage le flux de donn√©es entre les dispositifs d'entr√©e (comme la cam√©ra arri√®re) et les sorties.

Nous allons commencer par cr√©er une fonction pour configurer notre session de capture.

Cr√©ez `setupCaptureSession()` √† l'int√©rieur de `ViewController.swift` et instanciez une nouvelle `AVCaptureSession`.

```swift
func setupCaptureSession() {
        
        // cr√©e une nouvelle session de capture
        let captureSession = AVCaptureSession()
}
```

N'oubliez pas d'appeler cette nouvelle fonction depuis `ViewDidLoad()`.

```swift
override func viewDidLoad() {
        super.viewDidLoad()
        
        setupCaptureSession()
}
```

Ensuite, nous allons avoir besoin d'une r√©f√©rence √† la cam√©ra arri√®re. Nous pouvons utiliser un `DiscoverySession` pour interroger les dispositifs de capture disponibles en fonction de nos crit√®res de recherche.

Ajoutez le code suivant :

```swift
// recherche des dispositifs de capture disponibles
let availableDevices = AVCaptureDevice.DiscoverySession(deviceTypes: [.builtInWideAngleCamera], mediaType: AVMediaType.video, position: .back).devices

```

`AvailableDevices` contient maintenant une liste des dispositifs disponibles correspondant √† nos crit√®res de recherche.

Nous devons maintenant obtenir l'acc√®s √† notre `captureDevice` et l'ajouter comme entr√©e √† notre `captureSession`.

Ajoutez une entr√©e √† la session de capture.

```swift
// obtenir le dispositif de capture, ajouter l'entr√©e du dispositif √† la session de capture
do {
    if let captureDevice = availableDevices.first {
        captureSession.addInput(try AVCaptureDeviceInput(device: captureDevice))
    }
} catch {
    print(error.localizedDescription)
}
```

Le premier dispositif disponible sera la cam√©ra arri√®re. Nous cr√©ons un nouveau `AVCaptureDeviceInput` en utilisant notre dispositif de capture et l'ajoutons √† la session de capture.

Maintenant que nous avons configur√© notre entr√©e, nous pouvons commencer √† voir comment sortir ce que la cam√©ra capture.

Ajoutez une sortie vid√©o √† notre session de capture.

```swift
// configuration de la sortie, ajout de la sortie √† notre session de capture
let captureOutput = AVCaptureVideoDataOutput()
captureSession.addOutput(captureOutput)
```

`AVCaptureVideoDataOutput` est une sortie qui capture la vid√©o. Elle nous donne √©galement acc√®s aux images captur√©es pour le traitement avec une m√©thode d√©l√©gu√©e que nous verrons plus tard.

Ensuite, nous devons ajouter la sortie de la session de capture comme sous-couche √† notre vue.

Ajoutez la sortie de la session de capture comme sous-couche √† la vue du contr√¥leur de vue.

```swift
let previewLayer = AVCaptureVideoPreviewLayer(session: captureSession)
previewLayer.frame = view.frame
view.layer.addSublayer(previewLayer)

captureSession.startRunning()
```

Nous cr√©ons une couche bas√©e sur notre session de capture et ajoutons cette couche comme sous-couche √† notre vue. `CaptureSession.startRunning()` commence le flux des entr√©es vers les sorties que nous avons connect√©es pr√©c√©demment.

### üì∑ √âtape 4 : Permission d'utiliser la cam√©ra ? Permission accord√©e.

Presque tout le monde a ouvert une application pour la premi√®re fois et a √©t√© invit√© √† permettre √† l'application d'utiliser la cam√©ra. √Ä partir d'iOS 10, notre application plantera si nous ne demandons pas √† l'utilisateur avant d'essayer d'acc√©der √† la cam√©ra.

Acc√©dez √† `info.plist` et ajoutez une nouvelle cl√© nomm√©e `NSCameraUsageDescription`. Dans la colonne valeur, expliquez simplement √† l'utilisateur pourquoi votre application a besoin d'un acc√®s √† la cam√©ra.

Maintenant, lorsque l'utilisateur lance l'application pour la premi√®re fois, il sera invit√© √† permettre l'acc√®s √† la cam√©ra.

### üîä √âtape 5 : Obtenir le mod√®le.

Le c≈ìur de ce projet est tr√®s probablement le mod√®le de machine learning. Le mod√®le doit √™tre capable de prendre une image et de nous retourner une pr√©diction de ce qu'est l'image. Vous pouvez trouver des mod√®les entra√Æn√©s gratuits [ici](https://developer.apple.com/machine-learning/). Celui que j'ai choisi est ResNet50.

Une fois que vous avez obtenu votre mod√®le, faites-le glisser et d√©posez-le dans Xcode. Il g√©n√©rera automatiquement les classes n√©cessaires, vous fournissant une interface pour interagir avec votre mod√®le.

### üèõ √âtape 6 : Analyse d'image.

Pour analyser ce que la cam√©ra voit, nous devons somehow obtenir l'acc√®s aux images captur√©es par la cam√©ra.

Se conformer au `AVCaptureVideoDataOutputSampleBufferDelegate` nous donne une interface pour interagir et √™tre notifi√© chaque fois qu'une image est captur√©e par la cam√©ra.

Conformez `ViewController` au `AVCaptureVideoDataOutputSampleBufferDelegate`.

Nous devons dire √† notre sortie vid√©o que ViewController est son d√©l√©gu√© de tampon d'√©chantillon.

Ajoutez la ligne suivante dans `SetupCaptureSession()` :

```swift
captureOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "videoQueue"))

```

Ajoutez la fonction suivante :

```swift
func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let model = try? VNCoreMLModel(for: Resnet50().model) else { return }
        let request = VNCoreMLRequest(model: model) { (finishedRequest, error) in
            guard let results = finishedRequest.results as? [VNClassificationObservation] else { return }
            guard let Observation = results.first else { return }
            
            DispatchQueue.main.async(execute: {
                self.label.text = "\(Observation.identifier)"
            })
        }
        guard let pixelBuffer: CVPixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        
        // ex√©cute la requ√™te
        try? VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:]).perform([request])
    }
```

Chaque fois qu'une image est captur√©e, le d√©l√©gu√© est notifi√© en appelant `captureOutput()`. C'est un endroit parfait pour faire notre analyse d'image avec CoreML.

Tout d'abord, nous cr√©ons un `VNCoreMLModel` qui est essentiellement un mod√®le CoreML utilis√© avec le framework Vision. Nous le cr√©ons avec un mod√®le Resnet50.

Ensuite, nous cr√©ons notre requ√™te de vision. Dans le gestionnaire d'ach√®vement, nous mettons √† jour le UILabel √† l'√©cran avec l'identifiant retourn√© par le mod√®le. Nous convertissons ensuite l'image qui nous est pass√©e d'un `CMSampleBuffer` en un `CVPixelBuffer`. Qui est le format dont notre mod√®le a besoin pour l'analyse.

Enfin, nous ex√©cutons la requ√™te Vision avec un `VNImageRequestHandler`.

### üìù √âtape 7 : Cr√©er une √©tiquette.

La derni√®re √©tape consiste √† cr√©er un `UILabel` contenant la pr√©diction du mod√®le.

Cr√©ez un nouveau `UILabel` et positionnez-le en utilisant des contraintes.

```swift
let label: UILabel = {
        let label = UILabel()
        label.textColor = .white
        label.translatesAutoresizingMaskIntoConstraints = false
        label.text = "Label"
        label.font = label.font.withSize(30)
        return label
    }()

func setupLabel() {
        label.centerXAnchor.constraint(equalTo: view.centerXAnchor).isActive = true
        label.bottomAnchor.constraint(equalTo: view.bottomAnchor, constant: -50).isActive = true
}
```

N'oubliez pas d'ajouter l'√©tiquette comme sous-vue et d'appeler `setupLabel()` depuis `ViewDidLoad()`.

```swift
view.addSubview(label)
setupLabel()
```

Vous pouvez t√©l√©charger le projet termin√© depuis [GitHub ici](https://github.com/markmansur/CoreML-Vision-demo).

Aimez ce que vous voyez ? Donnez un pouce en l'air üëç √† cet article, suivez-moi sur [Twitter](https://twitter.com/MarkMansur2), [GitHub](https://github.com/markmansur), ou consultez [ma page personnelle](http://markmansur.me/).