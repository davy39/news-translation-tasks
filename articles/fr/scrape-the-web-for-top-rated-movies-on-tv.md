---
title: Le guide complet pour extraire les meilleurs films Ã  la tÃ©lÃ©vision depuis le
  web
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2020-01-03T22:30:01.000Z'
originalURL: https://freecodecamp.org/news/scrape-the-web-for-top-rated-movies-on-tv
coverImage: https://www.freecodecamp.org/news/content/images/2020/01/0_D52CsZmqCYvifA3M.jpeg
tags:
- name: Python
  slug: python
- name: '#Scrapy'
  slug: scrapy
- name: web scraping
  slug: web-scraping
seo_title: Le guide complet pour extraire les meilleurs films Ã  la tÃ©lÃ©vision depuis
  le web
seo_desc: 'By Bert Carremans

  In this article, I will show how to scrape the internet for top-rated films with
  the Scrapy framework. The goal of this web scraper is to find films that have a
  high user rating on The Movie Database. The list with these films will ...'
---

Par Bert Carremans

Dans cet article, je vais montrer comment extraire des informations sur les films les mieux notÃ©s avec le **_framework Scrapy_** (http://scrapy.org/). L'**_objectif_** de ce scraper web est de trouver des films qui ont une note Ã©levÃ©e sur [The Movie Database](https://www.themoviedb.org/). La liste de ces films sera stockÃ©e dans une **_base de donnÃ©es SQLite_** et **_envoyÃ©e par email_**. Ainsi, vous saurez que vous ne manquerez plus jamais un blockbuster Ã  la tÃ©lÃ©vision.

# Trouver une bonne page web Ã  scraper

Je commence par un guide TV en ligne pour trouver des films sur les chaÃ®nes de tÃ©lÃ©vision belges. Mais vous pourriez facilement adapter mon code pour l'utiliser sur n'importe quel autre site web. Pour faciliter votre tÃ¢che lors de l'extraction de films, assurez-vous que le site web que vous souhaitez scraper :

* a des balises HTML avec une **_classe ou un id comprÃ©hensible_**
* utilise des classes et des ids de maniÃ¨re **_cohÃ©rente_**
* a des **_URLs bien structurÃ©es_**
* contient toutes les **_chaÃ®nes de tÃ©lÃ©vision sur une seule page_**
* a une **_page sÃ©parÃ©e par jour de la semaine_**
* **_liste uniquement des films_** et aucun autre type de programme comme des Ã©missions en direct, des nouvelles, des reportages, etc. Sauf si vous pouvez facilement distinguer les films des autres types de programmes.

Avec les rÃ©sultats trouvÃ©s, nous allons scraper [**_The Movie Database_**](https://www.themoviedb.org/) (TMDB) pour obtenir la note du film et quelques autres informations.

# DÃ©cider quelles informations stocker

Je vais extraire les informations suivantes sur les films :

* titre du film
* chaÃ®ne de tÃ©lÃ©vision
* l'heure Ã  laquelle le film commence
* la date Ã  laquelle le film est diffusÃ© Ã  la tÃ©lÃ©vision
* genre
* intrigue
* date de sortie
* lien vers la page de dÃ©tails sur TMDB
* note TMDB

Vous pourriez complÃ©ter cette liste avec tous les acteurs, le rÃ©alisateur, des faits intÃ©ressants sur le film, etc. â€“ toutes les informations que vous aimeriez connaÃ®tre.

Dans Scrapy, ces informations seront stockÃ©es dans les champs d'un **_Item_**.

# CrÃ©er le projet Scrapy

Je vais supposer que vous avez Scrapy installÃ©. Si ce n'est pas le cas, vous pouvez suivre l'excellent [guide d'installation de Scrapy](http://doc.scrapy.org/en/latest/intro/install.html).

Lorsque Scrapy est installÃ©, ouvrez la ligne de commande et allez dans le rÃ©pertoire oÃ¹ vous souhaitez stocker le projet Scrapy. Ensuite, exÃ©cutez :

```
scrapy startproject topfilms
```

Cela crÃ©era une structure de dossiers pour le projet des meilleurs films comme montrÃ© ci-dessous. Vous pouvez ignorer le fichier topfilms.db pour l'instant. Il s'agit de la base de donnÃ©es SQLite que nous crÃ©erons dans le prochain article sur les Pipelines.

![Image](https://www.freecodecamp.org/news/content/images/2020/01/0_dZ6phochXc8Dq1L6.png)

# DÃ©finir les Items Scrapy

Nous allons travailler avec le fichier **_items.py_**. Items.py est crÃ©Ã© par dÃ©faut lors de la crÃ©ation de votre projet Scrapy.

Un `scrapy.Item` est un conteneur qui sera rempli pendant le scraping web. Il contiendra tous les champs que nous voulons extraire de la ou des pages web. Le contenu de l'Item peut Ãªtre accÃ©dÃ© de la mÃªme maniÃ¨re qu'un **_dictionnaire Python_**.

Ouvrez items.py et ajoutez une `classe scrapy.Item` avec les champs suivants :

```python
import scrapy
class TVGuideItem(scrapy.Item):
    title = scrapy.Field()
    channel = scrapy.Field()
    start_ts = scrapy.Field()
    film_date_long = scrapy.Field()
    film_date_short = scrapy.Field()
    genre = scrapy.Field()
    plot = scrapy.Field()
    rating = scrapy.Field()
    tmdb_link = scrapy.Field()
    release_date = scrapy.Field()
    nb_votes = scrapy.Field()
```

# Traiter les Items avec les Pipelines

AprÃ¨s avoir dÃ©marrÃ© un nouveau projet Scrapy, vous aurez un fichier appelÃ© **pipelines.py**. Ouvrez ce fichier et copiez-collez le code montrÃ© ci-dessous. Ensuite, je vous montrerai Ã©tape par Ã©tape ce que chaque partie du code fait.

```python
import sqlite3 as lite
con = None  # connexion Ã  la base de donnÃ©es
class StoreInDBPipeline(object):
    def __init__(self):
        self.setupDBCon()
        self.dropTopFilmsTable()
        self.createTopFilmsTable()
def process_item(self, item, spider):
    self.storeInDb(item)
    return item
def storeInDb(self, item):
    self.cur.execute("INSERT INTO topfilms(\
    title, \
    channel, \
    start_ts, \
    film_date_long, \
    film_date_short, \
    rating, \
    genre, \
    plot, \
    tmdb_link, \
    release_date, \
    nb_votes \
    ) \
    VALUES( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? )",
    (
    item['title'],
    item['channel'],
    item['start_ts'],
    item['film_date_long'],
    item['film_date_short'],
    float(item['rating']),
    item['genre'],
    item['plot'],
    item['tmdb_link'],
    item['release_date'],
    item['nb_votes']
    ))
    self.con.commit()
def setupDBCon(self):
    self.con = lite.connect('topfilms.db')
    self.cur = self.con.cursor()
def __del__(self):
    self.closeDB()
def createTopFilmsTable(self):
    self.cur.execute("CREATE TABLE IF NOT EXISTS topfilms(id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \
    title TEXT, \
    channel TEXT, \
    start_ts TEXT, \
    film_date_long TEXT, \
    film_date_short TEXT, \
    rating TEXT, \
    genre TEXT, \
    plot TEXT, \
    tmdb_link TEXT, \
    release_date TEXT, \
    nb_votes \
    )")
def dropTopFilmsTable(self):
    self.cur.execute("DROP TABLE IF EXISTS topfilms")
    
    def closeDB(self):
    self.con.close()
```

Tout d'abord, nous commenÃ§ons par importer le [package SQLite](https://docs.python.org/2/library/sqlite3.html) et lui donnons l'alias `lite`. Nous initialisons Ã©galement une variable `con` qui est utilisÃ©e pour la connexion Ã  la base de donnÃ©es.

## CrÃ©er une classe pour stocker les Items dans la base de donnÃ©es

Ensuite, vous crÃ©ez une [**_classe_**](https://docs.python.org/2/tutorial/classes.html) avec un nom logique. AprÃ¨s avoir activÃ© le pipeline dans le fichier de paramÃ¨tres (plus d'informations Ã  ce sujet plus tard), cette classe sera appelÃ©e.

```python
class StoreInDBPipeline(object):
```

## DÃ©finir la mÃ©thode constructeur

La mÃ©thode constructeur est la mÃ©thode avec le nom `__init__`. Cette mÃ©thode est automatiquement exÃ©cutÃ©e lors de la crÃ©ation d'une instance de la classe `StoreInDBPipeline`.

```python
def __init__(self):
    self.setupDBCon()
    self.dropTopFilmsTable()
    self.createTopFilmsTable()
```

Dans la mÃ©thode constructeur, nous lanÃ§ons trois autres mÃ©thodes qui sont dÃ©finies sous la mÃ©thode constructeur.

## MÃ©thode SetupDBCon

Avec la mÃ©thode `setupDBCon`, nous crÃ©ons la base de donnÃ©es `topfilms` (si elle n'existait pas encore) et Ã©tablissons une connexion avec la fonction `connect`.

```python
def setupDBCon(self):
    self.con = lite.connect('topfilms.db')
	self.cur = self.con.cursor()
```

Ici, nous utilisons l'alias lite pour le package SQLite. DeuxiÃ¨mement, nous crÃ©ons un objet Curseur avec la fonction `cursor`. Avec cet objet Curseur, nous pouvons exÃ©cuter des instructions SQL dans la base de donnÃ©es.

## MÃ©thode DropTopFilmsTable

La deuxiÃ¨me mÃ©thode appelÃ©e dans le constructeur est `dropTopFilmsTable`. Comme son nom l'indique, elle supprime la table dans la base de donnÃ©es SQLite.

Chaque fois que le scraper web est exÃ©cutÃ©, la base de donnÃ©es sera complÃ¨tement supprimÃ©e. C'est Ã  vous de dÃ©cider si vous souhaitez faire de mÃªme. Si vous souhaitez effectuer des requÃªtes ou des analyses sur les donnÃ©es des films, vous pourriez conserver les rÃ©sultats de chaque exÃ©cution.

Je veux simplement voir les films les mieux notÃ©s des jours Ã  venir et rien de plus. Par consÃ©quent, j'ai dÃ©cidÃ© de supprimer la base de donnÃ©es Ã  chaque exÃ©cution.

```python
def dropTopFilmsTable(self):
    self.cur.execute("DROP TABLE IF EXISTS topfilms")
```

Avec l'objet Curseur `cur`, nous exÃ©cutons l'instruction `DROP`.

## MÃ©thode CreateTopFilmsTable

AprÃ¨s avoir supprimÃ© la table des meilleurs films, nous devons la crÃ©er. Cela est fait par le dernier appel de mÃ©thode dans la mÃ©thode constructeur.

```python
def createTopFilmsTable(self):
    self.cur.execute("CREATE TABLE IF NOT EXISTS topfilms(id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \
    title TEXT, \
    channel TEXT, \
    start_ts TEXT, \
    film_date_long TEXT, \
    film_date_short TEXT, \
    rating TEXT, \
    genre TEXT, \
    plot TEXT, \
    tmdb_link TEXT, \
    release_date TEXT, \
    nb_votes \
    )")
```

Encore une fois, nous utilisons l'objet Curseur `cur` pour exÃ©cuter l'instruction `CREATE TABLE`. Les champs ajoutÃ©s Ã  la table des meilleurs films sont les mÃªmes que dans l'Item Scrapy que nous avons crÃ©Ã© prÃ©cÃ©demment. Pour simplifier les choses, j'utilise exactement les mÃªmes noms dans la table SQLite que dans l'Item. Seul le champ `id` est supplÃ©mentaire.

_**Note de cÃ´tÃ©**_ : une bonne application pour visualiser vos bases de donnÃ©es SQLite est le [plugin SQLite Manager dans Firefox](https://addons.mozilla.org/nl/firefox/addon/sqlite-manager/). Vous pouvez regarder ce [tutoriel SQLite Manager sur Youtube](https://youtu.be/y-yA7YT-7gw) pour apprendre Ã  utiliser ce plugin.

## MÃ©thode Process_item

Cette mÃ©thode doit Ãªtre implÃ©mentÃ©e dans la classe Pipeline et elle doit retourner un dict, un Item ou une exception DropItem. Dans notre scraper web, nous retournerons l'item.

```python
def process_item(self, item, spider):
    self.storeInDb(item)
	return item
```

Contrairement aux autres mÃ©thodes expliquÃ©es, elle a deux arguments supplÃ©mentaires. L'`item` qui a Ã©tÃ© scrapÃ© et le `spider` qui a scrapÃ© l'item. Ã€ partir de cette mÃ©thode, nous lanÃ§ons la mÃ©thode `storeInDb` et retournons ensuite l'item.

## MÃ©thode StoreInDb

Cette mÃ©thode exÃ©cute une instruction `INSERT` pour insÃ©rer l'item scrapÃ© dans la base de donnÃ©es SQLite.

```python
def storeInDb(self, item):
    self.cur.execute("INSERT INTO topfilms(\
    title, \
    channel, \
    start_ts, \
    film_date_long, \
    film_date_short, \
    rating, \
    genre, \
    plot, \
    tmdb_link, \
    release_date, \
    nb_votes \
    ) \
    VALUES( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? )",
                     (
                         item['title'],
                         item['channel'],
                         item['start_ts'],
                         item['film_date_long'],
                         item['film_date_short'],
                         float(item['rating']),
                         item['genre'],
                         item['plot'],
                         item['tmdb_link'],
                         item['release_date'],
                         item['nb_votes']
                     ))
    self.con.commit()
```

Les valeurs pour les champs de la table proviennent de l'item, qui est un argument pour cette mÃ©thode. Ces valeurs sont simplement appelÃ©es comme une valeur de dictionnaire (rappelons qu'un Item n'est rien de plus qu'un dictionnaire ?).

## Chaque constructeur a un... destructeur

Le contrepartie de la mÃ©thode constructeur est la mÃ©thode destructeur avec le nom `__del__`. Dans la mÃ©thode destructeur pour cette classe de pipelines, nous fermons la connexion Ã  la base de donnÃ©es.

```python
def __del__(self):
    self.closeDB()
```

## MÃ©thode CloseDB

```python
def closeDB(self):
    self.con.close()
```

Dans cette derniÃ¨re mÃ©thode, nous fermons la connexion Ã  la base de donnÃ©es avec la fonction `close`. Nous avons donc Ã©crit un pipeline entiÃ¨rement fonctionnel. Il reste encore une derniÃ¨re Ã©tape pour activer le pipeline.

## Activer le pipeline dans settings.py

Ouvrez le fichier **_settings.py_** et ajoutez le code suivant :

```python
ITEM_PIPELINES = {
    'topfilms.pipelines.StoreInDBPipeline':1
}
```

La **_valeur entiÃ¨re_** indique l'ordre dans lequel les pipelines sont exÃ©cutÃ©s. Comme nous n'avons qu'un seul pipeline, nous lui attribuons la valeur 1.

# CrÃ©er un Spider dans Scrapy

Maintenant, nous allons examiner le cÅ“ur de Scrapy, le **_Spider_**. C'est ici que le travail intensif de votre scraper web sera effectuÃ©. Je vais vous montrer Ã©tape par Ã©tape comment en crÃ©er un.

## Importer les packages nÃ©cessaires

Tout d'abord, nous allons importer les packages et modules nÃ©cessaires. Nous utilisons le module `CrawlSpider` pour suivre les liens dans le guide TV en ligne.

`Rule` et `LinkExtractor` sont utilisÃ©s pour dÃ©terminer quels liens nous voulons suivre.

Le module `config` contient certaines constantes comme `DOM_1, DOM_2` et `START_URL` qui sont utilisÃ©es dans le Spider. Le module config se trouve un rÃ©pertoire au-dessus du rÃ©pertoire actuel. C'est pourquoi vous voyez deux points avant le module config.

Et enfin, nous importons le `TVGuideItem`. Ce TVGuideItem sera utilisÃ© pour contenir les informations pendant le scraping.

```python
import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from fuzzywuzzy import fuzz
from ..config import *
from topfilms.items import TVGuideItem
```

## Dire au Spider oÃ¹ aller

DeuxiÃ¨mement, nous sous-classons la classe CrawlSpider. Cela est fait en insÃ©rant CrawlSpider comme argument pour la classe `TVGuideSpider`.

Nous donnons au Spider un `nom`, fournissons les `domaines autorisÃ©s` (par exemple, themoviedb.org) et les `start_urls`. Les start_urls est dans mon cas la page web du guide TV, donc vous devriez la changer par votre propre site web prÃ©fÃ©rÃ©.

Avec `rules` et l'argument `deny`, nous disons au Spider quelles URLs (ne pas) suivre sur l'URL de dÃ©part. L'URL Ã  ne pas suivre est spÃ©cifiÃ©e avec une expression rÃ©guliÃ¨re.

![Image](https://www.freecodecamp.org/news/content/images/2020/01/0_r0r11AyaIBEC7ODH.png)

Je ne suis pas intÃ©ressÃ© par les films qui ont Ã©tÃ© diffusÃ©s hier, ne permettez pas au Spider de suivre les URLs se terminant par Â« _gisteren_ Â».

D'accord, mais quelles URLs le Spider doit-il suivre ? Pour cela, j'utilise l'argument `restrict_xpaths`. Il dit de suivre toutes les URLs avec la `class="button button--beta"`. Ce sont en fait des URLs avec des films pour la semaine Ã  venir.

Enfin, avec l'argument `callback`, nous faisons savoir au Spider quoi faire lorsqu'il suit l'une des URLs. Il exÃ©cutera la fonction `parse_by_day`. Je vais expliquer cela dans la partie suivante.

```python
class TVGuideSpider(CrawlSpider):
    name = "tvguide"
    allowed_domains = [DOM_1, DOM_2]
    start_urls = [START_URL]
# Extraire les liens de la navigation par jour
    # Nous ne scraperons pas les films d'hier
    rules = (
        Rule(LinkExtractor(allow=(), deny=(r'\/gisteren'), restrict_xpaths=('//a[@class="button button--beta"]',)), callback="parse_by_day", follow= True),
    )
```

## Analyser les URLs suivies

La fonction `parse_by_day`, partie du TVGuideScraper, scrape les pages web avec l'aperÃ§u de tous les films par chaÃ®ne par jour. L'argument `response` provient de la `Request` qui a Ã©tÃ© lancÃ©e lors de l'exÃ©cution du programme de scraping web.

Sur la page web en cours de scraping, vous devez trouver les Ã©lÃ©ments HTML utilisÃ©s pour afficher les informations qui nous intÃ©ressent. Deux bons outils pour cela sont les [Chrome Developer Tools](https://developers.google.com/web/tools/chrome-devtools/) et le [plugin Firebug dans Firefox](https://addons.mozilla.org/nl/firefox/addon/firebug/).

Une chose que nous voulons stocker est la `date` des films que nous scrapons. Cette date peut Ãªtre trouvÃ©e dans le paragraphe (p) dans la div avec `class="grid__col__inner"`. Clairement, c'est quelque chose que vous devriez modifier pour la page que vous scrapez.

![Image](https://www.freecodecamp.org/news/content/images/2020/01/0_-EML6UFd2TbqVCmY.png)

Avec la `mÃ©thode xpath` de l'objet Response, nous extrayons le texte dans le paragraphe. J'ai appris beaucoup de cela dans le [tutoriel sur l'utilisation de la fonction xpath](http://zvon.org/comp/r/tut-XPath_1.html).

En utilisant `extract_first`, nous nous assurons de ne pas stocker cette date sous forme de liste. Sinon, cela nous posera des problÃ¨mes lors du stockage de la date dans la base de donnÃ©es SQLite.

Par la suite, j'effectue un nettoyage des donnÃ©es sur film_date_long et crÃ©e `film_date_short` avec le format AAAAMMJJ. J'ai crÃ©Ã© ce format AAAAMMJJ pour trier les films chronologiquement plus tard.

Ensuite, la chaÃ®ne de tÃ©lÃ©vision est scrapÃ©e. Si elle est dans la liste des `ALLOWED_CHANNELS` (dÃ©finie dans le module config), nous continuons Ã  scraper le titre et l'heure de dÃ©but. Ces informations sont stockÃ©es dans l'item, qui est initiÃ© par `TVGuideItem()`.

AprÃ¨s cela, nous voulons continuer Ã  scraper sur The Movie Database. Nous utiliserons l'URL [**https://www.themoviedb.org/search?query=**](https://www.themoviedb.org/search?query=) pour afficher les rÃ©sultats de recherche pour le film en cours de scraping. Ã€ cette URL, nous voulons ajouter le titre du film (`url_part` dans le code). Nous rÃ©utilisons simplement la partie URL trouvÃ©e dans le lien sur la page web du guide TV.

Avec cette URL, nous crÃ©ons une nouvelle requÃªte et continuons sur TMDB. Avec `request.meta['item'] = item`, nous ajoutons les donnÃ©es dÃ©jÃ  scrapÃ©es Ã  la requÃªte. De cette faÃ§on, nous pouvons continuer Ã  remplir notre TVGuideItem actuel.

`Yield request` lance effectivement la requÃªte.

```python
def parse_by_day(self, response):
    film_date_long = response.xpath('//div[@class="grid__col__inner"]/p/text()').extract_first()
    film_date_long = film_date_long.rsplit(',',1)[-1].strip()  # Supprimer le nom du jour et les espaces blancs
    # CrÃ©er une date de film avec un format court comme AAAAMMJJ pour trier les rÃ©sultats chronologiquement
    film_day_parts = film_date_long.split()
    months_list = ['janvier', 'fÃ©vrier', 'mars',
                  'avril', 'mai', 'juin', 'juillet',
                  'aoÃ»t', 'septembre', 'octobre',
                  'novembre', 'dÃ©cembre' ]
    year = str(film_day_parts[2])
    month = str(months_list.index(film_day_parts[1]) + 1).zfill(2)
    day = str(film_day_parts[0]).zfill(2)
    film_date_short = year + month + day
    for col_inner in response.xpath('//div[@class="grid__col__inner"]'):
        chnl = col_inner.xpath('.//div[@class="tv-guide__channel"]/h6/a/text()').extract_first()
        if chnl in ALLOWED_CHANNELS:
            for program in col_inner.xpath('.//div[@class="program"]'):
                item = TVGuideItem()
                item['channel'] = chnl
                item['title'] = program.xpath('.//div[@class="title"]/a/text()').extract_first()
                item['start_ts'] = program.xpath('.//div[@class="time"]/text()').extract_first()
                item['film_date_long'] = film_date_long
                item['film_date_short'] = film_date_short
                detail_link = program.xpath('.//div[@class="title"]/a/@href').extract_first()
                url_part = detail_link.rsplit('/',1)[-1]
                # Extraire les informations de la base de donnÃ©es de films www.themoviedb.org
                request = scrapy.Request("https://www.themoviedb.org/search?query="+url_part,callback=self.parse_tmdb)
                request.meta['item'] = item  # Passer l'item avec la requÃªte Ã  la page de dÃ©tails
    yield request
```

## Scraper des informations supplÃ©mentaires sur The Movie DataBase

Comme vous pouvez le voir dans la requÃªte crÃ©Ã©e dans la fonction `parse_by_day`, nous utilisons la fonction de rappel `parse_tmdb`. Cette fonction est utilisÃ©e lors de la requÃªte pour scraper le site web TMDB.

Dans la premiÃ¨re Ã©tape, nous obtenons les informations de l'item qui ont Ã©tÃ© passÃ©es par la fonction `parse_by_day`.

La page avec les rÃ©sultats de recherche sur TMDB peut Ã©ventuellement lister plusieurs rÃ©sultats de recherche pour le mÃªme titre de film (url_part passÃ©e dans la requÃªte). Nous vÃ©rifions Ã©galement s'il y a des rÃ©sultats avec `if tmddb_titles`.

![Image](https://www.freecodecamp.org/news/content/images/2020/01/0_ncBMqbk9fzZ-Szi0.png)

Nous utilisons le package [fuzzywuzzy](https://pypi.python.org/pypi/fuzzywuzzy) pour effectuer une correspondance floue sur les titres de films. Afin d'utiliser le package fuzzywuzzy, nous devons ajouter l'instruction `import` avec les instructions d'importation prÃ©cÃ©dentes.

```python
from fuzzywuzzy import fuzz
```

Si nous trouvons une correspondance de 90 %, nous utilisons ce rÃ©sultat de recherche pour faire le reste du scraping. Nous ne regardons plus les autres rÃ©sultats de recherche. Pour cela, nous utilisons l'instruction `break`.

Ensuite, nous recueillons le `genre`, la `note` et la `date de sortie` de la page des rÃ©sultats de recherche de maniÃ¨re similaire Ã  celle dont nous avons utilisÃ© la fonction xpath prÃ©cÃ©demment. Pour obtenir un format AAAAMMJJ pour la date de sortie, nous exÃ©cutons un traitement des donnÃ©es avec les fonctions `split` et `join`.

Encore une fois, nous voulons lancer une nouvelle requÃªte vers la page de dÃ©tails sur TMDB. Cette requÃªte appellera la fonction `parse_tmdb_detail` pour extraire l'intrigue du film et le nombre de votes sur TMDB. Cela est expliquÃ© dans la section suivante.

```python
def parse_tmdb(self, response):
    item = response.meta['item']  # Utiliser l'item passÃ©


    tmdb_titles = response.xpath('//a[@class="title result"]/text()').extract()
    if tmdb_titles:  # VÃ©rifier s'il y a des rÃ©sultats sur TMDB
        for tmdb_title in tmdb_titles:
            match_ratio = fuzz.ratio(item['title'], tmdb_title)
            if match_ratio > 90:
                item['genre'] = response.xpath('.//span[@class="genres"]/text()').extract_first()
                item['rating'] = response.xpath('//span[@class="vote_average"]/text()').extract_first()
                release_date = response.xpath('.//span[@class="release_date"]/text()').extract_first()
                release_date_parts = release_date.split('/')
                item['release_date'] = "/".join(
                    [release_date_parts[1].strip(), release_date_parts[0].strip(), release_date_parts[2].strip()])
                tmdb_link = "https://www.themoviedb.org" + response.xpath(
                    '//a[@class="title result"]/@href').extract_first()
                item['tmdb_link'] = tmdb_link
                # Extraire plus d'informations de la page de dÃ©tails
                request = scrapy.Request(tmdb_link, callback=self.parse_tmdb_detail)
                request.meta['item'] = item  # Passer l'item avec la requÃªte Ã  la page de dÃ©tails
    yield request
    break  # Nous ne considÃ©rons que la premiÃ¨re correspondance
    else:
        return
```

## Scraper l'intrigue du film depuis la page de dÃ©tails

La derniÃ¨re fonction que nous allons discuter est courte. Comme prÃ©cÃ©demment, nous obtenons l'item passÃ© par la fonction parse_tmdb et scrapons la page de dÃ©tails pour l'`intrigue` et le `nombre de votes`.

![Image](https://www.freecodecamp.org/news/content/images/2020/01/0_C-Tj8dZ8yxfx_3gV.png)

Ã€ ce stade, nous avons terminÃ© le scraping des informations pour le film. En d'autres termes, l'item pour le film est complÃ¨tement rempli. Scrapy utilisera ensuite le code Ã©crit dans les pipelines pour traiter ces donnÃ©es et les mettre dans la base de donnÃ©es.

```python
def parse_tmdb_detail(self, response):
    item = response.meta['item']  # Utiliser l'item passÃ©
    item['nb_votes'] = response.xpath('//span[@itemprop="ratingCount"]/text()').extract_first()
    item['plot'] = response.xpath('.//p[@id="overview"]/text()').extract_first()
    yield item
```

# Utiliser les Extensions dans Scrapy

Dans la section sur les Pipelines, nous avons dÃ©jÃ  vu comment nous stockons les rÃ©sultats du scraping dans une base de donnÃ©es SQLite. Maintenant, je vais vous montrer comment vous pouvez **_envoyer les rÃ©sultats du scraping par email._** De cette faÃ§on, vous obtenez un bel aperÃ§u des films les mieux notÃ©s pour la semaine Ã  venir dans votre boÃ®te mail.

## Importer les packages nÃ©cessaires

Nous allons travailler avec le fichier **_extensions.py_**. Ce fichier est automatiquement crÃ©Ã© dans le rÃ©pertoire racine lorsque vous avez crÃ©Ã© le projet Scrapy. Nous commenÃ§ons par importer les packages que nous utiliserons plus tard dans ce fichier.

```python
import logging
from scrapy import signals
from scrapy.exceptions import NotConfigured
import smtplib
import sqlite3 as lite
from config import *
```

Le package `logging` n'est pas vraiment requis. Mais ce package peut Ãªtre utile pour dÃ©boguer votre programme ou simplement pour Ã©crire quelques informations dans le journal. 
Le module `signals` nous aidera Ã  savoir quand l'araignÃ©e a Ã©tÃ© ouverte et fermÃ©e. Nous enverrons l'email avec les films aprÃ¨s que l'araignÃ©e ait fait son travail.

Du module `scrapy.exceptions`, nous importons la mÃ©thode `NotConfigured`. Cela sera levÃ© lorsque l'extension n'est pas configurÃ©e dans le fichier **_settings.py_**. ConcrÃ¨tement, le paramÃ¨tre `MYEXT_ENABLED` doit Ãªtre dÃ©fini sur `True`. Nous verrons cela plus tard dans le code.

Le package `smtplib` est importÃ© pour pouvoir envoyer l'email. J'utilise mon adresse Gmail pour envoyer l'email, mais vous pourriez adapter le code dans config.py pour utiliser un autre service d'email.

Enfin, nous importons le package `sqlite3` pour extraire les films les mieux notÃ©s de la base de donnÃ©es et importons `config` pour obtenir nos constantes.

## CrÃ©er la classe SendEmail dans les extensions

Tout d'abord, nous dÃ©finissons l'objet `logger`. Avec cet objet, nous pouvons Ã©crire des messages dans le journal Ã  certains Ã©vÃ©nements. Ensuite, nous crÃ©ons la classe `SendEmail` avec la mÃ©thode constructeur. Dans le constructeur, nous attribuons `FROMADDR` et `TOADDR` aux attributs correspondants de la classe. Ces constantes sont dÃ©finies dans le fichier **_config.py_**. J'ai utilisÃ© mon adresse Gmail pour les deux attributs.

```python
logger = logging.getLogger(__name__)
class SendEmail(object):
    def __init__(self):
        self.fromaddr = FROMADDR
        self.toaddr  = TOADDR
```

## Instancier l'objet d'extension

La premiÃ¨re mÃ©thode de l'objet `SendEmail` est `from_crawler`. La premiÃ¨re vÃ©rification que nous faisons est de savoir si `MYEXT_ENABLED` est activÃ© dans le fichier settings.py. Si ce n'est pas le cas, nous levons une exception `NotConfigured`. Lorsque cela se produit, le reste du code dans l'extension n'est pas exÃ©cutÃ©.

Dans le fichier **_settings.py_**, nous devons ajouter le code suivant pour activer cette extension.

```python
MYEXT_ENABLED = True
EXTENSIONS = {
    'topfilms.extensions.SendEmail': 500,
    'scrapy.telnet.TelnetConsole': None
}
```

Nous dÃ©finissons donc le drapeau boolÃ©en `MYEXT_ENABLED` sur `True`. Ensuite, nous ajoutons notre propre extension `SendEmail` au dictionnaire `EXTENSIONS`. La valeur entiÃ¨re de 500 spÃ©cifie l'ordre dans lequel l'extension doit Ãªtre exÃ©cutÃ©e. J'ai Ã©galement dÃ» dÃ©sactiver le `TelnetConsole`. Sinon, l'envoi de l'email ne fonctionnait pas. Cette extension est dÃ©sactivÃ©e en mettant `None` au lieu d'une valeur d'ordre entiÃ¨re.

Ensuite, nous instancions l'objet d'extension avec la fonction `cls()`. Ã€ cet objet d'extension, nous connectons certains `signals`. Nous sommes intÃ©ressÃ©s par les signaux `spider_opened` et `spider_closed`. Et enfin, nous retournons l'objet `ext`.

```
@classmethod
def from_crawler(cls, crawler):
    # premiÃ¨re vÃ©rification si l'extension doit Ãªtre activÃ©e et lever
    # NotConfigured sinon
    if not crawler.settings.getbool('MYEXT_ENABLED'):
        raise NotConfigured
    # instancier l'objet d'extension
    ext = cls()
    # connecter l'objet d'extension aux signaux
    crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)
    crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)
    # retourner l'objet d'extension
    return ext
```

## DÃ©finir les actions dans l'Ã©vÃ©nement spider_opened

Lorsque l'araignÃ©e a Ã©tÃ© ouverte, nous voulons simplement Ã©crire cela dans le journal. Par consÃ©quent, nous utilisons l'objet `logger` que nous avons crÃ©Ã© en haut du code. Avec la mÃ©thode `info`, nous Ã©crivons un message dans le journal. `Spider.name` est remplacÃ© par le nom que nous avons dÃ©fini dans le fichier TVGuideSpider.py.

```python
def spider_opened(self, spider):
    logger.info("opened spider %s", spider.name)
```

## Envoyer l'email aprÃ¨s l'Ã©vÃ©nement spider_closed

Dans la derniÃ¨re mÃ©thode de la classe `SendEmail`, nous envoyons l'email contenant l'aperÃ§u des films les mieux notÃ©s.

Encore une fois, nous envoyons une notification au journal que l'araignÃ©e a Ã©tÃ© fermÃ©e. DeuxiÃ¨mement, nous crÃ©ons une connexion Ã  la base de donnÃ©es SQLite contenant tous les films de la semaine Ã  venir pour les **_ALLOWED_CHANNELS_**. Nous sÃ©lectionnons les films avec une `note >= 6.5`. Vous pouvez changer la note pour un seuil plus Ã©levÃ© ou plus bas selon vos souhaits. Les films rÃ©sultants sont ensuite triÃ©s par `film_date_short`, qui a le format AAAAMMJJ et par l'heure de dÃ©but `start_ts`.

Nous rÃ©cupÃ©rons toutes les lignes dans le curseur `cur` et vÃ©rifions si nous avons des rÃ©sultats avec la fonction `len`. Il est possible de n'avoir aucun rÃ©sultat lorsque vous dÃ©finissez la note de seuil trop Ã©levÃ©e, par exemple.

Avec `for row in data`, nous parcourons chaque film rÃ©sultant. Nous extrayons toutes les informations intÃ©ressantes de la `row`. Pour certaines donnÃ©es, nous appliquons un encodage avec `encode('ascii','ignore')`. Cela permet d'ignorer certains des caractÃ¨res spÃ©ciaux comme 9, 0, 8, etc. Sinon, nous obtenons des erreurs lors de l'envoi de l'email.

Lorsque toutes les donnÃ©es sur le film sont recueillies, nous composons une variable de chaÃ®ne `topfilm`. Chaque `topfilm` est ensuite concatÃ©nÃ© Ã  la variable `topfilms_overview`, qui sera le message de l'email que nous envoyons. Si nous n'avons aucun film dans notre rÃ©sultat de requÃªte, nous le mentionnons dans un court message.

Ã€ la fin, nous envoyons le message avec l'adresse Gmail, grÃ¢ce au package `smtplib`.

```python
def spider_closed(self, spider):
    logger.info("closed spider %s", spider.name)
    # Obtenir les films avec une note supÃ©rieure Ã  un seuil
    topfilms_overview = ""
    con = lite.connect('topfilms.db')
    cur = con.execute(
        "SELECT title, channel, start_ts, film_date_long, plot, genre, release_date, rating, tmdb_link, nb_votes "
        "FROM topfilms "
        "WHERE rating >= 6.5 "
        "ORDER BY film_date_short, start_ts")


    data = cur.fetchall()
    if len(data) > 0:  # VÃ©rifier si nous avons des enregistrements dans le rÃ©sultat de la requÃªte
        for row in data:
            title = row[0].encode('ascii', 'ignore')
            channel = row[1]
            start_ts = row[2]
            film_date_long = row[3]
            plot = row[4].encode('ascii', 'ignore')
            genre = row[5]
            release_date = row[6].rstrip()
            rating = row[7]
            tmdb_link = row[8]
            nb_votes = row[9]
            topfilm = ' - '.join([title, channel, film_date_long, start_ts])
            topfilm = topfilm + "\r\n" + "Date de sortie : " + release_date
            topfilm = topfilm + "\r\n" + "Genre : " + str(genre)
            topfilm = topfilm + "\r\n" + "Note TMDB : " + rating + " sur " + nb_votes + " votes"
            topfilm = topfilm + "\r\n" + plot
            topfilm = topfilm + "\r\n" + "Plus d'informations sur : " + tmdb_link
            topfilms_overview = "\r\n\r\n".join([topfilms_overview, topfilm])
    con.close()
    if len(topfilms_overview) > 0:
        message = topfilms_overview
    else:
        message = "Il n'y a pas de films bien notÃ©s pour la semaine Ã  venir."
    msg = "\r\n".join([
        "De : " + self.fromaddr,
        "Ã€ : " + self.toaddr,
        "Sujet : AperÃ§u des meilleurs films",
        message
    ])
    username = UNAME
    password = PW
    server = smtplib.SMTP(GMAIL)
    server.ehlo()
    server.starttls()
    server.login(username, password)
    server.sendmail(self.fromaddr, self.toaddr, msg)
    server.quit()
```

## RÃ©sultat de l'envoi d'emails via Extensions

Le rÃ©sultat final de ce morceau de code est un aperÃ§u des films les mieux notÃ©s dans votre boÃ®te mail. Super ! Maintenant, vous n'avez plus besoin de chercher cela sur le guide TV en ligne.

![Image](https://www.freecodecamp.org/news/content/images/2020/01/0_SuRZuKi2RIkRJD3y.png)

# Astuces pour Ã©viter le bannissement de l'IP

Lorsque vous faites de nombreuses requÃªtes en peu de temps, vous risquez d'Ãªtre banni par le serveur. Dans cette section finale, je vais vous montrer quelques astuces pour Ã©viter le bannissement de l'IP.

## Retarder vos requÃªtes

Une faÃ§on simple d'Ã©viter le bannissement de l'IP est de **_faire une pause entre chaque requÃªte_**. Dans Scrapy, cela peut Ãªtre fait en dÃ©finissant simplement un paramÃ¨tre dans le fichier **_settings.py_**. Comme vous l'avez probablement remarquÃ©, le fichier settings.py a beaucoup de paramÃ¨tres commentÃ©s.

Recherchez le paramÃ¨tre `DOWNLOAD_DELAY` et dÃ©commentez-le. J'ai dÃ©fini la **_durÃ©e de la pause Ã  2 secondes_**. Selon le nombre de requÃªtes que vous devez faire, vous pouvez changer cela. Mais je le dÃ©finirais Ã  au moins 1 seconde.

```python
DOWNLOAD_DELAY=2
```

## MÃ©thode plus avancÃ©e pour Ã©viter le bannissement de l'IP

Par dÃ©faut, chaque fois que vous faites une requÃªte, vous le faites avec le **_mÃªme agent utilisateur_**. GrÃ¢ce au package `fake_useragent`, nous pouvons facilement changer l'agent utilisateur pour chaque requÃªte.

Tous les crÃ©dits pour ce morceau de code vont Ã  [Alecxe](https://github.com/alecxe/scrapy-fake-useragent) qui a Ã©crit un joli script Python pour utiliser le package fake_useragent.

Tout d'abord, nous crÃ©ons un dossier **_scrapy_fake_useragent_** dans le rÃ©pertoire racine de notre projet de scraper web. Dans ce dossier, nous ajoutons deux fichiers :

* **___init__.py_** qui est un fichier vide
* **_middleware.py_**

Pour utiliser ce [middleware](http://doc.scrapy.org/en/latest/topics/spider-middleware.html), nous devons l'activer dans le fichier **_settings.py_**. Cela se fait avec le code :

```python
DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddleware.useragent.UserAgentMiddleware': None,
    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,
}
```

Tout d'abord, nous dÃ©sactivons le `UserAgentMiddleware` par dÃ©faut de Scrapy en spÃ©cifiant _None_ au lieu d'une valeur entiÃ¨re. Ensuite, nous activons notre propre middleware `RandomUserAgentMiddleware`. Intuitivement, le middleware est un morceau de code qui est exÃ©cutÃ© **_pendant une requÃªte_**.

Dans le fichier **_middleware.py_**, nous ajoutons le code pour **_randomiser l'agent utilisateur_** pour chaque requÃªte. Assurez-vous d'avoir le package fake_useragent installÃ©. Du [package fake_usergent](https://pypi.python.org/pypi/fake-useragent), nous importons le module `UserAgent`. Celui-ci contient **_une liste de diffÃ©rents agents utilisateurs_**. Dans le constructeur de la classe RandomUserAgentMiddleware, nous instancions l'objet UserAgent. Dans la mÃ©thode **_process_request_**, nous dÃ©finissons l'agent utilisateur sur un agent utilisateur alÃ©atoire de l'objet `ua` dans l'en-tÃªte de la requÃªte.

```python
from fake_useragent import UserAgent
class RandomUserAgentMiddleware(object):
    def __init__(self):
        super(RandomUserAgentMiddleware, self).__init__()
        self.ua = UserAgent()
    def process_request(self, request, spider):
        request.headers.setdefault('User-Agent', self.ua.random)
```

# Conclusion

C'est tout ! J'espÃ¨re que vous avez maintenant une vue claire sur la faÃ§on d'utiliser Scrapy pour vos projets de scraping web.