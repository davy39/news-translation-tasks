---
title: 'Keras vs PyTorch : comment distinguer Aliens vs Predators avec l''apprentissage
  par transfert'
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2018-10-25T19:02:26.000Z'
originalURL: https://freecodecamp.org/news/keras-vs-pytorch-avp-transfer-learning-c8b852c31f02
coverImage: https://cdn-media-1.freecodecamp.org/images/1*BFIC_uZzi2v1p2254LLv2Q.png
tags:
- name: Deep Learning
  slug: deep-learning
- name: Machine Learning
  slug: machine-learning
- name: Python
  slug: python
- name: 'tech '
  slug: tech
- name: technology
  slug: technology
seo_title: 'Keras vs PyTorch : comment distinguer Aliens vs Predators avec l''apprentissage
  par transfert'
seo_desc: 'By Patryk Miziu≈Ça

  This article was written by Piotr Migda≈Ç, Rafa≈Ç Jakubanis and myself. In the previous
  post, they gave you an overview of the differences between Keras and PyTorch, aiming
  to help you pick the framework that‚Äôs better suited to your n...'
---

Par Patryk Miziu≈Ça

Cet article a √©t√© √©crit par [Piotr Migda≈Ç](http://p.migdal.pl/), [Rafa≈Ç Jakubanis](https://medium.com/@rafaljakubanis) et moi-m√™me. Dans le pr√©c√©dent article, ils vous ont donn√© un [aper√ßu des diff√©rences entre Keras et PyTorch](https://deepsense.ai/keras-or-pytorch/), visant √† vous aider √† choisir le framework le mieux adapt√© √† vos besoins.

Maintenant, il est temps pour un combat.

Nous allons opposer Keras et PyTorch, montrant leurs forces et faiblesses en action. Nous pr√©sentons un probl√®me r√©el, une question de vie ou de mort : distinguer les Aliens des Predators !

![Image](https://cdn-media-1.freecodecamp.org/images/1*fAS0pLDYQAwRvLiZtzPDCg.jpeg)
_Image tir√©e de notre dataset. Le Predator et l'Alien sont profond√©ment int√©ress√©s par l'IA._

Nous allons effectuer une classification d'images, l'une des t√¢ches de vision par ordinateur o√π le deep learning excelle. Comme l'entra√Ænement √† partir de z√©ro est irr√©alisable dans la plupart des cas (car il n√©cessite beaucoup de donn√©es), nous allons effectuer un apprentissage par transfert en utilisant ResNet-50 pr√©-entra√Æn√© sur ImageNet. Nous allons √™tre aussi pratiques que possible, pour montrer √† la fois les diff√©rences conceptuelles et les conventions.

En m√™me temps, nous allons garder le code assez minimal, pour le rendre clair et facile √† lire et √† r√©utiliser. Voir les [notebooks sur GitHub](https://github.com/deepsense-ai/Keras-PyTorch-AvP-transfer-learning), les [kernels Kaggle](https://www.kaggle.com/pmigdal/alien-vs-predator-images/kernels) ou les [versions Neptune avec des graphiques](https://app.neptune.ml/deepsense-ai/Keras-vs-PyTorch).

### Attendez, qu'est-ce que l'apprentissage par transfert ? Et pourquoi ResNet-50 ?

> En pratique, tr√®s peu de gens entra√Ænent un r√©seau de neurones convolutionnel entier √† partir de z√©ro (avec une initialisation al√©atoire), car il est relativement rare d'avoir un dataset de taille suffisante. Au lieu de cela, il est courant de pr√©-entra√Æner un ConvNet sur un tr√®s grand dataset (par exemple, ImageNet, qui contient 1,2 million d'images avec 1000 cat√©gories), puis d'utiliser le ConvNet soit comme une initialisation, soit comme un extracteur de caract√©ristiques fixe pour la t√¢che qui nous int√©resse. ‚Äî _Andrej Karpathy, [Transfer Learning ‚Äî CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/transfer-learning/)_

L'[apprentissage par transfert](http://cs231n.github.io/transfer-learning/) est un processus qui consiste √† apporter de minuscules ajustements √† un r√©seau entra√Æn√© sur une t√¢che donn√©e pour en effectuer une autre, similaire.

Dans notre cas, nous travaillons avec le mod√®le ResNet-50 entra√Æn√© pour classer les images du dataset [ImageNet](http://image-net.org/index). Il est suffisant pour [apprendre beaucoup de textures et de motifs](https://distill.pub/2017/feature-visualization/) qui peuvent √™tre utiles dans d'autres t√¢ches visuelles, m√™me aussi √©tranges que ce cas Alien vs. Predator. Ainsi, nous utiliserons beaucoup moins de puissance de calcul pour obtenir de bien meilleurs r√©sultats.

Dans notre cas, nous allons le faire de la mani√®re la plus simple possible :

* garder les couches convolutionnelles pr√©-entra√Æn√©es (appel√©es extracteur de caract√©ristiques), avec leurs poids gel√©s, et
* supprimer les couches denses originales et les remplacer par de nouvelles couches denses que nous utiliserons pour l'entra√Ænement.

![Image](https://cdn-media-1.freecodecamp.org/images/1*BFIC_uZzi2v1p2254LLv2Q.png)

Alors, quel r√©seau devons-nous choisir comme extracteur de caract√©ristiques ?

[ResNet-50](http://dgschwend.github.io/netscope/#/preset/resnet-50) est un mod√®le populaire pour la classification d'images ImageNet (AlexNet, VGG, GoogLeNet, Inception, Xception sont d'autres mod√®les populaires). Il s'agit d'une architecture de r√©seau de neurones profond de 50 couches bas√©e sur des [connexions r√©siduelles](https://blog.waya.ai/deep-residual-learning-9610bb62c355), qui sont des connexions qui ajoutent des modifications √† chaque couche, plut√¥t que de changer compl√®tement le signal.

ResNet √©tait l'√©tat de l'art sur ImageNet en 2015. Depuis, des [architectures plus r√©centes avec des scores plus √©lev√©s sur ImageNet](https://www.eff.org/ai/metrics) ont √©t√© invent√©es. Cependant, elles ne sont pas n√©cessairement meilleures pour g√©n√©raliser √† d'autres datasets (voir l'article arXiv [Do Better ImageNet Models Transfer Better?](https://arxiv.org/abs/1805.08974)).

D'accord, il est temps de plonger dans le code.

### Que le match commence !

Nous allons configurer notre d√©fi Alien vs. Predator en sept √©tapes :

0. Pr√©parer le dataset  
1. Importer les d√©pendances  
2. Cr√©er des g√©n√©rateurs de donn√©es  
3. Cr√©er le r√©seau  
4. Entra√Æner le mod√®le  
5. Sauvegarder et charger le mod√®le  
6. Faire des pr√©dictions sur des images de test

Nous compl√©tons cet article de blog avec du code Python dans des Jupyter Notebooks ([Keras-ResNet50.ipynb](https://github.com/deepsense-ai/Keras-PyTorch-AvP-transfer-learning/blob/master/Keras-ResNet50.ipynb), [PyTorch-ResNet50.ipynb](https://github.com/deepsense-ai/Keras-PyTorch-AvP-transfer-learning/blob/master/PyTorch-ResNet50.ipynb)). Cet environnement est plus pratique pour le prototypage que les scripts bruts, car nous pouvons l'ex√©cuter cellule par cellule et examiner la sortie.

Tr√®s bien, c'est parti !

### 0. Pr√©parer le dataset

Nous avons cr√©√© un dataset en effectuant une recherche Google avec les mots ¬´ alien ¬ª et ¬´ predator ¬ª. Nous avons sauvegard√© des miniatures JPG (environ 250√ó250 pixels) et filtr√© manuellement les r√©sultats. Voici quelques exemples :

![Image](https://cdn-media-1.freecodecamp.org/images/1*QmyVYru66iPvvWcITUKobg.png)

Nous avons divis√© nos donn√©es en deux parties :

* Donn√©es d'entra√Ænement (347 √©chantillons par classe) ‚Äî utilis√©es pour entra√Æner le r√©seau.
* Donn√©es de validation (100 √©chantillons par classe) ‚Äî non utilis√©es pendant l'entra√Ænement, mais n√©cessaires pour v√©rifier la performance du mod√®le sur des donn√©es pr√©c√©demment invisibles.

Keras n√©cessite que les datasets soient organis√©s en dossiers de la mani√®re suivante :

Si vous voulez voir le processus d'organisation des donn√©es en r√©pertoires, consultez le fichier data_prep.ipynb. Vous pouvez t√©l√©charger le dataset depuis [Kaggle](https://www.kaggle.com/pmigdal/alien-vs-predator-images).

### 1. Importer les d√©pendances

D'abord, les d√©tails techniques. Nous supposons que vous avez Python 3.5+, Keras 2.2.2 (avec TensorFlow 1.10.1 en backend) et PyTorch 0.4.1. Consultez le fichier [requirements.txt](https://github.com/deepsense-ai/Keras-PyTorch-AvP-transfer-learning/blob/master/requirements.txt) dans le d√©p√¥t.

Donc, d'abord, nous devons importer les modules requis. Nous allons s√©parer le code en Keras, PyTorch et commun (requis dans les deux).

Nous pouvons v√©rifier les versions des frameworks en tapant `keras.__version__` et `torch.__version__`, respectivement.

### 2. Cr√©er des g√©n√©rateurs de donn√©es

Normalement, les images ne peuvent pas toutes √™tre charg√©es en une fois, car cela serait trop pour la m√©moire. En m√™me temps, nous voulons b√©n√©ficier de l'augmentation de performance du GPU en traitant quelques images √† la fois. Nous chargeons donc les images par _lots_ (par exemple, 32 images √† la fois) en utilisant des g√©n√©rateurs de donn√©es. Chaque passage √† travers l'ensemble du dataset est appel√© une _√©poque_.

Nous utilisons √©galement des g√©n√©rateurs de donn√©es pour le pr√©traitement : nous redimensionnons et normalisons les images pour les rendre comme ResNet-50 les aime (224 x 224 px, avec des canaux de couleur mis √† l'√©chelle). Et enfin, mais non des moindres, nous utilisons des g√©n√©rateurs de donn√©es pour perturber al√©atoirement les images √† la vol√©e :

![Image](https://cdn-media-1.freecodecamp.org/images/1*G4WdPAp5x6Z22WiV4mMBqw.png)

Effectuer de tels changements est appel√© _augmentation de donn√©es_. Nous l'utiliserons pour montrer √† un r√©seau de neurones quels types de transformations n'ont pas d'importance. Ou, pour le dire autrement, nous allons nous entra√Æner sur un dataset potentiellement infini en g√©n√©rant de nouvelles images bas√©es sur le dataset original.

Presque toutes les t√¢ches visuelles b√©n√©ficient, √† des degr√©s divers, de l'augmentation de donn√©es pour l'entra√Ænement. Pour plus d'informations sur l'augmentation de donn√©es, voir [appliqu√©e aux photos de plancton](http://benanne.github.io/2015/03/17/plankton.html) ou [comment l'utiliser dans Keras](https://machinelearningmastery.com/image-augmentation-deep-learning-keras/). Dans notre cas, nous appliquons al√©atoirement des cisaillements, des zooms et des retournements horizontaux √† nos aliens et predators.

Ici, nous cr√©ons des g√©n√©rateurs qui :

* chargent les donn√©es depuis les dossiers,
* normalisent les donn√©es (√† la fois l'entra√Ænement et la validation), et
* augmentent les donn√©es (uniquement l'entra√Ænement).

Dans Keras, vous obtenez des augmentations int√©gr√©es et la m√©thode `preprocess_input` normalisant les images mises dans ResNet-50, mais vous n'avez aucun contr√¥le sur leur ordre. Dans PyTorch, vous devez normaliser les images manuellement, mais vous pouvez organiser les augmentations de la mani√®re que vous souhaitez.

Il y a aussi d'autres nuances : par exemple, Keras remplit par d√©faut le reste de l'image augment√©e avec les pixels de bordure (comme vous pouvez le voir sur l'image ci-dessus) tandis que PyTorch le laisse en noir. Chaque fois qu'un framework g√®re votre t√¢che beaucoup mieux que l'autre, examinez de plus pr√®s pour voir s'ils effectuent le pr√©traitement de mani√®re identique ; nous parions qu'ils ne le font pas.

### 3. Cr√©er le r√©seau

L'√©tape suivante consiste √† importer un mod√®le ResNet-50 pr√©-entra√Æn√©, ce qui est un jeu d'enfant dans les deux cas. Nous allons geler toutes les couches convolutionnelles de ResNet-50 et n'entra√Æner que les deux derni√®res couches enti√®rement connect√©es (denses). Comme notre t√¢che de classification n'a que 2 classes (contre 1000 classes d'ImageNet), nous devons ajuster la derni√®re couche.

Ici, nous :

* chargeons le r√©seau pr√©-entra√Æn√©, coupons sa t√™te et gelons ses poids,
* ajoutons des couches denses personnalis√©es (nous choisissons 128 neurones pour la couche cach√©e), et
* d√©finissons l'optimiseur et la fonction de perte.

Nous chargeons ResNet-50 depuis Keras et PyTorch sans effort. Ils offrent √©galement de nombreuses autres architectures pr√©-entra√Æn√©es bien connues : voir le [zoo de mod√®les de Keras](https://keras.io/applications/) et le [zoo de mod√®les de PyTorch](https://pytorch.org/docs/stable/torchvision/models.html). Alors, quelles sont les diff√©rences ?

Dans Keras, nous pouvons importer uniquement les couches d'extraction de caract√©ristiques, sans charger de donn√©es superflues (`include_top=False`). Nous cr√©ons ensuite un mod√®le de mani√®re fonctionnelle, en utilisant les entr√©es et sorties du mod√®le de base. Ensuite, nous utilisons `model.compile(...)` pour y int√©grer la fonction de perte, l'optimiseur et d'autres m√©triques.

Dans PyTorch, le mod√®le est un objet Python. Dans le cas de `models.resnet50`, les couches denses sont stock√©es dans l'attribut `model.fc`. Nous allons les √©craser. La fonction de perte et les optimiseurs sont des objets s√©par√©s. Pour l'optimiseur, nous devons explicitement passer une liste de param√®tres que nous voulons qu'il mette √† jour.

Dans PyTorch, nous devons explicitement sp√©cifier ce que nous voulons charger sur le GPU en utilisant la m√©thode `.to(device)`. Nous devons l'√©crire chaque fois que nous voulons mettre un objet sur le GPU, si disponible. Eh bien...

![Image](https://cdn-media-1.freecodecamp.org/images/1*woYU8o65zMwH4UHvjWj7NA.jpeg)
_Cadre de 'AVP : Alien vs. Predator' : l'ordinateur de poignet des Predators. Nous sommes assez s√ªrs que le Predator pourrait l'utiliser pour calculer logsoftmax._

Le gel des couches fonctionne de mani√®re similaire. Cependant, dans [The Batch Normalization layer of Keras is broken](http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/) (dans la version actuelle ; merci √† Przemys≈Çaw Pobrotyn pour avoir soulev√© ce probl√®me), vous verrez que certaines couches sont modifi√©es de toute fa√ßon, m√™me avec `trainable=False`.

Keras et PyTorch traitent la log-loss de mani√®re diff√©rente.

Dans Keras, un r√©seau pr√©dit des probabilit√©s (a une fonction [softmax](https://medium.com/@uniqtech/understand-the-softmax-function-in-minutes-f3a59641e86d) int√©gr√©e), et ses fonctions de co√ªt int√©gr√©es supposent qu'elles travaillent avec des probabilit√©s.

Dans PyTorch, nous avons plus de libert√©, mais la mani√®re pr√©f√©r√©e est de retourner des logits. Cela est fait pour des raisons num√©riques, effectuer softmax puis log-loss signifie faire des op√©rations `log(exp(x))` inutiles. Donc, au lieu d'utiliser softmax, nous utilisons `LogSoftmax` (et `NLLLoss`) ou les combinons en une seule fonction de perte `nn.CrossEntropyLoss`.

### 4. Entra√Æner le mod√®le

D'accord, ResNet est charg√©, alors pr√©parons-nous √† l'affrontement spatial !

![Image](https://cdn-media-1.freecodecamp.org/images/1*uuFsm4SiVj5TbWtGPtZ0IQ.png)
_Cadre de 'AVP : Alien vs. Predator' : le vaisseau m√®re des Predators. Oui, nous avons entendu dire qu'il n'y a pas de rumbles dans l'espace, mais rien n'est impossible pour les Aliens et les Predators._

Maintenant, nous allons proc√©der √† l'√©tape la plus importante ‚Äî l'entra√Ænement du mod√®le. Nous devons passer les donn√©es, calculer la fonction de perte et modifier les poids du r√©seau en cons√©quence. Alors que nous avions d√©j√† quelques diff√©rences entre Keras et PyTorch dans l'augmentation des donn√©es, la longueur du code √©tait similaire. Pour l'entra√Ænement... la diff√©rence est massive. Voyons comment cela fonctionne !

Ici, nous :

* entra√Ænons le mod√®le, et
* mesurons la fonction de perte (log-loss) et la pr√©cision pour les ensembles d'entra√Ænement et de validation.

Dans Keras, `model.fit_generator` effectue l'entra√Ænement... et c'est tout ! L'entra√Ænement dans Keras est aussi simple que cela. Et comme vous pouvez le trouver dans le notebook, Keras nous donne √©galement une barre de progression et une fonction de chronom√©trage gratuitement. Mais si vous voulez faire quelque chose de non standard, alors la douleur commence...

PyTorch est √† l'autre extr√©mit√©. Tout est explicite ici. Vous avez besoin de plus de lignes pour construire l'entra√Ænement de base, mais vous pouvez librement changer et personnaliser tout ce que vous voulez.

![Image](https://cdn-media-1.freecodecamp.org/images/1*2WwHB_QkewusJM_ELzYXmg.jpeg)
_Shuriken du Predator revenant automatiquement √† son propri√©taire. Pr√©f√©reriez-vous impl√©menter sa capacit√© de suivi dans Keras ou PyTorch ?_

Changeons de vitesse et analysons le code d'entra√Ænement de PyTorch. Nous avons des boucles imbriqu√©es, it√©rant sur :

* les √©poques,
* les phases d'entra√Ænement et de validation, et
* les lots.

La boucle d'√©poque ne fait rien d'autre que r√©p√©ter le code √† l'int√©rieur. Les phases d'entra√Ænement et de validation sont faites pour trois raisons :

* Certaines couches sp√©ciales, comme la [normalisation par lots](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html) (pr√©sente dans ResNet-50) et le [dropout](https://deepsense.ai/wp-content/uploads/2018/10/srivastava14a.pdf) (absent dans ResNet-50), fonctionnent diff√©remment pendant l'entra√Ænement et la validation. Nous d√©finissons leur comportement par `model.train()` et `model.eval()`, respectivement.
* Nous utilisons diff√©rentes images pour l'entra√Ænement et pour la validation, bien s√ªr.
* La chose la plus importante et la moins surprenante : nous entra√Ænons le r√©seau uniquement pendant l'entra√Ænement. Les commandes magiques `optimizer.zero_grad()`, `loss.backward()` et `optimizer.step()` (dans cet ordre) font le travail. Si vous savez ce qu'est la [r√©tropropagation](https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/), vous appr√©ciez leur √©l√©gance.

Nous prenons ensuite soin de calculer les pertes d'√©poque et les impressions nous-m√™mes.

### 5. Sauvegarder et charger le mod√®le

#### Sauvegarde

Une fois notre r√©seau entra√Æn√©, souvent avec des co√ªts computationnels et temporels √©lev√©s, il est bon de le garder pour plus tard. Globalement, il existe deux types de sauvegardes :

* sauvegarder toute l'architecture du mod√®le et les poids entra√Æn√©s (et l'√©tat de l'optimiseur) dans un fichier, et
* sauvegarder les poids entra√Æn√©s dans un fichier (en gardant l'architecture du mod√®le dans le code).

C'est √† vous de choisir la m√©thode.

Ici, nous :

* sauvegardons le mod√®le.

Une ligne de code suffit dans les deux frameworks. Dans Keras, vous pouvez soit tout sauvegarder dans un fichier [HDF5](https://www.h5py.org/), soit sauvegarder les poids dans HDF5 et l'architecture dans un fichier JSON lisible. Au fait : [vous pouvez ensuite charger le mod√®le et l'ex√©cuter dans le navigateur](https://medium.com/tensorflow/train-on-google-colab-and-run-on-the-browser-a-case-study-8a45f9b1474e).

![Image](https://cdn-media-1.freecodecamp.org/images/1*WQupj2OpGHFnD6CrgSaPxA.jpeg)
_Cadre de 'Alien : R√©surrection' : l'Alien √©volue, tout comme PyTorch._

Actuellement, les cr√©ateurs de PyTorch [recommandent de sauvegarder uniquement les poids](https://pytorch.org/docs/stable/notes/serialization.html). Ils d√©conseillent de sauvegarder tout le mod√®le car l'API √©volue encore.

#### Chargement

Le chargement des mod√®les est aussi simple que la sauvegarde. Vous devez simplement vous souvenir de la m√©thode de sauvegarde que vous avez choisie et des chemins des fichiers.

Ici, nous :

* chargeons le mod√®le.

Dans Keras, nous pouvons charger un mod√®le √† partir d'un fichier JSON, au lieu de le cr√©er en Python (au moins lorsque nous n'utilisons pas de couches personnalis√©es). Ce type de s√©rialisation le rend pratique pour transf√©rer des mod√®les.

PyTorch peut utiliser n'importe quel code Python. Donc, nous devons pratiquement recr√©er un mod√®le en Python.

Le chargement des poids du mod√®le est similaire dans les deux frameworks.

### 6. Faire des pr√©dictions sur des images de test

Tr√®s bien, il est enfin temps de faire quelques pr√©dictions ! Pour v√©rifier √©quitablement la qualit√© de notre solution, nous allons demander au mod√®le de pr√©dire le type de monstres √† partir d'images non utilis√©es pour l'entra√Ænement. Nous pouvons utiliser l'ensemble de validation, ou toute autre image.

Ici, nous :

* chargeons et pr√©traitons les images de test,
* pr√©disons les cat√©gories d'images, et
* montrons les images et les pr√©dictions.

La pr√©diction, comme l'entra√Ænement, fonctionne par lots (ici nous utilisons un lot de 3 ; bien que nous puissions certainement aussi utiliser un lot de 1).

Dans Keras et PyTorch, nous devons charger et pr√©traiter les donn√©es. Une erreur de d√©butant est d'oublier l'√©tape de pr√©traitement (y compris la mise √† l'√©chelle des couleurs). Cela peut fonctionner, mais entra√Ænera des pr√©dictions moins bonnes (puisqu'il voit effectivement les m√™mes formes mais avec des couleurs et des contrastes diff√©rents).

Dans PyTorch, il y a deux √©tapes suppl√©mentaires, car nous devons :

* convertir les logits en probabilit√©s, et
* transf√©rer les donn√©es vers le CPU et convertir en NumPy (heureusement, les messages d'erreur sont assez clairs lorsque nous oublions cette √©tape).

Et voici ce que nous obtenons :

![Image](https://cdn-media-1.freecodecamp.org/images/1*a6XeWuUwwtBjFMfUlN-eyw.png)

Cela fonctionne !

Et qu'en est-il des autres images ? Si vous ne trouvez rien (ou personne) d'autre, essayez d'utiliser des photos de vos coll√®gues. üòâ

### Conclusion

Comme vous pouvez le voir, Keras et PyTorch diff√®rent consid√©rablement en termes de d√©finition, de modification, d'entra√Ænement, d'√©valuation et d'exportation des mod√®les de deep learning standard. Pour certaines parties, il s'agit purement de diff√©rentes conventions d'API, tandis que pour d'autres, des diff√©rences fondamentales entre les niveaux d'abstraction sont impliqu√©es.

Keras fonctionne √† un niveau d'abstraction beaucoup plus √©lev√©. Il est beaucoup plus plug&play, et g√©n√©ralement plus succinct, mais au d√©triment de la flexibilit√©.

PyTorch fournit un code plus explicite et d√©taill√©. Dans la plupart des cas, cela signifie un code d√©bogable et flexible, avec un faible surco√ªt. Pourtant, l'entra√Ænement est beaucoup plus verbeux dans PyTorch. Cela fait mal, mais cela offre parfois beaucoup de flexibilit√©.

L'apprentissage par transfert est un grand sujet. Essayez de modifier vos param√®tres (par exemple, les couches denses, l'optimiseur, le taux d'apprentissage, l'augmentation) ou choisissez une architecture de r√©seau diff√©rente.

Avez-vous essay√© l'apprentissage par transfert pour la reconnaissance d'images ? Consid√©rez la liste ci-dessous pour quelques inspirations :

* [Chihuahua vs. muffin, sheepdog vs. mop, shrew vs. kiwi](https://twistedsifter.com/2016/03/puppy-or-bagel-meme-gallery/) (serve d√©j√† de [benchmark int√©ressant pour la vision par ordinateur](https://medium.freecodecamp.org/chihuahua-or-muffin-my-search-for-the-best-computer-vision-api-cbda4d6b425d))
* Images originales vs. images photoshop√©es
* Artichaut vs. brocoli vs. chou-fleur
* Zerg vs. Protoss vs. Orc vs. Elfe
* Meme ou pas meme
* [Est-ce une image d'un oiseau ?](https://xkcd.com/1425/)
* [Est-ce que c'est huggable ?](https://www.reddit.com/r/MachineLearning/comments/4casci/can_i_hug_that_i_trained_a_classifier_to_tell_you/)

Choisissez Keras ou PyTorch, s√©lectionnez un dataset, et faites-nous savoir comment cela s'est pass√© dans la section des commentaires ci-dessous üòâ

Au fait, en novembre, nous organisons une [s√©rie de formations pratiques](https://deepsense.ai/machine-learning-and-deep-learning-training/) o√π vous pouvez en apprendre davantage sur Keras et PyTorch. Piotr Migda≈Ç et moi-m√™me dirigerons certaines des sessions, alors n'h√©sitez pas √† y jeter un coup d'≈ìil.