---
title: Asym√©trie et Kurtosis ‚Äì Distributions Asym√©triques Positives et N√©gatives en
  Statistiques Expliqu√©es
subtitle: ''
author: freeCodeCamp
co_authors: []
series: null
date: '2021-06-16T20:56:49.000Z'
originalURL: https://freecodecamp.org/news/skewness-and-kurtosis-in-statistics-explained
coverImage: https://www.freecodecamp.org/news/content/images/2021/06/stats-article-image.jpg
tags:
- name: Advanced Mathematics
  slug: advanced-mathematics
- name: Math
  slug: math
- name: MathJax
  slug: mathjax
- name: statistics
  slug: statistics
seo_title: Asym√©trie et Kurtosis ‚Äì Distributions Asym√©triques Positives et N√©gatives
  en Statistiques Expliqu√©es
seo_desc: 'By Rishit Dagli

  In this article, I''ll explain two important concepts in statistics: skewness and
  kurtosis. And don''t worry ‚Äì you won''t need to know very much math to understand
  these concepts and learn how to apply them.

  What are Density Curves?

  Let''...'
---

Par Rishit Dagli

Dans cet article, je vais expliquer deux concepts importants en statistiques : l'asym√©trie et le kurtosis. Et ne vous inqui√©tez pas ‚Äì vous n'aurez pas besoin de conna√Ætre beaucoup de math√©matiques pour comprendre ces concepts et apprendre √† les appliquer.

## Qu'est-ce que les Courbes de Densit√© ?

Parlons d'abord un peu des courbes de densit√©, car l'asym√©trie et le kurtosis sont bas√©s sur elles. Elles sont simplement un moyen pour nous de repr√©senter une distribution. Voyons ce que je veux dire √† travers un exemple.

Supposons que vous devez enregistrer les tailles de beaucoup de personnes. Donc, votre distribution a, disons, 20 cat√©gories repr√©sentant la plage des r√©sultats (58-59 pouces, 59-60 pouces ... 78-79). Vous pouvez tracer un histogramme repr√©sentant ces cat√©gories et le nombre de personnes dont la taille tombe dans chaque cat√©gorie.

![Image](https://www.freecodecamp.org/news/content/images/2021/06/Percent-of-population-vs.-Height.png)
_Histogramme de la taille par rapport √† la population_

Eh bien, vous pourriez faire cela pour des milliers de personnes, donc vous n'√™tes pas int√©ress√© par le nombre exact ‚Äì plut√¥t par le pourcentage ou la probabilit√© de ces cat√©gories.

J'ai √©galement explicitement mentionn√© que vous avez une distribution plut√¥t grande puisque les pourcentages sont souvent inutiles pour les petites distributions.

Si vous utilisez des pourcentages avec des nombres plus petits, je fais souvent r√©f√©rence √† cela comme _mentir_ avec des statistiques ‚Äì c'est une d√©claration qui est techniquement correcte mais cr√©e la mauvaise impression dans nos esprits.

Permettez-moi de vous donner un exemple : un √©tudiant est extr√™mement excit√© et dit √† tout le monde dans sa classe qu'il a fait une am√©lioration de 100 % dans ses notes ! Mais ce qu'il ne dit pas, c'est que ses notes sont pass√©es de 2/30 √† 4/30 üòÇ.

J'esp√®re que vous voyez maintenant clairement le probl√®me de l'utilisation des pourcentages avec des nombres plus petits.

Revenons aux courbes de densit√©, lorsque vous travaillez avec une grande distribution, vous voulez avoir des cat√©gories plus granulaires. Donc, vous faites de chaque cat√©gorie qui √©tait large de 1 pouce, maintenant 2 cat√©gories chacune large de \(\frac{1}{2}\) pouce. Peut-√™tre que vous voulez obtenir encore plus granulaire et commencer √† utiliser des cat√©gories larges de \(\frac{1}{4}\) pouce. Pouvez-vous deviner o√π je veux en venir avec cela ?

√Ä un moment donn√©, nous obtenons un nombre infini de telles cat√©gories avec une longueur infiniment petite. Cela nous permet de cr√©er une courbe √† partir de cet histogramme que nous avions pr√©c√©demment divis√© en cat√©gories discr√®tes. Voyez notre courbe de densit√© ci-dessous trac√©e √† partir de l'histogramme.

![Image](https://www.freecodecamp.org/news/content/images/2021/06/image-155.png)
_Courbe de densit√© de probabilit√© pour notre distribution_

### Pourquoi faire tout cet effort ?

Excellente question ! Comme vous l'avez peut-√™tre devin√©, j'aime expliquer avec des exemples, alors regardons une autre courbe de densit√© pour que ce soit un peu plus facile √† comprendre. N'h√©sitez pas √† sauter l'√©quation de la courbe √† ce stade si vous n'avez pas travaill√© avec des distributions auparavant.

Vous pouvez √©galement suivre et cr√©er les graphiques et visualisations de cet article vous-m√™me via [ce projet Geogebra](https://www.geogebra.org/classic/barxehx4) (il fonctionne dans le navigateur).

$$ f(x) = \frac{1}{0.4 \sqrt{2 \pi} } \cdot e^{-\frac{1}{2} (\frac{x - 1.6}{0.4})^2} $$

![Image](https://www.freecodecamp.org/news/content/images/2021/06/image-9.png)

Alors maintenant, si je vous demande "Quel pourcentage de ma distribution se trouve dans la cat√©gorie 1 - 1.6 ?" Eh bien, vous calculez simplement l'aire sous la courbe entre 1 et 1.6, comme ceci :

$$ \int_{1}^{1.6} f(x) \,dx $$

![Image](https://www.freecodecamp.org/news/content/images/2021/06/image-11.png)

Il serait √©galement relativement facile pour vous de r√©pondre √† des questions similaires √† partir de la courbe de densit√© comme : "Quel pourcentage de la distribution est inf√©rieur √† 1.2 ?" ou "Quel pourcentage de la distribution est sup√©rieur √† 1.2 ?"

Vous pouvez maintenant probablement voir pourquoi l'effort de cr√©er cette courbe de densit√© en vaut la peine et comment elle vous permet de faire des inf√©rences facilement üöÄ.

## Distributions Asym√©triques

Parlons maintenant un peu des distributions asym√©triques ‚Äì c'est-√†-dire celles qui ne sont pas aussi agr√©ables et sym√©triques que les courbes que nous avons vues pr√©c√©demment. Nous en parlerons de mani√®re plus intuitive en utilisant les id√©es de moyenne et de m√©diane.

√Ä partir de l'image du graphique de la courbe de densit√©, essayez de d√©terminer o√π se situerait la m√©diane de cette distribution. Peut-√™tre √©tait-il facile pour vous de le d√©terminer ‚Äì la courbe est sym√©trique et vous avez peut-√™tre conclu que la m√©diane est 1.6 puisqu'elle √©tait sym√©trique autour de \(x=1.6\).

Une autre fa√ßon de proc√©der serait de dire que la m√©diane est la valeur o√π l'aire sous la courbe √† gauche de celle-ci et l'aire sous la courbe √† droite de celle-ci sont √©gales.

![Image](https://www.freecodecamp.org/news/content/images/2021/06/skewness-and-kurtosis-blog.png)

Nous parlons de cette id√©e car elle nous permet √©galement de calculer la m√©diane pour les courbes de densit√© non sym√©triques.

En guise d'exemple ici, je montre deux distributions asym√©triques tr√®s courantes et comment l'id√©e des aires √©gales que nous venons de discuter nous aide √† trouver leurs m√©dianes. Si nous avons essay√© de d√©terminer notre m√©diane √† l'≈ìil nu, voici ce que nous obtiendrions puisque nous voulons que les aires de chaque c√¥t√© soient √©gales.

![Image](https://www.freecodecamp.org/news/content/images/2021/06/Eyballing-median.png)
_D√©termination de la m√©diane √† l'≈ìil nu pour les courbes asym√©triques_

Vous pouvez √©galement calculer la moyenne √† travers ces courbes de densit√©. Peut-√™tre avez-vous d√©j√† essay√© de calculer la moyenne vous-m√™me, mais remarquez que si vous utilisez la formule g√©n√©rale pour calculer la moyenne :

$$ moyenne = \frac{\sum a_n}{n} $$

vous pourriez remarquer un d√©faut : nous prenons en compte les valeurs \( x \) mais nous avons √©galement des probabilit√©s associ√©es √† ces valeurs. Et il est logique de tenir compte de cela √©galement.

Nous modifions donc la mani√®re dont nous calculons la moyenne en utilisant des moyennes pond√©r√©es. Nous aurons maintenant √©galement un terme \(w_n\) repr√©sentant les poids associ√©s :

$$ moyenne = \frac{\sum{a_n \cdot w_n}}{n} $$

Nous utiliserons donc l'id√©e que nous venons de discuter pour calculer la moyenne √† partir de notre courbe de densit√©.

Vous pouvez √©galement comprendre cela de mani√®re plus intuitive comme le point sur l'axe des x o√π vous pourriez placer un point d'appui et √©quilibrer la courbe si elle √©tait un objet solide. Cette id√©e devrait vous aider √† mieux comprendre la recherche de la moyenne √† partir de notre courbe de densit√©.

Mais une autre fa√ßon vraiment int√©ressante de voir cela serait comme la coordonn√©e x du point sur cette courbe o√π l'inertie rotationnelle serait nulle.

Vous avez peut-√™tre d√©j√† compris comment nous pouvons localiser la moyenne pour les courbes sym√©triques : notre m√©diane et notre moyenne se situent au m√™me point, le point de sym√©trie.

Nous utiliserons l'id√©e que nous venons de discuter, en pla√ßant un point d'appui sur l'axe des x et en √©quilibrant la courbe, pour d√©terminer √† l'≈ìil nu la moyenne pour les graphiques asym√©triques comme ceux que nous avons vus pr√©c√©demment lors du calcul de la m√©diane.

![Image](https://www.freecodecamp.org/news/content/images/2021/06/mean.png)

Nous allons bient√¥t discuter de l'id√©e d'asym√©trie plus en d√©tail. Mais √† ce stade, _g√©n√©ralement parlant_, vous pouvez identifier la direction o√π votre courbe est asym√©trique. Si la m√©diane est √† droite de la moyenne, alors elle est asym√©trique n√©gativement. Et si la moyenne est √† droite de la m√©diane, alors elle est asym√©trique positivement.

Plus tard dans cet article, pour simplifier, nous appellerons √©galement la partie √©troite de ces courbes une "queue".

## Qu'est-ce que les Moments ?

Avant de parler davantage de l'asym√©trie et du kurtosis, explorons un peu l'id√©e des moments. Plus tard, nous utiliserons ce concept pour d√©velopper une id√©e de mesure de l'asym√©trie et du kurtosis dans notre distribution.

Nous utiliserons un petit ensemble de donn√©es, [1, 2, 3, 3, 3, 6]. Ces nombres signifient que vous avez des points qui sont √† 1 unit√© de l'origine, 2 unit√©s de l'origine, et ainsi de suite.

Nous nous soucions donc beaucoup des distances par rapport √† l'origine dans notre ensemble de donn√©es. Nous pouvons repr√©senter la distance moyenne par rapport √† l'origine dans nos donn√©es en √©crivant :

$$ \frac{\sum a_n -0}{n} = \frac{\sum a_n}{n} $$

C'est ce que nous appelons notre premier moment. En calculant cela pour notre ensemble de donn√©es d'exemple, nous obtenons 3, mais si nous changeons notre ensemble de donn√©es et rendons tous les √©l√©ments √©gaux √† 3,

$$ [1, 2, 3, 3, 3, 6] \rightarrow [3, 3, 3, 3, 3, 3] $$

vous verrez que notre premier moment reste le m√™me. Pouvez-nous concevoir quelque chose pour diff√©rencier nos deux ensembles de donn√©es qui ont des premiers moments √©gaux ? (PS : C'est le deuxi√®me moment.)

Nous allons calculer la somme moyenne des distances au carr√© plut√¥t que la somme moyenne des distances :

$$ \frac{\sum (a_n)^2}{n} $$

Notre deuxi√®me moment pour notre ensemble de donn√©es original est 11,33 et pour notre nouvel ensemble de donn√©es est 9. Remarquez que la magnitude du deuxi√®me moment est plus grande pour notre ensemble de donn√©es original que pour le nouveau. De plus, nous avons une valeur plus √©lev√©e pour le deuxi√®me moment dans l'ensemble de donn√©es original car il est plus √©tal√© et a une distance quadratique moyenne plus grande.

Essentiellement, nous disons que nous avons quelques valeurs dans notre ensemble de donn√©es original plus grandes que la valeur moyenne, qui, lorsqu'elles sont √©lev√©es au carr√©, augmentent notre deuxi√®me moment de beaucoup.

Voici une fa√ßon int√©ressante de penser aux moments ‚Äì supposons que notre distribution est une masse, et alors le premier moment serait le centre de la masse, et le deuxi√®me moment serait l'inertie rotationnelle.

Vous pouvez √©galement voir que notre deuxi√®me moment d√©pend fortement de notre premier moment. Mais nous sommes int√©ress√©s √† conna√Ætre les informations que le deuxi√®me moment peut nous donner ind√©pendamment.

Pour ce faire, nous calculons les distances au carr√© par rapport √† la moyenne ou au premier moment plut√¥t que par rapport √† l'origine.

$$ \frac{\sum (a_n- \mu_{1}^{'})^2 }{n} $$

Avez-vous remarqu√© que nous avons √©galement d√©riv√© intuitivement une formule pour la variance ? √Ä l'avenir, vous verrez comment nous utilisons les id√©es dont nous venons de parler pour mesurer l'asym√©trie et le kurtosis.

## Introduction √† l'Asym√©trie et au Kurtosis

Voyons comment nous pouvons utiliser l'id√©e des moments dont nous avons parl√© pr√©c√©demment pour d√©terminer comment nous pouvons mesurer l'asym√©trie (dont vous avez d√©j√† une certaine id√©e) et le kurtosis.

### Qu'est-ce que l'Asym√©trie ?

Prenons l'id√©e des moments dont nous avons parl√© il y a un instant et essayons de calculer le troisi√®me moment. Comme vous l'avez peut-√™tre devin√©, nous pouvons calculer les cubes de nos distances. Mais comme nous en avons discut√© ci-dessus, nous sommes plus int√©ress√©s √† voir les informations suppl√©mentaires que le troisi√®me moment fournit.

Nous voulons donc soustraire le deuxi√®me moment de notre troisi√®me moment. Plus tard, nous appellerons √©galement ce facteur l'ajustement du moment. Notre moment ajust√© ressemblera √† ceci :

$$ asym√©trie = \frac{\sum (a_n - \mu)^3 }{n \cdot \sigma ^3} $$

Ce moment ajust√© est ce que nous appelons _asym√©trie_. Il nous aide √† mesurer l'asym√©trie dans les donn√©es.

Des donn√©es parfaitement sym√©triques auraient une valeur d'asym√©trie de 0. Une valeur d'asym√©trie n√©gative implique qu'une distribution a sa queue du c√¥t√© gauche de la distribution, tandis qu'une valeur d'asym√©trie positive a sa queue du c√¥t√© droit de la distribution.

![Image](https://www.freecodecamp.org/news/content/images/2021/06/image-100.png)
_Asym√©trie positive et asym√©trie n√©gative_

√Ä ce stade, il peut sembler que le calcul de l'asym√©trie serait assez difficile √† faire puisque dans les formules nous utilisons la moyenne de la population \( \mu \) et l'√©cart-type de la population \( \sigma \) auxquels nous n'aurions pas acc√®s lors de la prise d'un √©chantillon.

Au lieu de cela, vous n'avez que la moyenne de l'√©chantillon et l'√©cart-type de l'√©chantillon, donc nous verrons bient√¥t comment vous pouvez utiliser ceux-ci.

### Qu'est-ce que le Kurtosis ?

Comme vous l'avez peut-√™tre devin√©, cette fois nous allons calculer notre quatri√®me moment ou utiliser la quatri√®me puissance de nos distances. Et comme nous en avons parl√© plus t√¥t, nous sommes int√©ress√©s √† voir les informations suppl√©mentaires que cela fournit, donc nous allons √©galement soustraire le facteur d'ajustement.

C'est ce que nous appelons _kurtosis_ ou une mesure de savoir si nos donn√©es ont beaucoup de valeurs aberrantes ou tr√®s peu de valeurs aberrantes. Cela ressemblera √† ceci :

$$ kurtosis = \frac{\sum (a_n - \mu)^4 }{n \cdot \sigma ^4} $$

Un meilleur terme pour ce qui se passe ici est de d√©terminer si la distribution est √† queue lourde ou √† queue l√©g√®re. Nous pouvons comparer cela √† une distribution normale.

Si vous faites une simple substitution, vous verrez que le kurtosis pour une distribution normale est 3. Et puisque nous sommes int√©ress√©s √† comparer le kurtosis √† la distribution normale, souvent nous utilisons l'exc√®s de kurtosis qui soustrait simplement 3 de l'√©quation ci-dessus.

![Image](https://www.freecodecamp.org/news/content/images/2021/06/positive-negative-kurtosis.png)
_Kurtosis positif et n√©gatif (Adapt√© de Analytics Vidhya)_

Cela revient essentiellement √† forcer le kurtosis de notre distribution normale √† √™tre 0 pour une comparaison plus facile. Donc, si notre distribution a un kurtosis positif, cela indique une distribution √† queue lourde tandis qu'un kurtosis n√©gatif indique une distribution √† queue l√©g√®re. Graphiquement, cela ressemblerait √† quelque chose comme l'image ci-dessus.

## Ajustement de l'√âchantillonnage

Un probl√®me avec les √©quations que nous venons de construire est qu'elles contiennent deux termes, la moyenne de la distribution \( \mu \) et l'√©cart-type de la distribution \( \sigma \). Mais nous prenons un √©chantillon d'observations, donc nous n'avons pas les param√®tres pour toute la distribution. Nous n'aurions que la moyenne de l'√©chantillon et l'√©cart-type de l'√©chantillon.

Pour garder cet article concentr√©, nous ne parlerons pas en d√©tail des termes d'ajustement de l'√©chantillonnage puisque les degr√©s de libert√© ne sont pas dans le cadre de cet article.

L'id√©e est d'utiliser notre moyenne d'√©chantillon \( \bar{x} \) et notre √©cart-type d'√©chantillon \( s \) pour estimer ces valeurs pour notre distribution. Nous devrons √©galement ajuster notre degr√© de libert√© dans ces √©quations.

Ne vous inqui√©tez pas si vous ne comprenez pas compl√®tement ce concept √† ce stade. Nous pouvons continuer quand m√™me. Cela nous am√®ne √† modifier les √©quations dont nous avons parl√© pr√©c√©demment comme suit :

$$ asym√©trie = \frac{\sum (a_n - \bar{x})^3 }{s^3} \cdot \frac{n}{(n-1)(n-2)} $$

$$ kurtosis = \frac{\sum (a_n - \bar{x})^4 }{s^4} \cdot \frac{n(n+1)}{(n-1)(n-2)(n-3)} - \frac{3(n-1)^2}{(n-2)(n-3)} $$

## Comment Impl√©menter cela en Python

Enfin, terminons en voyant comment vous pouvez mesurer l'asym√©trie et le kurtosis en Python avec un exemple. Au cas o√π vous voudriez suivre et essayer le code, vous pouvez suivre avec [ce Notebook Colab](https://colab.research.google.com/drive/1pbWIz7X7_k5iNZ5w2x6eQUPz24l7yfLm?usp=sharing) o√π nous mesurons l'asym√©trie et le kurtosis d'un ensemble de donn√©es.

Il est assez simple de l'impl√©menter en Python avec [Scipy](https://www.scipy.org/). Il dispose de m√©thodes pour mesurer facilement l'asym√©trie et le kurtosis pour une distribution avec des m√©thodes pr√©-construites.

Le bloc de code ci-dessous montre comment mesurer l'asym√©trie et le kurtosis pour l'ensemble de donn√©es [Boston housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html), mais vous pourriez √©galement l'utiliser pour vos propres distributions.

```python
from scipy.stats import skew
from scipy.stats import kurtosis

skew(data["MEDV"].dropna())
kurtosis(data["MEDV"].dropna())
```

## **Merci d'avoir lu !**

Merci d'√™tre rest√© avec moi jusqu'√† la fin. J'esp√®re que vous avez beaucoup appris de cet article.

Je suis ravi de voir si cet article vous a aid√© √† mieux comprendre ces deux id√©es tr√®s importantes. Si vous avez des commentaires ou des suggestions pour moi, n'h√©sitez pas √† [me contacter sur Twitter](https://twitter.com/rishit_dagli).